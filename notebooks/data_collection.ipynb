{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Data collection from various sources"
      ],
      "metadata": {
        "id": "nyC5sG9KzYpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "\n",
        "API_KEY = \"rl_zbkvT7h5FY8576ZfSWCLhfXRW\"\n",
        "TOPICS = [\"git\", \"bash\", \"tar\", \"gzip\", \"grep\", \"venv\", \"docker\", \"ssh\", \"curl\", \"awk\"]\n",
        "MAX_QUESTIONS_PER_TOPIC = 5\n",
        "OUTPUT_FILE = \"command_line_qa_dataset.json\"\n",
        "\n",
        "def html_to_text(html):\n",
        "    \"\"\"Convert HTML to clean text with preserved code blocks\"\"\"\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    # Format code blocks\n",
        "    for pre in soup.find_all('pre'):\n",
        "        pre.string = f\"\\n```\\n{pre.get_text()}\\n```\\n\"\n",
        "\n",
        "    return soup.get_text().strip()\n",
        "\n",
        "def fetch_questions(topic):\n",
        "    \"\"\"Fetch multiple questions for a topic\"\"\"\n",
        "    questions = []\n",
        "\n",
        "    try:\n",
        "        response = requests.get(\n",
        "            \"https://api.stackexchange.com/2.3/questions\",\n",
        "            params={\n",
        "                \"order\": \"desc\",\n",
        "                \"sort\": \"votes\",\n",
        "                \"tagged\": topic,\n",
        "                \"site\": \"stackoverflow\",\n",
        "                \"filter\": \"withbody\",\n",
        "                \"pagesize\": MAX_QUESTIONS_PER_TOPIC,\n",
        "                \"key\": API_KEY\n",
        "            }\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "        items = response.json().get(\"items\", [])\n",
        "\n",
        "        for item in items:\n",
        "            if \"accepted_answer_id\" not in item:\n",
        "                continue\n",
        "\n",
        "            answer = fetch_answer(item['accepted_answer_id'])\n",
        "            if answer:\n",
        "                questions.append({\n",
        "                    \"topic\": topic,\n",
        "                    \"question\": {\n",
        "                        \"id\": item[\"question_id\"],\n",
        "                        \"title\": item[\"title\"],\n",
        "                        \"body\": html_to_text(item[\"body\"]),\n",
        "                        \"url\": item[\"link\"],\n",
        "                        \"votes\": item[\"score\"],\n",
        "                        \"tags\": item[\"tags\"],\n",
        "                        \"created_at\": datetime.fromtimestamp(item[\"creation_date\"]).isoformat()\n",
        "                    },\n",
        "                    \"answer\": {\n",
        "                        \"id\": answer[\"answer_id\"],\n",
        "                        \"body\": html_to_text(answer[\"body\"]),\n",
        "                        \"votes\": answer[\"score\"],\n",
        "                        \"accepted\": answer[\"is_accepted\"],\n",
        "                        \"created_at\": datetime.fromtimestamp(answer[\"creation_date\"]).isoformat()\n",
        "                    }\n",
        "                })\n",
        "                time.sleep(1)  # Respect API rate limits\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error fetching {topic}: {str(e)}\")\n",
        "\n",
        "    return questions\n",
        "\n",
        "def fetch_answer(answer_id):\n",
        "    \"\"\"Fetch a single answer\"\"\"\n",
        "    try:\n",
        "        response = requests.get(\n",
        "            f\"https://api.stackexchange.com/2.3/answers/{answer_id}\",\n",
        "            params={\n",
        "                \"site\": \"stackoverflow\",\n",
        "                \"filter\": \"withbody\",\n",
        "                \"key\": API_KEY\n",
        "            }\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"items\"][0]\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def build_dataset():\n",
        "    \"\"\"Build complete dataset across all topics\"\"\"\n",
        "    dataset = {\n",
        "        \"metadata\": {\n",
        "            \"source\": \"Stack Overflow API\",\n",
        "            \"created_at\": datetime.now().isoformat(),\n",
        "            \"topics\": TOPICS,\n",
        "            \"questions_per_topic\": MAX_QUESTIONS_PER_TOPIC\n",
        "        },\n",
        "        \"qna_pairs\": []\n",
        "    }\n",
        "\n",
        "    print(\"üöÄ Building command-line Q&A dataset...\\n\")\n",
        "\n",
        "    for topic in TOPICS:\n",
        "        print(f\"üîç Fetching {MAX_QUESTIONS_PER_TOPIC} questions about {topic}...\")\n",
        "        questions = fetch_questions(topic)\n",
        "        dataset[\"qna_pairs\"].extend(questions)\n",
        "        print(f\"‚úÖ Got {len(questions)} questions with accepted answers\")\n",
        "        time.sleep(2)  # Additional rate limiting\n",
        "\n",
        "    # Save to file\n",
        "    with open(OUTPUT_FILE, 'w') as f:\n",
        "        json.dump(dataset, f, indent=2)\n",
        "\n",
        "    print(f\"\\nüéâ Dataset saved to {OUTPUT_FILE}\")\n",
        "    print(f\"Total Q&A pairs collected: {len(dataset['qna_pairs'])}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    build_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EEyEmTlYrPI",
        "outputId": "2fcdf0ad-a898-4f3c-aca7-482e85ee0947"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Building command-line Q&A dataset...\n",
            "\n",
            "üîç Fetching 5 questions about git...\n",
            "‚úÖ Got 5 questions with accepted answers\n",
            "üîç Fetching 5 questions about bash...\n",
            "‚úÖ Got 5 questions with accepted answers\n",
            "üîç Fetching 5 questions about tar...\n",
            "‚úÖ Got 5 questions with accepted answers\n",
            "üîç Fetching 5 questions about gzip...\n",
            "‚úÖ Got 5 questions with accepted answers\n",
            "üîç Fetching 5 questions about grep...\n",
            "‚úÖ Got 5 questions with accepted answers\n",
            "üîç Fetching 5 questions about venv...\n",
            "‚úÖ Got 2 questions with accepted answers\n",
            "üîç Fetching 5 questions about docker...\n",
            "‚úÖ Got 5 questions with accepted answers\n",
            "üîç Fetching 5 questions about ssh...\n",
            "‚úÖ Got 4 questions with accepted answers\n",
            "üîç Fetching 5 questions about curl...\n",
            "‚úÖ Got 5 questions with accepted answers\n",
            "üîç Fetching 5 questions about awk...\n",
            "‚úÖ Got 4 questions with accepted answers\n",
            "\n",
            "üéâ Dataset saved to command_line_qa_dataset.json\n",
            "Total Q&A pairs collected: 45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "\n",
        "API_KEY = \"rl_zbkvT7h5FY8576ZfSWCLhfXRW\"\n",
        "OUTPUT_FILE = \"command_line_qa_dataset.json\"\n",
        "\n",
        "# More reliable filter that's known to work\n",
        "DEFAULT_FILTER = \"!-MBr1_IzpA7I9nsmVLyoWwR*5QqS7qyDqR\"\n",
        "\n",
        "# Slightly reduced but comprehensive topic list\n",
        "COMMAND_LINE_TOPICS = [\n",
        "    \"git\", \"bash\", \"terminal\", \"shell\",\n",
        "    \"awk\", \"sed\", \"grep\", \"find\",\n",
        "    \"ssh\", \"curl\", \"wget\", \"rsync\",\n",
        "    \"tar\", \"gzip\", \"zip\", \"unzip\",\n",
        "    \"docker\", \"kubectl\", \"vagrant\",\n",
        "    \"vim\", \"emacs\", \"nano\",\n",
        "    \"cron\", \"systemd\", \"htop\",\n",
        "    \"python\", \"pip\", \"conda\", \"venv\"\n",
        "]\n",
        "\n",
        "def html_to_text(html):\n",
        "    \"\"\"Convert HTML to clean text with preserved code blocks\"\"\"\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    # Format code blocks\n",
        "    for pre in soup.find_all('pre'):\n",
        "        pre.string = f\"\\n```\\n{pre.get_text()}\\n```\\n\"\n",
        "\n",
        "    return soup.get_text().strip()\n",
        "\n",
        "def fetch_questions(topic, max_questions=50):\n",
        "    \"\"\"Fetch questions for a topic with reliable filters\"\"\"\n",
        "    questions = []\n",
        "    page = 1\n",
        "\n",
        "    print(f\"\\nüìö Fetching questions about {topic}...\")\n",
        "\n",
        "    while len(questions) < max_questions:\n",
        "        try:\n",
        "            # Using a simpler, more reliable filter\n",
        "            response = requests.get(\n",
        "                \"https://api.stackexchange.com/2.3/questions\",\n",
        "                params={\n",
        "                    \"order\": \"desc\",\n",
        "                    \"sort\": \"votes\",\n",
        "                    \"tagged\": topic,\n",
        "                    \"site\": \"stackoverflow\",\n",
        "                    \"filter\": \"withbody\",\n",
        "                    \"pagesize\": 50,  # More reliable page size\n",
        "                    \"page\": page,\n",
        "                    \"key\": API_KEY\n",
        "                },\n",
        "                timeout=10\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            for item in data.get(\"items\", []):\n",
        "                if \"accepted_answer_id\" in item:\n",
        "                    answer = fetch_answer(item['accepted_answer_id'])\n",
        "                    if answer:\n",
        "                        questions.append(create_qa_pair(item, answer, topic))\n",
        "                        if len(questions) >= max_questions:\n",
        "                            break\n",
        "\n",
        "            if not data.get(\"has_more\", False):\n",
        "                break\n",
        "\n",
        "            page += 1\n",
        "            time.sleep(1)  # Respect API rate limits\n",
        "\n",
        "            print(f\"  ‚úÖ Collected {len(questions)}/{max_questions} questions\", end='\\r')\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ö†Ô∏è Error fetching {topic}: {str(e)}\")\n",
        "            time.sleep(5)\n",
        "            break\n",
        "\n",
        "    return questions\n",
        "\n",
        "def fetch_answer(answer_id):\n",
        "    \"\"\"Fetch an answer with reliable filter\"\"\"\n",
        "    try:\n",
        "        response = requests.get(\n",
        "            f\"https://api.stackexchange.com/2.3/answers/{answer_id}\",\n",
        "            params={\n",
        "                \"site\": \"stackoverflow\",\n",
        "                \"filter\": \"withbody\",\n",
        "                \"key\": API_KEY\n",
        "            },\n",
        "            timeout=5\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"items\"][0]\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error fetching answer {answer_id}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def create_qa_pair(question, answer, topic):\n",
        "    \"\"\"Create a structured Q&A pair\"\"\"\n",
        "    return {\n",
        "        \"topic\": topic,\n",
        "        \"question\": {\n",
        "            \"id\": question[\"question_id\"],\n",
        "            \"title\": question[\"title\"],\n",
        "            \"body\": html_to_text(question[\"body\"]),\n",
        "            \"url\": question[\"link\"],\n",
        "            \"votes\": question[\"score\"],\n",
        "            \"tags\": question[\"tags\"],\n",
        "            \"created_at\": datetime.fromtimestamp(question[\"creation_date\"]).isoformat()\n",
        "        },\n",
        "        \"answer\": {\n",
        "            \"id\": answer[\"answer_id\"],\n",
        "            \"body\": html_to_text(answer[\"body\"]),\n",
        "            \"votes\": answer[\"score\"],\n",
        "            \"accepted\": answer[\"is_accepted\"],\n",
        "            \"created_at\": datetime.fromtimestamp(answer[\"creation_date\"]).isoformat()\n",
        "        }\n",
        "    }\n",
        "\n",
        "def build_dataset():\n",
        "    \"\"\"Build complete dataset with reliable API calls\"\"\"\n",
        "    dataset = {\n",
        "        \"metadata\": {\n",
        "            \"source\": \"Stack Overflow API\",\n",
        "            \"created_at\": datetime.now().isoformat(),\n",
        "            \"topics\": COMMAND_LINE_TOPICS\n",
        "        },\n",
        "        \"qna_pairs\": []\n",
        "    }\n",
        "\n",
        "    print(\"üöÄ Building reliable command-line Q&A dataset\")\n",
        "    print(f\"üìÇ Will save to {OUTPUT_FILE}\\n\")\n",
        "\n",
        "    for topic in COMMAND_LINE_TOPICS:\n",
        "        questions = fetch_questions(topic)\n",
        "        dataset[\"qna_pairs\"].extend(questions)\n",
        "        print(f\"‚úÖ {topic}: Collected {len(questions)} Q&A pairs\")\n",
        "        time.sleep(2)  # Additional rate limiting\n",
        "\n",
        "    # Save to file\n",
        "    with open(OUTPUT_FILE, 'w') as f:\n",
        "        json.dump(dataset, f, indent=2)\n",
        "\n",
        "    print(f\"\\nüéâ Dataset saved to {OUTPUT_FILE}\")\n",
        "    print(f\"Total Q&A pairs collected: {len(dataset['qna_pairs'])}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    build_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnK6oRszb9rm",
        "outputId": "38faa583-f08d-49ad-9ffb-4e129e0fef9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Building reliable command-line Q&A dataset\n",
            "üìÇ Will save to command_line_qa_dataset.json\n",
            "\n",
            "\n",
            "üìö Fetching questions about git...\n",
            "‚úÖ git: Collected 50 Q&A pairs\n",
            "\n",
            "üìö Fetching questions about bash...\n",
            "‚úÖ bash: Collected 50 Q&A pairs\n",
            "\n",
            "üìö Fetching questions about terminal...\n",
            "‚úÖ terminal: Collected 50 Q&A pairs\n",
            "\n",
            "üìö Fetching questions about shell...\n",
            "‚úÖ shell: Collected 50 Q&A pairs\n",
            "\n",
            "üìö Fetching questions about awk...\n",
            "‚úÖ awk: Collected 43 Q&A pairs\n",
            "\n",
            "üìö Fetching questions about sed...\n",
            "‚úÖ sed: Collected 50 Q&A pairs\n",
            "\n",
            "üìö Fetching questions about grep...\n",
            "‚úÖ grep: Collected 50 Q&A pairs\n",
            "\n",
            "üìö Fetching questions about find...\n",
            "‚úÖ find: Collected 50 Q&A pairs\n",
            "\n",
            "üìö Fetching questions about ssh...\n",
            "‚úÖ ssh: Collected 50 Q&A pairs\n",
            "\n",
            "üìö Fetching questions about curl...\n",
            "‚úÖ curl: Collected 50 Q&A pairs\n",
            "\n",
            "üìö Fetching questions about wget...\n",
            "‚úÖ wget: Collected 50 Q&A pairs\n",
            "\n",
            "üìö Fetching questions about rsync...\n",
            "‚úÖ rsync: Collected 50 Q&A pairs\n",
            "\n",
            "üìö Fetching questions about tar...\n",
            "‚úÖ tar: Collected 50 Q&A pairs\n",
            "\n",
            "üìö Fetching questions about gzip...\n",
            "‚úÖ gzip: Collected 50 Q&A pairs\n",
            "\n",
            "üìö Fetching questions about zip...\n",
            "‚úÖ zip: Collected 50 Q&A pairs\n",
            "\n",
            "üìö Fetching questions about unzip...\n",
            "‚úÖ unzip: Collected 50 Q&A pairs\n",
            "\n",
            "üìö Fetching questions about docker...\n",
            "‚úÖ docker: Collected 41 Q&A pairs\n",
            "\n",
            "üìö Fetching questions about kubectl...\n",
            "‚úÖ kubectl: Collected 50 Q&A pairs\n",
            "\n",
            "üìö Fetching questions about vagrant...\n",
            "‚úÖ vagrant: Collected 50 Q&A pairs\n",
            "\n",
            "üìö Fetching questions about vim...\n",
            "‚úÖ vim: Collected 50 Q&A pairs\n",
            "\n",
            "üìö Fetching questions about emacs...\n",
            "‚úÖ emacs: Collected 50 Q&A pairs\n",
            "\n",
            "üìö Fetching questions about nano...\n",
            "‚úÖ nano: Collected 50 Q&A pairs\n",
            "\n",
            "üìö Fetching questions about cron...\n",
            "‚úÖ cron: Collected 44 Q&A pairs\n",
            "\n",
            "üìö Fetching questions about systemd...\n",
            "‚úÖ systemd: Collected 50 Q&A pairs\n",
            "\n",
            "üìö Fetching questions about htop...\n",
            "‚úÖ htop: Collected 50 Q&A pairs\n",
            "\n",
            "üìö Fetching questions about python...\n",
            "‚úÖ python: Collected 50 Q&A pairs\n",
            "\n",
            "üìö Fetching questions about pip...\n",
            "‚úÖ pip: Collected 50 Q&A pairs\n",
            "\n",
            "üìö Fetching questions about conda...\n",
            "‚úÖ conda: Collected 50 Q&A pairs\n",
            "\n",
            "üìö Fetching questions about venv...\n",
            "‚úÖ venv: Collected 6 Q&A pairs\n",
            "\n",
            "üéâ Dataset saved to command_line_qa_dataset.json\n",
            "Total Q&A pairs collected: 1384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "import time\n",
        "import requests\n",
        "import re\n",
        "\n",
        "# Configuration - USE YOUR OWN TOKEN!\n",
        "GITHUB_TOKEN = \"your_github_token_here\"  # Replace with your token\n",
        "REPO_NAME = \"git-guides/git-guide\"  # Corrected repository name\n",
        "OUTPUT_FILE = \"git_guides_qa.json\"\n",
        "\n",
        "def fetch_discussions_graphql(repo_name):\n",
        "    \"\"\"Fetch discussions from GitHub GraphQL API\"\"\"\n",
        "    owner, repo = repo_name.split('/')\n",
        "    query_template = \"\"\"\n",
        "    query($cursor: String) {\n",
        "      repository(owner: \"%s\", name: \"%s\") {\n",
        "        hasDiscussionsEnabled\n",
        "        discussions(first: 100, after: $cursor) {\n",
        "          pageInfo {\n",
        "            hasNextPage\n",
        "            endCursor\n",
        "          }\n",
        "          nodes {\n",
        "            title\n",
        "            body\n",
        "            url\n",
        "            createdAt\n",
        "            upvoteCount\n",
        "            answer {\n",
        "              body\n",
        "            }\n",
        "            comments(first: 15) {\n",
        "              nodes {\n",
        "                body\n",
        "                isAnswer\n",
        "              }\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    \"\"\" % (owner, repo)\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {GITHUB_TOKEN}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    # First check if discussions are enabled\n",
        "    try:\n",
        "        check_response = requests.post(\n",
        "            \"https://api.github.com/graphql\",\n",
        "            json={\"query\": query_template, \"variables\": {}},\n",
        "            headers=headers\n",
        "        )\n",
        "        check_response.raise_for_status()\n",
        "        check_data = check_response.json()\n",
        "\n",
        "        if 'errors' in check_data:\n",
        "            print(f\"‚ö†Ô∏è GraphQL errors: {check_data['errors'][0]['message']}\")\n",
        "            return []\n",
        "\n",
        "        if not check_data['data']['repository']['hasDiscussionsEnabled']:\n",
        "            print(f\"‚ö†Ô∏è Discussions not enabled in {repo_name}\")\n",
        "            return []\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error checking discussions status: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "    # Now fetch discussions\n",
        "    discussions = []\n",
        "    has_next_page = True\n",
        "    cursor = None\n",
        "\n",
        "    print(f\"üîç Fetching discussions from {repo_name}...\")\n",
        "\n",
        "    while has_next_page:\n",
        "        variables = {\"cursor\": cursor} if cursor else {}\n",
        "        try:\n",
        "            response = requests.post(\n",
        "                \"https://api.github.com/graphql\",\n",
        "                json={\"query\": query_template, \"variables\": variables},\n",
        "                headers=headers\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            if 'errors' in data:\n",
        "                print(f\"‚ö†Ô∏è GraphQL errors: {data['errors'][0]['message']}\")\n",
        "                break\n",
        "\n",
        "            discussion_data = data['data']['repository']['discussions']\n",
        "            nodes = discussion_data['nodes']\n",
        "\n",
        "            for node in nodes:\n",
        "                # Extract answer (either direct answer or from comments)\n",
        "                answer_body = None\n",
        "                if node.get('answer'):\n",
        "                    answer_body = node['answer']['body']\n",
        "                else:\n",
        "                    for comment in node['comments']['nodes']:\n",
        "                        if comment['isAnswer']:\n",
        "                            answer_body = comment['body']\n",
        "                            break\n",
        "\n",
        "                if answer_body:\n",
        "                    discussions.append({\n",
        "                        \"question\": node['title'],\n",
        "                        \"question_body\": node['body'],\n",
        "                        \"answer\": answer_body,\n",
        "                        \"url\": node['url'],\n",
        "                        \"created_at\": node['createdAt'],\n",
        "                        \"upvotes\": node['upvoteCount']\n",
        "                    })\n",
        "\n",
        "            # Handle pagination\n",
        "            page_info = discussion_data['pageInfo']\n",
        "            has_next_page = page_info['hasNextPage']\n",
        "            cursor = page_info['endCursor']\n",
        "\n",
        "            print(f\"üì• Collected {len(discussions)} discussions so far...\")\n",
        "            time.sleep(0.5)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error: {str(e)}\")\n",
        "            break\n",
        "\n",
        "    return discussions\n",
        "\n",
        "def clean_markdown(text):\n",
        "    \"\"\"Clean markdown content\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "    # Remove excessive newlines\n",
        "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "    # Clean code blocks\n",
        "    text = text.replace(\"```\\n\\n\", \"```\\n\")\n",
        "    return text.strip()\n",
        "\n",
        "def save_dataset(data):\n",
        "    \"\"\"Save dataset to JSON file\"\"\"\n",
        "    dataset = {\n",
        "        \"metadata\": {\n",
        "            \"source\": \"github\",\n",
        "            \"repository\": REPO_NAME,\n",
        "            \"created_at\": datetime.now().isoformat(),\n",
        "            \"discussion_count\": len(data)\n",
        "        },\n",
        "        \"qnas\": data\n",
        "    }\n",
        "\n",
        "    with open(OUTPUT_FILE, 'w') as f:\n",
        "        json.dump(dataset, f, indent=2)\n",
        "\n",
        "    print(f\"\\n‚úÖ Dataset saved to {OUTPUT_FILE}\")\n",
        "    print(f\"Total Q&A pairs collected: {len(data)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Fetch discussions\n",
        "    discussions = fetch_discussions_graphql(REPO_NAME)\n",
        "\n",
        "    # Clean markdown content\n",
        "    for qa in discussions:\n",
        "        qa['question'] = clean_markdown(qa['question'])\n",
        "        qa['question_body'] = clean_markdown(qa['question_body'])\n",
        "        qa['answer'] = clean_markdown(qa['answer'])\n",
        "\n",
        "    # Save results\n",
        "    save_dataset(discussions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKviEJrcb_Mc",
        "outputId": "062d67ec-6bca-42d3-b683-4a6505d48788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Collecting GitHub Discussions...\n",
            "\n",
            "üìö Processing github/docs...\n",
            "‚ö†Ô∏è github/docs: Discussions not enabled\n",
            "‚ö†Ô∏è github/docs: No discussions found or not enabled\n",
            "\n",
            "üìö Processing facebook/react...\n",
            "‚ö†Ô∏è facebook/react: Discussions not enabled\n",
            "‚ö†Ô∏è facebook/react: No discussions found or not enabled\n",
            "\n",
            "üìö Processing docker/for-linux...\n",
            "‚ö†Ô∏è docker/for-linux: Discussions not enabled\n",
            "‚ö†Ô∏è docker/for-linux: No discussions found or not enabled\n",
            "\n",
            "üìö Processing python/cpython...\n",
            "‚ö†Ô∏è python/cpython: Discussions not enabled\n",
            "‚ö†Ô∏è python/cpython: No discussions found or not enabled\n",
            "\n",
            "üìö Processing pypa/pipx...\n",
            "‚úÖ pypa/pipx: 11 answered discussions\n",
            "\n",
            "üìö Processing pypa/virtualenv...\n",
            "‚úÖ pypa/virtualenv: 7 answered discussions\n",
            "\n",
            "üìö Processing git/git...\n",
            "‚ö†Ô∏è git/git: Discussions not enabled\n",
            "‚ö†Ô∏è git/git: No discussions found or not enabled\n",
            "\n",
            "üéâ Dataset saved to github_qa_dataset.json\n",
            "Total Discussions Collected: 18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "import time\n",
        "import requests\n",
        "import re\n",
        "\n",
        "# Configuration - USE YOUR OWN TOKEN!\n",
        "# GITHUB_TOKEN = \"your_github_token_here\"  # Replace with your token\n",
        "REPO_NAME = \"github/git-guide\"  # Corrected repository name\n",
        "OUTPUT_FILE = \"git_guides_qa.json\"\n",
        "\n",
        "def fetch_discussions_graphql(repo_name):\n",
        "    \"\"\"Fetch discussions from GitHub GraphQL API\"\"\"\n",
        "    owner, repo = repo_name.split('/')\n",
        "    query_template = \"\"\"\n",
        "    query($cursor: String) {\n",
        "      repository(owner: \"%s\", name: \"%s\") {\n",
        "        hasDiscussionsEnabled\n",
        "        discussions(first: 100, after: $cursor) {\n",
        "          pageInfo {\n",
        "            hasNextPage\n",
        "            endCursor\n",
        "          }\n",
        "          nodes {\n",
        "            title\n",
        "            body\n",
        "            url\n",
        "            createdAt\n",
        "            upvoteCount\n",
        "            answer {\n",
        "              body\n",
        "            }\n",
        "            comments(first: 15) {\n",
        "              nodes {\n",
        "                body\n",
        "                isAnswer\n",
        "              }\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    \"\"\" % (owner, repo)\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {GITHUB_TOKEN}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    # First check if discussions are enabled\n",
        "    try:\n",
        "        check_response = requests.post(\n",
        "            \"https://api.github.com/graphql\",\n",
        "            json={\"query\": query_template, \"variables\": {}},\n",
        "            headers=headers\n",
        "        )\n",
        "        check_response.raise_for_status()\n",
        "        check_data = check_response.json()\n",
        "\n",
        "        if 'errors' in check_data:\n",
        "            print(f\"‚ö†Ô∏è GraphQL errors: {check_data['errors'][0]['message']}\")\n",
        "            return []\n",
        "\n",
        "        if not check_data['data']['repository']['hasDiscussionsEnabled']:\n",
        "            print(f\"‚ö†Ô∏è Discussions not enabled in {repo_name}\")\n",
        "            return []\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error checking discussions status: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "    # Now fetch discussions\n",
        "    discussions = []\n",
        "    has_next_page = True\n",
        "    cursor = None\n",
        "\n",
        "    print(f\"üîç Fetching discussions from {repo_name}...\")\n",
        "\n",
        "    while has_next_page:\n",
        "        variables = {\"cursor\": cursor} if cursor else {}\n",
        "        try:\n",
        "            response = requests.post(\n",
        "                \"https://api.github.com/graphql\",\n",
        "                json={\"query\": query_template, \"variables\": variables},\n",
        "                headers=headers\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            if 'errors' in data:\n",
        "                print(f\"‚ö†Ô∏è GraphQL errors: {data['errors'][0]['message']}\")\n",
        "                break\n",
        "\n",
        "            discussion_data = data['data']['repository']['discussions']\n",
        "            nodes = discussion_data['nodes']\n",
        "\n",
        "            for node in nodes:\n",
        "                # Extract answer (either direct answer or from comments)\n",
        "                answer_body = None\n",
        "                if node.get('answer'):\n",
        "                    answer_body = node['answer']['body']\n",
        "                else:\n",
        "                    for comment in node['comments']['nodes']:\n",
        "                        if comment['isAnswer']:\n",
        "                            answer_body = comment['body']\n",
        "                            break\n",
        "\n",
        "                if answer_body:\n",
        "                    discussions.append({\n",
        "                        \"question\": node['title'],\n",
        "                        \"question_body\": node['body'],\n",
        "                        \"answer\": answer_body,\n",
        "                        \"url\": node['url'],\n",
        "                        \"created_at\": node['createdAt'],\n",
        "                        \"upvotes\": node['upvoteCount']\n",
        "                    })\n",
        "\n",
        "            # Handle pagination\n",
        "            page_info = discussion_data['pageInfo']\n",
        "            has_next_page = page_info['hasNextPage']\n",
        "            cursor = page_info['endCursor']\n",
        "\n",
        "            print(f\"üì• Collected {len(discussions)} discussions so far...\")\n",
        "            time.sleep(0.5)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error: {str(e)}\")\n",
        "            break\n",
        "\n",
        "    return discussions\n",
        "\n",
        "def clean_markdown(text):\n",
        "    \"\"\"Clean markdown content\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "    # Remove excessive newlines\n",
        "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "    # Clean code blocks\n",
        "    text = text.replace(\"```\\n\\n\", \"```\\n\")\n",
        "    return text.strip()\n",
        "\n",
        "def save_dataset(data):\n",
        "    \"\"\"Save dataset to JSON file\"\"\"\n",
        "    dataset = {\n",
        "        \"metadata\": {\n",
        "            \"source\": \"github\",\n",
        "            \"repository\": REPO_NAME,\n",
        "            \"created_at\": datetime.now().isoformat(),\n",
        "            \"discussion_count\": len(data)\n",
        "        },\n",
        "        \"qnas\": data\n",
        "    }\n",
        "\n",
        "    with open(OUTPUT_FILE, 'w') as f:\n",
        "        json.dump(dataset, f, indent=2)\n",
        "\n",
        "    print(f\"\\n‚úÖ Dataset saved to {OUTPUT_FILE}\")\n",
        "    print(f\"Total Q&A pairs collected: {len(data)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Fetch discussions\n",
        "    discussions = fetch_discussions_graphql(REPO_NAME)\n",
        "\n",
        "    # Clean markdown content\n",
        "    for qa in discussions:\n",
        "        qa['question'] = clean_markdown(qa['question'])\n",
        "        qa['question_body'] = clean_markdown(qa['question_body'])\n",
        "        qa['answer'] = clean_markdown(qa['answer'])\n",
        "\n",
        "    # Save results\n",
        "    save_dataset(discussions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdj0kwJ_hOoL",
        "outputId": "2b7ef3eb-70ba-42ad-f94a-ba99d69fe3b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è GraphQL errors: Could not resolve to a Repository with the name 'github/git-guide'.\n",
            "\n",
            "‚úÖ Dataset saved to git_guides_qa.json\n",
            "Total Q&A pairs collected: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nTAS8YlQrHPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "N2Cgv41vzeR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "formatted_data = []\n",
        "\n",
        "for file in [\"file1.json\", \"file2.json\"]:\n",
        "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
        "        raw_data = json.load(f)\n",
        "\n",
        "    qna_pairs = raw_data[\"qna_pairs\"]\n",
        "\n",
        "    for pair in qna_pairs:\n",
        "        title = pair[\"question\"].get(\"title\", \"\").strip()\n",
        "        body = pair[\"question\"][\"body\"].strip()\n",
        "        answer = pair[\"answer\"][\"body\"].strip()\n",
        "\n",
        "        # instruction = f\"{title}\\n\\n{body}\" if title else body\n",
        "\n",
        "        formatted_data.append({\n",
        "            \"instruction\": title,\n",
        "            \"output\": answer\n",
        "        })\n",
        "\n",
        "# Save as JSONL\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "with open(\"data/command_qa.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for item in formatted_data:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"‚úÖ Data saved using title + body as instruction: data/command_qa.jsonl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2i5qJhYQIBh",
        "outputId": "54b20e85-d9e0-4c34-8ea8-a4c120f18fa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Data saved using title + body as instruction: data/command_qa.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "def extract_commands(text):\n",
        "    return re.findall(r'```(?:bash|sh)?\\n(.*?)\\n```', text, re.DOTALL)\n",
        "\n",
        "processed_data = []\n",
        "seen_questions = set()\n",
        "\n",
        "# Process Stack Overflow-style files\n",
        "for file in [\"file1.json\", \"file2.json\"]:\n",
        "    data = json.load(open(file))\n",
        "    for pair in data[\"qna_pairs\"]:\n",
        "        # Extract commands from answer\n",
        "        commands = extract_commands(pair[\"answer\"][\"body\"])\n",
        "        if not commands: continue\n",
        "\n",
        "        # Create clean Q&A pair\n",
        "        question = f\"{pair['question']['title']} {pair['question']['body']}\".strip()\n",
        "        answer = commands[0].strip()  # Take first valid command\n",
        "\n",
        "        # Remove shell prompts and line numbers\n",
        "        answer = re.sub(r'^\\$\\s*|\\s*#\\s*\\d+', '', answer, flags=re.MULTILINE)\n",
        "\n",
        "        if question not in seen_questions:\n",
        "            seen_questions.add(question)\n",
        "            processed_data.append({\"question\": question, \"answer\": answer})\n",
        "\n",
        "# # Process GitHub-style file\n",
        "# github_data = json.load(open(\"file3.json\"))\n",
        "# for qna in github_data[\"qnas\"]:\n",
        "#     commands = extract_commands(qna[\"answer\"])\n",
        "#     if commands:\n",
        "#         answer = commands[0].strip()\n",
        "#         answer = re.sub(r'^\\$\\s*', '', answer, flags=re.MULTILINE)\n",
        "\n",
        "#         if qna[\"question\"] not in seen_questions:\n",
        "#             seen_questions.add(qna[\"question\"])\n",
        "#             processed_data.append({\"question\": qna[\"question\"], \"answer\": answer})\n",
        "\n",
        "# Save processed data\n",
        "with open(\"processed_data.json\", \"w\") as f:\n",
        "    json.dump(processed_data, f, indent=2)"
      ],
      "metadata": {
        "id": "iik9WSMdzglZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(processed_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0ZlEw-S0Rf_",
        "outputId": "33ba0c93-4ca4-461f-cf72-c3d87d86562c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "973"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "def extract_commands(text):\n",
        "    return re.findall(r'```(?:bash|sh)?\\n(.*?)\\n```', text, re.DOTALL)\n",
        "\n",
        "def clean_text_block(text):\n",
        "    # Remove leading/trailing spaces on each line\n",
        "    lines = [line.strip() for line in text.strip().splitlines()]\n",
        "    # Remove empty lines and normalize to max 1 blank line\n",
        "    cleaned = []\n",
        "    blank_line = False\n",
        "    for line in lines:\n",
        "        if not line:\n",
        "            if not blank_line:\n",
        "                cleaned.append('')\n",
        "                blank_line = True\n",
        "        else:\n",
        "            cleaned.append(line)\n",
        "            blank_line = False\n",
        "    return '\\n'.join(cleaned).strip()\n",
        "\n",
        "processed_data = []\n",
        "seen_questions = set()\n",
        "\n",
        "for file in [\"file1.json\", \"file2.json\"]:\n",
        "    with open(file, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    for pair in data.get(\"qna_pairs\", []):\n",
        "        question = f\"{pair['question']['title']} {pair['question']['body']}\".strip()\n",
        "\n",
        "        if question in seen_questions:\n",
        "            continue\n",
        "\n",
        "        raw_answer = pair.get(\"answer\", {}).get(\"body\", \"\")\n",
        "\n",
        "        # Extract and clean commands\n",
        "        commands = extract_commands(raw_answer)\n",
        "        cleaned_commands = []\n",
        "        for cmd in commands:\n",
        "            cmd = re.sub(r'^\\s*\\$\\s*', '', cmd, flags=re.MULTILINE)\n",
        "            cmd = re.sub(r'^\\s*#.*$', '', cmd, flags=re.MULTILINE)\n",
        "            cmd = re.sub(r'^\\s*\\d+\\s+', '', cmd, flags=re.MULTILINE)\n",
        "            cmd_cleaned = clean_text_block(cmd)\n",
        "            if cmd_cleaned:\n",
        "                cleaned_commands.append(cmd_cleaned)\n",
        "\n",
        "        # Remove code blocks from explanation\n",
        "        explanation = re.sub(r'```(?:bash|sh)?\\n.*?\\n```', '', raw_answer, flags=re.DOTALL)\n",
        "        explanation_clean = clean_text_block(explanation)\n",
        "\n",
        "        # Combine both parts cleanly\n",
        "        full_answer_parts = []\n",
        "        if cleaned_commands:\n",
        "            full_answer_parts.append('\\n'.join(cleaned_commands))\n",
        "        if explanation_clean:\n",
        "            full_answer_parts.append(explanation_clean)\n",
        "\n",
        "        full_answer = '\\n\\n'.join(full_answer_parts).strip()\n",
        "\n",
        "        if full_answer:\n",
        "            processed_data.append({\"question\": question, \"answer\": full_answer})\n",
        "            seen_questions.add(question)\n",
        "\n",
        "# Save clean output\n",
        "with open(\"processed_data.json\", \"w\") as f:\n",
        "    json.dump(processed_data, f, indent=2)\n"
      ],
      "metadata": {
        "id": "IPQvs8XWHc9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(processed_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7BKFvJMIXqh",
        "outputId": "75b3ccdb-b66d-4651-c843-146602a2f407"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1297"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YjPNGH9AKNou"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}