{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12201828,"sourceType":"datasetVersion","datasetId":7686045},{"sourceId":12204959,"sourceType":"datasetVersion","datasetId":7688308},{"sourceId":439408,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":358312,"modelId":379636}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-18T17:00:22.398348Z","iopub.execute_input":"2025-06-18T17:00:22.398970Z","iopub.status.idle":"2025-06-18T17:00:22.409286Z","shell.execute_reply.started":"2025-06-18T17:00:22.398938Z","shell.execute_reply":"2025-06-18T17:00:22.408575Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/command-qa/command_qa.jsonl\n/kaggle/input/model/transformers/default/1/kaggle/working/qlora_tinyllama_cli_final/adapter_model.safetensors\n/kaggle/input/model/transformers/default/1/kaggle/working/qlora_tinyllama_cli_final/training_args.bin\n/kaggle/input/model/transformers/default/1/kaggle/working/qlora_tinyllama_cli_final/adapter_config.json\n/kaggle/input/model/transformers/default/1/kaggle/working/qlora_tinyllama_cli_final/README.md\n/kaggle/input/model/transformers/default/1/kaggle/working/qlora_tinyllama_cli_final/tokenizer.json\n/kaggle/input/model/transformers/default/1/kaggle/working/qlora_tinyllama_cli_final/tokenizer_config.json\n/kaggle/input/model/transformers/default/1/kaggle/working/qlora_tinyllama_cli_final/special_tokens_map.json\n/kaggle/input/model/transformers/default/1/kaggle/working/qlora_tinyllama_cli_final/tokenizer.model\n/kaggle/input/title-answer/command_qa (1).jsonl\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# !pip install -q transformers datasets accelerate peft bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T17:00:25.517593Z","iopub.execute_input":"2025-06-18T17:00:25.518282Z","iopub.status.idle":"2025-06-18T17:00:25.521489Z","shell.execute_reply.started":"2025-06-18T17:00:25.518258Z","shell.execute_reply":"2025-06-18T17:00:25.520842Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# !pip install bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T17:00:25.522669Z","iopub.execute_input":"2025-06-18T17:00:25.522955Z","iopub.status.idle":"2025-06-18T17:00:25.537498Z","shell.execute_reply.started":"2025-06-18T17:00:25.522936Z","shell.execute_reply":"2025-06-18T17:00:25.536840Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"from datasets import load_dataset\n\ndata_path = \"/kaggle/input/title-answer/command_qa (1).jsonl\"\ndataset = load_dataset(\"json\", data_files=data_path, split=\"train\")\n\nprint(dataset[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T17:05:15.793399Z","iopub.execute_input":"2025-06-18T17:05:15.794044Z","iopub.status.idle":"2025-06-18T17:05:17.052059Z","shell.execute_reply.started":"2025-06-18T17:05:15.794017Z","shell.execute_reply":"2025-06-18T17:05:17.051475Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff34676da2ec4aa8915a51a4b6073d22"}},"metadata":{}},{"name":"stdout","text":"{'instruction': 'How do I undo the most recent local commits in Git?', 'output': 'Undo a commit & redo\\n\\n```\\n$ git commit -m \"Something terribly misguided\" # (0: Your Accident)\\n$ git reset HEAD~                              # (1)\\n# === If you just want to undo the commit, stop here! ===\\n[ edit files as necessary ]                    # (2)\\n$ git add .                                    # (3)\\n$ git commit -c ORIG_HEAD                      # (4)\\n\\n```\\n\\n\\ngit reset is the command responsible for the undo. It will undo your last commit while leaving your working tree (the state of your files on disk) untouched. You\\'ll need to add them again before you can commit them again.\\nMake corrections to working tree files.\\ngit add anything that you want to include in your new commit.\\nCommit the changes, reusing the old commit message. reset copied the old head to .git/ORIG_HEAD; commit with -c ORIG_HEAD will open an editor, which initially contains the log message from the old commit and allows you to edit it. If you do not need to edit the message, you could use the -C option.\\n\\nAlternatively, to edit the previous commit (or just its commit message), commit --amend will add changes within the current index to the previous commit.\\nTo remove (not revert) a commit that has been pushed to the server, rewriting history with git push origin main --force[-with-lease] is necessary. It\\'s almost always a bad idea to use --force; prefer --force-with-lease instead, and as noted in the git manual:\\n\\nYou should understand the implications of rewriting history if you amend a commit that has already been published.\\n\\n\\nFurther Reading\\nYou can use git reflog to determine the SHA-1 for the commit to which you wish to revert. Once you have this value, use the sequence of commands as explained above.\\n\\nHEAD~ is the same as HEAD~1. The article What is the HEAD in git? is helpful if you want to uncommit multiple commits.'}\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import re\nimport re\n\ndef clean_instruction(example):\n    text = example['instruction']\n    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n    text = \"\\n\".join(line.rstrip() for line in text.splitlines())\n    text = re.sub(r'[ \\t]{2,}', ' ', text)  # collapse tabs or multiple spaces\n    example['instruction'] = text.strip()\n    return example\n\n\ndataset_process = dataset.map(clean_instruction)\n\nprint(dataset_process[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T17:05:17.053180Z","iopub.execute_input":"2025-06-18T17:05:17.053690Z","iopub.status.idle":"2025-06-18T17:05:23.253927Z","shell.execute_reply.started":"2025-06-18T17:05:17.053669Z","shell.execute_reply":"2025-06-18T17:05:23.253104Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1429 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64d32f27066740f49ca0b30bdf2116fd"}},"metadata":{}},{"name":"stdout","text":"{'instruction': 'How do I undo the most recent local commits in Git?', 'output': 'Undo a commit & redo\\n\\n```\\n$ git commit -m \"Something terribly misguided\" # (0: Your Accident)\\n$ git reset HEAD~                              # (1)\\n# === If you just want to undo the commit, stop here! ===\\n[ edit files as necessary ]                    # (2)\\n$ git add .                                    # (3)\\n$ git commit -c ORIG_HEAD                      # (4)\\n\\n```\\n\\n\\ngit reset is the command responsible for the undo. It will undo your last commit while leaving your working tree (the state of your files on disk) untouched. You\\'ll need to add them again before you can commit them again.\\nMake corrections to working tree files.\\ngit add anything that you want to include in your new commit.\\nCommit the changes, reusing the old commit message. reset copied the old head to .git/ORIG_HEAD; commit with -c ORIG_HEAD will open an editor, which initially contains the log message from the old commit and allows you to edit it. If you do not need to edit the message, you could use the -C option.\\n\\nAlternatively, to edit the previous commit (or just its commit message), commit --amend will add changes within the current index to the previous commit.\\nTo remove (not revert) a commit that has been pushed to the server, rewriting history with git push origin main --force[-with-lease] is necessary. It\\'s almost always a bad idea to use --force; prefer --force-with-lease instead, and as noted in the git manual:\\n\\nYou should understand the implications of rewriting history if you amend a commit that has already been published.\\n\\n\\nFurther Reading\\nYou can use git reflog to determine the SHA-1 for the commit to which you wish to revert. Once you have this value, use the sequence of commands as explained above.\\n\\nHEAD~ is the same as HEAD~1. The article What is the HEAD in git? is helpful if you want to uncommit multiple commits.'}\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# for datast in range(len(dataset)):\n#     print(dataset[datast]['instruction'])\n\ntotal_length = 0\nmax_length = 0\n\nfor item in dataset_process:\n    instruction_length = len(item['instruction'])\n    total_length += instruction_length\n    if instruction_length > max_length:\n        max_length = instruction_length\n\naverage_length = total_length / len(dataset_process)\n\nprint(\"Max instruction length:\", max_length)\nprint(\"Average instruction length:\", average_length)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T17:05:23.254618Z","iopub.execute_input":"2025-06-18T17:05:23.254814Z","iopub.status.idle":"2025-06-18T17:05:23.295641Z","shell.execute_reply.started":"2025-06-18T17:05:23.254797Z","shell.execute_reply":"2025-06-18T17:05:23.294943Z"}},"outputs":[{"name":"stdout","text":"Max instruction length: 170\nAverage instruction length: 52.87263820853744\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"total_length = 0\nmax_length = 0\n\nfor item in dataset_process:\n    output_length = len(item['output'])\n    total_length += output_length\n    if output_length > max_length:\n        max_length = output_length\n\naverage_length = total_length / len(dataset_process)\n\nprint(\"Max output length:\", max_length)\nprint(\"Average output length:\", average_length)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T17:05:23.296950Z","iopub.execute_input":"2025-06-18T17:05:23.297176Z","iopub.status.idle":"2025-06-18T17:05:23.338317Z","shell.execute_reply.started":"2025-06-18T17:05:23.297160Z","shell.execute_reply":"2025-06-18T17:05:23.337756Z"}},"outputs":[{"name":"stdout","text":"Max output length: 15203\nAverage output length: 896.0468859342197\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\ntokenizer.pad_token = tokenizer.eos_token\n\ndef tokenize(example):\n    # Format the full text\n    text = f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['output']}\"\n    \n    # Tokenize with proper formatting\n    tokenized = tokenizer(\n        text,\n        truncation=True,\n        max_length=2048,\n        padding=\"max_length\",  # Ensures fixed-length sequences\n        return_tensors=\"pt\"    # Returns PyTorch tensors\n    )\n    \n    # Create labels (mask instruction tokens)\n    instruction_text = f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n\"\n    instruction_tokens = tokenizer(\n        instruction_text,\n        truncation=True,\n        max_length=150,\n        add_special_tokens=False\n    )\n    \n    # Create labels (-100 for instruction, token IDs for response)\n    labels = [-100] * len(instruction_tokens[\"input_ids\"]) + tokenized[\"input_ids\"][0][len(instruction_tokens[\"input_ids\"]):].tolist()\n    \n    return {\n        \"input_ids\": tokenized[\"input_ids\"][0].tolist(),  # Convert to list\n        \"attention_mask\": tokenized[\"attention_mask\"][0].tolist(),\n        \"labels\": labels\n    }\n\n\n# Process dataset with error handling\ntry:\n    tokenized_dataset = dataset_process.map(\n        tokenize,\n        batched=False,  # Process one example at a time for stability\n        remove_columns=[\"instruction\", \"output\"]\n    )\nexcept Exception as e:\n    print(f\"Error during tokenization: {e}\")\n    # Add debug to find problematic examples\n    for i, example in enumerate(dataset):\n        try:\n            tokenize(example)\n        except Exception as ex:\n            print(f\"Error in example {i}: {ex}\")\n            print(example)\n            break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T17:05:23.343528Z","iopub.execute_input":"2025-06-18T17:05:23.344346Z","iopub.status.idle":"2025-06-18T17:05:28.786931Z","shell.execute_reply.started":"2025-06-18T17:05:23.344321Z","shell.execute_reply":"2025-06-18T17:05:28.786086Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a0d9159d2e143cb9eabc53d2eb21992"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1220cf941914a7c99ab67cfa249f0fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95c7ce18b7ad464a95a909b83223ffab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebead82efa50450c98c4229d1a459276"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1429 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd0e05fe4ea14e85bf14348ccf70580a"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T17:05:28.788250Z","iopub.execute_input":"2025-06-18T17:05:28.788501Z","iopub.status.idle":"2025-06-18T17:05:28.800681Z","shell.execute_reply.started":"2025-06-18T17:05:28.788477Z","shell.execute_reply":"2025-06-18T17:05:28.800226Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"tokenized_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T17:05:28.801292Z","iopub.execute_input":"2025-06-18T17:05:28.801529Z","iopub.status.idle":"2025-06-18T17:05:28.882251Z","shell.execute_reply.started":"2025-06-18T17:05:28.801507Z","shell.execute_reply":"2025-06-18T17:05:28.881692Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 1286\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 143\n    })\n})"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"tokenized_dataset['train'][10]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T17:02:59.545502Z","iopub.execute_input":"2025-06-18T17:02:59.545719Z","iopub.status.idle":"2025-06-18T17:04:32.980197Z","shell.execute_reply.started":"2025-06-18T17:02:59.545702Z","shell.execute_reply":"2025-06-18T17:04:32.979450Z"}},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nDownloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\nSuccessfully installed bitsandbytes-0.46.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T17:04:32.983901Z","iopub.execute_input":"2025-06-18T17:04:32.984145Z","iopub.status.idle":"2025-06-18T17:04:32.988018Z","shell.execute_reply.started":"2025-06-18T17:04:32.984117Z","shell.execute_reply":"2025-06-18T17:04:32.987432Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\nimport torch\n\nbase_model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n\n# tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n# tokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model_id,\n    load_in_4bit=True,\n    torch_dtype=torch.float16,\n    device_map={\"\": 0}\n)\nmodel = prepare_model_for_kbit_training(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T17:04:35.703808Z","iopub.execute_input":"2025-06-18T17:04:35.704043Z","iopub.status.idle":"2025-06-18T17:05:09.553636Z","shell.execute_reply.started":"2025-06-18T17:04:35.704026Z","shell.execute_reply":"2025-06-18T17:05:09.553111Z"}},"outputs":[{"name":"stderr","text":"2025-06-18 17:04:46.562368: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750266286.762398      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750266286.815177      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f79900ae32c433594b480b8d0c4f6ab"}},"metadata":{}},{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3764aca67b2f4c67b4be0dd0063ad17b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a07f96514f34cd89fcd1035f00ab698"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=32,\n    lora_alpha=64,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T17:05:33.832731Z","iopub.execute_input":"2025-06-18T17:05:33.833304Z","iopub.status.idle":"2025-06-18T17:05:33.945500Z","shell.execute_reply.started":"2025-06-18T17:05:33.833279Z","shell.execute_reply":"2025-06-18T17:05:33.944724Z"}},"outputs":[{"name":"stdout","text":"trainable params: 4,505,600 || all params: 1,104,553,984 || trainable%: 0.4079\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"sample = tokenized_dataset[\"train\"][0]\nprint(f\"Input IDs shape: {len(sample['input_ids'])}\")\nprint(f\"Attention mask shape: {len(sample['attention_mask'])}\")\nprint(f\"Labels shape: {len(sample['labels'])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T17:05:36.074105Z","iopub.execute_input":"2025-06-18T17:05:36.074837Z","iopub.status.idle":"2025-06-18T17:05:36.083035Z","shell.execute_reply.started":"2025-06-18T17:05:36.074814Z","shell.execute_reply":"2025-06-18T17:05:36.082349Z"}},"outputs":[{"name":"stdout","text":"Input IDs shape: 2048\nAttention mask shape: 2048\nLabels shape: 2048\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\nfrom transformers import DataCollatorForLanguageModeling\n\n# import torch\n# torch.cuda.set_device(device)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./qlora_tinyllama_cli\",\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-4,\n    fp16=True,\n    report_to=\"none\"\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    # label_names=[\"input_ids\", \"attention_mask\", \"labels\"]\n)\n\n\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T17:06:22.437162Z","iopub.execute_input":"2025-06-18T17:06:22.437413Z","iopub.status.idle":"2025-06-18T18:12:26.186640Z","shell.execute_reply.started":"2025-06-18T17:06:22.437397Z","shell.execute_reply":"2025-06-18T18:12:26.185961Z"}},"outputs":[{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [80/80 1:05:14, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1.825600</td>\n      <td>1.869084</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=80, training_loss=1.872579574584961, metrics={'train_runtime': 3963.2701, 'train_samples_per_second': 0.324, 'train_steps_per_second': 0.02, 'total_flos': 1.634233982386176e+16, 'train_loss': 1.872579574584961, 'epoch': 0.995334370139969})"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T18:12:31.174567Z","iopub.execute_input":"2025-06-18T18:12:31.174815Z","iopub.status.idle":"2025-06-18T18:12:31.413803Z","shell.execute_reply.started":"2025-06-18T18:12:31.174798Z","shell.execute_reply":"2025-06-18T18:12:31.412979Z"}},"outputs":[{"name":"stdout","text":"Wed Jun 18 18:12:31 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   61C    P0             35W /  250W |    4673MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"!pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T18:13:30.736432Z","iopub.execute_input":"2025-06-18T18:13:30.736709Z","iopub.status.idle":"2025-06-18T18:13:34.569672Z","shell.execute_reply.started":"2025-06-18T18:13:30.736690Z","shell.execute_reply":"2025-06-18T18:13:34.568940Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.31.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nCollecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec, evaluate\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed evaluate-0.4.3 fsspec-2025.3.0\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import evaluate\ntrainer.evaluate()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T18:13:36.751209Z","iopub.execute_input":"2025-06-18T18:13:36.751859Z","iopub.status.idle":"2025-06-18T18:15:31.478278Z","shell.execute_reply.started":"2025-06-18T18:13:36.751829Z","shell.execute_reply":"2025-06-18T18:15:31.477501Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='72' max='72' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [72/72 01:53]\n    </div>\n    "},"metadata":{}},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 1.8690844774246216,\n 'eval_runtime': 114.4551,\n 'eval_samples_per_second': 1.249,\n 'eval_steps_per_second': 0.629,\n 'epoch': 0.995334370139969}"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"# Move model/data to GPU 1\n# model = model.to(1)\n# input_tensor = input_tensor.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# from IPython.display import clear_output\n\n# # Clear GPU cache\n# torch.cuda.empty_cache()\n\n# # Clear notebook output (optional)\n# clear_output()\n\n# # Verify memory is freed\n# print(f\"Memory allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n# print(f\"Memory reserved: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.save_model(\"./qlora_tinyllama_cli_final\")\ntokenizer.save_pretrained(\"./qlora_tinyllama_cli_final\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T18:13:04.085386Z","iopub.execute_input":"2025-06-18T18:13:04.085661Z","iopub.status.idle":"2025-06-18T18:13:04.387550Z","shell.execute_reply.started":"2025-06-18T18:13:04.085644Z","shell.execute_reply":"2025-06-18T18:13:04.386957Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"('./qlora_tinyllama_cli_final/tokenizer_config.json',\n './qlora_tinyllama_cli_final/special_tokens_map.json',\n './qlora_tinyllama_cli_final/tokenizer.model',\n './qlora_tinyllama_cli_final/added_tokens.json',\n './qlora_tinyllama_cli_final/tokenizer.json')"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"from transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"./qlora_tinyllama_cli_final\", tokenizer=tokenizer, device=0)\nprompt = (\n    \"### Instruction:\\n\"\n    \"How can I delete a remote Git branch?\\n\\n\"\n    \"### Response:\\n\"\n)\n\noutput = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7)[0][\"generated_text\"]\nprint(output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T18:15:31.479509Z","iopub.execute_input":"2025-06-18T18:15:31.479731Z","iopub.status.idle":"2025-06-18T18:15:42.603519Z","shell.execute_reply.started":"2025-06-18T18:15:31.479714Z","shell.execute_reply":"2025-06-18T18:15:42.602791Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"### Instruction:\nHow can I delete a remote Git branch?\n\n### Response:\nYou can do this using Git's remote delete command:\n\n```\ngit push --delete remote/branchname\n\n```\n\nThis will delete the branch from the remote and remove it from your local repository.\nThat's it.\n\nNote that if a branch has been merged into master or origin/master, it won't be deleted. In these cases, you will need to do a new push to delete the branch.\n\n```\ngit push origin --delete branchname\n\n```\n\nIf you still need to delete a remote branch, you can use the delete-remote command:\n\n```\ngit remote delete remote/branchname\n\n```\n\nThis will remove the remote and the branch from the remote repository.\nNote that you can't delete a remote branch that is referenced by a local branch.\n\nA couple of side notes:\n\n* Do not use --delete-branch when deleting a branch that has been merged into a remote repository. This is because Git does not remove the remote branch.\n\n* Be careful when using the --delete-branch option when deleting a remote branch that has been merged into the remote repository. If you make a mistake while deleting, you may end up\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"from transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"./qlora_tinyllama_cli_final\", tokenizer=tokenizer, device=0)\nprompt = (\n    \"### Instruction:\\n\"\n    \"How can I delete a remote Git branch?\\n\\n\"\n    \"### Response:\\n\"\n)\n\noutput = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7)[0][\"generated_text\"]\nprint(output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T18:17:32.389642Z","iopub.execute_input":"2025-06-18T18:17:32.389916Z","iopub.status.idle":"2025-06-18T18:17:42.673052Z","shell.execute_reply.started":"2025-06-18T18:17:32.389896Z","shell.execute_reply":"2025-06-18T18:17:42.672414Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"### Instruction:\nHow can I delete a remote Git branch?\n\n### Response:\nYou can delete a remote Git branch with the following command:\n\ngit push origin --delete BRANCH_NAME\n\nThis will delete the branch with the same name as the remote branch, which you need to have created locally before.\n\nIf you want to delete the branch from the remote repository without deleting it locally, you can use the following command:\n\ngit push origin --delete BRANCH_NAME:BRANCH_NAME\n\nNote that the --delete option is not necessary for all remote branches (the default behavior is not to delete remote branches). You can also use the --force option to delete branches that you don't want to delete, and you can use the --force-with-lease option to delete branches that are currently being merged.\n\nTo delete any local branch that is not remotely related to a remote branch, you can use the following command:\n\ngit branch -m BRANCH_NAME\n\nThis will remove any local branch that is not remotely related to a remote branch, which includes your local branches that are not remotely related to any remote branches.\n\nFor example, if you have a local branch called \"master\" that is not related to any remote branch\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"!zip -r /kaggle/working/qlora_tinyllama_cli_final.zip /kaggle/working/qlora_tinyllama_cli_final","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T18:17:52.614095Z","iopub.execute_input":"2025-06-18T18:17:52.614368Z","iopub.status.idle":"2025-06-18T18:17:53.915556Z","shell.execute_reply.started":"2025-06-18T18:17:52.614350Z","shell.execute_reply":"2025-06-18T18:17:53.914631Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/qlora_tinyllama_cli_final/ (stored 0%)\n  adding: kaggle/working/qlora_tinyllama_cli_final/adapter_config.json (deflated 53%)\n  adding: kaggle/working/qlora_tinyllama_cli_final/tokenizer.json (deflated 85%)\n  adding: kaggle/working/qlora_tinyllama_cli_final/adapter_model.safetensors","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":" (deflated 8%)\n  adding: kaggle/working/qlora_tinyllama_cli_final/tokenizer_config.json (deflated 68%)\n  adding: kaggle/working/qlora_tinyllama_cli_final/README.md (deflated 66%)\n  adding: kaggle/working/qlora_tinyllama_cli_final/training_args.bin (deflated 52%)\n  adding: kaggle/working/qlora_tinyllama_cli_final/tokenizer.model (deflated 55%)\n  adding: kaggle/working/qlora_tinyllama_cli_final/special_tokens_map.json (deflated 73%)\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"eval_prompts = [\n    \"Create a new Git branch and switch to it.\",\n    \"Compress the folder reports into reports.tar.gz.\",\n    \"List all Python files in the current directory recursively.\",\n    \"Set up a virtual environment and install requests.\",\n    \"Fetch only the first ten lines of a file named output.log.\",\n    \"Clone a repository using git command.\",  # Edge case 1\n    \"How can I delete a remote Git branch?\",        # Edge case 2\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T18:18:10.844041Z","iopub.execute_input":"2025-06-18T18:18:10.844387Z","iopub.status.idle":"2025-06-18T18:18:10.848837Z","shell.execute_reply.started":"2025-06-18T18:18:10.844359Z","shell.execute_reply":"2025-06-18T18:18:10.848239Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# pip install -U bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T18:18:23.236602Z","iopub.execute_input":"2025-06-18T18:18:23.236864Z","iopub.status.idle":"2025-06-18T18:18:23.240459Z","shell.execute_reply.started":"2025-06-18T18:18:23.236846Z","shell.execute_reply":"2025-06-18T18:18:23.239727Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"pipe = pipeline(\"text-generation\", model=\"./qlora_tinyllama_cli_final\", tokenizer=tokenizer, device=0)\nprompt = (\n    \"### Instruction:\\n\"\n    \"How to push in to git?\\n\\n\"\n    \"### Response:\\n\"\n)\n\noutput = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.3)[0][\"generated_text\"]\nprint(output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T18:22:24.207866Z","iopub.execute_input":"2025-06-18T18:22:24.208480Z","iopub.status.idle":"2025-06-18T18:22:34.422828Z","shell.execute_reply.started":"2025-06-18T18:22:24.208454Z","shell.execute_reply":"2025-06-18T18:22:34.422145Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"### Instruction:\nHow to push in to git?\n\n### Response:\nYou can use the following command to push to the remote repository:\n\n```\ngit push origin master\n\n```\n\nThis will push the changes from your local repository to the remote repository.\n\nIf you want to push to a specific branch, you can use the following command:\n\n```\ngit push origin branch_name\n\n```\n\nThis will push the changes to the branch named branch_name.\n\nIf you want to push to a specific commit, you can use the following command:\n\n```\ngit push origin HEAD:commit_id\n\n```\n\nThis will push the changes to the commit with the given commit ID.\n\nIf you want to push to a specific commit, you can use the following command:\n\n```\ngit push origin commit_id:commit_id\n\n```\n\nThis will push the changes to the commit with the given commit ID.\n\nIf you want to push to a specific branch and commit, you can use the following command:\n\n```\ngit push origin branch_name:commit_id\n\n```\n\nThis will push the changes to the branch named branch_name and the commit with the given commit ID.\n\nIf you want\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom peft import PeftModel\nimport torch\n\n# Load base model\nbase_model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\nadapter_path = \"/kaggle/input/model/transformers/default/1/kaggle/working/qlora_tinyllama_cli_final\"\n\ntokenizer = AutoTokenizer.from_pretrained(base_model_id)\ntokenizer.pad_token = tokenizer.eos_token\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    base_model_id,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n\n# Load and merge PEFT adapter\npeft_model = PeftModel.from_pretrained(base_model, adapter_path)\n# merged_model = peft_model.merge_and_unload()\n\n# Build pipelines\npipe_base = pipeline(\"text-generation\", model=base_model, tokenizer=tokenizer)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pipe = pipeline(\"text-generation\", model=\"./qlora_tinyllama_cli_final\", tokenizer=tokenizer, device=0)\n# prompt = (\n#     \"### Instruction:\\n\"\n#     \"How to push in to git?\\n\\n\"\n#     \"### Response:\\n\"\n# )\n\n# output = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.3)[0][\"generated_text\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T18:24:27.490797Z","iopub.execute_input":"2025-06-18T18:24:27.491069Z","iopub.status.idle":"2025-06-18T18:24:30.611755Z","shell.execute_reply.started":"2025-06-18T18:24:27.491049Z","shell.execute_reply":"2025-06-18T18:24:30.610949Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"def get_outputs(pipe, prompts):\n    outputs = []\n    for prompt in prompts:\n        full_prompt = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n        result = pipe(full_prompt, max_new_tokens=200, do_sample=False)[0]['generated_text']\n        response = result.split(\"### Response:\")[-1].strip()\n        outputs.append(response)\n    return outputs\n\nbase_outputs = get_outputs(pipe_base, eval_prompts)\n# finetuned_outputs = get_outputs(pipe_finetuned, eval_prompts)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T18:19:04.687262Z","iopub.execute_input":"2025-06-18T18:19:04.687783Z","iopub.status.idle":"2025-06-18T18:20:13.103412Z","shell.execute_reply.started":"2025-06-18T18:19:04.687760Z","shell.execute_reply":"2025-06-18T18:20:13.102617Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"finetuned_outputs=get_outputs(pipe, eval_prompts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T18:25:31.841750Z","iopub.execute_input":"2025-06-18T18:25:31.842337Z","iopub.status.idle":"2025-06-18T18:26:11.069658Z","shell.execute_reply.started":"2025-06-18T18:25:31.842312Z","shell.execute_reply":"2025-06-18T18:26:11.069093Z"}},"outputs":[{"name":"stderr","text":"You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"finetuned_outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T18:26:11.070723Z","iopub.execute_input":"2025-06-18T18:26:11.070926Z","iopub.status.idle":"2025-06-18T18:26:11.075668Z","shell.execute_reply.started":"2025-06-18T18:26:11.070903Z","shell.execute_reply":"2025-06-18T18:26:11.074944Z"}},"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"['You can create a new branch by typing git branch -m branchname.\\n\\nThen you can switch to it by typing git checkout branchname.\\n\\nYou can also switch back to the previous branch by typing git checkout -b branchname.\\n\\nYou can also switch to a branch by typing git checkout branchname.\\n\\nYou can also switch back to the previous branch by typing git checkout -b branchname.\\n\\nYou can also switch to a branch by typing git checkout branchname.\\n\\nYou can also switch back to the previous branch by typing git checkout -b branchname.\\n\\nYou can also switch to a branch by typing git checkout branchname.\\n\\nYou can also switch back to the previous branch by typing git checkout -b branchname.\\n\\nYou can also switch to a branch by typing git checkout branchname.\\n\\nYou can also switch back to the previous branch by typing git checkout -b branchname.\\n\\nYou can also switch',\n 'You can use tar command to compress the folder reports into reports.tar.gz.\\n\\n```\\ntar -czvf reports.tar.gz reports\\n\\n```\\n\\nThis will create a tar archive named reports.tar.gz containing all the files in the reports directory.\\n\\n```\\ntar -czvf reports.tar.gz reports\\n\\n```\\n\\nThis will create a tar archive named reports.tar.gz containing all the files in the reports directory.\\n\\n```\\ntar -czvf reports.tar.gz reports\\n\\n```\\n\\nThis will create a tar archive named reports.tar.gz containing all the files in the reports directory.\\n\\n```\\ntar -czvf reports.tar.gz reports\\n\\n```\\n\\nThis will create a tar archive named reports.tar.gz containing all the files in the reports directory.\\n\\n```\\ntar -czvf reports.tar.gz reports\\n\\n```',\n \"You can use the os.listdir() function to list all the files in the current directory.\\n\\n```\\nimport os\\n\\nfor root, dirs, files in os.walk('.'):\\n    for file in files:\\n        print(os.path.join(root, file))\\n\\n```\\n\\nThis will output:\\n\\n```\\n.\\n..\\n\\n```\\n\\n\\nA: You can use os.walk() function to recursively list all files and directories in the current directory.\\nimport os\\n\\nfor root, dirs, files in os.walk('.'):\\n    for file in files:\\n        print(os.path.join(root, file))\\n\\n\\nA: You can use os.walk() function to recursively list all files and directories in the current directory.\\nimport os\\n\\nfor root, dirs, files in os.walk('.'):\\n    for file in files:\",\n 'You can use pip to install requests:\\n\\n```\\npip install requests\\n\\n```\\n\\nYou can also use pip to install a specific version of requests:\\n\\n```\\npip install requests==2.18.4\\n\\n```\\n\\nYou can also use pip to install a specific version of requests from a specific repository:\\n\\n```\\npip install requests[security]\\n\\n```\\n\\nYou can also use pip to install a specific version of requests from a specific repository and a specific branch:\\n\\n```\\npip install requests[security]==2.18.4\\n\\n```\\n\\nYou can also use pip to install a specific version of requests from a specific repository and a specific branch and a specific commit:\\n\\n```\\npip install requests[security]==2.18.4@sha256:1234567890abcdef01234567890abcdef',\n '```\\nwget -qO- -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t',\n 'You can use the following command to clone a repository:\\n\\n```\\ngit clone <url>\\n\\n```\\n\\nReplace <url> with the URL of the repository you want to clone.\\n\\nFor example:\\n\\n```\\ngit clone https://github.com/username/repo.git\\n\\n```\\n\\nThis will clone the repository named repo.git into the current directory.\\n\\nYou can also specify the branch to clone from. For example:\\n\\n```\\ngit clone https://github.com/username/repo.git master\\n\\n```\\n\\nThis will clone the repository named repo.git into the current directory, and create a branch named master.\\n\\nYou can also specify the remote repository to clone from. For example:\\n\\n```\\ngit clone https://github.com/username/repo.git origin/master\\n\\n```\\n\\nThis will clone the repository named repo.git into the current directory, and create a branch named',\n 'You can use the git branch -d command to delete a remote branch.\\n\\n```\\ngit branch -d branch_name\\n\\n```\\n\\nThis will delete the branch and all its commits.\\n\\n```\\ngit branch -d branch_name\\n\\n```\\n\\nThis will delete the branch and all its commits, but will not delete the branch itself.\\n\\n```\\ngit branch -D branch_name\\n\\n```\\n\\nThis will delete the branch and all its commits, but will also delete the branch itself.\\n\\n```\\ngit branch -D branch_name\\n\\n```\\n\\nThis will delete the branch and all its commits, but will also delete the branch itself.\\n\\n```\\ngit branch -D branch_name\\n\\n```\\n\\nThis will delete the branch and all its commits, but will also delete the branch itself.\\n\\n```\\ngit branch -D branch_name\\n\\n```\\n\\nThis will delete the branch and all']"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"len(finetuned_outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T18:26:29.202234Z","iopub.execute_input":"2025-06-18T18:26:29.202922Z","iopub.status.idle":"2025-06-18T18:26:29.207229Z","shell.execute_reply.started":"2025-06-18T18:26:29.202895Z","shell.execute_reply":"2025-06-18T18:26:29.206533Z"}},"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"7"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T18:20:18.756648Z","iopub.execute_input":"2025-06-18T18:20:18.757162Z","iopub.status.idle":"2025-06-18T18:20:18.762021Z","shell.execute_reply.started":"2025-06-18T18:20:18.757138Z","shell.execute_reply":"2025-06-18T18:20:18.761380Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"['git checkout -b branch_name\\n\\nThis will create a new branch called branch_name.\\n\\ngit checkout branch_name will switch to the branch you just created.\\n\\ngit checkout -b branch_name will create a new branch called branch_name.\\n\\ngit checkout branch_name will switch to the branch you just created.\\n\\ngit checkout branch_name will switch to the branch you just created.\\n\\ngit checkout branch_name will switch to the branch you just created.\\n\\ngit checkout branch_name will switch to the branch you just created.\\n\\ngit checkout branch_name will switch to the branch you just created.\\n\\ngit checkout branch_name will switch to the branch you just created.\\n\\ngit checkout branch_name will switch to the branch you just created.\\n\\ngit checkout branch_name will switch to the branch you just created.\\n\\ngit checkout branch_name will switch to the branch you just created.',\n 'You can use tar command to compress the folder reports into reports.tar.gz.\\n\\n```\\ntar -czvf reports.tar.gz reports\\n\\n```\\n\\nThis will create a compressed tar archive named reports.tar.gz containing all the files in the directory reports.\\n\\n```\\ntar -czvf reports.tar.gz reports\\n\\n```\\n\\nThis will create a compressed tar archive named reports.tar.gz containing all the files in the directory reports.\\n\\n```\\ntar -czvf reports.tar.gz reports\\n\\n```\\n\\nThis will create a compressed tar archive named reports.tar.gz containing all the files in the directory reports.\\n\\n```\\ntar -czvf reports.tar.gz reports\\n\\n```\\n\\nThis will create a compressed tar archive named reports.tar.gz containing all the files in the directory reports.\\n\\n```\\ntar -czvf reports.',\n \"You can use the os.walk() function to recursively list all files in a directory.\\n\\n```\\nimport os\\n\\ndef list_files(path):\\n    for root, dirs, files in os.walk(path):\\n        for file in files:\\n            print(os.path.join(root, file))\\n\\n\\n# Example usage\\nlist_files('path/to/directory')\\n\\n```\\n\\n\\nA: You can use os.walk() to recursively list all files in a directory.\\nimport os\\n\\ndef list_files(path):\\n    for root, dirs, files in os.walk(path):\\n        for file in files:\\n            print(os.path.join(root, file))\\n\\n\\n# Example usage\\nlist_files('path/to/directory')\\n\\n\\nA: You can use os.walk() to recursively list all files in a directory\",\n 'You can use pip to install requests.\\n\\n```\\npip install requests\\n\\n```\\n\\nYou can also use pip to install a specific version of requests.\\n\\n```\\npip install requests==2.18.4\\n\\n```\\n\\nYou can also use pip to install a specific version of requests from a specific repository.\\n\\n```\\npip install requests[security]\\n\\n```\\n\\nYou can also use pip to install a specific version of requests from a specific repository.\\n\\n```\\npip install requests[security]==2.18.4\\n\\n```\\n\\nYou can also use pip to install a specific version of requests from a specific repository.\\n\\n```\\npip install requests[security]==2.18.4\\n\\n```\\n\\nYou can also use pip to install a specific version of requests from a specific repository.\\n\\n```\\npip install requests[security]==2.18.4\\n\\n```',\n 'You can use the head command to get the first ten lines of a file.\\n\\n```\\nhead -n 10 output.log\\n\\n```\\n\\nThis will output the first ten lines of the file.\\n\\n```\\nhead -n 10 output.log\\n\\n```\\n\\nThis will output the first ten lines of the file.\\n\\n```\\nhead -n 10 output.log\\n\\n```\\n\\nThis will output the first ten lines of the file.\\n\\n```\\nhead -n 10 output.log\\n\\n```\\n\\nThis will output the first ten lines of the file.\\n\\n```\\nhead -n 10 output.log\\n\\n```\\n\\nThis will output the first ten lines of the file.\\n\\n```\\nhead -n 10 output.log\\n\\n```\\n\\nThis will output the first ten lines of the file.\\n\\n```\\nhead -n 10',\n 'You can use the following command to clone a repository:\\n\\n```\\ngit clone <repo_url>\\n\\n```\\n\\nReplace <repo_url> with the URL of the repository you want to clone.\\n\\nFor example:\\n\\n```\\ngit clone https://github.com/username/repo.git\\n\\n```\\n\\nThis will clone the repository named repo.git into the current directory.\\n\\nYou can also use the following command to clone a repository with a specific branch:\\n\\n```\\ngit clone <repo_url> <branch_name>\\n\\n```\\n\\nReplace <repo_url> with the URL of the repository you want to clone, and replace <branch_name> with the name of the branch you want to clone.\\n\\nFor example:\\n\\n```\\ngit clone https://github.com/username/repo.git master\\n\\n```\\n\\nThis will clone the repository named repo.git into the current directory',\n 'You can use the git branch -d command to delete a remote branch.\\n\\n```\\ngit branch -d branch_name\\n\\n```\\n\\nThis will delete the branch and all its commits.\\n\\n```\\ngit branch -d branch_name\\n\\n```\\n\\nThis will delete the branch and all its commits, including any local branches that are forked from it.\\n\\n```\\ngit branch -d branch_name --delete\\n\\n```\\n\\nThis will delete the branch and all its commits, including any local branches that are forked from it.\\n\\n```\\ngit branch -d branch_name --delete\\n\\n```\\n\\nThis will delete the branch and all its commits, including any local branches that are forked from it.\\n\\n```\\ngit branch -d branch_name --delete\\n\\n```\\n\\nThis will delete the branch and all its commits, including any local branches that are forked from it.\\n\\n```\\ngit']"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(len(base_outputs)):\n    print(\"_______________________________________\")\n    print(f\"{i+1}:𝓕𝓲𝓷𝓮 𝓣𝓾𝓷𝓮 𝓶𝓸𝓭𝓮𝓵 𝓞𝓾𝓽𝓾𝓹𝓽: {finetuned_outputs[i]}, \\n {i+1}:𝓑𝓪𝓼𝓮 𝓶𝓸𝓭𝓮𝓵 𝓞𝓾𝓽𝓾𝓹𝓽: {base_outputs[i]}\")\n    print(\"_______________________________________\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T18:32:46.431606Z","iopub.execute_input":"2025-06-18T18:32:46.432349Z","iopub.status.idle":"2025-06-18T18:32:46.436908Z","shell.execute_reply.started":"2025-06-18T18:32:46.432324Z","shell.execute_reply":"2025-06-18T18:32:46.436148Z"}},"outputs":[{"name":"stdout","text":"_______________________________________\n1:𝓕𝓲𝓷𝓮 𝓣𝓾𝓷𝓮 𝓶𝓸𝓭𝓮𝓵 𝓞𝓾𝓽𝓾𝓹𝓽: You can create a new branch by typing git branch -m branchname.\n\nThen you can switch to it by typing git checkout branchname.\n\nYou can also switch back to the previous branch by typing git checkout -b branchname.\n\nYou can also switch to a branch by typing git checkout branchname.\n\nYou can also switch back to the previous branch by typing git checkout -b branchname.\n\nYou can also switch to a branch by typing git checkout branchname.\n\nYou can also switch back to the previous branch by typing git checkout -b branchname.\n\nYou can also switch to a branch by typing git checkout branchname.\n\nYou can also switch back to the previous branch by typing git checkout -b branchname.\n\nYou can also switch to a branch by typing git checkout branchname.\n\nYou can also switch back to the previous branch by typing git checkout -b branchname.\n\nYou can also switch, \n 1:𝓑𝓪𝓼𝓮 𝓶𝓸𝓭𝓮𝓵 𝓞𝓾𝓽𝓾𝓹𝓽: git checkout -b branch_name\n\nThis will create a new branch called branch_name.\n\ngit checkout branch_name will switch to the branch you just created.\n\ngit checkout -b branch_name will create a new branch called branch_name.\n\ngit checkout branch_name will switch to the branch you just created.\n\ngit checkout branch_name will switch to the branch you just created.\n\ngit checkout branch_name will switch to the branch you just created.\n\ngit checkout branch_name will switch to the branch you just created.\n\ngit checkout branch_name will switch to the branch you just created.\n\ngit checkout branch_name will switch to the branch you just created.\n\ngit checkout branch_name will switch to the branch you just created.\n\ngit checkout branch_name will switch to the branch you just created.\n\ngit checkout branch_name will switch to the branch you just created.\n_______________________________________\n_______________________________________\n2:𝓕𝓲𝓷𝓮 𝓣𝓾𝓷𝓮 𝓶𝓸𝓭𝓮𝓵 𝓞𝓾𝓽𝓾𝓹𝓽: You can use tar command to compress the folder reports into reports.tar.gz.\n\n```\ntar -czvf reports.tar.gz reports\n\n```\n\nThis will create a tar archive named reports.tar.gz containing all the files in the reports directory.\n\n```\ntar -czvf reports.tar.gz reports\n\n```\n\nThis will create a tar archive named reports.tar.gz containing all the files in the reports directory.\n\n```\ntar -czvf reports.tar.gz reports\n\n```\n\nThis will create a tar archive named reports.tar.gz containing all the files in the reports directory.\n\n```\ntar -czvf reports.tar.gz reports\n\n```\n\nThis will create a tar archive named reports.tar.gz containing all the files in the reports directory.\n\n```\ntar -czvf reports.tar.gz reports\n\n```, \n 2:𝓑𝓪𝓼𝓮 𝓶𝓸𝓭𝓮𝓵 𝓞𝓾𝓽𝓾𝓹𝓽: You can use tar command to compress the folder reports into reports.tar.gz.\n\n```\ntar -czvf reports.tar.gz reports\n\n```\n\nThis will create a compressed tar archive named reports.tar.gz containing all the files in the directory reports.\n\n```\ntar -czvf reports.tar.gz reports\n\n```\n\nThis will create a compressed tar archive named reports.tar.gz containing all the files in the directory reports.\n\n```\ntar -czvf reports.tar.gz reports\n\n```\n\nThis will create a compressed tar archive named reports.tar.gz containing all the files in the directory reports.\n\n```\ntar -czvf reports.tar.gz reports\n\n```\n\nThis will create a compressed tar archive named reports.tar.gz containing all the files in the directory reports.\n\n```\ntar -czvf reports.\n_______________________________________\n_______________________________________\n3:𝓕𝓲𝓷𝓮 𝓣𝓾𝓷𝓮 𝓶𝓸𝓭𝓮𝓵 𝓞𝓾𝓽𝓾𝓹𝓽: You can use the os.listdir() function to list all the files in the current directory.\n\n```\nimport os\n\nfor root, dirs, files in os.walk('.'):\n    for file in files:\n        print(os.path.join(root, file))\n\n```\n\nThis will output:\n\n```\n.\n..\n\n```\n\n\nA: You can use os.walk() function to recursively list all files and directories in the current directory.\nimport os\n\nfor root, dirs, files in os.walk('.'):\n    for file in files:\n        print(os.path.join(root, file))\n\n\nA: You can use os.walk() function to recursively list all files and directories in the current directory.\nimport os\n\nfor root, dirs, files in os.walk('.'):\n    for file in files:, \n 3:𝓑𝓪𝓼𝓮 𝓶𝓸𝓭𝓮𝓵 𝓞𝓾𝓽𝓾𝓹𝓽: You can use the os.walk() function to recursively list all files in a directory.\n\n```\nimport os\n\ndef list_files(path):\n    for root, dirs, files in os.walk(path):\n        for file in files:\n            print(os.path.join(root, file))\n\n\n# Example usage\nlist_files('path/to/directory')\n\n```\n\n\nA: You can use os.walk() to recursively list all files in a directory.\nimport os\n\ndef list_files(path):\n    for root, dirs, files in os.walk(path):\n        for file in files:\n            print(os.path.join(root, file))\n\n\n# Example usage\nlist_files('path/to/directory')\n\n\nA: You can use os.walk() to recursively list all files in a directory\n_______________________________________\n_______________________________________\n4:𝓕𝓲𝓷𝓮 𝓣𝓾𝓷𝓮 𝓶𝓸𝓭𝓮𝓵 𝓞𝓾𝓽𝓾𝓹𝓽: You can use pip to install requests:\n\n```\npip install requests\n\n```\n\nYou can also use pip to install a specific version of requests:\n\n```\npip install requests==2.18.4\n\n```\n\nYou can also use pip to install a specific version of requests from a specific repository:\n\n```\npip install requests[security]\n\n```\n\nYou can also use pip to install a specific version of requests from a specific repository and a specific branch:\n\n```\npip install requests[security]==2.18.4\n\n```\n\nYou can also use pip to install a specific version of requests from a specific repository and a specific branch and a specific commit:\n\n```\npip install requests[security]==2.18.4@sha256:1234567890abcdef01234567890abcdef, \n 4:𝓑𝓪𝓼𝓮 𝓶𝓸𝓭𝓮𝓵 𝓞𝓾𝓽𝓾𝓹𝓽: You can use pip to install requests.\n\n```\npip install requests\n\n```\n\nYou can also use pip to install a specific version of requests.\n\n```\npip install requests==2.18.4\n\n```\n\nYou can also use pip to install a specific version of requests from a specific repository.\n\n```\npip install requests[security]\n\n```\n\nYou can also use pip to install a specific version of requests from a specific repository.\n\n```\npip install requests[security]==2.18.4\n\n```\n\nYou can also use pip to install a specific version of requests from a specific repository.\n\n```\npip install requests[security]==2.18.4\n\n```\n\nYou can also use pip to install a specific version of requests from a specific repository.\n\n```\npip install requests[security]==2.18.4\n\n```\n_______________________________________\n_______________________________________\n5:𝓕𝓲𝓷𝓮 𝓣𝓾𝓷𝓮 𝓶𝓸𝓭𝓮𝓵 𝓞𝓾𝓽𝓾𝓹𝓽: ```\nwget -qO- -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t 10 -t, \n 5:𝓑𝓪𝓼𝓮 𝓶𝓸𝓭𝓮𝓵 𝓞𝓾𝓽𝓾𝓹𝓽: You can use the head command to get the first ten lines of a file.\n\n```\nhead -n 10 output.log\n\n```\n\nThis will output the first ten lines of the file.\n\n```\nhead -n 10 output.log\n\n```\n\nThis will output the first ten lines of the file.\n\n```\nhead -n 10 output.log\n\n```\n\nThis will output the first ten lines of the file.\n\n```\nhead -n 10 output.log\n\n```\n\nThis will output the first ten lines of the file.\n\n```\nhead -n 10 output.log\n\n```\n\nThis will output the first ten lines of the file.\n\n```\nhead -n 10 output.log\n\n```\n\nThis will output the first ten lines of the file.\n\n```\nhead -n 10\n_______________________________________\n_______________________________________\n6:𝓕𝓲𝓷𝓮 𝓣𝓾𝓷𝓮 𝓶𝓸𝓭𝓮𝓵 𝓞𝓾𝓽𝓾𝓹𝓽: You can use the following command to clone a repository:\n\n```\ngit clone <url>\n\n```\n\nReplace <url> with the URL of the repository you want to clone.\n\nFor example:\n\n```\ngit clone https://github.com/username/repo.git\n\n```\n\nThis will clone the repository named repo.git into the current directory.\n\nYou can also specify the branch to clone from. For example:\n\n```\ngit clone https://github.com/username/repo.git master\n\n```\n\nThis will clone the repository named repo.git into the current directory, and create a branch named master.\n\nYou can also specify the remote repository to clone from. For example:\n\n```\ngit clone https://github.com/username/repo.git origin/master\n\n```\n\nThis will clone the repository named repo.git into the current directory, and create a branch named, \n 6:𝓑𝓪𝓼𝓮 𝓶𝓸𝓭𝓮𝓵 𝓞𝓾𝓽𝓾𝓹𝓽: You can use the following command to clone a repository:\n\n```\ngit clone <repo_url>\n\n```\n\nReplace <repo_url> with the URL of the repository you want to clone.\n\nFor example:\n\n```\ngit clone https://github.com/username/repo.git\n\n```\n\nThis will clone the repository named repo.git into the current directory.\n\nYou can also use the following command to clone a repository with a specific branch:\n\n```\ngit clone <repo_url> <branch_name>\n\n```\n\nReplace <repo_url> with the URL of the repository you want to clone, and replace <branch_name> with the name of the branch you want to clone.\n\nFor example:\n\n```\ngit clone https://github.com/username/repo.git master\n\n```\n\nThis will clone the repository named repo.git into the current directory\n_______________________________________\n_______________________________________\n7:𝓕𝓲𝓷𝓮 𝓣𝓾𝓷𝓮 𝓶𝓸𝓭𝓮𝓵 𝓞𝓾𝓽𝓾𝓹𝓽: You can use the git branch -d command to delete a remote branch.\n\n```\ngit branch -d branch_name\n\n```\n\nThis will delete the branch and all its commits.\n\n```\ngit branch -d branch_name\n\n```\n\nThis will delete the branch and all its commits, but will not delete the branch itself.\n\n```\ngit branch -D branch_name\n\n```\n\nThis will delete the branch and all its commits, but will also delete the branch itself.\n\n```\ngit branch -D branch_name\n\n```\n\nThis will delete the branch and all its commits, but will also delete the branch itself.\n\n```\ngit branch -D branch_name\n\n```\n\nThis will delete the branch and all its commits, but will also delete the branch itself.\n\n```\ngit branch -D branch_name\n\n```\n\nThis will delete the branch and all, \n 7:𝓑𝓪𝓼𝓮 𝓶𝓸𝓭𝓮𝓵 𝓞𝓾𝓽𝓾𝓹𝓽: You can use the git branch -d command to delete a remote branch.\n\n```\ngit branch -d branch_name\n\n```\n\nThis will delete the branch and all its commits.\n\n```\ngit branch -d branch_name\n\n```\n\nThis will delete the branch and all its commits, including any local branches that are forked from it.\n\n```\ngit branch -d branch_name --delete\n\n```\n\nThis will delete the branch and all its commits, including any local branches that are forked from it.\n\n```\ngit branch -d branch_name --delete\n\n```\n\nThis will delete the branch and all its commits, including any local branches that are forked from it.\n\n```\ngit branch -d branch_name --delete\n\n```\n\nThis will delete the branch and all its commits, including any local branches that are forked from it.\n\n```\ngit\n_______________________________________\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T18:33:38.650902Z","iopub.execute_input":"2025-06-18T18:33:38.651197Z","iopub.status.idle":"2025-06-18T18:33:43.558204Z","shell.execute_reply.started":"2025-06-18T18:33:38.651176Z","shell.execute_reply":"2025-06-18T18:33:43.557330Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.0)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge_score) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rouge_score) (2024.2.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=d3f65db6313320dac9e6290164344d9e4839953c3aa1fd520466096cb23e50b7\n  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"import evaluate\n\n# Initialize metrics\nbleu = evaluate.load(\"bleu\")\nrouge = evaluate.load(\"rouge\")\n\nreferences = [\n    \"git checkout -b branch_name\",\n    \"tar -czvf reports.tar.gz reports\",\n    \"import os\\nfor root, dirs, files in os.walk(path): print(file)\",\n    \"pip install requests\",\n    \"head -n 10 output.log\",\n    \"git clone https://github.com/username/repo.git\",\n    \"git push origin :branch_name\"\n]\n\n\nbleu_base = bleu.compute(\n    predictions=base_outputs,\n    references=[[ref] for ref in references]\n)\n\nbleu_finetuned = bleu.compute(\n    predictions=finetuned_outputs,\n    references=[[ref] for ref in references]\n)\n\n# ROUGE expects plain string lists\nrouge_base = rouge.compute(\n    predictions=base_outputs,\n    references=references,\n    rouge_types=[\"rougeL\"]\n)\n\nrouge_finetuned = rouge.compute(\n    predictions=finetuned_outputs,\n    references=references,\n    rouge_types=[\"rougeL\"]\n)\n\n# Print results\nprint(\"📏 BLEU Scores:\")\nprint(\"Base:      \", bleu_base[\"bleu\"])\nprint(\"Fine-tuned:\", bleu_finetuned[\"bleu\"])\n\nprint(\"\\n📏 ROUGE-L Scores:\")\nprint(\"Base:      \", rouge_base[\"rougeL\"])\nprint(\"Fine-tuned:\", rouge_finetuned[\"rougeL\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T18:38:12.878715Z","iopub.execute_input":"2025-06-18T18:38:12.879497Z","iopub.status.idle":"2025-06-18T18:38:14.009027Z","shell.execute_reply.started":"2025-06-18T18:38:12.879470Z","shell.execute_reply":"2025-06-18T18:38:14.008427Z"}},"outputs":[{"name":"stdout","text":"📏 BLEU Scores:\nBase:       0.39041142250733285\nFine-tuned: 0.3507226337696886\n\n📏 ROUGE-L Scores:\nBase:       0.9427639370845717\nFine-tuned: 0.8266996616935094\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}