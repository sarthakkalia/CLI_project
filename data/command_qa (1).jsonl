{"instruction": "How do I undo the most recent local commits in Git?", "output": "Undo a commit & redo\n\n```\n$ git commit -m \"Something terribly misguided\" # (0: Your Accident)\n$ git reset HEAD~                              # (1)\n# === If you just want to undo the commit, stop here! ===\n[ edit files as necessary ]                    # (2)\n$ git add .                                    # (3)\n$ git commit -c ORIG_HEAD                      # (4)\n\n```\n\n\ngit reset is the command responsible for the undo. It will undo your last commit while leaving your working tree (the state of your files on disk) untouched. You'll need to add them again before you can commit them again.\nMake corrections to working tree files.\ngit add anything that you want to include in your new commit.\nCommit the changes, reusing the old commit message. reset copied the old head to .git/ORIG_HEAD; commit with -c ORIG_HEAD will open an editor, which initially contains the log message from the old commit and allows you to edit it. If you do not need to edit the message, you could use the -C option.\n\nAlternatively, to edit the previous commit (or just its commit message), commit --amend will add changes within the current index to the previous commit.\nTo remove (not revert) a commit that has been pushed to the server, rewriting history with git push origin main --force[-with-lease] is necessary. It's almost always a bad idea to use --force; prefer --force-with-lease instead, and as noted in the git manual:\n\nYou should understand the implications of rewriting history if you amend a commit that has already been published.\n\n\nFurther Reading\nYou can use git reflog to determine the SHA-1 for the commit to which you wish to revert. Once you have this value, use the sequence of commands as explained above.\n\nHEAD~ is the same as HEAD~1. The article What is the HEAD in git? is helpful if you want to uncommit multiple commits."}
{"instruction": "How do I delete a Git branch locally and remotely?", "output": "Executive Summary\n\n```\ngit push -d <remote_name> <branchname>   # Delete remote\ngit branch -d <branchname>               # Delete local\n\n```\n\nNote: In most cases, <remote_name> will be origin.\nDelete Local Branch\nTo delete the local branch, use one of the following:\n\n```\ngit branch -d <branch_name>\ngit branch -D <branch_name>\n\n```\n\n\nThe -d option is an alias for --delete, which only deletes the branch if it has already been fully merged in its upstream branch.\nThe -D option is an alias for --delete --force, which deletes the branch \"irrespective of its merged status.\" [Source: man git-branch]\nAs of Git v2.3, git branch -d (delete) learned to honor the -f (force) flag.\nYou will receive an error if you try to delete the currently selected branch.\n\nDelete Remote Branch\nAs of Git v1.7.0, you can delete a remote branch using\n\n```\n$ git push <remote_name> --delete <branch_name>\n\n```\n\nwhich might be easier to remember than\n\n```\n$ git push <remote_name> :<branch_name>\n\n```\n\nwhich was added in Git v1.5.0 \"to delete a remote branch or a tag.\"\nStarting with Git v2.8.0, you can also use git push with the -d option as an alias for --delete. Therefore, the version of Git you have installed will dictate whether you need to use the easier or harder syntax.\nDelete Remote Branch [Original Answer from 5-Jan-2010]\nFrom Chapter 3 of Pro Git by Scott Chacon:\n\nDeleting Remote Branches\nSuppose you‚Äôre done with a remote branch ‚Äî say, you and your collaborators are finished with a feature and have merged it into your remote‚Äôs main branch (or whatever branch your stable code-line is in). You can delete a remote branch using the rather obtuse syntax git push [remotename] :[branch]. If you want to delete your serverfix branch from the server, you run the following:\n\n```\n$ git push origin :serverfix\nTo git@github.com:schacon/simplegit.git\n - [deleted]         serverfix\n\n```\n\nBoom. No more branches on your server. You may want to dog-ear this page, because you‚Äôll need that command, and you‚Äôll likely forget the syntax. A way to remember this command is by recalling the git push [remotename] [localbranch]:[remotebranch] syntax that we went over a bit earlier. If you leave off the [localbranch] portion, then you‚Äôre basically saying, ‚ÄúTake nothing on my side and make it be [remotebranch].‚Äù\n\nI ran git push origin :bugfix, and it worked beautifully. Scott Chacon was right‚ÄîI will want to dog-ear that page (or virtually dog ear-by answering this on Stack Overflow).\nFetch changes\nFinally, execute the following on other machines to propagate changes:\n\n```\n# Fetch changes from all remotes and locally delete \n# remote deleted branches/tags etc\n# --prune will do the job :-;\ngit fetch --all --prune\n\n```"}
{"instruction": "What is the difference between &#39;git pull&#39; and &#39;git fetch&#39;?", "output": "In the simplest terms, git pull does a git fetch followed by a git merge.\n\ngit fetch updates your remote-tracking branches under refs/remotes/<remote>/. This operation is safe to run at any time since it never changes any of your local branches under refs/heads.\ngit pull brings a local branch up-to-date with its remote version, while also updating your other remote-tracking branches.\nFrom the Git documentation for git pull:\n\ngit pull runs git fetch with the given parameters and then depending on configuration options or command line flags, will call either git rebase or git merge to reconcile diverging branches."}
{"instruction": "How can I rename a local Git branch?", "output": "To rename the current branch:\n\n```\ngit branch -m <newname>\n\n```\n\nTo rename a branch while pointed to any branch:\n\n```\ngit branch -m <oldname> <newname>\n\n```\n\n-m is short for --move.\n\nTo push the  local branch and reset the upstream branch:\n\n```\ngit push origin -u <newname>\n\n```\n\nTo delete the  remote branch:\n\n```\ngit push origin --delete <oldname>\n\n```\n\n\nTo create a git rename alias:\n\n```\ngit config --global alias.rename 'branch -m'\n\n```\n\n\nOn Windows or another case-insensitive filesystem, use -M if there are only capitalization changes in the name. Otherwise, Git will throw a \"branch already exists\" error.\n\n```\ngit branch -M <newname>\n\n```"}
{"instruction": "How do I undo &#39;git add&#39; before commit?", "output": "To unstage a specific file\n\n```\ngit reset <file>\n\n```\n\nThat will remove the file from the current index (the \"about to be committed\" list) without changing anything else.\nTo unstage all files from the current change set:\n\n```\ngit reset\n\n```\n\n\nIn old versions of Git, the above commands are equivalent to git reset HEAD <file> and git reset HEAD respectively, and will fail if HEAD is undefined (because you haven't yet made any commits in your repository) or ambiguous (because you created a branch called HEAD, which is a stupid thing that you shouldn't do). This was changed in Git 1.8.2, though, so in modern versions of Git you can use the commands above even prior to making your first commit:\n\n\"git reset\" (without options or parameters) used to error out when\nyou do not have any commits in your history, but it now gives you\nan empty index (to match non-existent commit you are not even on).\n\nDocumentation: git reset"}
{"instruction": "How do I get the directory where a Bash script is located from within the script itself?", "output": "```\n#!/usr/bin/env bash\n\nSCRIPT_DIR=$( cd -- \"$( dirname -- \"${BASH_SOURCE[0]}\" )\" &> /dev/null && pwd )\n\n```\n\nis a useful one-liner which will give you the full directory name of the script no matter where it is being called from.\nIt will work as long as the last component of the path used to find the script is not a symlink (directory links are OK).  If you also want to resolve any links to the script itself, you need a multi-line solution:\n\n```\n#!/usr/bin/env bash\n\nget_script_dir()\n{\n    local SOURCE_PATH=\"${BASH_SOURCE[0]}\"\n    local SYMLINK_DIR\n    local SCRIPT_DIR\n    # Resolve symlinks recursively\n    while [ -L \"$SOURCE_PATH\" ]; do\n        # Get symlink directory\n        SYMLINK_DIR=\"$( cd -P \"$( dirname \"$SOURCE_PATH\" )\" >/dev/null 2>&1 && pwd )\"\n        # Resolve symlink target (relative or absolute)\n        SOURCE_PATH=\"$(readlink \"$SOURCE_PATH\")\"\n        # Check if candidate path is relative or absolute\n        if [[ $SOURCE_PATH != /* ]]; then\n            # Candidate path is relative, resolve to full path\n            SOURCE_PATH=$SYMLINK_DIR/$SOURCE_PATH\n        fi\n    done\n    # Get final script directory path from fully resolved source path\n    SCRIPT_DIR=\"$(cd -P \"$( dirname \"$SOURCE_PATH\" )\" >/dev/null 2>&1 && pwd)\"\n    echo \"$SCRIPT_DIR\"\n}\n\necho \"get_script_dir: $(get_script_dir)\"\n\n```\n\nThis last one will work with any combination of aliases, source, bash -c, symlinks, etc.\nBeware: if you cd to a different directory before running this snippet, the result may be incorrect!\nAlso, watch out for $CDPATH gotchas, and stderr output side effects if the user has smartly overridden cd to redirect output to stderr instead (including escape sequences, such as when calling update_terminal_cwd >&2 on Mac). Adding >/dev/null 2>&1 at the end of your cd command will take care of both possibilities.\nTo understand how it works, try running this more verbose form:\n\n```\n#!/usr/bin/env bash\n\nSOURCE=${BASH_SOURCE[0]}\nwhile [ -L \"$SOURCE\" ]; do # resolve $SOURCE until the file is no longer a symlink\n  TARGET=$(readlink \"$SOURCE\")\n  if [[ $TARGET == /* ]]; then\n    echo \"SOURCE '$SOURCE' is an absolute symlink to '$TARGET'\"\n    SOURCE=$TARGET\n  else\n    DIR=$( dirname \"$SOURCE\" )\n    echo \"SOURCE '$SOURCE' is a relative symlink to '$TARGET' (relative to '$DIR')\"\n    SOURCE=$DIR/$TARGET # if $SOURCE was a relative symlink, we need to resolve it relative to the path where the symlink file was located\n  fi\ndone\necho \"SOURCE is '$SOURCE'\"\nRDIR=$( dirname \"$SOURCE\" )\nDIR=$( cd -P \"$( dirname \"$SOURCE\" )\" >/dev/null 2>&1 && pwd )\nif [ \"$DIR\" != \"$RDIR\" ]; then\n  echo \"DIR '$RDIR' resolves to '$DIR'\"\nfi\necho \"DIR is '$DIR'\"\n\n```\n\nAnd it will print something like:\n\n```\nSOURCE './scriptdir.sh' is a relative symlink to 'sym2/scriptdir.sh' (relative to '.')\nSOURCE is './sym2/scriptdir.sh'\nDIR './sym2' resolves to '/home/ubuntu/dotfiles/fo fo/real/real1/real2'\nDIR is '/home/ubuntu/dotfiles/fo fo/real/real1/real2'\n\n```"}
{"instruction": "How do I check if a directory exists or not in a Bash shell script?", "output": "To check if a directory exists:\n\n```\nif [ -d \"$DIRECTORY\" ]; then\n  echo \"$DIRECTORY does exist.\"\nfi\n\n```\n\nTo check if a directory does not exist:\n\n```\nif [ ! -d \"$DIRECTORY\" ]; then\n  echo \"$DIRECTORY does not exist.\"\nfi\n\n```\n\n\nHowever, as Jon Ericson points out, subsequent commands may not work as intended if you do not take into account that a symbolic link to a directory will also pass this check.\nE.g. running this:\n\n```\nln -s \"$ACTUAL_DIR\" \"$SYMLINK\"\nif [ -d \"$SYMLINK\" ]; then \n  rmdir \"$SYMLINK\" \nfi\n\n```\n\nWill produce the error message:\n\n```\nrmdir: failed to remove `symlink': Not a directory\n\n```\n\nSo symbolic links may have to be treated differently, if subsequent commands expect directories:\n\n```\nif [ -d \"$LINK_OR_DIR\" ]; then \n  if [ -L \"$LINK_OR_DIR\" ]; then\n    # It is a symlink!\n    # Symbolic link specific commands go here.\n    rm \"$LINK_OR_DIR\"\n  else\n    # It's a directory!\n    # Directory command goes here.\n    rmdir \"$LINK_OR_DIR\"\n  fi\nfi\n\n```\n\n\nTake particular note of the double-quotes used to wrap the variables. The reason for this is explained by 8jean in another answer.\nIf the variables contain spaces or other unusual characters it will probably cause the script to fail."}
{"instruction": "How do I tell if a file does not exist in Bash?", "output": "The test command (written as [ here) has a \"not\" logical operator, ! (exclamation mark):\n\n```\nif [ ! -f /tmp/foo.txt ]; then\n    echo \"File not found!\"\nfi\n\n```"}
{"instruction": "Echo newline in Bash prints literal \\n", "output": "Use printf instead:\n\n```\nprintf \"hello\\nworld\\n\"\n\n```\n\nprintf behaves more consistently across different environments than echo."}
{"instruction": "How to check if a string contains a substring in Bash", "output": "You can use Marcus's answer (* wildcards) outside a case statement, too, if you use double brackets:\n\n```\nstring='My long string'\nif [[ $string == *\"My long\"* ]]; then\n  echo \"It's there!\"\nfi\n\n```\n\nNote that spaces in the needle string need to be placed between double quotes, and the * wildcards should be outside. Also note that a simple comparison operator is used (i.e. ==), not the regex operator =~."}
{"instruction": "Shell command to tar directory excluding certain files/folders", "output": "You can have multiple exclude options for tar so\n\n```\n$ tar --exclude='./folder' --exclude='./upload/folder2' -zcvf /backup/filename.tgz .\n\n```\n\netc will work. Make sure to put --exclude before the source and destination items."}
{"instruction": "How do I tar a directory of files and folders without including the directory itself?", "output": "```\ncd my_directory/ && tar -zcvf ../my_dir.tgz . && cd - \n\n```\n\nshould do the job in one line. It works well for hidden files as well. \"*\" doesn't expand hidden files by path name expansion at least in bash. Below is my experiment:\n\n```\n$ mkdir my_directory\n$ touch my_directory/file1\n$ touch my_directory/file2\n$ touch my_directory/.hiddenfile1\n$ touch my_directory/.hiddenfile2\n$ cd my_directory/ && tar -zcvf ../my_dir.tgz . && cd ..\n./\n./file1\n./file2\n./.hiddenfile1\n./.hiddenfile2\n$ tar ztf my_dir.tgz\n./\n./file1\n./file2\n./.hiddenfile1\n./.hiddenfile2\n\n```"}
{"instruction": "Tar a directory, but don&#39;t store full absolute paths in the archive", "output": "```\ntar -cjf site1.tar.bz2 -C /var/www/site1 .\n\n```\n\nIn the above example, tar will change to directory /var/www/site1 before doing its thing because the option -C /var/www/site1 was given.\nFrom man tar:\n\n```\nOTHER OPTIONS\n\n  -C, --directory DIR\n       change to directory DIR\n\n```"}
{"instruction": "Utilizing multi core for tar+gzip/bzip compression/decompression", "output": "You can use pigz instead of gzip, which does gzip compression on multiple cores.  Instead of using the -z option, you would pipe it through pigz:\n\n```\ntar cf - paths-to-archive | pigz > archive.tar.gz\n\n```\n\nBy default, pigz uses the number of available cores, or eight if it could not query that.  You can ask for more with -p n, e.g. -p 32.  pigz has the same options as gzip, so you can request better compression with -9.  E.g.\n\n```\ntar cf - paths-to-archive | pigz -9 -p 32 > archive.tar.gz\n\n```"}
{"instruction": "How to uncompress a tar.gz in another directory", "output": "```\ngzip -dc archive.tar.gz | tar -xf - -C /destination\n\n```\n\nor, with GNU tar\n\n```\ntar xzf archive.tar.gz -C /destination\n\n```"}
{"instruction": "How are zlib, gzip and zip related? What do they have in common and how are they different?", "output": "Short form:\n.zip is an archive format using, usually, the Deflate compression method.  The .gz gzip format is for single files, also using the Deflate compression method.  Often gzip is used in combination with tar to make a compressed archive format, .tar.gz.  The zlib library provides Deflate compression and decompression code for use by zip, gzip, png (which uses the zlib wrapper on deflate data), and many other applications.\nLong form:\nThe ZIP format was developed by Phil Katz as an open format with an open specification, where his implementation, PKZIP, was shareware.  It is an archive format that stores files and their directory structure, where each file is individually compressed.  The file type is .zip.  The files, as well as the directory structure, can optionally be encrypted.\nThe ZIP format supports several compression methods:\n\n```\n    0 - The file is stored (no compression)\n    1 - The file is Shrunk\n    2 - The file is Reduced with compression factor 1\n    3 - The file is Reduced with compression factor 2\n    4 - The file is Reduced with compression factor 3\n    5 - The file is Reduced with compression factor 4\n    6 - The file is Imploded\n    7 - Reserved for Tokenizing compression algorithm\n    8 - The file is Deflated\n    9 - Enhanced Deflating using Deflate64(tm)\n   10 - PKWARE Data Compression Library Imploding (old IBM TERSE)\n   11 - Reserved by PKWARE\n   12 - File is compressed using BZIP2 algorithm\n   13 - Reserved by PKWARE\n   14 - LZMA\n   15 - Reserved by PKWARE\n   16 - IBM z/OS CMPSC Compression\n   17 - Reserved by PKWARE\n   18 - File is compressed using IBM TERSE (new)\n   19 - IBM LZ77 z Architecture \n   20 - deprecated (use method 93 for zstd)\n   93 - Zstandard (zstd) Compression \n   94 - MP3 Compression \n   95 - XZ Compression \n   96 - JPEG variant\n   97 - WavPack compressed data\n   98 - PPMd version I, Rev 1\n   99 - AE-x encryption marker (see APPENDIX E)\n\n```\n\nMethods 1 to 7 are historical and are not in use.  Methods 9 through 98 are relatively recent additions and are in varying, small amounts of use.  The only method in truly widespread use in the ZIP format is method 8, Deflate, and to some smaller extent method 0, which is no compression at all.  Virtually every .zip file that you will come across in the wild will use exclusively methods 8 and 0, likely just method 8.  (Method 8 also has a means to effectively store the data with no compression and relatively little expansion, and Method 0 cannot be streamed whereas Method 8 can be.)\nThe ISO/IEC 21320-1:2015 standard for file containers is a restricted zip format, such as used in Java archive files (.jar), Office Open XML files (Microsoft Office .docx, .xlsx, .pptx), Office Document Format files (.odt, .ods, .odp), and EPUB files (.epub). That standard limits the compression methods to 0 and 8, as well as other constraints such as no encryption or signatures.\nAround 1990, the Info-ZIP group wrote portable, free, open-source implementations of zip and unzip utilities, supporting compression with the Deflate format, and decompression of that and the earlier formats.  This greatly expanded the use of the .zip format.\nIn the early '90s, the gzip format was developed as a replacement for the Unix compress utility, derived from the Deflate code in the Info-ZIP utilities.  Unix compress was designed to compress a single file or stream, appending a .Z to the file name.  compress uses the LZW compression algorithm, which at the time was under patent and its free use was in dispute by the patent holders.  Though some specific implementations of Deflate were patented by Phil Katz, the format was not, and so it was possible to write a Deflate implementation that did not infringe on any patents.  That implementation has not been so challenged in the last 20+ years.  The Unix gzip utility was intended as a drop-in replacement for compress, and in fact is able to decompress compress-compressed data (assuming that you were able to parse that sentence).  gzip appends a .gz to the file name.  gzip uses the Deflate compressed data format, which compresses quite a bit better than Unix compress, has very fast decompression, and adds a CRC-32 as an integrity check for the data.  The header format also permits the storage of more information than the compress format allowed, such as the original file name and the file modification time.\nThough compress only compresses a single file, it was common to use the tar utility to create an archive of files, their attributes, and their directory structure into a single .tar file, and then compress it with compress to make a .tar.Z file.  In fact, the tar utility had and still has the option to do the compression at the same time, instead of having to pipe the output of tar to compress.  This all carried forward to the gzip format, and tar has an option to compress directly to the .tar.gz format.  The tar.gz format compresses better than the .zip approach, since the compression of a .tar can take advantage of redundancy across files, especially many small files.  .tar.gz is the most common archive format in use on Unix due to its very high portability, but there are more effective compression methods in use as well, so you will often see .tar.bz2 and .tar.xz archives.\nUnlike .tar, .zip has a central directory at the end, which provides a list of the contents. That and the separate compression provides random access to the individual entries in a .zip file. A .tar file would have to be decompressed and scanned from start to end in order to build a directory, which is how a .tar file is listed.\nShortly after the introduction of gzip, around the mid-1990s, the same patent dispute called into question the free use of the .gif image format, very widely used on bulletin boards and the World Wide Web (a new thing at the time).  So a small group created the PNG losslessly compressed image format, with file type .png, to replace .gif.  That format also uses the Deflate format for compression, which is applied after filters on the image data expose more of the redundancy.  In order to promote widespread usage of the PNG format, two free code libraries were created.  libpng and zlib.  libpng handled all of the features of the PNG format, and zlib provided the compression and decompression code for use by libpng, as well as for other applications.  zlib was adapted from the gzip code.\nAll of the mentioned patents have since expired.\nThe zlib library supports Deflate compression and decompression, and three kinds of wrapping around the deflate streams.  Those are no wrapping at all (\"raw\" deflate), zlib wrapping, which is used in the PNG format data blocks, and gzip wrapping, to provide gzip routines for the programmer.  The main difference between zlib and gzip wrapping is that the zlib wrapping is more compact, six bytes vs. a minimum of 18 bytes for gzip, and the integrity check, Adler-32, runs faster than the CRC-32 that gzip uses.  Raw deflate is used by programs that read and write the .zip format, which is another format that wraps around deflate compressed data.\nzlib is now in wide use for data transmission and storage.  For example, most HTTP transactions by servers and browsers compress and decompress the data using zlib, specifically HTTP header Content-Encoding: deflate means deflate compression method wrapped inside the zlib data format.\nDifferent implementations of deflate can result in different compressed output for the same input data, as evidenced by the existence of selectable compression levels that allow trading off compression effectiveness for CPU time. zlib and PKZIP are not the only implementations of deflate compression and decompression. Both the 7-Zip archiving utility and Google's zopfli library have the ability to use much more CPU time than zlib in order to squeeze out the last few bits possible when using the deflate format, reducing compressed sizes by a few percent as compared to zlib's highest compression level. The pigz utility, a parallel implementation of gzip, includes the option to use zlib (compression levels 1-9) or zopfli (compression level 11), and somewhat mitigates the time impact of using zopfli by splitting the compression of large files over multiple processors and cores."}
{"instruction": "How do I tar a directory of files and folders without including the directory itself?", "output": "```\ncd my_directory/ && tar -zcvf ../my_dir.tgz . && cd - \n\n```\n\nshould do the job in one line. It works well for hidden files as well. \"*\" doesn't expand hidden files by path name expansion at least in bash. Below is my experiment:\n\n```\n$ mkdir my_directory\n$ touch my_directory/file1\n$ touch my_directory/file2\n$ touch my_directory/.hiddenfile1\n$ touch my_directory/.hiddenfile2\n$ cd my_directory/ && tar -zcvf ../my_dir.tgz . && cd ..\n./\n./file1\n./file2\n./.hiddenfile1\n./.hiddenfile2\n$ tar ztf my_dir.tgz\n./\n./file1\n./file2\n./.hiddenfile1\n./.hiddenfile2\n\n```"}
{"instruction": "Utilizing multi core for tar+gzip/bzip compression/decompression", "output": "You can use pigz instead of gzip, which does gzip compression on multiple cores.  Instead of using the -z option, you would pipe it through pigz:\n\n```\ntar cf - paths-to-archive | pigz > archive.tar.gz\n\n```\n\nBy default, pigz uses the number of available cores, or eight if it could not query that.  You can ask for more with -p n, e.g. -p 32.  pigz has the same options as gzip, so you can request better compression with -9.  E.g.\n\n```\ntar cf - paths-to-archive | pigz -9 -p 32 > archive.tar.gz\n\n```"}
{"instruction": "How to uncompress a tar.gz in another directory", "output": "```\ngzip -dc archive.tar.gz | tar -xf - -C /destination\n\n```\n\nor, with GNU tar\n\n```\ntar xzf archive.tar.gz -C /destination\n\n```"}
{"instruction": "Command Line Tool - Error - xcrun: error: unable to find utility &quot;xcodebuild&quot;, not a developer tool or in PATH", "output": "I solved that problem by setting the Command Line Tools in Xcode. Go to:\n\nXcode > Preferences > Locations\n\nAnd select the command line tool from the dropdown. If you have only one version of Xcode installed, there should be only one option. If you have several versions of Xcode, then you must choose the one you need.\nUpdate (added image for reference)"}
{"instruction": "Find all files containing a specific text (string) on Linux?", "output": "Do the following:\n\n```\ngrep -rnw '/path/to/somewhere/' -e 'pattern'\n\n```\n\n\n-r or -R is recursive,\n-n is line number, and\n-w stands for match the whole word.\n-l (lower-case L) can be added to just give the file name of matching files.\n-e is the pattern used during the search\n\nAlong with these, --exclude, --include, --exclude-dir flags could be used for efficient searching:\n\nThis will only search through those files which have .c or .h extensions:\n\n```\ngrep --include=\\*.{c,h} -rnw '/path/to/somewhere/' -e \"pattern\"\n\n```\n\n\nThis will exclude searching all the files ending with .o extension:\n\n```\ngrep --exclude=\\*.o -rnw '/path/to/somewhere/' -e \"pattern\"\n\n```\n\n\nFor directories it's possible to exclude one or more directories using the --exclude-dir parameter. For example, this will exclude the dirs dir1/, dir2/ and all of them matching *.dst/:\n\n```\ngrep --exclude-dir={dir1,dir2,*.dst} -rnw '/path/to/search/' -e \"pattern\"\n\n```\n\n\n\nThis works very well for me, to achieve almost the same purpose like yours.\nFor more options, see man grep."}
{"instruction": "grep: show lines surrounding each match", "output": "For BSD or GNU grep you can use -B num to set how many lines before the match and -A num for the number of lines after the match.\n\n```\ngrep -B 3 -A 2 foo README.txt\n\n```\n\nIf you want the same number of lines before and after you can use -C num.\n\n```\ngrep -C 3 foo README.txt\n\n```\n\nThis will show 3 lines before and 3 lines after."}
{"instruction": "How do I recursively grep all directories and subdirectories?", "output": "```\ngrep -r \"texthere\" .\n\n```\n\nThe first parameter represents the regular expression to search for, while the second one represents the directory that should be searched. In this case, . means the current directory.\nNote: This works for GNU grep, and on some platforms like Solaris you must specifically use GNU grep as opposed to legacy implementation.  For Solaris this is the ggrep command."}
{"instruction": "How to grep (search through) committed code in the Git history", "output": "You should use the pickaxe (-S) option of git log.\nTo search for Foo:\n\n```\ngit log -SFoo -- path_containing_change\ngit log -SFoo --since=2009.1.1 --until=2010.1.1 -- path_containing_change\n\n```\n\nSee Git history - find lost line by keyword for more.\n-S (named pickaxe) comes originally from a git diff option (Git v0.99, May 2005).\nThen -S (pickaxe) was ported to git log in May 2006 with Git 1.4.0-rc1.\n\nAs Jakub Narƒôbski commented:\n\nthis looks for differences that introduce or remove an instance of <string>.\nIt usually means \"revisions where you added or removed line with 'Foo'\".\n\nthe --pickaxe-regex option allows you to use extended POSIX regex instead of searching for a string.\nExample (from git log): git log -S\"frotz\\(nitfol\" --pickaxe-regex\n\n\n\nAs Rob commented, this search is case-sensitive - he opened a follow-up question on how to search case-insensitive.\n\nHi Angel notes in the comments:\n\nExecuting a git log -G<regexp> --branches --all (the -G is same as -S but for regexes) does same thing as the accepted one (git grep <regexp> $(git rev-list --all)), but it soooo much faster!\nThe accepted answer was still searching for text after ‚âà10 minutes of me running it, whereas this one gives results after ‚âà4 seconds ü§∑‚Äç‚ôÇÔ∏è.\nThe output here is more useful as well"}
{"instruction": "How can I use grep to show just filenames on Linux?", "output": "The standard option grep -l (that is a lowercase L) could do this.\nFrom the Unix standard:\n\n```\n-l\n    (The letter ell.) Write only the names of files containing selected\n    lines to standard output. Pathnames are written once per file searched.\n    If the standard input is searched, a pathname of (standard input) will\n    be written, in the POSIX locale. In other locales, standard input may be\n    replaced by something more appropriate in those locales.\n\n```\n\nYou also do not need -H in this case."}
{"instruction": "No module named pip in venv but pip installed", "output": "I ran into the exact same problem after installing Python 3.13 on WSL. Suddenly, all my existing virtual environments (created with Python 3.12) broke in VSCode. I was getting the \"Invalid Python interpreter\" error, Pylance couldn't resolve any imports, and pip appeared to be missing‚Äîeven though I could see it in the venv/bin folder.\nHere‚Äôs what fixed it for me:\nFirst, check what your system python3 now points to:\n\n```\npython3 --version\nwhich python3\n\n```\n\nIn my case, it was now Python 3.13, which explains why stuff started breaking. Your virtual environment still points to the Python 3.12 binary internally, but VSCode (and maybe even pip) is trying to use 3.13 instead.\nYou can confirm that by looking at the pyvenv.cfg file inside your venv:\n\n```\ncat venv/pyvenv.cfg\n\n```\n\nYou should see something like:\n\n```\nhome = /usr/bin/python3.12\n\n```\n\nIf that's the case, then you just need to tell VSCode to use that exact interpreter. Open the command palette (Ctrl+Shift+P) in VSCode, choose ‚ÄúPython: Select Interpreter‚Äù, and manually select the path to your virtualenv‚Äôs Python binary:\n\n```\n/path/to/your/venv/bin/python\n\n```\n\nAlso, double-check the shebang in your pip script:\n\n```\nhead -n 1 venv/bin/pip\n\n```\n\nIf it says #!/usr/bin/python3, that might now point to Python 3.13, which breaks the venv. You can fix this by rebuilding the venv with the correct Python version:\n\n```\npython3.12 -m venv --upgrade-deps venv\n\n```\n\nOr, if that doesn‚Äôt work cleanly:\n\n```\nrm -rf venv\npython3.12 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n\n```\n\nAnd yeah, ensurepip being disabled for system Python is normal on Ubuntu. Just make sure you have the necessary packages installed:\n\n```\nsudo apt install python3.12-venv python3.12-distutils\n\n```\n\nOnce I manually selected the right interpreter in VSCode and fixed the pip shebang, everything worked again‚ÄîIntelliSense, linting, imports, etc. Hope that helps."}
{"instruction": "python venv install skips component file &quot;pointer.png&quot;", "output": "It's because it's not present in tool.setuptools.package-data in pyproject.toml file.\n\n```\n[tool.setuptools.package-data]\n\"*\" = [\"*.fs\", \"*.vs\", \"*.inc\", \"*.gif\"]\n\n```\n\nWith the previous configuration, you add all this extensions in your package as you can see in the next screenshot (content of the package uploaded on pypi).\n\nSo adding the png extension should work:\n\n```\n[tool.setuptools.package-data]\n\"*\" = [\"*.fs\", \"*.vs\", \"*.inc\", \"*.gif\", \"*.png\"]\n\n```"}
{"instruction": "How is Docker different from a virtual machine?", "output": "Docker originally used LinuX Containers (LXC), but later switched to runC (formerly known as libcontainer), which runs in the same operating system as its host. This allows it to share a lot of the host operating system resources. Also, it uses a layered filesystem (AuFS) and manages networking.\nAuFS is a layered file system, so you can have a read only part and a write part which are merged together. One could have the common parts of the operating system as read only (and shared amongst all of your containers) and then give each container its own mount for writing.\nSo, let's say you have a 1¬†GB container image; if you wanted to use a full VM, you would need to have 1¬†GB x number of VMs you want. With Docker and AuFS you can share the bulk of the 1¬†GB between all the containers and if you have 1000 containers you still might only have a little over 1¬†GB of space for the containers OS (assuming they are all running the same OS image).\nA full virtualized system gets its own set of resources allocated to it, and does minimal sharing. You get more isolation, but it is much heavier (requires more resources). With Docker you get less isolation, but the containers are lightweight (require fewer resources). So you could easily run thousands of containers on a host, and it won't even blink. Try doing that with Xen, and unless you have a really big host, I don't think it is possible.\nA full virtualized system usually takes minutes to start, whereas Docker/LXC/runC containers take seconds, and often even less than a second.\nThere are pros and cons for each type of virtualized system. If you want full isolation with guaranteed resources, a full VM is the way to go. If you just want to isolate processes from each other and want to run a ton of them on a reasonably sized host, then Docker/LXC/runC seems to be the way to go.\nFor more information, check out this set of blog posts which do a good job of explaining how LXC works.\n\nWhy is deploying software to a docker image (if that's the right term) easier than simply deploying to a consistent production environment?\n\nDeploying a consistent production environment is easier said than done. Even if you use tools like Chef and Puppet, there are always OS updates and other things that change between hosts and environments.\nDocker gives you the ability to snapshot the OS into a shared image, and makes it easy to deploy on other Docker hosts. Locally, dev, qa, prod, etc.: all the same image. Sure you can do this with other tools, but not nearly as easily or fast.\nThis is great for testing; let's say you have thousands of tests that need to connect to a database, and each test needs a pristine copy of the database and will make changes to the data. The classic approach to this is to reset the database after every test either with custom code or with tools like Flyway - this can be very time-consuming and means that tests must be run serially. However, with Docker you could create an image of your database and run up one instance per test, and then run all the tests in parallel since you know they will all be running against the same snapshot of the database. Since the tests are running in parallel and in Docker containers they could run all on the same box at the same time and should finish much faster. Try doing that with a full VM.\nFrom comments...\n\nInteresting! I suppose I'm still confused by the notion of \"snapshot[ting] the OS\". How does one do that without, well, making an image of the OS?\n\nWell, let's see if I can explain. You start with a base image, and then make your changes, and commit those changes using docker, and it creates an image. This image contains only the differences from the base. When you want to run your image, you also need the base, and it layers your image on top of the base using a layered file system: as mentioned above, Docker uses AuFS. AuFS merges the different layers together and you get what you want; you just need to run it. You can keep adding more and more images (layers) and it will continue to only save the diffs. Since Docker typically builds on top of ready-made images from a registry, you rarely have to \"snapshot\" the whole OS yourself."}
{"instruction": "From inside of a Docker container, how do I connect to the localhost of the machine?", "output": "If you are using Docker-for-mac or Docker-for-Windows 18.03+, connect to your MySQL service using the host host.docker.internal (instead of the 127.0.0.1 in your connection string).\nIf you are using Docker-for-Linux 20.10.0+, you can also use the host host.docker.internal if you started your Docker container with the --add-host host.docker.internal:host-gateway option, or added the following snippet in your docker-compose.yml file:\n\n```\nextra_hosts:\n    - \"host.docker.internal:host-gateway\"\n\n```\n\nOtherwise, read below\n\nTLDR\nUse --network=\"host\" in your docker run command, then 127.0.0.1 in your Docker container will point to your Docker host.\nNote: This mode only works on Docker for Linux, per the documentation.\n\nNote on Docker container networking modes\nDocker offers different networking modes when running containers. Depending on the mode you choose you would connect to your MySQL database running on the Docker host differently.\ndocker run --network=\"bridge\" (default)\nDocker creates a bridge named docker0 by default. Both the Docker host and the Docker containers have an IP address on that bridge.\nOn the Docker host, type sudo ip addr show docker0 you will have an output looking like:\n\n```\n[vagrant@docker:~] $ sudo ip addr show docker0\n4: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default\n    link/ether 56:84:7a:fe:97:99 brd ff:ff:ff:ff:ff:ff\n    inet 172.17.42.1/16 scope global docker0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::5484:7aff:fefe:9799/64 scope link\n       valid_lft forever preferred_lft forever\n\n```\n\nSo here my Docker host has the IP address 172.17.42.1 on the docker0 network interface.\nNow start a new container and get a shell on it: docker run --rm -it ubuntu:trusty bash and within the container type ip addr show eth0 to discover how its main network interface is set up:\n\n```\nroot@e77f6a1b3740:/# ip addr show eth0\n863: eth0: <BROADCAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\n    link/ether 66:32:13:f0:f1:e3 brd ff:ff:ff:ff:ff:ff\n    inet 172.17.1.192/16 scope global eth0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::6432:13ff:fef0:f1e3/64 scope link\n       valid_lft forever preferred_lft forever\n\n```\n\nHere my container has the IP address 172.17.1.192. Now look at the routing table:\n\n```\nroot@e77f6a1b3740:/# route\nKernel IP routing table\nDestination     Gateway         Genmask         Flags Metric Ref    Use Iface\ndefault         172.17.42.1     0.0.0.0         UG    0      0        0 eth0\n172.17.0.0      *               255.255.0.0     U     0      0        0 eth0\n\n```\n\nSo the IP address of the Docker host 172.17.42.1 is set as the default route and is accessible from your container.\n\n```\nroot@e77f6a1b3740:/# ping 172.17.42.1\nPING 172.17.42.1 (172.17.42.1) 56(84) bytes of data.\n64 bytes from 172.17.42.1: icmp_seq=1 ttl=64 time=0.070 ms\n64 bytes from 172.17.42.1: icmp_seq=2 ttl=64 time=0.201 ms\n64 bytes from 172.17.42.1: icmp_seq=3 ttl=64 time=0.116 ms\n\n```\n\ndocker run --network=\"host\"\nAlternatively you can run a Docker container with network settings set to host. Such a container will share the network stack with the Docker host and from the container point of view, localhost (or 127.0.0.1) will refer to the Docker host.\nBe aware that any port opened in your Docker container would be opened on the Docker host. And this without requiring the -p or -P docker run option.\nIP configuration on my Docker host:\n\n```\n[vagrant@docker:~] $ ip addr show eth0\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\n    link/ether 08:00:27:98:dc:aa brd ff:ff:ff:ff:ff:ff\n    inet 10.0.2.15/24 brd 10.0.2.255 scope global eth0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::a00:27ff:fe98:dcaa/64 scope link\n       valid_lft forever preferred_lft forever\n\n```\n\nAnd from a Docker container in host mode:\n\n```\n[vagrant@docker:~] $ docker run --rm -it --network=host ubuntu:trusty ip addr show eth0\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\n    link/ether 08:00:27:98:dc:aa brd ff:ff:ff:ff:ff:ff\n    inet 10.0.2.15/24 brd 10.0.2.255 scope global eth0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::a00:27ff:fe98:dcaa/64 scope link\n       valid_lft forever preferred_lft forever\n\n```\n\nAs you can see, both the Docker host and Docker container share the exact same network interface and as such have the same IP address.\n\nConnecting to MySQL from containers\nBridge mode\nTo access MySQL running on the Docker host from containers in bridge mode, you need to make sure the MySQL service is listening for connections on the 172.17.42.1 IP address.\nTo do so, make sure you have either bind-address = 172.17.42.1 or bind-address = 0.0.0.0 in your MySQL configuration file (my.cnf).\nIf you need to set an environment variable with the IP address of the gateway, you can run the following code in a container:\n\n```\nexport DOCKER_HOST_IP=$(route -n | awk '/UG[ \\t]/{print $2}')\n\n```\n\nThen in your application, use the DOCKER_HOST_IP environment variable to open the connection to MySQL.\nNote: if you use bind-address = 0.0.0.0, your MySQL server will listen for connections on all network interfaces. That means your MySQL server could be reached from the Internet; make sure to set up firewall rules accordingly.\nNote 2: if you use bind-address = 172.17.42.1 your MySQL server won't listen for connections made to 127.0.0.1. Processes running on the Docker host that would want to connect to MySQL would have to use the 172.17.42.1 IP address.\nHost mode\nTo access MySQL running on the docker host from containers in host mode, you can keep bind-address = 127.0.0.1 in your MySQL configuration and connect to 127.0.0.1 from your containers:\n\n```\n[vagrant@docker:~] $ docker run --rm -it --network=host mysql mysql -h 127.0.0.1 -uroot -p\nEnter password:\nWelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 36\nServer version: 5.5.41-0ubuntu0.14.04.1 (Ubuntu)\n\nCopyright (c) 2000, 2014, Oracle and/or its affiliates. All rights reserved.\n\nOracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners.\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n\nmysql>\n\n```\n\nNote: Do use mysql -h 127.0.0.1 and not mysql -h localhost; otherwise the MySQL client would try to connect using a Unix socket."}
{"instruction": "What is the difference between the &#39;COPY&#39; and &#39;ADD&#39; commands in a Dockerfile?", "output": "You should check the ADD and COPY documentation for a more detailed description of their behaviors, but in a nutshell, the major difference is that ADD can do more than COPY:\n\nADD allows <src> to be a URL\nReferring to comments below, the ADD documentation states that:\n\n\nIf  is a local tar archive in a recognized compression format (identity, gzip, bzip2 or xz) then it is unpacked as a directory. Resources from remote URLs are not decompressed.\n\nNote that the Best practices for writing Dockerfiles suggests using COPY where the magic of ADD is not required. Otherwise, you (since you had to look up this answer) are likely to get surprised someday when you mean to copy keep_this_archive_intact.tar.gz into your container, but instead, you spray the contents onto your filesystem."}
{"instruction": "Copying files from Docker container to host", "output": "In order to copy a file from a container to the host, you can use the command\n\n```\ndocker cp <containerId>:/file/path/within/container /host/path/target\n\n```\n\nHere's an example:\n\n```\n$ sudo docker cp goofy_roentgen:/out_read.jpg .\n\n```\n\nHere goofy_roentgen is the container name I got from the following command:\n\n```\n$ sudo docker ps\n\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS                                            NAMES\n1b4ad9311e93        bamos/openface      \"/bin/bash\"         33 minutes ago      Up 33 minutes       0.0.0.0:8000->8000/tcp, 0.0.0.0:9000->9000/tcp   goofy_roentgen\n\n```\n\nYou can also use (part of) the Container ID. The following command is equivalent to the first\n\n```\n$ sudo docker cp 1b4a:/out_read.jpg .\n\n```"}
{"instruction": "What is the difference between CMD and ENTRYPOINT in a Dockerfile?", "output": "Docker has a default entrypoint which is /bin/sh -c but does not have a default command.\nWhen you run docker like this:\ndocker run -i -t ubuntu bash\nthe entrypoint is the default /bin/sh -c, the image is ubuntu and the command is bash.\nThe command is run via the entrypoint. i.e., the actual thing that gets executed is /bin/sh -c bash. This allowed Docker to implement RUN quickly by relying on the shell's parser.\nLater on, people asked to be able to customize this, so ENTRYPOINT and --entrypoint were introduced.\nEverything after the image name, ubuntu in the example above, is the command and is passed to the entrypoint. When using the CMD instruction, it is exactly as if you were executing\ndocker run -i -t ubuntu <cmd>\nThe parameter of the entrypoint is <cmd>.\nYou will also get the same result if you instead type this command docker run -i -t ubuntu: a bash shell will start in the container because in the ubuntu Dockerfile a default CMD is specified:\nCMD [\"bash\"].\nAs everything is passed to the entrypoint, you can have a very nice behavior from your images. @Jiri example is good, it shows how to use an image as a \"binary\". When using [\"/bin/cat\"] as entrypoint and then doing docker run img /etc/passwd, you get it, /etc/passwd is the command and is passed to the entrypoint so the end result execution is simply /bin/cat /etc/passwd.\nAnother example would be to have any cli as entrypoint. For instance, if you have a redis image, instead of running docker run redisimg redis -H something -u toto get key, you can simply have ENTRYPOINT [\"redis\", \"-H\", \"something\", \"-u\", \"toto\"] and then run like this for the same result: docker run redisimg get key."}
{"instruction": "How do I copy a folder from remote to local using scp?", "output": "```\nscp -r user@your.server.example.com:/path/to/foo /home/user/Desktop/\n\n```\n\nBy not including the trailing '/' at the end of foo, you will copy the directory itself (including contents), rather than only the contents of the directory.\nFrom man scp (See online manual)\n\n-r Recursively copy entire directories"}
{"instruction": "ssh &quot;permissions are too open&quot;", "output": "The keys need to be read-writable only by you:\n\n```\nchmod 600 ~/.ssh/id_rsa\n\n```\n\nAlternatively, the keys can be only readable by you (this also blocks your write access):\n\n```\nchmod 400 ~/.ssh/id_rsa\n\n```\n\n600 appears to be better in most cases, because you don't need to change file permissions later to edit it. (See the comments for more nuances)\nThe relevant portion from the manpage (man ssh)\n\n\n```\n ~/.ssh/id_rsa\n         Contains the private key for authentication.  These files contain sensitive \n         data and should be readable by the user but not\n         accessible by others (read/write/execute).  ssh will simply ignore a private \n         key file if it is              \n         accessible by others.  It is possible to specify a\n         passphrase when generating the key which will be used to encrypt the sensitive \n         part of this file using 3DES.\n\n ~/.ssh/identity.pub\n ~/.ssh/id_dsa.pub\n ~/.ssh/id_ecdsa.pub\n ~/.ssh/id_rsa.pub\n         Contains the public key for authentication.  These files are not sensitive and \n         can (but need not) be readable by anyone.\n\n```"}
{"instruction": "How to specify the private SSH-key to use when executing shell command on Git?", "output": "Something like this should work (suggested by orip):\n\n```\nssh-agent bash -c 'ssh-add /somewhere/yourkey; git clone git@github.com:user/project.git'\n\n```\n\nif you prefer subshells, you could try the following (though it is more fragile):\n\n```\nssh-agent $(ssh-add /somewhere/yourkey; git clone git@github.com:user/project.git)\n\n```\n\nGit will invoke SSH which will find its agent by environment variable; this will, in turn, have the key loaded.\nAlternatively, setting HOME may also do the trick, provided you are willing to setup a directory that contains only a .ssh directory as HOME; this may either contain an identity.pub, or a config file setting IdentityFile."}
{"instruction": "How do I remove the passphrase for the SSH key without having to create a new key?", "output": "Short answer:\n\n```\n$ ssh-keygen -p\n\n```\n\nThis will then prompt you to enter the keyfile location, the old passphrase, and the new passphrase (which can be left blank to have no passphrase).\n\nIf you would like to do it all on one line without prompts do:\n\n```\n$ ssh-keygen -p [-P old_passphrase] [-N new_passphrase] [-f keyfile]\n\n```\n\nImportant: Beware that when executing commands they will typically be logged in your ~/.bash_history file (or similar) in plain text including all arguments provided (i.e. the passphrases in this case). It is, therefore, is recommended that you use the first option unless you have a specific reason to do otherwise.   \nNotice though that you can still use -f keyfile without having to specify -P nor -N, and that the keyfile defaults to ~/.ssh/id_rsa, so in many cases, it's not even needed.\nYou might want to consider using ssh-agent, which can cache the passphrase for a time. The latest versions of gpg-agent also support the protocol that is used by ssh-agent."}
{"instruction": "How do I POST JSON data with cURL?", "output": "You need to set your content-type to application/json. But -d (or --data) sends the Content-Type application/x-www-form-urlencoded by default, which is not accepted on Spring's side.\nLooking at the curl man page, I think you can use -H (or --header):\n\n```\n-H \"Content-Type: application/json\"\n\n```\n\nFull example:\n\n```\ncurl --header \"Content-Type: application/json\" \\\n  --request POST \\\n  --data '{\"username\":\"xyz\",\"password\":\"xyz\"}' \\\n  http://localhost:3000/api/login\n\n```\n\n(-H is short for --header, -d for --data)\nNote that -request POST is optional if you use -d, as the -d flag implies a POST request.\n\nOn Windows, things are slightly different. See the comment thread."}
{"instruction": "How do I get a YouTube video thumbnail from the YouTube API?", "output": "Each YouTube video has four generated images. They are predictably formatted as follows:\n\n```\nhttps://img.youtube.com/vi/<insert-youtube-video-id-here>/0.jpg\nhttps://img.youtube.com/vi/<insert-youtube-video-id-here>/1.jpg\nhttps://img.youtube.com/vi/<insert-youtube-video-id-here>/2.jpg\nhttps://img.youtube.com/vi/<insert-youtube-video-id-here>/3.jpg\n\n```\n\nThe first one in the list is a full size image and others are thumbnail images. The default thumbnail image (i.e., one of 1.jpg, 2.jpg, 3.jpg) is:\n\n```\nhttps://img.youtube.com/vi/<insert-youtube-video-id-here>/default.jpg\n\n```\n\nFor the high quality version of the thumbnail use a URL similar to this:\n\n```\nhttps://img.youtube.com/vi/<insert-youtube-video-id-here>/hqdefault.jpg\n\n```\n\nThere is also a medium quality version of the thumbnail, using a URL similar to the HQ:\n\n```\nhttps://img.youtube.com/vi/<insert-youtube-video-id-here>/mqdefault.jpg\n\n```\n\nFor the standard definition version of the thumbnail, use a URL similar to this:\n\n```\nhttps://img.youtube.com/vi/<insert-youtube-video-id-here>/sddefault.jpg\n\n```\n\nFor the maximum resolution version of the thumbnail use a URL similar to this:\n\n```\nhttps://img.youtube.com/vi/<insert-youtube-video-id-here>/maxresdefault.jpg\n\n```\n\nAll of the above URLs are available over HTTP too. Additionally, the slightly shorter hostname i3.ytimg.com works in place of img.youtube.com in the example URLs above.\nAlternatively, you can use the YouTube Data API (v3) to get thumbnail images."}
{"instruction": "How to send a header using a HTTP request through a cURL call?", "output": "GET:\nwith JSON:\n\n```\ncurl -i -H \"Accept: application/json\" -H \"Content-Type: application/json\" http://hostname/resource\n\n```\n\nwith XML:\n\n```\ncurl -H \"Accept: application/xml\" -H \"Content-Type: application/xml\" -X GET http://hostname/resource\n\n```\n\nPOST:\nFor posting data:\n\n```\ncurl --data \"param1=value1&param2=value2\" http://hostname/resource\n\n```\n\nFor file upload:\n\n```\ncurl --form \"fileupload=@filename.txt\" http://hostname/resource\n\n```\n\nRESTful HTTP Post:\n\n```\ncurl -X POST -d @filename http://hostname/resource\n\n```\n\nFor logging into a site (auth):\n\n```\ncurl -d \"username=admin&password=admin&submit=Login\" --dump-header headers http://localhost/Login\ncurl -L -b headers http://localhost/\n\n```\n\nSource"}
{"instruction": "How do I measure request and response times at once using cURL?", "output": "From this brilliant blog post...  https://blog.josephscott.org/2011/10/14/timing-details-with-curl/\ncURL supports formatted output for the details of the request (see the cURL manpage for details, under -w, ‚Äìwrite-out <format>). For our purposes we‚Äôll focus just on the timing details that are provided. Times below are in seconds.\n\nCreate a new file, curl-format.txt, and paste in:\n\n```\n     time_namelookup:  %{time_namelookup}s\\n\n        time_connect:  %{time_connect}s\\n\n     time_appconnect:  %{time_appconnect}s\\n\n    time_pretransfer:  %{time_pretransfer}s\\n\n       time_redirect:  %{time_redirect}s\\n\n  time_starttransfer:  %{time_starttransfer}s\\n\n                     ----------\\n\n          time_total:  %{time_total}s\\n\n\n```\n\n\nMake a request:\n\n```\n curl -w \"@curl-format.txt\" -o /dev/null -s \"http://wordpress.com/\"\n\n```\n\nOr on Windows, it's...\n\n```\n curl -w \"@curl-format.txt\" -o NUL -s \"http://wordpress.com/\"\n\n```\n\n\n\n\nWhat this does:\n-w \"@curl-format.txt\" tells cURL to use our format file\n-o /dev/null redirects the output of the request to /dev/null\n-s\ntells cURL not to show a progress meter\n\"http://wordpress.com/\" is\nthe URL we are requesting. Use quotes particularly if your URL has \"&\" query string parameters\n\nAnd here is what you get back:\n\n```\n   time_namelookup:  0.001s\n      time_connect:  0.037s\n   time_appconnect:  0.000s\n  time_pretransfer:  0.037s\n     time_redirect:  0.000s\ntime_starttransfer:  0.092s\n                   ----------\n        time_total:  0.164s\n\n```\n\nI have not yet seen an option to output the results in microseconds, but if you're aware of one, post in the comments below.\n\nMake a Linux/Mac shortcut (alias)\n\n```\nalias curltime=\"curl -w \\\"@$HOME/.curl-format.txt\\\" -o /dev/null -s \"\n\n```\n\nThen you can simply call...\n\n```\ncurltime wordpress.org\n\n```\n\nThanks to commenter Pete Doyle!\n\nMake a Linux/Mac stand-alone script\nThis script does not require a separate .txt file to contain the formatting.\nCreate a new file, curltime, somewhere in your executable path, and paste in:\n\n```\n#!/bin/bash\n\ncurl -w @- -o /dev/null -s \"$@\" <<'EOF'\n    time_namelookup:  %{time_namelookup}\\n\n       time_connect:  %{time_connect}\\n\n    time_appconnect:  %{time_appconnect}\\n\n   time_pretransfer:  %{time_pretransfer}\\n\n      time_redirect:  %{time_redirect}\\n\n time_starttransfer:  %{time_starttransfer}\\n\n                    ----------\\n\n         time_total:  %{time_total}\\n\nEOF\n\n```\n\nThen call it the same way as the alias:\n\n```\ncurltime wordpress.org\n\n```\n\n\nMake a Windows shortcut (aka BAT file)\nCreate a new text file called curltime.bat in the same folder as curl.exe and curl-format.txt, and paste in the following line:\n\n```\ncurl -w \"@%~dp0curl-format.txt\" -o NUL -s %*\n\n```\n\nThen from the command line you can simply call:\n\n```\ncurltime wordpress.org\n\n```\n\n(Make sure the folder is listed in your Windows PATH variable to be able to use the command from any folder.)"}
{"instruction": "How do I get cURL to not show the progress bar?", "output": "```\ncurl -s http://google.com > temp.html\n\n```\n\nworks for curl version 7.19.5 on Ubuntu 9.10 (no progress bar). But if for some reason that does not work on your platform, you could always redirect stderr to /dev/null:\n\n```\ncurl  http://google.com 2>/dev/null > temp.html\n\n```"}
{"instruction": "Bash tool to get nth line from a file", "output": "head and pipe with tail will be slow for a huge file. I would suggest sed like this:\n\n```\nsed 'NUMq;d' file\n\n```\n\nWhere NUM is the number of the line you want to print; so, for example, sed '10q;d' file to print the 10th line of file.\nExplanation:\nNUMq will quit immediately when the line number is NUM.\nd will delete the line instead of printing it; this is inhibited on the last line because the q causes the rest of the script to be skipped when quitting.\nIf you have NUM in a variable, you will want to use double quotes instead of single:\n\n```\nsed \"${NUM}q;d\" file\n\n```"}
{"instruction": "Find and kill a process in one line using bash and regex", "output": "In bash, using only the basic tools listed in your question(1), you should be able to do:\n\n```\nkill $(ps aux | grep '[p]ython csp_build.py' | awk '{print $2}')\n\n```\n\nDetails on its workings are as follows:\n\nThe ps gives you the list of all the processes.\nThe grep filters that based on your search string, [p] is a trick to stop you picking up the actual grep process itself.\nThe awk just gives you the second field of each line, which is the PID.\nThe $(x) construct means to execute x then take its output and put it on the command line. The output of that ps pipeline inside that construct above is the list of process IDs so you end up with a command like kill 1234 1122 7654.\n\nHere's a transcript showing it in action:\n\n```\npax> sleep 3600 &\n[1] 2225\npax> sleep 3600 &\n[2] 2226\npax> sleep 3600 &\n[3] 2227\npax> sleep 3600 &\n[4] 2228\npax> sleep 3600 &\n[5] 2229\npax> kill $(ps aux | grep '[s]leep' | awk '{print $2}')\n[5]+  Terminated              sleep 3600\n[1]   Terminated              sleep 3600\n[2]   Terminated              sleep 3600\n[3]-  Terminated              sleep 3600\n[4]+  Terminated              sleep 3600\n\n```\n\nand you can see it terminating all the sleepers.\nExplaining the grep '[p]ython csp_build.py' bit in a bit more detail: when you do sleep 3600 & followed by ps -ef | grep sleep, you tend to get two processes with sleep in it, the sleep 3600 and the grep sleep (because they both have sleep in them, that's not rocket science).\nHowever, ps -ef | grep '[s]leep' won't create a grep process with sleep in it, it instead creates one with the command grep '[s]leep' and here's the tricky bit: the grep doesn't find that one, because it's looking for the regular expression \"any character from the character class [s] (which is basically just s) followed by leep.\nIn other words, it's looking for sleep but the grep process is grep '[s]leep' which doesn't have the text sleep in it.\nWhen I was shown this (by someone here on SO), I immediately started using it because\n\nit's one less process than adding | grep -v grep; and\nit's elegant and sneaky, a rare combination :-)\n\n\n(1) If you're not limited to using those basic tools, there's a nifty pgrep command which will find processes based on certain criteria (assuming you have it available on your system, of course).\nFor example, you can use pgrep sleep to output the process IDs for all sleep commands (by default, it matches the process name). If you want to match the entire command line as shown in ps, you can do something like pgrep -f 'sleep 9999'.\nAs an aside, it doesn't list itself if you do pgrep pgrep, so the tricky filter method shown above is not necessary in this case.\nYou can check that the processes are the ones you're interested in by using -a to show the full process names. You can also limit the scope to your own processes (or a specific set of users) with -u or -U. See the man page for pgrep/pkill for more options.\nOnce you're satisfied it will only show the processes you're interested in, you can then use pkill with the same parameters to send a signal to all those processes."}
{"instruction": "How to remove double-quotes in jq output for parsing json files in bash?", "output": "Use the -r (or --raw-output) option to emit raw strings as output:\n\n```\njq -r '.name' <json.txt\n\n```"}
{"instruction": "What is the difference between sed and awk?", "output": "sed is a stream editor. It works with streams of characters on a per-line basis. It has a primitive programming language that includes goto-style loops and simple conditionals (in addition to pattern matching and address matching). There are essentially only two \"variables\": pattern space and hold space. Readability of scripts can be difficult. Mathematical operations are extraordinarily awkward at best.\nThere are various versions of sed with different levels of support for command line options and language features.\nawk is oriented toward delimited fields on a per-line basis. It has much more robust programming constructs including if/else, while, do/while and for (C-style and array iteration). There is complete support for variables and single-dimension associative arrays plus (IMO) kludgey multi-dimension arrays. Mathematical operations resemble those in C. It has printf and functions. The \"K\" in \"AWK\" stands for \"Kernighan\" as in \"Kernighan and Ritchie\" of the book \"C Programming Language\" fame (not to forget Aho and Weinberger). One could conceivably write a detector of academic plagiarism using awk.\nGNU awk (gawk) has numerous extensions, including true multidimensional arrays in the latest version. There are other variations of awk including mawk and nawk.\nBoth programs use regular expressions for selecting and processing text.\nI would tend to use sed where there are patterns in the text. For example, you could replace all the negative numbers in some text that are in the form \"minus-sign followed by a sequence of digits\" (e.g. \"-231.45\") with the \"accountant's brackets\" form (e.g. \"(231.45)\") using this (which has room for improvement):\n\n```\nsed 's/-\\([0-9.]\\+\\)/(\\1)/g' inputfile\n\n```\n\nI would use awk when the text looks more like rows and columns or, as awk refers to them \"records\" and \"fields\". If I was going to do a similar operation as above, but only on the third field in a simple comma delimited file I might do something like:\n\n```\nawk -F, 'BEGIN {OFS = \",\"} {gsub(\"-([0-9.]+)\", \"(\" substr($3, 2) \")\", $3); print}' inputfile\n\n```\n\nOf course those are just very simple examples that don't illustrate the full range of capabilities that each has to offer."}
{"instruction": "How do I undo the most recent local commits in Git?", "output": "Undo a commit & redo\n\n```\n$ git commit -m \"Something terribly misguided\" # (0: Your Accident)\n$ git reset HEAD~                              # (1)\n# === If you just want to undo the commit, stop here! ===\n[ edit files as necessary ]                    # (2)\n$ git add .                                    # (3)\n$ git commit -c ORIG_HEAD                      # (4)\n\n```\n\n\ngit reset is the command responsible for the undo. It will undo your last commit while leaving your working tree (the state of your files on disk) untouched. You'll need to add them again before you can commit them again.\nMake corrections to working tree files.\ngit add anything that you want to include in your new commit.\nCommit the changes, reusing the old commit message. reset copied the old head to .git/ORIG_HEAD; commit with -c ORIG_HEAD will open an editor, which initially contains the log message from the old commit and allows you to edit it. If you do not need to edit the message, you could use the -C option.\n\nAlternatively, to edit the previous commit (or just its commit message), commit --amend will add changes within the current index to the previous commit.\nTo remove (not revert) a commit that has been pushed to the server, rewriting history with git push origin main --force[-with-lease] is necessary. It's almost always a bad idea to use --force; prefer --force-with-lease instead, and as noted in the git manual:\n\nYou should understand the implications of rewriting history if you amend a commit that has already been published.\n\n\nFurther Reading\nYou can use git reflog to determine the SHA-1 for the commit to which you wish to revert. Once you have this value, use the sequence of commands as explained above.\n\nHEAD~ is the same as HEAD~1. The article What is the HEAD in git? is helpful if you want to uncommit multiple commits."}
{"instruction": "How do I delete a Git branch locally and remotely?", "output": "Executive Summary\n\n```\ngit push -d <remote_name> <branchname>   # Delete remote\ngit branch -d <branchname>               # Delete local\n\n```\n\nNote: In most cases, <remote_name> will be origin.\nDelete Local Branch\nTo delete the local branch, use one of the following:\n\n```\ngit branch -d <branch_name>\ngit branch -D <branch_name>\n\n```\n\n\nThe -d option is an alias for --delete, which only deletes the branch if it has already been fully merged in its upstream branch.\nThe -D option is an alias for --delete --force, which deletes the branch \"irrespective of its merged status.\" [Source: man git-branch]\nAs of Git v2.3, git branch -d (delete) learned to honor the -f (force) flag.\nYou will receive an error if you try to delete the currently selected branch.\n\nDelete Remote Branch\nAs of Git v1.7.0, you can delete a remote branch using\n\n```\n$ git push <remote_name> --delete <branch_name>\n\n```\n\nwhich might be easier to remember than\n\n```\n$ git push <remote_name> :<branch_name>\n\n```\n\nwhich was added in Git v1.5.0 \"to delete a remote branch or a tag.\"\nStarting with Git v2.8.0, you can also use git push with the -d option as an alias for --delete. Therefore, the version of Git you have installed will dictate whether you need to use the easier or harder syntax.\nDelete Remote Branch [Original Answer from 5-Jan-2010]\nFrom Chapter 3 of Pro Git by Scott Chacon:\n\nDeleting Remote Branches\nSuppose you‚Äôre done with a remote branch ‚Äî say, you and your collaborators are finished with a feature and have merged it into your remote‚Äôs main branch (or whatever branch your stable code-line is in). You can delete a remote branch using the rather obtuse syntax git push [remotename] :[branch]. If you want to delete your serverfix branch from the server, you run the following:\n\n```\n$ git push origin :serverfix\nTo git@github.com:schacon/simplegit.git\n - [deleted]         serverfix\n\n```\n\nBoom. No more branches on your server. You may want to dog-ear this page, because you‚Äôll need that command, and you‚Äôll likely forget the syntax. A way to remember this command is by recalling the git push [remotename] [localbranch]:[remotebranch] syntax that we went over a bit earlier. If you leave off the [localbranch] portion, then you‚Äôre basically saying, ‚ÄúTake nothing on my side and make it be [remotebranch].‚Äù\n\nI ran git push origin :bugfix, and it worked beautifully. Scott Chacon was right‚ÄîI will want to dog-ear that page (or virtually dog ear-by answering this on Stack Overflow).\nFetch changes\nFinally, execute the following on other machines to propagate changes:\n\n```\n# Fetch changes from all remotes and locally delete \n# remote deleted branches/tags etc\n# --prune will do the job :-;\ngit fetch --all --prune\n\n```"}
{"instruction": "What is the difference between &#39;git pull&#39; and &#39;git fetch&#39;?", "output": "In the simplest terms, git pull does a git fetch followed by a git merge.\n\ngit fetch updates your remote-tracking branches under refs/remotes/<remote>/. This operation is safe to run at any time since it never changes any of your local branches under refs/heads.\ngit pull brings a local branch up-to-date with its remote version, while also updating your other remote-tracking branches.\nFrom the Git documentation for git pull:\n\ngit pull runs git fetch with the given parameters and then depending on configuration options or command line flags, will call either git rebase or git merge to reconcile diverging branches."}
{"instruction": "How can I rename a local Git branch?", "output": "To rename the current branch:\n\n```\ngit branch -m <newname>\n\n```\n\nTo rename a branch while pointed to any branch:\n\n```\ngit branch -m <oldname> <newname>\n\n```\n\n-m is short for --move.\n\nTo push the  local branch and reset the upstream branch:\n\n```\ngit push origin -u <newname>\n\n```\n\nTo delete the  remote branch:\n\n```\ngit push origin --delete <oldname>\n\n```\n\n\nTo create a git rename alias:\n\n```\ngit config --global alias.rename 'branch -m'\n\n```\n\n\nOn Windows or another case-insensitive filesystem, use -M if there are only capitalization changes in the name. Otherwise, Git will throw a \"branch already exists\" error.\n\n```\ngit branch -M <newname>\n\n```"}
{"instruction": "How do I undo &#39;git add&#39; before commit?", "output": "To unstage a specific file\n\n```\ngit reset <file>\n\n```\n\nThat will remove the file from the current index (the \"about to be committed\" list) without changing anything else.\nTo unstage all files from the current change set:\n\n```\ngit reset\n\n```\n\n\nIn old versions of Git, the above commands are equivalent to git reset HEAD <file> and git reset HEAD respectively, and will fail if HEAD is undefined (because you haven't yet made any commits in your repository) or ambiguous (because you created a branch called HEAD, which is a stupid thing that you shouldn't do). This was changed in Git 1.8.2, though, so in modern versions of Git you can use the commands above even prior to making your first commit:\n\n\"git reset\" (without options or parameters) used to error out when\nyou do not have any commits in your history, but it now gives you\nan empty index (to match non-existent commit you are not even on).\n\nDocumentation: git reset"}
{"instruction": "How do I force &quot;git pull&quot; to overwrite local files?", "output": "‚ö† Warning:\nAny uncommitted local change to tracked files will be lost, even if staged.\nBut any local file that's not tracked by Git will not be affected.\n\n\nFirst, update all origin/<branch> refs to latest:\n\n```\ngit fetch --all\n\n```\n\nBackup your current branch (e.g. main):\n\n```\ngit branch backup-main\n\n```\n\nJump to the latest commit on origin/main and checkout those files:\n\n```\ngit reset --hard origin/main\n\n```\n\nExplanation:\ngit fetch downloads the latest from remote without trying to merge or rebase anything.\ngit reset resets the master branch to what you just fetched. The --hard option changes all the files in your working tree to match the files in origin/main.\n\nMaintain current local commits\n[*]: It's worth noting that it is possible to maintain current local commits by creating a branch from main before resetting:\n\n```\ngit checkout main\ngit branch new-branch-to-save-current-commits\ngit fetch --all\ngit reset --hard origin/main\n\n```\n\nAfter this, all of the old commits will be kept in new-branch-to-save-current-commits.\nUncommitted changes\nUncommitted changes, even if staged (with git add), will be lost. Make sure to stash or commit anything you need. For example, run the following:\n\n```\ngit stash\n\n```\n\nAnd later (after git reset), reapply these uncommitted changes:\n\n```\ngit stash pop\n\n```\n\n\nWhich may create merge conflicts."}
{"instruction": "How to check out a remote Git branch?", "output": "The answer has been split depending on whether there is one remote repository configured or multiple. The reason for this is that for the single remote case, some of the commands can be simplified as there is less ambiguity.\nUpdated for Git 2.23: For older versions, see the section at the end.\nWith One Remote\nIn both cases, start by fetching from the remote repository to make sure you have all the latest changes downloaded.\n\n```\n$ git fetch\n\n```\n\nThis will fetch all of the remote branches for you. You can see the branches available for checkout with:\n\n```\n$ git branch -v -a\n\n...\nremotes/origin/test\n\n```\n\nThe branches that start with remotes/* can be thought of as read only copies of the remote branches. To work on a branch you need to create a local branch from it. This is done with the Git command switch (since Git 2.23) by giving it the name of the remote branch (minus the remote name):\n\n```\n$ git switch test\n\n```\n\nIn this case Git is guessing (can be disabled with --no-guess) that you are trying to checkout and track the remote branch with the same name.\nWith Multiple Remotes\nIn the case where multiple remote repositories exist, the remote repository needs to be explicitly named.\nAs before, start by fetching the latest remote changes:\n\n```\n$ git fetch origin\n\n```\n\nThis will fetch all of the remote branches for you. You can see the branches available for checkout with:\n\n```\n$ git branch -v -a\n\n```\n\nWith the remote branches in hand, you now need to check out the branch you are interested in with -c to create a new local branch:\n\n```\n$ git switch -c test origin/test\n\n```\n\nFor more information about using git switch:\n\n```\n$ man git-switch\n\n```\n\nPrior to Git 2.23\ngit switch was added in Git 2.23, prior to this git checkout was used to switch branches.\nTo checkout out with only a single remote repository:\n\n```\ngit checkout test\n\n```\n\nif there are multiple remote repositories configured then it becomes a bit longer\n\n```\ngit checkout -b test <name of remote>/test\n\n```"}
{"instruction": "How do I make Git forget about a file that was tracked, but is now in .gitignore?", "output": ".gitignore will prevent untracked files from being added (without an add -f) to the set of files tracked by Git. However, Git will continue to track any files that are already being tracked.\nUpdated Answer in 2024\nDo NOT use git rm --cached <file> if you ever want to see that file again. It will remove it from git, and also your local machine.\nIf you want to keep the file locally, but remove it from git tracking, use the answer by Konstantin. In short, use the following instead of git rm:\ngit update-index --skip-worktree <file>\nHowever, according to the official git documentation:\n\nUsers often try to use the assume-unchanged and skip-worktree bits to tell Git to ignore changes to files that are tracked. This does not work as expected, since Git may still check working tree files against the index when performing certain operations. In general, Git does not provide a way to ignore changes to tracked files, so alternate solutions are recommended.\n\nTherefore, you should still consider using the original answer below.\nOriginal Answer\nWARNING: This will remove the physical file from your local machine and other developers' machines on your or their next git pull.\nTo stop tracking a file, we must remove it from the index:\n\n```\ngit rm --cached <file>\n\n```\n\nTo remove a folder and all files in the folder recursively:\n\n```\ngit rm -r --cached <folder>\n\n```\n\nThe removal of the file from the head revision will happen on the next commit."}
{"instruction": "How do I remove local (untracked) files from the current Git working tree?", "output": "git-clean - Remove untracked files from the working tree\nSynopsis\n\n```\ngit clean [-d] [-f] [-i] [-n] [-q] [-e <pattern>] [-x | -X] [--] <path>‚Ä¶‚Äã\n\n```\n\nDescription\nCleans the working tree by recursively removing files that are not under version control, starting from the current directory.\nNormally, only files unknown to Git are removed, but if the -x option is specified, ignored files are also removed. This can, for example, be useful to remove all build products.\nIf any optional <path>... arguments are given, only those paths are affected.\n\n\nStep 1 is to show what will be deleted by using the -n option:\n\n```\n# Print out the list of files and directories which will be removed (dry run)\ngit clean -n -d\n\n```\n\nClean Step - beware: this will delete files:\n\n```\n# Delete the files from the repository\ngit clean -f\n\n```\n\n\nTo remove directories, run git clean -f -d or git clean -fd\nTo remove ignored files, run git clean -f -X or git clean -fX\nTo remove ignored and non-ignored files, run git clean -f -x or git clean -fx\n\nNote the case difference on the X for the two latter commands.\nIf clean.requireForce is set to \"true\" (the default) in your configuration, one needs to specify -f otherwise nothing will actually happen.\nAgain see the git-clean docs for more information.\n\n\nOptions\n-f, --force\nIf the Git configuration variable clean.requireForce is not set to\nfalse, git clean will refuse to run unless given -f, -n or -i.\n-x\nDon‚Äôt use the standard ignore rules read from .gitignore (per\ndirectory) and $GIT_DIR/info/exclude, but do still use the ignore\nrules given with -e options. This allows removing all untracked files,\nincluding build products. This can be used (possibly in conjunction\nwith git reset) to create a pristine working directory to test a clean\nbuild.\n-X\nRemove only files ignored by Git. This may be useful to rebuild\neverything from scratch, but keep manually created files.\n-n, --dry-run\nDon‚Äôt actually remove anything, just show what would be done.\n-d\nRemove untracked directories in addition to untracked files. If an\nuntracked directory is managed by a different Git repository, it is\nnot removed by default. Use -f option twice if you really want to\nremove such a directory."}
{"instruction": "How to modify existing, unpushed commit messages?", "output": "Amending the most recent commit message\n\n```\ngit commit --amend\n\n```\n\nwill open your editor, allowing you to change the commit message of the most recent commit. Additionally, you can set the commit message directly in the command line with:\n\n```\ngit commit --amend -m \"New commit message\"\n\n```\n\n‚Ä¶however, this can make multi-line commit messages or small corrections more cumbersome to enter.\nMake sure you don't have any working copy changes staged before doing this or they will get committed too. (Unstaged changes will not get committed.)\nChanging the message of a commit that you've already pushed to your remote branch\nIf you've already pushed your commit up to your remote branch, then - after amending your commit locally (as described above) - you'll also need to force push the commit with:\n\n```\ngit push <remote> <branch> --force\n# Or\ngit push <remote> <branch> -f\n\n```\n\nWarning: force-pushing will overwrite the remote branch with the state of your local one. If there are commits on the remote branch that you don't have in your local branch, you will lose those commits.\nWarning: be cautious about amending commits that you have already shared with other people. Amending commits essentially rewrites them to have different SHA IDs, which poses a problem if other people have copies of the old commit that you've rewritten. Anyone who has a copy of the old commit will need to synchronize their work with your newly re-written commit, which can sometimes be difficult, so make sure you coordinate with others when attempting to rewrite shared commit history, or just avoid rewriting shared commits altogether.\n\nPerform an interactive rebase\nAnother option is to use interactive rebase.\nThis allows you to edit any message you want to update even if it's not the latest message.\nIn order to do a Git squash, follow these steps:\n\n```\n// n is the number of commits up to the last commit you want to be able to edit\ngit rebase -i HEAD~n\n\n```\n\nOnce you squash your commits - choose the e/r for editing the message:\n\nImportant note about interactive rebase\nWhen you use git rebase -i HEAD~n there can be more than n commits. Git will \"collect\" all the commits in the last n commits, and if there was a merge somewhere in between that range you will see all the commits as well, so the outcome will be n + .\nGood tip:\nIf you have to do it for more than a single branch and you might face conflicts when amending the content, set up git rerere and let Git resolve those conflicts automatically for you.\n\nDocumentation\n\ngit-commit(1) Manual Page\n\ngit-rebase(1) Manual Page\n\ngit-push(1) Manual Page"}
{"instruction": "How do I revert a Git repository to a previous commit?", "output": "This depends a lot on what you mean by \"revert\".\nTemporarily switch to a different commit\nIf you want to temporarily go back to it, fool around, then come back to where you are, all you have to do is check out the desired commit:\n\n```\n# This will detach your HEAD, that is, leave you with no branch checked out:\ngit checkout 0d1d7fc32\n\n```\n\nOr if you want to make commits while you're there, go ahead and make a new branch while you're at it:\n\n```\ngit checkout -b old-state 0d1d7fc32\n\n```\n\nTo go back to where you were, just check out the branch you were on again. (If you've made changes, as always when switching branches, you'll have to deal with them as appropriate. You could reset to throw them away; you could stash, checkout, stash pop to take them with you; you could commit them to a branch there if you want a branch there.)\nHard delete unpublished commits\nIf, on the other hand, you want to really get rid of everything you've done since then, there are two possibilities. One, if you haven't published any of these commits, simply reset:\n\n```\n# This will destroy any local modifications.\n# Don't do it if you have uncommitted work you want to keep.\ngit reset --hard 0d1d7fc32\n\n# Alternatively, if there's work to keep:\ngit stash\ngit reset --hard 0d1d7fc32\ngit stash pop\n# This saves the modifications, then reapplies that patch after resetting.\n# You could get merge conflicts, if you've modified things which were\n# changed since the commit you reset to.\n\n```\n\nIf you mess up, you've already thrown away your local changes, but you can at least get back to where you were before by resetting again.\nUndo published commits with new commits\nOn the other hand, if you've published the work, you probably don't want to reset the branch, since that's effectively rewriting history. In that case, you could indeed revert the commits. In many enterprise organisations, the concept of \"protected\" branches will even prevent history from being rewritten on some major branches. In this case, reverting is your only option.\nWith Git, revert has a very specific meaning: create a commit with the reverse patch to cancel it out. This way you don't rewrite any history.\nFirst figure out what commits to revert. Depending on the technique chosen below, you want to either revert only the merge commits, or only the non-merge commits.\n\n```\n# This lists all merge commits between 0d1d7fc and HEAD:\ngit log --merges --pretty=format:\"%h\" 0d1d7fc..HEAD | tr '\\n' ' '\n\n# This lists all non merge commits between 0d1d7fc and HEAD:\ngit log --no-merges --pretty=format:\"%h\" 0d1d7fc..HEAD | tr '\\n' ' '\n\n```\n\nNote: if you revert multiple commits, the order matters. Start with the most recent commit.\n\n```\n# This will create three separate revert commits, use non merge commits only:\ngit revert a867b4af 25eee4ca 0766c053\n\n# It also takes ranges. This will revert the last two commits:\ngit revert HEAD~2..HEAD\n\n# Similarly, you can revert a range of commits using commit hashes (non inclusive of first hash):\ngit revert 0d1d7fc..a867b4a\n\n# Reverting a merge commit. You can also use a range of merge commits here.\ngit revert -m 1 <merge_commit_sha>\n\n# To get just one, you could use `rebase -i` to squash them afterwards\n# Or, you could do it manually (be sure to do this at top level of the repo)\n# get your index and work tree into the desired state, without changing HEAD:\ngit checkout 0d1d7fc32 .\n\n# Then commit. Be sure and write a good message describing what you just did\ngit commit\n\n```\n\nThe git-revert manpage actually covers a lot of this in its description. Another useful link is this git-scm.com section discussing git-revert.\nIf you decide you didn't want to revert after all, you can revert the revert (as described here) or reset back to before the revert (see the previous section).\nYou may also find this answer helpful in this case:\nHow can I move HEAD back to a previous location? (Detached head) & Undo commits"}
{"instruction": "How do I change the URI (URL) for a remote Git repository?", "output": "First, view the existing remotes to verify which URL is currently set:\n\n```\ngit remote -v\n\n```\n\nThen, you can set it with:\n\n```\ngit remote set-url origin <NEW_GIT_URL_HERE>\n\n```\n\nSee git help remote. You also can edit .git/config and change the URLs there.\nYou're not in any danger of losing history unless you do something very silly (and if you're worried, just make a copy of your repo, since your repo is your history.)"}
{"instruction": "Move the most recent commit(s) to a new branch with Git", "output": "WARNING: You need to store uncommitted edits to your stash before doing this, using git stash. Once complete, you can retrieve the stashed uncommitted edits with git stash pop. git reset hard command will remove all changes!\nMoving to an existing branch\nIf you want to move your commits to an existing branch, it will look like this:\n\n```\ngit checkout existingbranch\ngit merge branchToMoveCommitFrom\ngit checkout branchToMoveCommitFrom\ngit reset --hard HEAD~3 # Go back 3 commits. You *will* lose uncommitted work.\ngit checkout existingbranch\n\n```\n\nMoving to a new branch\nWARNING: This method works because you are creating a new branch with the first command: git branch newbranch. If you want to move commits to an existing branch you need to merge your changes into the existing branch before executing git reset --hard HEAD~3 (see Moving to an existing branch above). If you don't merge your changes first, they will be lost.\nUnless there are other circumstances involved, this can be easily done by branching and rolling back.\n\n```\n# Note: Any changes not committed will be lost.\ngit branch newbranch      # Create a new branch, saving the desired commits\ngit checkout master       # checkout master, this is the place you want to go back\ngit reset --hard HEAD~3   # Move master back by 3 commits (Make sure you know how many commits you need to go back)\ngit checkout newbranch    # Go to the new branch that still has the desired commits\n\n```\n\nBut do make sure how many commits to go back. Alternatively, you can instead of HEAD~3, simply provide the hash of the commit (or the reference like origin/master) you want to \"revert back to\" on the master (/current) branch, e.g:\n\n```\ngit reset --hard a1b2c3d4\n\n```\n\nNote: You will only be \"losing\" commits from the master branch, but don't worry, you'll have those commits in newbranch! An easy way to check that, after completing the 4 step sequence of commands above, is by looking at git log -n4 which will show the history of newbranch actually retained the 3 commits (and the reason is that newbranch was created at the time those changes were already commited on master!). They have only been removed from master, as git reset only affected the branch that was checked out at the time of its execution, i.e. master (see git reset description: Reset current HEAD to the specified state). git status however will not show any checkouts on the newbranch, which might be surprising at first but that is actually expected.\nLastly, you may need to force push your latest changes to main repo:\n\n```\ngit push origin master --force\n\n```\n\nWARNING: With Git version 2.0 and later, if you later git rebase the new branch upon the original (master) branch, you may need an explicit --no-fork-point option during the rebase to avoid losing the carried-over commits.  Having branch.autosetuprebase always set makes this more likely.  See John Mellor's answer for details."}
{"instruction": "How do I discard unstaged changes in Git?", "output": "Another quicker way is:\n\n```\ngit stash save --keep-index --include-untracked\n\n```\n\nYou don't need to include --include-untracked if you don't want to be thorough about it.\nAfter that, you can drop that stash with a git stash drop command if you like."}
{"instruction": "Reset local repository branch to be just like remote repository HEAD", "output": "Setting your branch to exactly match the remote branch can be done in two steps:\n\n```\ngit fetch origin\ngit reset --hard origin/master\n\n```\n\nIf you want to save your current branch's state before doing this (just in case), you can do:\n\n```\ngit commit -a -m \"Saving my work, just in case\"\ngit branch my-saved-work\n\n```\n\nNow your work is saved on the branch \"my-saved-work\" in case you decide you want it back (or want to look at it later or diff it against your updated branch).\nNote: the first example assumes that the remote repo's name is origin and that the branch named master in the remote repo matches the currently checked-out branch in your local repo, since that is in line with the example given in the question. If you are trying to reset to the default branch in a more recent repository, it is likely that it will be main.\nBTW, this situation that you're in looks an awful lot like a common case where a push has been done into the currently checked out branch of a non-bare repository. Did you recently push into your local repo? If not, then no worries -- something else must have caused these files to unexpectedly end up modified. Otherwise, you should be aware that it's not recommended to push into a non-bare repository (and not into the currently checked-out branch, in particular)."}
{"instruction": "How can I reset or revert a file to a specific revision?", "output": "Assuming the hash of the commit you want is c5f567:\n\n```\ngit checkout c5f567 -- file1/to/restore file2/to/restore\n\n```\n\nThe git checkout man page gives more information.\nIf you want to revert to the commit before c5f567, append ~1 (where 1 is the number of commits you want to go back, it can be anything):\n\n```\ngit checkout c5f567~1 -- file1/to/restore file2/to/restore\n\n```\n\nAs a side note, I've always been uncomfortable with this command because it's used for both ordinary things (changing between branches) and unusual, destructive things (discarding changes in the working directory).\nFor the meaning of -- in the command, refer to In Git, what does -- (dash dash) mean?\n\nThere is also a new git restore command that is specifically designed for restoring working copy files that have been modified. If your git is new enough you can use this command, but the documentation comes with a warning:\n\nTHIS COMMAND IS EXPERIMENTAL. THE BEHAVIOR MAY CHANGE.\n\nBecause git restore is experimental, it should not yet be promoted as the primary answer to this question. When the command is no longer marked as \"experimental\", then this answer can be amended to promote the use of git restore. [At the time of writing, the git restore command has been marked as \"experimental\" for at least four years.]"}
{"instruction": "How do I push a new local branch to a remote Git repository and track it too?", "output": "In Git 1.7.0 and later, you can checkout a new branch:\n\n```\ngit checkout -b <branch>\n\n```\n\nEdit files, add and commit. Then push with the -u (short for --set-upstream) option:\n\n```\ngit push -u origin <branch>\n\n```\n\nGit will set up the tracking information during the push."}
{"instruction": "How do I squash my last N commits together?", "output": "Use git rebase -i <after-this-commit> and replace \"pick\" on the second and subsequent commits with \"squash\" or \"fixup\", as described in the manual.\nIn this example, <after-this-commit> is either the SHA1 hash or the relative location from the HEAD of the current branch from which commits are analyzed for the rebase command. For example, if the user wishes to view 5 commits from the current HEAD in the past, the command is git rebase -i HEAD~5."}
{"instruction": "How to determine the URL that a local Git repository was originally cloned from", "output": "To obtain only the remote URL:\n\n```\ngit config --get remote.origin.url\n\n```\n\nIf you require full output, and you are on a network that can reach the remote repo where the origin resides:\n\n```\ngit remote show origin\n\n```\n\nWhen using git clone (from GitHub, or any source repository for that matter) the default name for the source of the clone is \"origin\". Using git remote show will display the information about this remote name. The first few lines should show:\n\n```\nC:\\Users\\jaredpar\\VsVim> git remote show origin\n* remote origin\n  Fetch URL: git@github.com:jaredpar/VsVim.git\n  Push  URL: git@github.com:jaredpar/VsVim.git\n  HEAD branch: master\n  Remote branches:\n\n```\n\nIf you want to use the value in a script, you would use the first command listed in this answer."}
{"instruction": "How do I add an empty directory to a Git repository?", "output": "Another way to make a directory stay (almost) empty (in the repository) is to create a .gitignore file inside that directory that contains these four lines:\n\n```\n# Ignore everything in this directory\n*\n# Except this file\n!.gitignore\n\n```\n\nThen you don't have to get the order right the way that you have to do in m104's solution.\nThis also gives the benefit that files in that directory won't show up as \"untracked\" when you do a git status.\nMaking @GreenAsJade's comment persistent:\n\nI think it's worth noting that this solution does precisely what the question asked for, but is not perhaps what many people looking at this question will have been looking for. This solution guarantees that the directory remains empty. It says \"I truly never want files checked in here\". As opposed to \"I don't have any files to check in here, yet, but I need the directory here, files may be coming later\"."}
{"instruction": "How do I resolve merge conflicts in a Git repository?", "output": "Try:\n\n```\ngit mergetool\n\n```\n\nIt opens a GUI that steps you through each conflict, and you get to choose how to merge.  Sometimes it requires a bit of hand editing afterwards, but usually it's enough by itself.  It is much better than doing the whole thing by hand certainly.\n\nAs per Josh Glover's comment:\n\n[This command]\ndoesn't necessarily open a GUI unless you install one. Running git mergetool for me resulted in vimdiff being used. You can install\none of the following tools to use it instead: meld, opendiff,\nkdiff3, tkdiff, xxdiff, tortoisemerge, gvimdiff, diffuse,\necmerge, p4merge, araxis, vimdiff, emerge.\n\n\nBelow is a sample procedure using vimdiff to resolve merge conflicts, based on this link.\n\nRun the following commands in your terminal\n\n```\ngit config merge.tool vimdiff\ngit config merge.conflictstyle diff3\ngit config mergetool.prompt false\n\n```\n\nThis will set vimdiff as the default merge tool.\n\nRun the following command in your terminal\n\n```\ngit mergetool\n\n```\n\n\nYou will see a vimdiff display in the following format:\n\n```\n  ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï¶‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï¶‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n  ‚ïë       ‚ïë      ‚ïë        ‚ïë\n  ‚ïë LOCAL ‚ïë BASE ‚ïë REMOTE ‚ïë\n  ‚ïë       ‚ïë      ‚ïë        ‚ïë\n  ‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï©‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï©‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n  ‚ïë                       ‚ïë\n  ‚ïë        MERGED         ‚ïë\n  ‚ïë                       ‚ïë\n  ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n\n```\n\nThese 4 views are\n\nLOCAL: this is the file from the current branch\nBASE: the common ancestor, how this file looked before both changes\nREMOTE: the file you are merging into your branch\nMERGED: the merge result; this is what gets saved in the merge commit and used in the future\n\nYou can navigate among these views using ctrl+w. You can directly reach the MERGED view using ctrl+w followed by j.\nMore information about vimdiff navigation is here and here.\n\nYou can edit the MERGED view like this:\n\nIf you want to get changes from REMOTE\n\n```\n:diffg RE\n\n```\n\n\nIf you want to get changes from BASE\n\n```\n:diffg BA\n\n```\n\n\nIf you want to get changes from LOCAL\n\n```\n:diffg LO\n\n```\n\n\n\n\nSave, Exit, Commit, and Clean up\n:wqa save and exit from vi\ngit commit -m \"message\"\ngit clean Remove extra files (e.g. *.orig). Warning: It will remove all untracked files, if you won't pass any arguments."}
{"instruction": "How can I delete a remote tag?", "output": "You can push an 'empty' reference to the remote tag name:\n\n```\ngit push origin :tagname\n\n```\n\nOr, more expressively, use the --delete option (or -d if your git version is older than 1.8.0):\n\n```\ngit push --delete origin tagname\n\n```\n\nNote that git has tag namespace and branch namespace so you may use the same name for a branch and for a tag. If you want to make sure that you cannot accidentally remove the branch instead of the tag, you can specify full ref which will never delete a branch:\n\n```\ngit push origin :refs/tags/tagname\n\n```\n\nIf you also need to delete the local tag, use:\n\n```\ngit tag --delete tagname\n\n```\n\nor\n\n```\ngit tag -d tagname\n\n```\n\n\nBackground\nPushing a branch, tag, or other ref to a remote repository involves specifying \"which repo, what source, what destination?\"\n\n```\ngit push remote-repo source-ref:destination-ref\n\n```\n\nA real world example where you push your master branch to the origin's master branch is:\n\n```\ngit push origin refs/heads/master:refs/heads/master\n\n```\n\nWhich because of default paths, can be shortened to:\n\n```\ngit push origin master:master\n\n```\n\nTags work the same way:\n\n```\ngit push origin refs/tags/release-1.0:refs/tags/release-1.0\n\n```\n\nWhich can also be shortened to:\n\n```\ngit push origin release-1.0:release-1.0\n\n```\n\nBy omitting the source ref (the part before the colon), you push 'nothing' to the destination, deleting the ref on the remote end."}
{"instruction": "Undo a Git merge that hasn&#39;t been pushed yet", "output": "With git reflog check which commit is one prior the merge (git reflog will be a better option than git log). Then you can reset it using:\n\n```\ngit reset --hard commit_sha\n\n```\n\nThere's also another way:\n\n```\ngit reset --hard HEAD~1\n\n```\n\nIt will get you back 1 commit.\nBe aware that any modified and uncommitted/unstashed files will be reset to their unmodified state. To keep them either stash changes away or see --merge option below.  \n\nAs @Velmont suggested below in his answer, in this direct case using:\n\n```\ngit reset --hard ORIG_HEAD\n\n```\n\nmight yield better results, as it should preserve your changes. ORIG_HEAD will point to a commit directly before merge has occurred, so you don't have to hunt for it yourself.\n\nA further tip is to use the --merge switch instead of --hard since it doesn't reset files unnecessarily:\n\n```\ngit reset --merge ORIG_HEAD\n\n```\n\n\n--merge\nResets the index and updates the files in the working tree that are different between <commit> and HEAD, but keeps those which are different between the index and working tree (i.e. which have changes which have not been added)."}
{"instruction": "How do I clone all remote branches?", "output": "First, clone a remote Git repository and cd into it:\n\n```\n$ git clone git://example.com/myproject\n$ cd myproject\n\n```\n\nNext, look at the local branches in your repository:\n\n```\n$ git branch\n* master\n\n```\n\nBut there are other branches hiding in your repository! See these using the -a flag:\n\n```\n$ git branch -a\n* master\n  remotes/origin/HEAD\n  remotes/origin/master\n  remotes/origin/v1.0-stable\n  remotes/origin/experimental\n\n```\n\nTo take a quick peek at an upstream branch, check it out directly:\n\n```\n$ git checkout origin/experimental\n\n```\n\nTo work on that branch, create a local tracking branch, which is done automatically by:\n\n```\n$ git checkout experimental\n\nBranch experimental set up to track remote branch experimental from origin.\nSwitched to a new branch 'experimental'\n\n```\n\nHere, \"new branch\" simply means that the branch is taken from the index and created locally for you.  As the previous line tells you, the branch is being set up to track the remote branch, which usually means the origin/branch_name branch.\nYour local branches should now show:\n\n```\n$ git branch\n* experimental\n  master\n\n```\n\nYou can track more than one remote repository using git remote:\n\n```\n$ git remote add win32 git://example.com/users/joe/myproject-win32-port\n$ git branch -a\n* master\n  remotes/origin/HEAD\n  remotes/origin/master\n  remotes/origin/v1.0-stable\n  remotes/origin/experimental\n  remotes/win32/master\n  remotes/win32/new-widgets\n\n```\n\nAt this point, things are getting pretty crazy, so run gitk to see what's going on:\n\n```\n$ gitk --all &\n\n```"}
{"instruction": "How do I update or sync a forked repository on GitHub?", "output": "In your local clone of your forked repository, you can add the original GitHub repository as a \"remote\".  (\"Remotes\" are like nicknames for the URLs of repositories - origin is one, for example.)  Then you can fetch all the branches from that upstream repository, and rebase your work to continue working on the upstream version.  In terms of commands that might look like:\n\n```\n# Add the remote, call it \"upstream\":\n\ngit remote add upstream https://github.com/whoever/whatever.git\n\n# Fetch all the branches of that remote into remote-tracking branches\n\ngit fetch upstream\n\n# Make sure that you're on your main branch:\n\ngit checkout main\n\n# Rewrite your main branch so that any commits of yours that\n# aren't already in upstream/main are replayed on top of that\n# other branch:\n\ngit rebase upstream/main\n\n```\n\nIf you don't want to rewrite the history of your main branch, (for example because other people may have cloned it) then you should replace the last command with git merge upstream/main.  However, for making further pull requests that are as clean as possible, it's probably better to rebase.\n\nIf you've rebased your branch onto upstream/main you may need to force the push in order to push it to your own forked repository on GitHub.  You'd do that with:\n\n```\ngit push -f origin main\n\n```\n\nYou only need to use the -f the first time after you've rebased."}
{"instruction": "How do I remove a submodule?", "output": "Since git1.8.3 (April 22d, 2013):\n\nThere was no Porcelain way to say \"I no longer am interested in this submodule\", once you express your interest in a submodule with \"git submodule init\".\n\"git submodule deinit\" is the way to do so.\n\nThe deletion process also uses git rm (since git1.8.5 October 2013).\nSummary\nThe 3-steps removal process would then be:\n\n```\n0. mv a/submodule a/submodule_tmp\n\n1. git submodule deinit -f -- a/submodule    \n2. rm -rf .git/modules/a/submodule\n3. git rm -f a/submodule\n# Note: a/submodule (no trailing slash)\n\n# or, if you want to leave it in your working tree and have done step 0\n3.   git rm --cached a/submodule\n3bis mv a/submodule_tmp a/submodule\n\n```\n\nExplanation\nrm -rf: This is mentioned in Daniel Schroeder's answer, and summarized by Eonil in the comments:\n\nThis leaves .git/modules/<path-to-submodule>/ unchanged.\nSo if you once delete a submodule with this method and re-add them again, it will not be possible because repository already been corrupted.\n\n\ngit rm: See commit 95c16418:\n\nCurrently using \"git rm\" on a submodule removes the submodule's work tree from that of the superproject and the gitlink from the index.\nBut the submodule's section in .gitmodules is left untouched, which is a leftover of the now removed submodule and might irritate users (as opposed to the setting in .git/config, this must stay as a reminder that the user showed interest in this submodule so it will be repopulated later when an older commit is checked out).\n\n\nLet \"git rm\" help the user by not only removing the submodule from the work tree but by also removing the \"submodule.<submodule name>\" section from the .gitmodules file and stage both.\n\n\ngit submodule deinit: It stems from this patch:\n\nWith \"git submodule init\" the user is able to tell git they care about one or more submodules and wants to have it populated on the next call to \"git submodule update\".\nBut currently there is no easy way they can tell git they do not care about a submodule anymore and wants to get rid of the local work tree (unless the user knows a lot about submodule internals and removes the \"submodule.$name.url\" setting from .git/config together with the work tree himself).\n\n\nHelp those users by providing a 'deinit' command.\nThis removes the whole submodule.<name> section from .git/config either for the given\nsubmodule(s) (or for all those which have been initialized if '.' is given).\nFail if the current work tree contains modifications unless forced.\nComplain when for a submodule given on the command line the url setting can't be found in .git/config, but nonetheless don't fail.\n\nThis takes care if the (de)initialization steps (.git/config and .git/modules/xxx)\nSince git1.8.5, the git rm takes also care of the:\n\n'add' step which records the url of a submodule in the .gitmodules file: it is need to removed for you.\nthe submodule special entry (as illustrated by this question): the git rm removes it from the index:\ngit rm --cached path_to_submodule (no trailing slash)\nThat will remove that directory stored in the index with a special mode \"160000\", marking it as a submodule root directory.\n\nIf you forget that last step, and try to add what was a submodule as a regular directory, you would get error message like:\n\n```\ngit add mysubmodule/file.txt \nPath 'mysubmodule/file.txt' is in submodule 'mysubmodule'\n\n```\n\n\nNote: since Git 2.17 (Q2 2018), git submodule deinit is no longer a shell script.\nIt is a call to a C function.\nSee commit 2e61273, commit 1342476 (14 Jan 2018) by Prathamesh Chavan (pratham-pc).\n(Merged by Junio C Hamano -- gitster -- in commit ead8dbe, 13 Feb 2018)\n\n```\ngit ${wt_prefix:+-C \"$wt_prefix\"} submodule--helper deinit \\\n  ${GIT_QUIET:+--quiet} \\\n  ${prefix:+--prefix \"$prefix\"} \\\n  ${force:+--force} \\\n  ${deinit_all:+--all} \"$@\"\n\n```"}
{"instruction": "How do I delete a commit from a branch?", "output": "Careful: git reset --hard WILL DELETE YOUR WORKING DIRECTORY CHANGES.\nBe sure to stash any local changes you want to keep before running this command.\nAssuming you are sitting on that commit, then this command will wack it...\n\n```\ngit reset --hard HEAD~1\n\n```\n\nThe HEAD~1 means the commit before head.\nOr, you could look at the output of git log, find the commit id of the commit you want to back up to, and then do this:\n\n```\ngit reset --hard <sha1-commit-id>\n\n```\n\n\nIf you already pushed it, you will need to do a force push to get rid of it...\n\n```\ngit push origin HEAD --force\n\n```\n\nHowever, if others may have pulled it, then you would be better off starting a new branch.  Because when they pull, it will just merge it into their work, and you will get it pushed back up again.\nIf you already pushed, it may be better to use git revert, to create a \"mirror image\" commit that will undo the changes.  However, both commits will be in the log.\n\nFYI: git reset --hard HEAD is great if you want to get rid of WORK IN PROGRESS.It will reset you back to the most recent commit, and erase all the changes in your working tree and index.\ngit stash does the same except you can restore it later if you need, versus permanently delete with reset hard mode. Check your stashes by using git stash list and git stash show 'stash@123'\n\nLastly, if you need to find a commit that you \"deleted\", it is typically present in git reflog unless you have garbage collected your repository."}
{"instruction": "Undoing a git rebase", "output": "The easiest way would be to find the head commit of the branch as it was immediately before the rebase started in the reflog...\n\n```\ngit reflog\n\n```\n\nand to reset the current branch to it.\nSuppose the old commit was HEAD@{2} in the ref log:\n\n```\ngit reset --soft \"HEAD@{2}\"\n\n```\n\n(If you do not want to retain the working copy changes, you can use --hard instead of --soft)\nYou can check the history of the candidate old head by just doing a git log \"HEAD@{2}\".\nIf you've not disabled per branch reflogs you should be able to simply do git reflog \"branchname@{1}\" as a rebase detaches the branch head before reattaching to the final head. I would double-check this behavior, though, as I haven't verified it recently.\nPer default, all reflogs are activated for non-bare repositories:\n\n```\n[core]\n    logAllRefUpdates = true\n\n```"}
{"instruction": "How do I get the current branch name in Git?", "output": "```\ngit branch\n\n```\n\nshould show all the local branches of your repo. The starred branch is your current branch.\n\nTo retrieve only the name of the branch you are on:\n\n```\ngit rev-parse --abbrev-ref HEAD\n\n```\n\nVersion 2.22 adds the --show-current option to ‚Äùprint the name of the current branch‚Äù. The combination also works for freshly initialized repositories before the first commit:\n\n```\ngit branch --show-current\n\n```"}
{"instruction": "How do I clone a specific Git branch?", "output": "```\ngit clone --single-branch --branch <branchname> <remote-repo>\n\n```\n\nThe --single-branch option is valid from version 1.7.10 and later.\nPlease see also the other answer which many people prefer.\nYou may also want to make sure you understand the difference. And the difference is: by invoking git clone --branch <branchname> url you're fetching all the branches and checking out one. That may, for instance, mean that your repository has a 5kB documentation or wiki branch and 5GB data branch. And whenever you want to edit your frontpage, you may end up cloning 5GB of data.\nAgain, that is not to say git clone --branch is not the way to accomplish that, it's just that it's not always what you want to accomplish, when you're asking about cloning a specific branch."}
{"instruction": "Make an existing Git branch track a remote branch?", "output": "Given a branch foo and a remote upstream:\nAs of Git 1.8.0:\n\n```\ngit branch -u upstream/foo\n\n```\n\nOr, if local branch foo is not the current branch:\n\n```\ngit branch -u upstream/foo foo\n\n```\n\nOr, if you like to type longer commands, these are equivalent to the above two:\n\n```\ngit branch --set-upstream-to=upstream/foo\n\ngit branch --set-upstream-to=upstream/foo foo\n\n```\n\nAs of Git 1.7.0 (before 1.8.0):\n\n```\ngit branch --set-upstream foo upstream/foo\n\n```\n\nNotes:\n\nAll of the above commands will cause local branch foo to track remote branch foo from remote upstream.\nThe old (1.7.x) syntax is deprecated in favor of the new (1.8+) syntax.  The new syntax is intended to be more intuitive and easier to remember.\nDefining an upstream branch will fail when run against newly-created remotes that have not already been fetched. In that case, run git fetch upstream beforehand.\n\n\nSee also: Why do I need to do `--set-upstream` all the time?"}
{"instruction": "Remove a file from a Git repository without deleting it from the local filesystem", "output": "The git rm documentation states:\n\nWhen --cached is given, the staged content has to match either the tip of the branch or the file on disk, allowing the file to be removed from just the index.\n\nSo, for a single file:\n\n```\ngit rm --cached file_to_remove.txt\n\n```\n\nand for a single directory:\n\n```\ngit rm --cached -r directory_to_remove\n\n```"}
{"instruction": "Move existing, uncommitted work to a new branch in Git", "output": "Update 2020 / Git 2.23\nGit 2.23 adds the new switch subcommand in an attempt to clear some of the confusion that comes from the overloaded usage of checkout (switching branches, restoring files, detaching HEAD, etc.)\nStarting with this version of Git, replace the checkout command with:\n\n```\ngit switch -c <new-branch>\n\n```\n\nThe behavior is identical and remains unchanged.\n\nBefore Update 2020 / Git 2.23\nUse the following:\n\n```\ngit checkout -b <new-branch>\n\n```\n\nThis will leave your current branch as it is, create and checkout a new branch and keep all your changes. You can then stage changes in files to commit with:\n\n```\ngit add <files>\n\n```\n\nand commit to your new branch with:\n\n```\ngit commit -m \"<Brief description of this commit>\"\n\n```\n\nThe changes in the working directory and changes staged in index do not belong to any branch yet. This changes the branch where those modifications would end in.\nYou don't reset your original branch, it stays as it is. The last commit on <old-branch> will still be the same. Therefore you checkout -b and then commit."}
{"instruction": "View the change history of a file using Git versioning", "output": "For a graphical view, use gitk:\n\n```\ngitk [filename]\n\n```\n\nTo follow the file across file renames:\n\n```\ngitk --follow [filename]\n\n```"}
{"instruction": "How do I stash only one file out of multiple files that have changed?", "output": "```\ngit stash push -p -m \"my commit message\"\n\n```\n\n-p let's you select the hunks that should be stashed; whole files can be selected as well.\nYou'll be prompted with a few actions for each hunk:\n\n```\n   y - stash this hunk\n   n - do not stash this hunk\n   q - quit; do not stash this hunk or any of the remaining ones\n   a - stash this hunk and all later hunks in the file\n   d - do not stash this hunk or any of the later hunks in the file\n   g - select a hunk to go to\n   / - search for a hunk matching the given regex\n   j - leave this hunk undecided, see next undecided hunk\n   J - leave this hunk undecided, see next hunk\n   k - leave this hunk undecided, see previous undecided hunk\n   K - leave this hunk undecided, see previous hunk\n   s - split the current hunk into smaller hunks\n   e - manually edit the current hunk\n   ? - print help\n\n```"}
{"instruction": "How do I create a remote Git branch?", "output": "Simple Git 2.0+ solution:\nAs of Git 2.0, the behavior has become simpler:\nYou can configure git with push.default = current to make life easier:\nI added this so now I can just push a new branch upstream with\n\n```\n$ git push -u\n\n```\n\n-u will track remote branch of the same name. Now with this configuration, you will auto-guess the remote reference to git push. From git.config documentation:\n\npush.default\nDefines the action git push should take if no refspec is explicitly given.\npush.default = current - push the current branch to update a branch with the\nsame name on the receiving end. Works in both central and non-central workflows.\n\nFor me, this is a good simplification of my day-to-day Git workflow. The configuration setting takes care of the 'usual' use case where you add a branch locally and want to create it remotely. Also, I can just as easily create local branches from remotes by just doing git co remote_branch_name (as opposed to using --set-upstream-to flag).\nI know this question and the accepted answers are rather old, but the behavior has changed so that now configuration options exist to make your workflow simpler.\nTo add to your global Git configuration, run this on the command line:\n\n```\n$ git config --global push.default current\n\n```"}
{"instruction": "Commit only part of a file&#39;s changes in Git", "output": "You can use:\n\n```\ngit add --patch <filename>\n\n```\n\nor for short:\n\n```\ngit add -p <filename>\n\n```\n\nGit will break down your file into what it thinks are sensible \"hunks\" (portions of the file). It will then prompt you with this question:\n\n```\nStage this hunk [y,n,q,a,d,/,j,J,g,s,e,?]?\n\n```\n\nHere is a description of each option:\n\ny stage this hunk for the next commit\nn do not stage this hunk for the next commit\nq quit; do not stage this hunk or any of the remaining hunks\na stage this hunk and all later hunks in the file\nd do not stage this hunk or any of the later hunks in the file\ng select a hunk to go to\n/ search for a hunk matching the given regex\nj leave this hunk undecided, see next undecided hunk\nJ leave this hunk undecided, see next hunk\nk leave this hunk undecided, see previous undecided hunk\nK leave this hunk undecided, see previous hunk\ns split the current hunk into smaller hunks\ne manually edit the current hunk\n\nYou can then edit the hunk manually by replacing +/- by # (thanks veksen)\n\n\n? print hunk help\n\nIf the file is not in the repository yet, you can first do git add -N <filename>. Afterwards you can go on with git add -p <filename>.\nAfterwards, you can use:\n\ngit diff --staged to check that you staged the correct changes\ngit reset -p to unstage mistakenly added hunks\ngit commit -v to view your commit while you edit the commit message.\n\nNote this is far different than the git format-patch command, whose purpose is to parse commit data into a .patch files.\nReference for future: Git Tools - Interactive Staging"}
{"instruction": "How do I list all the files in a commit?", "output": "Preferred Way (because it's a plumbing command; meant to be programmatic):\n\n```\n$ git diff-tree --no-commit-id --name-only bd61ad98 -r\nindex.html\njavascript/application.js\njavascript/ie6.js\n\n```\n\nAnother Way (less preferred for scripts, because it's a porcelain command; meant to be user-facing)\n\n```\n$ git show --pretty=\"\" --name-only bd61ad98    \nindex.html\njavascript/application.js\njavascript/ie6.js\n\n```\n\n\n\nThe --no-commit-id suppresses the commit ID output.\nThe --pretty argument specifies an empty format string to avoid the cruft at the beginning.\nThe --name-only argument shows only the file names that were affected (Thanks Hank). Use --name-status instead, if you want to see what happened to each file (Deleted, Modified, Added)\nThe -r argument is to recurse into sub-trees"}
{"instruction": "How do you push a tag to a remote repository using Git?", "output": "To push specific, one tag do following\ngit push origin tag_name"}
{"instruction": "Git is not working after macOS update (&quot;xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools&quot;)", "output": "The problem is that Xcode Command-line Tools needs to be updated due to a MacOs update.\n\nDid not run into this on Sonoma.\n\nMaybe Apple fixed the process?\n\n\nUpdated for Ventura\n\nAfter opening the terminal after restarting, I tried to go to my code, and do a git status, and I got an error and prompt for command line software agreement:\nSo press space until you get to the [agree, print, cancel] option, so careful hit space to scroll down to the end, if you blow past It you have to run a command to get it back. Use sudo xcodebuild -license to get to it again.\nJust be careful on scrolling down and enter agree and press return and it will launch into an update.\n\nThen I tried to use git after the install, and it prompted me to install Xcode tools again.\nI followed my own advice from previous years (see below), and went to https://developer.apple.com/download/all and downloaded\n\"Command Line Tools for Xcode 14\" (You have to log in with your Apple ID and enter MFA code, so have all the devices you need for that handy. Then select \"Command Line Tools for Xcode 14\", or if you want to get into the alphas or betas, that's up to you. But stable releases are probably the best choice for software developers.\n\nYou have to either download the tools from CLI or the developer page and before you can use git, you need to reboot!!! Or you will get stuck in a loop of prompt & downloading\nRebooting will break the loop and complete the installation of your CLI tools including git so that you can get back to work\nSolutions for previous years, these may or may not be valid these days as the downloads page has changed significantly:\nPREVIOUS YEARS SOLUTIONS, probably #2 is most helpful.\n*** Solution #1:\nGo back to your terminal and enter:\n\n```\nxcode-select --install\n\n```\n\nYou'll then receive the following output:\n\n```\nxcode-select: note: install requested for command line developer tools\n\n```\n\nYou will then be prompted in a window to update Xcode Command Line tools. (which could take a while)\nOpen a new terminal window and your development tools should be returned.\nAddition: With any major or semi-major update you'll need to update the command line tools in order to get them functioning properly again. Check Xcode with any update. This goes beyond Mojave...\nAfter that restart your terminal\nAlternatively, IF that fails, and it might.... you'll get a pop-up box saying \"Software not found on server\", proceed to solution 2.\n*** Solution #2: (Preferred method)\nIf you hit xcode-select --install and it doesn't find the software, log into Apple Developer, and install it via webpage.\nLog in or sign up here:\nhttps://developer.apple.com/download/more/\nLook for: \"Command Line Tools for Xcode 14.x\" in the list of downloads\nThen click the dmg and download. (See previous image above) either way, you will probably wind up at an apple downloads webpage."}
{"instruction": "Difference between &quot;git add -A&quot; and &quot;git add .&quot;", "output": "This answer only applies to Git version 1.x. For Git version 2.x, see other answers.\n\nSummary:\n\ngit add -A stages all changes\n\ngit add . stages new files and modifications, without deletions (on the current directory and its subdirectories).\n\ngit add -u stages modifications and deletions, without new files\n\n\n\nDetail:\ngit add -A is equivalent to  git add .; git add -u.\nThe important point about git add . is that it looks at the working tree and adds all those paths to the staged changes if they are either changed or are new and not ignored, it does not stage any 'rm' actions.\ngit add -u looks at all the already tracked files and stages the changes to those files if they are different or if they have been removed. It does not add any new files, it only stages changes to already tracked files.\ngit add -A is a handy shortcut for doing both of those.\nYou can test the differences out with something like this (note that for Git version 2.x your output for git add . git status will be different):\n\n```\ngit init\necho Change me > change-me\necho Delete me > delete-me\ngit add change-me delete-me\ngit commit -m initial\n\necho OK >> change-me\nrm delete-me\necho Add me > add-me\n\ngit status\n# Changed but not updated:\n#   modified:   change-me\n#   deleted:    delete-me\n# Untracked files:\n#   add-me\n\ngit add .\ngit status\n\n# Changes to be committed:\n#   new file:   add-me\n#   modified:   change-me\n# Changed but not updated:\n#   deleted:    delete-me\n\ngit reset\n\ngit add -u\ngit status\n\n# Changes to be committed:\n#   modified:   change-me\n#   deleted:    delete-me\n# Untracked files:\n#   add-me\n\ngit reset\n\ngit add -A\ngit status\n\n# Changes to be committed:\n#   new file:   add-me\n#   modified:   change-me\n#   deleted:    delete-me\n\n```"}
{"instruction": "How do I make git use the editor of my choice for editing commit messages?", "output": "Setting the default editor for Git\nPick one:\n\nSet core.editor in your Git config:\n\n```\ngit config --global core.editor \"vim\"\n\n```\n\n\nSet the GIT_EDITOR environment variable:\n\n```\nexport GIT_EDITOR=vim\n\n```\n\n\n\n\nSetting the default editor for all programs\nSet the standardized VISUAL and EDITOR environment variables*:\n\n```\nexport VISUAL=vim\nexport EDITOR=\"$VISUAL\"\n\n```\n\nNOTE: Setting both is not necessarily needed, but some programs may not use the more-correct VISUAL. See VISUAL vs. EDITOR.\n\nFixing compatibility issues\nSome editors require a --wait flag, or they will open a blank page. For example:\n\nSublime Text (if correctly set up; or use the full path to the executable in place of subl):\n\n```\nexport VISUAL=\"subl --wait\"\n\n```\n\n\nVS Code (after adding the shell command):\n\n```\nexport VISUAL=\"code --wait\"\n\n```"}
{"instruction": "What does cherry-picking a commit with Git mean?", "output": "Cherry-picking in Git means choosing a commit from one branch and applying it to another.\nThis contrasts with other ways such as merge and rebase which normally apply many commits to another branch.\nIt's also possible to cherry-pick multiple commits but merge is the preferred way over cherry-picking.\n\nMake sure you are on the branch you want to apply the commit to.\n\n```\ngit switch master\n\n```\n\n\nExecute the following:\n\n```\ngit cherry-pick <commit-hash>\n\n```\n\n\n\nN.B.:\n\nIf you cherry-pick from a public branch, you should consider using\n\n```\ngit cherry-pick -x <commit-hash>\n\n```\n\nThis will generate a standardized commit message. This way, you (and your co-workers) can still keep track of the origin of the commit and may avoid merge conflicts in the future.\n\nIf you have notes attached to the commit they do not follow the cherry-pick. To bring them over as well, You have to use:\n\n```\ngit notes copy <from> <to>\n\n```\n\n\n\nAdditional links:\n\ngit official guide page\ngit cherry-pick guide"}
{"instruction": "How do I modify a specific commit?", "output": "Use git rebase. For example, to modify commit bbc643cd, run:\n\n```\ngit rebase --interactive bbc643cd~\n\n```\n\nPlease note the tilde ~ at the end of the command, because you need to reapply commits on top of the previous commit of bbc643cd (i.e. bbc643cd~).\nIn the default editor, modify pick to edit in the line mentioning bbc643cd.\nSave the file and exit. git will interpret and automatically execute the commands in the file. You will find yourself in the previous situation in which you just had created commit bbc643cd.\nAt this point, bbc643cd is your last commit and you can easily amend it. Make your changes and then commit them with the command:\n\n```\ngit commit --all --amend --no-edit\n\n```\n\nAfter that, return back to the previous HEAD commit using:\n\n```\ngit rebase --continue\n\n```\n\nWARNING: Note that this will change the SHA-1 of that commit as well as all children -- in other words, this rewrites the history from that point forward. You can break repos doing this if you push using the command git push --force."}
{"instruction": "How can I change the commit author for a single commit?", "output": "Interactive rebase off of a point earlier in the history than the commit you need to modify (git rebase -i <earliercommit>). In the list of commits being rebased, change the text from pick to edit next to the hash of the one you want to modify. Then when git prompts you to change the commit, use this:\n\n```\ngit commit --amend --author=\"Author Name <email@address.com>\" --no-edit\n\n```\n\n\nFor example, if your commit history is A-B-C-D-E-F with F as HEAD, and you want to change the author of C and D, then you would...\n\nSpecify git rebase -i B (here is an example of what you will see after executing the git rebase -i B command)\n\n\nif you need to edit A, use git rebase -i --root\n\nChange the lines for both C and D from pick to edit\nExit the editor (for vim, this would be pressing Esc and then typing :wq).\nOnce the rebase started, it would first pause at C\nYou would git commit --amend --author=\"Author Name <email@address.com>\"\nThen git rebase --continue\nIt would pause again at D\nThen you would git commit --amend --author=\"Author Name <email@address.com>\" again\ngit rebase --continue\nThe rebase would complete.\nUse git push -f to update your origin with the updated commits."}
{"instruction": "How do I find and restore a deleted file in a Git repository?", "output": "Find the last commit that affected the given path. As the file isn't in the HEAD commit, that previous commit must have deleted it.\n\n```\ngit rev-list -n 1 HEAD -- <file_path>\n\n```\n\nThen checkout the version at the commit before, using the caret (^) symbol:\n\n```\ngit checkout <deleting_commit>^ -- <file_path>\n\n```\n\nOr in one command, if $file is the file in question.\n\n```\ngit checkout $(git rev-list -n 1 HEAD -- \"$file\")^ -- \"$file\"\n\n```\n\n\nIf you are using zsh and have the EXTENDED_GLOB option enabled, the caret symbol won't work. You can use ~1 instead.\n\n```\ngit checkout $(git rev-list -n 1 HEAD -- \"$file\")~1 -- \"$file\"\n\n```"}
{"instruction": "`git fetch` a remote branch", "output": "Update: Using Git Switch\nAll of the information written below was accurate, but a new command, git switch has been added that simplifies the effort.\nIf daves_branch exists on the remote repository, but not on your local branch, you can simply type:\n\n```\ngit switch daves_branch\n\n```\n\nSince you do not have the branch locally, this will automatically make switch look on the remote repo.  It will then also automatically set up remote branch tracking.\nNote that if daves_branch doesn't exist locally you'll need to git fetch first before using switch.\n\nOriginal Post\nYou need to create a local branch that tracks a remote branch. The following command will create a local branch named daves_branch, tracking the remote branch origin/daves_branch. When you push your changes the remote branch will be updated.\nFor most recent versions of Git:\n\n```\ngit checkout --track origin/daves_branch\n\n```\n\n--track is shorthand for git checkout -b [branch] [remotename]/[branch] where [remotename] is origin in this case and [branch] is twice the same, daves_branch in this case.\nFor Git 1.5.6.5 you needed this:\n\n```\ngit checkout --track -b daves_branch origin/daves_branch\n\n```\n\nFor Git 1.7.2.3 and higher, this is enough (it might have started earlier, but this is the earliest confirmation I could find quickly):\n\n```\ngit checkout daves_branch\n\n```\n\nNote that with recent Git versions, this command will not create a local branch and will put you in a 'detached HEAD' state. If you want a local branch, use the --track option.\nFull details are here: 3.5 Git Branching - Remote Branches, Tracking Branches"}
{"instruction": "How do I change the author and committer name/email for multiple commits?", "output": "This answer uses git-filter-branch, for which the docs now give this warning:\ngit filter-branch has a plethora of pitfalls that can produce non-obvious manglings of the intended history rewrite (and can leave you with little time to investigate such problems since it has such abysmal performance). These safety and performance issues cannot be backward compatibly fixed and as such, its use is not recommended. Please use an alternative history filtering tool such as git filter-repo. If you still need to use git filter-branch, please carefully read SAFETY (and PERFORMANCE) to learn about the land mines of filter-branch, and then vigilantly avoid as many of the hazards listed there as reasonably possible.\n\nChanging the author (or committer) would require rewriting all of the history.  If you're okay with that and think it's worth it then you should check out git filter-branch.  The manual page includes several examples to get you started.  Also note that you can use environment variables to change the name of the author, committer, dates, etc. -- see the \"Environment Variables\" section of the git manual page.\nSpecifically, you can fix all the wrong author names and emails for all branches and tags with this command (source: GitHub help):\n\n```\n#!/bin/sh\n\ngit filter-branch --env-filter '\nOLD_EMAIL=\"your-old-email@example.com\"\nCORRECT_NAME=\"Your Correct Name\"\nCORRECT_EMAIL=\"your-correct-email@example.com\"\nif [ \"$GIT_COMMITTER_EMAIL\" = \"$OLD_EMAIL\" ]\nthen\n    export GIT_COMMITTER_NAME=\"$CORRECT_NAME\"\n    export GIT_COMMITTER_EMAIL=\"$CORRECT_EMAIL\"\nfi\nif [ \"$GIT_AUTHOR_EMAIL\" = \"$OLD_EMAIL\" ]\nthen\n    export GIT_AUTHOR_NAME=\"$CORRECT_NAME\"\n    export GIT_AUTHOR_EMAIL=\"$CORRECT_EMAIL\"\nfi\n' --tag-name-filter cat -- --branches --tags\n\n```\n\nFor using alternative history filtering tool git filter-repo, you can first install it and construct a git-mailmap according to the format of gitmailmap.\n\n```\nProper Name <proper@email.xx> Commit Name <commit@email.xx>\n\n```\n\nAnd then run filter-repo with the created mailmap:\n\n```\ngit filter-repo --mailmap git-mailmap\n\n```"}
{"instruction": "How do I clone a Git repository into a specific folder?", "output": "Option A:\n\n```\ngit clone git@github.com:whatever folder-name\n\n```\n\nErgo, for right here use:\n\n```\ngit clone git@github.com:whatever .\n\n```\n\nOption B:\nMove the .git folder, too. Note that the .git folder is hidden in most graphical file explorers, so be sure to show hidden files.\n\n```\nmv /where/it/is/right/now/* /where/I/want/it/\nmv /where/it/is/right/now/.* /where/I/want/it/\n\n```\n\nThe first line grabs all normal files, the second line grabs dot-files. It is also possibe to do it in one line by enabling dotglob (i.e. shopt -s dotglob) but that is probably a bad solution if you are asking the question this answer answers.\nBetter yet:\nKeep your working copy somewhere else, and create a symbolic link. Like this:\n\n```\nln -s /where/it/is/right/now /the/path/I/want/to/use\n\n```\n\nFor your case this would be something like:\n\n```\nln -sfn /opt/projectA/prod/public /httpdocs/public\n\n```\n\nWhich easily could be changed to test if you wanted it, i.e.:\n\n```\nln -sfn /opt/projectA/test/public /httpdocs/public\n\n```\n\nwithout moving files around. Added -fn in case someone is copying these lines (-f is force,  -n avoid some often unwanted interactions with already and non-existing links).\nIf you just want it to work, use Option A, if someone else is going to look at what you have done, use Option C."}
{"instruction": "How do I make Git ignore file mode (chmod) changes?", "output": "Try:\n\n```\ngit config core.fileMode false\n\n```\n\nFrom git-config(1):\n\n\n```\ncore.fileMode\n    Tells Git if the executable bit of files in the working tree\n    is to be honored.\n\n    Some filesystems lose the executable bit when a file that is\n    marked as executable is checked out, or checks out a\n    non-executable file with executable bit on. git-clone(1)\n    or git-init(1) probe the filesystem to see if it handles the \n    executable bit correctly and this variable is automatically\n    set as necessary.\n\n    A repository, however, may be on a filesystem that handles\n    the filemode correctly, and this variable is set to true when\n    created, but later may be made accessible from another\n    environment that loses the filemode (e.g. exporting ext4\n    via CIFS mount, visiting a Cygwin created repository with Git\n    for Windows or Eclipse). In such a case it may be necessary\n    to set this variable to false. See git-update-index(1).\n\n    The default is true (when core.filemode is not specified\n    in the config file).\n\n```\n\n\nThe -c flag can be used to set this option for one-off commands:\n\n```\ngit -c core.fileMode=false diff\n\n```\n\nTyping the -c core.fileMode=false can be bothersome and so you can set this flag for all git repos or just for one git repo:\n\n```\n# this will set your the flag for your user for all git repos (modifies `$HOME/.gitconfig`)\n# WARNING: this will be override by local config, fileMode value is automatically selected with latest version of git.\n# This mean that if git detect your current filesystem is compatible it will set local core.fileMode to true when you clone or init a repository.\n# Tool like cygwin emulation will be detected as compatible and so your local setting WILL BE SET to true no matter what you set in global setting.\ngit config --global core.fileMode false\n\n# this will set the flag for one git repo (modifies `$current_git_repo/.git/config`)\ngit config core.fileMode false\n\n```\n\nAdditionally, git clone and git init explicitly set core.fileMode to true in the repo config as discussed in Git global core.fileMode false overridden locally on clone\nWarning\ncore.fileMode is not the best practice and should be used carefully. This setting only covers the executable bit of mode and never the read/write bits. In many cases you think you need this setting because you did something like chmod -R 777, making all your files executable. But in most projects most files don't need and should not be executable for security reasons.\nThe proper way to solve this kind of situation is to handle folder and file permission separately, with something like:\n\n```\nfind . -type d -exec chmod a+rwx {} \\; # Make folders traversable and read/write\nfind . -type f -exec chmod a+rw {} \\;  # Make files read/write\n\n```\n\nIf you do that, you'll never need to use core.fileMode, except in very rare environment."}
{"instruction": "How do I get the directory where a Bash script is located from within the script itself?", "output": "```\n#!/usr/bin/env bash\n\nSCRIPT_DIR=$( cd -- \"$( dirname -- \"${BASH_SOURCE[0]}\" )\" &> /dev/null && pwd )\n\n```\n\nis a useful one-liner which will give you the full directory name of the script no matter where it is being called from.\nIt will work as long as the last component of the path used to find the script is not a symlink (directory links are OK).  If you also want to resolve any links to the script itself, you need a multi-line solution:\n\n```\n#!/usr/bin/env bash\n\nget_script_dir()\n{\n    local SOURCE_PATH=\"${BASH_SOURCE[0]}\"\n    local SYMLINK_DIR\n    local SCRIPT_DIR\n    # Resolve symlinks recursively\n    while [ -L \"$SOURCE_PATH\" ]; do\n        # Get symlink directory\n        SYMLINK_DIR=\"$( cd -P \"$( dirname \"$SOURCE_PATH\" )\" >/dev/null 2>&1 && pwd )\"\n        # Resolve symlink target (relative or absolute)\n        SOURCE_PATH=\"$(readlink \"$SOURCE_PATH\")\"\n        # Check if candidate path is relative or absolute\n        if [[ $SOURCE_PATH != /* ]]; then\n            # Candidate path is relative, resolve to full path\n            SOURCE_PATH=$SYMLINK_DIR/$SOURCE_PATH\n        fi\n    done\n    # Get final script directory path from fully resolved source path\n    SCRIPT_DIR=\"$(cd -P \"$( dirname \"$SOURCE_PATH\" )\" >/dev/null 2>&1 && pwd)\"\n    echo \"$SCRIPT_DIR\"\n}\n\necho \"get_script_dir: $(get_script_dir)\"\n\n```\n\nThis last one will work with any combination of aliases, source, bash -c, symlinks, etc.\nBeware: if you cd to a different directory before running this snippet, the result may be incorrect!\nAlso, watch out for $CDPATH gotchas, and stderr output side effects if the user has smartly overridden cd to redirect output to stderr instead (including escape sequences, such as when calling update_terminal_cwd >&2 on Mac). Adding >/dev/null 2>&1 at the end of your cd command will take care of both possibilities.\nTo understand how it works, try running this more verbose form:\n\n```\n#!/usr/bin/env bash\n\nSOURCE=${BASH_SOURCE[0]}\nwhile [ -L \"$SOURCE\" ]; do # resolve $SOURCE until the file is no longer a symlink\n  TARGET=$(readlink \"$SOURCE\")\n  if [[ $TARGET == /* ]]; then\n    echo \"SOURCE '$SOURCE' is an absolute symlink to '$TARGET'\"\n    SOURCE=$TARGET\n  else\n    DIR=$( dirname \"$SOURCE\" )\n    echo \"SOURCE '$SOURCE' is a relative symlink to '$TARGET' (relative to '$DIR')\"\n    SOURCE=$DIR/$TARGET # if $SOURCE was a relative symlink, we need to resolve it relative to the path where the symlink file was located\n  fi\ndone\necho \"SOURCE is '$SOURCE'\"\nRDIR=$( dirname \"$SOURCE\" )\nDIR=$( cd -P \"$( dirname \"$SOURCE\" )\" >/dev/null 2>&1 && pwd )\nif [ \"$DIR\" != \"$RDIR\" ]; then\n  echo \"DIR '$RDIR' resolves to '$DIR'\"\nfi\necho \"DIR is '$DIR'\"\n\n```\n\nAnd it will print something like:\n\n```\nSOURCE './scriptdir.sh' is a relative symlink to 'sym2/scriptdir.sh' (relative to '.')\nSOURCE is './sym2/scriptdir.sh'\nDIR './sym2' resolves to '/home/ubuntu/dotfiles/fo fo/real/real1/real2'\nDIR is '/home/ubuntu/dotfiles/fo fo/real/real1/real2'\n\n```"}
{"instruction": "How do I check if a directory exists or not in a Bash shell script?", "output": "To check if a directory exists:\n\n```\nif [ -d \"$DIRECTORY\" ]; then\n  echo \"$DIRECTORY does exist.\"\nfi\n\n```\n\nTo check if a directory does not exist:\n\n```\nif [ ! -d \"$DIRECTORY\" ]; then\n  echo \"$DIRECTORY does not exist.\"\nfi\n\n```\n\n\nHowever, as Jon Ericson points out, subsequent commands may not work as intended if you do not take into account that a symbolic link to a directory will also pass this check.\nE.g. running this:\n\n```\nln -s \"$ACTUAL_DIR\" \"$SYMLINK\"\nif [ -d \"$SYMLINK\" ]; then \n  rmdir \"$SYMLINK\" \nfi\n\n```\n\nWill produce the error message:\n\n```\nrmdir: failed to remove `symlink': Not a directory\n\n```\n\nSo symbolic links may have to be treated differently, if subsequent commands expect directories:\n\n```\nif [ -d \"$LINK_OR_DIR\" ]; then \n  if [ -L \"$LINK_OR_DIR\" ]; then\n    # It is a symlink!\n    # Symbolic link specific commands go here.\n    rm \"$LINK_OR_DIR\"\n  else\n    # It's a directory!\n    # Directory command goes here.\n    rmdir \"$LINK_OR_DIR\"\n  fi\nfi\n\n```\n\n\nTake particular note of the double-quotes used to wrap the variables. The reason for this is explained by 8jean in another answer.\nIf the variables contain spaces or other unusual characters it will probably cause the script to fail."}
{"instruction": "How do I tell if a file does not exist in Bash?", "output": "The test command (written as [ here) has a \"not\" logical operator, ! (exclamation mark):\n\n```\nif [ ! -f /tmp/foo.txt ]; then\n    echo \"File not found!\"\nfi\n\n```"}
{"instruction": "Echo newline in Bash prints literal \\n", "output": "Use printf instead:\n\n```\nprintf \"hello\\nworld\\n\"\n\n```\n\nprintf behaves more consistently across different environments than echo."}
{"instruction": "How to check if a string contains a substring in Bash", "output": "You can use Marcus's answer (* wildcards) outside a case statement, too, if you use double brackets:\n\n```\nstring='My long string'\nif [[ $string == *\"My long\"* ]]; then\n  echo \"It's there!\"\nfi\n\n```\n\nNote that spaces in the needle string need to be placed between double quotes, and the * wildcards should be outside. Also note that a simple comparison operator is used (i.e. ==), not the regex operator =~."}
{"instruction": "How to concatenate string variables in Bash", "output": "```\nfoo=\"Hello\"\nfoo=\"${foo} World\"\necho \"${foo}\"\n> Hello World\n\n```\n\nIn general to concatenate two variables you can just write them one after another:\n\n```\na='Hello'\nb='World'\nc=\"${a} ${b}\"\necho \"${c}\"\n> Hello World\n\n```"}
{"instruction": "What does &quot; 2&gt;&amp;1 &quot; mean?", "output": "File descriptor 1 is the standard output (stdout).\nFile descriptor 2 is the standard error (stderr).\nAt first, 2>1 may look like a good way to redirect stderr to stdout. However, it will actually be interpreted as \"redirect stderr to a file named 1\".\n& indicates that what follows and precedes is a file descriptor, and not a filename. Thus, we use 2>&1. Consider >& to be a redirect merger operator."}
{"instruction": "How can I check if a program exists from a Bash script?", "output": "Answer\nPOSIX compatible:\n\n```\ncommand -v <the_command>\n\n```\n\nExample use:\n\n```\nif ! command -v <the_command> >/dev/null 2>&1\nthen\n    echo \"<the_command> could not be found\"\n    exit 1\nfi\n\n```\n\nFor Bash specific environments:\n\n```\nhash <the_command> # For regular commands. Or...\ntype <the_command> # To check built-ins and keywords\n\n```\n\nExplanation\nAvoid which. Not only is it an external process you're launching for doing very little (meaning builtins like hash, type or command are way cheaper), you can also rely on the builtins to actually do what you want, while the effects of external commands can easily vary from system to system.\nWhy care?\n\nMany operating systems have a which that doesn't even set an exit status, meaning the if which foo won't even work there and will always report that foo exists, even if it doesn't (note that some POSIX shells appear to do this for hash too).\nMany operating systems make which do custom and evil stuff like change the output or even hook into the package manager.\n\nSo, don't use which. Instead use one of these:\n\n```\ncommand -v foo >/dev/null 2>&1 || { echo >&2 \"I require foo but it's not installed.  Aborting.\"; exit 1; }\n\n```\n\n\n```\ntype foo >/dev/null 2>&1 || { echo >&2 \"I require foo but it's not installed.  Aborting.\"; exit 1; }\n\n```\n\n\n```\nhash foo 2>/dev/null || { echo >&2 \"I require foo but it's not installed.  Aborting.\"; exit 1; }\n\n```\n\n(Minor side-note: some will suggest 2>&- is the same 2>/dev/null but shorter ‚Äì this is untrue.  2>&- closes FD 2 which causes an error in the program when it tries to write to stderr, which is very different from successfully writing to it and discarding the output (and dangerous!))\n(Additional minor side-note: some will suggest &>/dev/null, but this is not POSIX compliant)\nIf your hash bang is /bin/sh then you should care about what POSIX says. type and hash's exit codes aren't terribly well defined by POSIX, and hash is seen to exit successfully when the command doesn't exist (haven't seen this with type yet).  command's exit status is well defined by POSIX, so that one is probably the safest to use.\nIf your script uses bash though, POSIX rules don't really matter anymore and both type and hash become perfectly safe to use. type now has a -P to search just the PATH and hash has the side-effect that the command's location will be hashed (for faster lookup next time you use it), which is usually a good thing since you probably check for its existence in order to actually use it.\nAs a simple example, here's a function that runs gdate if it exists, otherwise date:\n\n```\ngnudate() {\n    if hash gdate 2>/dev/null; then\n        gdate \"$@\"\n    else\n        date \"$@\"\n    fi\n}\n\n```"}
{"instruction": "How do I split a string on a delimiter in Bash?", "output": "You can set the internal field separator (IFS) variable, and then let it parse into an array. When this happens in a command, then the assignment to IFS only takes place to that single command's environment (to read ). It then parses the input according to the IFS variable value into an array, which we can then iterate over.\nThis example will parse one line of items separated by ;, pushing it into an array:\n\n```\nIFS=';' read -ra ADDR <<< \"$IN\"\nfor i in \"${ADDR[@]}\"; do\n  # process \"$i\"\ndone\n\n```\n\nThis other example is for processing the whole content of $IN, each time one line of input separated by ;:\n\n```\nwhile IFS=';' read -ra ADDR; do\n  for i in \"${ADDR[@]}\"; do\n    # process \"$i\"\n  done\ndone <<< \"$IN\"\n\n```"}
{"instruction": "Extract filename and extension in Bash", "output": "First, get file name without the path:\n\n```\nfilename=$(basename -- \"$fullfile\")\nextension=\"${filename##*.}\"\nfilename=\"${filename%.*}\"\n\n```\n\nAlternatively, you can focus on the last '/' of the path instead of the '.' which should work even if you have unpredictable file extensions:\n\n```\nfilename=\"${fullfile##*/}\"\n\n```\n\nYou may want to check the documentation :\n\nOn the web at section \"3.5.3 Shell Parameter Expansion\"\nIn the bash manpage at section called \"Parameter Expansion\""}
{"instruction": "How do I parse command line arguments in Bash?", "output": "Bash Space-Separated (e.g., --option argument)\n\n```\ncat >/tmp/demo-space-separated.sh <<'EOF'\n#!/bin/bash\n\nPOSITIONAL_ARGS=()\n\nwhile [[ $# -gt 0 ]]; do\n  case $1 in\n    -e|--extension)\n      EXTENSION=\"$2\"\n      shift # past argument\n      shift # past value\n      ;;\n    -s|--searchpath)\n      SEARCHPATH=\"$2\"\n      shift # past argument\n      shift # past value\n      ;;\n    --default)\n      DEFAULT=YES\n      shift # past argument\n      ;;\n    -*|--*)\n      echo \"Unknown option $1\"\n      exit 1\n      ;;\n    *)\n      POSITIONAL_ARGS+=(\"$1\") # save positional arg\n      shift # past argument\n      ;;\n  esac\ndone\n\nset -- \"${POSITIONAL_ARGS[@]}\" # restore positional parameters\n\necho \"FILE EXTENSION  = ${EXTENSION}\"\necho \"SEARCH PATH     = ${SEARCHPATH}\"\necho \"DEFAULT         = ${DEFAULT}\"\necho \"Number files in SEARCH PATH with EXTENSION:\" $(ls -1 \"${SEARCHPATH}\"/*.\"${EXTENSION}\" | wc -l)\n\nif [[ -n $1 ]]; then\n    echo \"Last line of file specified as non-opt/last argument:\"\n    tail -1 \"$1\"\nfi\nEOF\n\nchmod +x /tmp/demo-space-separated.sh\n\n/tmp/demo-space-separated.sh -e conf -s /etc /etc/hosts\n\n```\n\nOutput from copy-pasting the block above\n\n```\nFILE EXTENSION  = conf\nSEARCH PATH     = /etc\nDEFAULT         =\nNumber files in SEARCH PATH with EXTENSION: 14\nLast line of file specified as non-opt/last argument:\n#93.184.216.34    example.com\n\n```\n\nUsage\n\n```\ndemo-space-separated.sh -e conf -s /etc /etc/hosts\n\n```\n\n\nBash Equals-Separated (e.g., --option=argument)\n\n```\ncat >/tmp/demo-equals-separated.sh <<'EOF'\n#!/bin/bash\n\nfor i in \"$@\"; do\n  case $i in\n    -e=*|--extension=*)\n      EXTENSION=\"${i#*=}\"\n      shift # past argument=value\n      ;;\n    -s=*|--searchpath=*)\n      SEARCHPATH=\"${i#*=}\"\n      shift # past argument=value\n      ;;\n    --default)\n      DEFAULT=YES\n      shift # past argument with no value\n      ;;\n    -*|--*)\n      echo \"Unknown option $i\"\n      exit 1\n      ;;\n    *)\n      ;;\n  esac\ndone\n\necho \"FILE EXTENSION  = ${EXTENSION}\"\necho \"SEARCH PATH     = ${SEARCHPATH}\"\necho \"DEFAULT         = ${DEFAULT}\"\necho \"Number files in SEARCH PATH with EXTENSION:\" $(ls -1 \"${SEARCHPATH}\"/*.\"${EXTENSION}\" | wc -l)\n\nif [[ -n $1 ]]; then\n    echo \"Last line of file specified as non-opt/last argument:\"\n    tail -1 $1\nfi\nEOF\n\nchmod +x /tmp/demo-equals-separated.sh\n\n/tmp/demo-equals-separated.sh -e=conf -s=/etc /etc/hosts\n\n```\n\nOutput from copy-pasting the block above\n\n```\nFILE EXTENSION  = conf\nSEARCH PATH     = /etc\nDEFAULT         =\nNumber files in SEARCH PATH with EXTENSION: 14\nLast line of file specified as non-opt/last argument:\n#93.184.216.34    example.com\n\n```\n\nUsage\n\n```\ndemo-equals-separated.sh -e=conf -s=/etc /etc/hosts\n\n```\n\n\nTo better understand ${i#*=} search for \"Substring Removal\" in this guide. It is functionally equivalent to `sed 's/[^=]*=//' <<< \"$i\"` which calls a needless subprocess or `echo \"$i\" | sed 's/[^=]*=//'` which calls two needless subprocesses.\n\nUsing bash with getopt[s]\ngetopt(1) limitations (older, relatively-recent getopt versions):\n\ncan't handle arguments that are empty strings\ncan't handle arguments with embedded whitespace\n\nMore recent getopt versions don't have these limitations. For more information, see these docs.\n\nPOSIX getopts\nAdditionally, the POSIX shell and others offer getopts which doen't have these limitations. I've included a simplistic getopts example.\n\n```\ncat >/tmp/demo-getopts.sh <<'EOF'\n#!/bin/sh\n\n# A POSIX variable\nOPTIND=1         # Reset in case getopts has been used previously in the shell.\n\n# Initialize our own variables:\noutput_file=\"\"\nverbose=0\n\nwhile getopts \"h?vf:\" opt; do\n  case \"$opt\" in\n    h|\\?)\n      show_help\n      exit 0\n      ;;\n    v)  verbose=1\n      ;;\n    f)  output_file=$OPTARG\n      ;;\n  esac\ndone\n\nshift $((OPTIND-1))\n\n[ \"${1:-}\" = \"--\" ] && shift\n\necho \"verbose=$verbose, output_file='$output_file', Leftovers: $@\"\nEOF\n\nchmod +x /tmp/demo-getopts.sh\n\n/tmp/demo-getopts.sh -vf /etc/hosts foo bar\n\n```\n\nOutput from copy-pasting the block above\n\n```\nverbose=1, output_file='/etc/hosts', Leftovers: foo bar\n\n```\n\nUsage\n\n```\ndemo-getopts.sh -vf /etc/hosts foo bar\n\n```\n\nThe advantages of getopts are:\n\nIt's more portable, and will work in other shells like dash.\nIt can handle multiple single options like -vf filename in the typical Unix way, automatically.\n\nThe disadvantage of getopts is that it can only handle short options (-h, not --help) without additional code.\nThere is a getopts tutorial which explains what all of the syntax and variables mean.  In bash, there is also help getopts, which might be informative."}
{"instruction": "How do I set a variable to the output of a command in Bash?", "output": "In addition to backticks `command`, command substitution can be done with $(command) or \"$(command)\", which I find easier to read, and allows for nesting.\n\n```\nOUTPUT=\"$(ls -1)\"\necho \"${OUTPUT}\"\n\nMULTILINE=\"$(ls \\\n   -1)\"\necho \"${MULTILINE}\"\n\n```\n\nQuoting (\") does matter to preserve multi-line variable values and it is safer to use with whitespace and special characters such as (*) and therefore advised; it is, however, optional on the right-hand side of an assignment when word splitting is not performed, so OUTPUT=$(ls -1) would work fine."}
{"instruction": "How to check if a variable is set in Bash", "output": "(Usually) The right way\n\n```\nif [ -z ${var+x} ]; then echo \"var is unset\"; else echo \"var is set to '$var'\"; fi\n\n```\n\nwhere ${var+x} is a parameter expansion which evaluates to nothing if var is unset, and substitutes the string x otherwise.\nQuotes Digression\nQuotes can be omitted (so we can say ${var+x} instead of \"${var+x}\") because this syntax & usage guarantees this will only expand to something that does not require quotes (since it either expands to x (which contains no word breaks so it needs no quotes), or to nothing (which results in [ -z  ], which conveniently evaluates to the same value (true) that [ -z \"\" ] does as well)).\nHowever, while quotes can be safely omitted, and it was not immediately obvious to all (it wasn't even apparent to the first author of this quotes explanation who is also a major Bash coder), it would sometimes be better to write the solution with quotes as [ -z \"${var+x}\" ], at the very small possible cost of an O(1) speed penalty.  The first author also added this as a comment next to the code using this solution giving the URL to this answer, which now also includes the explanation for why the quotes can be safely omitted.\n(Often) The wrong way\n\n```\nif [ -z \"$var\" ]; then echo \"var is blank\"; else echo \"var is set to '$var'\"; fi\n\n```\n\nThis is often wrong because it doesn't distinguish between a variable that is unset and a variable that is set to the empty string. That is to say, if var='', then the above solution will output \"var is blank\".\nThe distinction between unset and \"set to the empty string\" is essential in situations where the user has to specify an extension, or additional list of properties, and that not specifying them defaults to a non-empty value, whereas specifying the empty string should make the script use an empty extension or list of additional properties.\nThe distinction may not be essential in every scenario though. In those cases  [ -z \"$var\" ] will be just fine."}
{"instruction": "Loop through an array of strings in Bash?", "output": "You can use it like this:\n\n```\n## declare an array variable\ndeclare -a arr=(\"element1\" \"element2\" \"element3\")\n\n## now loop through the above array\nfor i in \"${arr[@]}\"\ndo\n   echo \"$i\"\n   # or do whatever with individual element of the array\ndone\n\n# You can access them using echo \"${arr[0]}\", \"${arr[1]}\" also\n\n```\n\nAlso works for multi-line array declaration\n\n```\ndeclare -a arr=(\"element1\" \n                \"element2\" \"element3\"\n                \"element4\"\n                )\n\n```"}
{"instruction": "How to reload .bashrc settings without logging out and back in again?", "output": "You can enter the long form command:\n\n```\nsource ~/.bashrc\n\n```\n\nor you can use the shorter version of the command:\n\n```\n. ~/.bashrc\n\n```"}
{"instruction": "How do I iterate over a range of numbers defined by variables in Bash?", "output": "```\nfor i in $(seq 1 $END); do echo $i; done\n```\n\nedit: I prefer seq over the other methods because I can actually remember it ;)"}
{"instruction": "Looping through the content of a file in Bash", "output": "One way to do it is:\n\n```\nwhile read p; do\n  echo \"$p\"\ndone <peptides.txt\n\n```\n\nAs pointed out in the comments, this has the side effects of trimming leading whitespace, interpreting backslash sequences, and skipping the last line if it's missing a terminating linefeed. If these are concerns, you can do:\n\n```\nwhile IFS=\"\" read -r p || [ -n \"$p\" ]\ndo\n  printf '%s\\n' \"$p\"\ndone < peptides.txt\n\n```\n\n\nExceptionally, if the loop body may read from standard input, you can open the file using a different file descriptor:\n\n```\nwhile read -u 10 p; do\n  ...\ndone 10<peptides.txt\n\n```\n\nHere, 10 is just an arbitrary number (different from 0, 1, 2)."}
{"instruction": "How can I count all the lines of code in a directory recursively?", "output": "Try:\n\n```\nfind . -name '*.php' | xargs wc -l\n\n```\n\nor (when file names include special characters such as spaces)\n\n```\nfind . -name '*.php' | sed 's/.*/\"&\"/' | xargs  wc -l\n\n```\n\nThe SLOCCount tool may help as well.\nIt will give an accurate source lines of code count for whatever\nhierarchy you point it at, as well as some additional stats.\nSorted output:\nfind . -name '*.php' | xargs wc -l | sort -nr"}
{"instruction": "How to redirect and append both standard output and standard error to a file with Bash", "output": "```\ncmd >>file.txt 2>&1\n\n```\n\nBash executes the redirects from left to right as follows:\n\n>>file.txt: Open file.txt in append mode and redirect stdout there.\n2>&1: Redirect stderr to \"where stdout is currently going\". In this case, that is a file opened in append mode. In other words, the &1 reuses the file descriptor which stdout currently uses."}
{"instruction": "Check existence of input argument in a Bash shell script", "output": "It is:\n\n```\nif [ $# -eq 0 ]\n  then\n    echo \"No arguments supplied\"\nfi\n\n```\n\nThe $# variable will tell you the number of input arguments the script was passed.\nOr you can check if an argument is an empty string or not like:\n\n```\nif [ -z \"$1\" ]\n  then\n    echo \"No argument supplied\"\nfi\n\n```\n\nThe -z switch will test if the expansion of \"$1\" is a null string or not. If it is a null string then the body is executed."}
{"instruction": "How do I prompt for Yes/No/Cancel input in a Linux shell script?", "output": "A widely available method to get user input at a shell prompt is the read command. Here is a demonstration:\n\n```\nwhile true; do\n    read -p \"Do you wish to install this program? \" yn\n    case $yn in\n        [Yy]* ) make install; break;;\n        [Nn]* ) exit;;\n        * ) echo \"Please answer yes or no.\";;\n    esac\ndone\n\n```\n\n\nAnother method, pointed out by Steven Huwig, is Bash's select command. Here is the same example using select:\n\n```\necho \"Do you wish to install this program?\"\nselect yn in \"Yes\" \"No\"; do\n    case $yn in\n        Yes ) make install; break;;\n        No ) exit;;\n    esac\ndone\n\n```\n\nWith select you don't need to sanitize the input ‚Äì it displays the available choices, and you type a number corresponding to your choice. It also loops automatically, so there's no need for a while true loop to retry if they give invalid input. If you want to allow more flexible input (accepting the words of the options, rather than just their number), you can alter it like this:\n\n```\necho \"Do you wish to install this program?\"\nselect strictreply in \"Yes\" \"No\"; do\n    relaxedreply=${strictreply:-$REPLY}\n    case $relaxedreply in\n        Yes | yes | y ) make install; break;;\n        No  | no  | n ) exit;;\n    esac\ndone\n\n```\n\n\nAlso, L√©a Gris demonstrated a way to make the request language agnostic in her answer. Adapting my first example to better serve multiple languages might look like this:\n\n```\nset -- $(locale LC_MESSAGES)\nyesexpr=\"$1\"; noexpr=\"$2\"; yesword=\"$3\"; noword=\"$4\"\n\nwhile true; do\n    read -p \"Install (${yesword} / ${noword})? \" yn\n    if [[ \"$yn\" =~ $yesexpr ]]; then make install; exit; fi\n    if [[ \"$yn\" =~ $noexpr ]]; then exit; fi\n    echo \"Answer ${yesword} / ${noword}.\"\ndone\n\n```\n\nObviously other communication strings remain untranslated here (Install, Answer) which would need to be addressed in a more fully completed translation, but even a partial translation would be helpful in many cases.\n\nFinally, please check out the excellent answer by F. Hauri."}
{"instruction": "How to specify the private SSH-key to use when executing shell command on Git?", "output": "Something like this should work (suggested by orip):\n\n```\nssh-agent bash -c 'ssh-add /somewhere/yourkey; git clone git@github.com:user/project.git'\n\n```\n\nif you prefer subshells, you could try the following (though it is more fragile):\n\n```\nssh-agent $(ssh-add /somewhere/yourkey; git clone git@github.com:user/project.git)\n\n```\n\nGit will invoke SSH which will find its agent by environment variable; this will, in turn, have the key loaded.\nAlternatively, setting HOME may also do the trick, provided you are willing to setup a directory that contains only a .ssh directory as HOME; this may either contain an identity.pub, or a config file setting IdentityFile."}
{"instruction": "Make a Bash alias that takes a parameter?", "output": "Bash alias does not directly accept parameters. You will have to create a function.\nalias does not accept parameters but a function can be called just like an alias. For example:\n\n```\nmyfunction() {\n    #do things with parameters like $1 such as\n    mv \"$1\" \"$1.bak\"\n    cp \"$2\" \"$1\"\n}\n\n\nmyfunction old.conf new.conf #calls `myfunction`\n\n```\n\nBy the way, Bash functions defined in your .bashrc and other files are available as commands within your shell. So for instance you can call the earlier function like this \n\n```\n$ myfunction original.conf my.conf\n\n```"}
{"instruction": "How to convert a string to lower case in Bash", "output": "There are various ways:\nPOSIX standard\ntr\n\n```\n$ echo \"$a\" | tr '[:upper:]' '[:lower:]'\nhi all\n\n```\n\nAWK\n\n```\n$ echo \"$a\" | awk '{print tolower($0)}'\nhi all\n\n```\n\nNon-POSIX\nYou may run into portability issues with the following examples:\nBash 4.0\n\n```\n$ echo \"${a,,}\"\nhi all\n\n```\n\nsed\n\n```\n$ echo \"$a\" | sed -e 's/\\(.*\\)/\\L\\1/'\nhi all\n# this also works:\n$ sed -e 's/\\(.*\\)/\\L\\1/' <<< \"$a\"\nhi all\n\n```\n\nPerl\n\n```\n$ echo \"$a\" | perl -ne 'print lc'\nhi all\n\n```\n\nBash\n\n```\nlc(){\n    case \"$1\" in\n        [A-Z])\n        n=$(printf \"%d\" \"'$1\")\n        n=$((n+32))\n        printf \\\\$(printf \"%o\" \"$n\")\n        ;;\n        *)\n        printf \"%s\" \"$1\"\n        ;;\n    esac\n}\nword=\"I Love Bash\"\nfor((i=0;i<${#word};i++))\ndo\n    ch=\"${word:$i:1}\"\n    lc \"$ch\"\ndone\n\n```\n\nNote: YMMV on this one. Doesn't work for me (GNU bash version 4.2.46 and 4.0.33 (and same behaviour 2.05b.0 but nocasematch  is not implemented)) even with using shopt -u nocasematch;. Unsetting that nocasematch causes [[ \"fooBaR\" == \"FOObar\" ]] to match OK BUT inside case weirdly [b-z] are incorrectly matched by [A-Z]. Bash is confused by the double-negative (\"unsetting nocasematch\")! :-)"}
{"instruction": "echo that outputs to stderr", "output": "You could do this, which facilitates reading:\n\n```\n>&2 echo \"error\"\n\n```\n\n>&2 copies file descriptor #2 to file descriptor #1. Therefore, after this redirection is performed, both file descriptors will refer to the same file: the one file descriptor #2 was originally referring to. For more information see the Bash Hackers Illustrated Redirection Tutorial."}
{"instruction": "How to redirect output to a file and stdout", "output": "The command you want is named tee:\n\n```\nfoo | tee output.file\n\n```\n\nFor example, if you only care about stdout:\n\n```\nls -a | tee output.file\n\n```\n\nIf you want to include stderr, do:\n\n```\nprogram [arguments...] 2>&1 | tee outfile\n\n```\n\n2>&1 redirects channel 2 (stderr/standard error) into channel 1 (stdout/standard output), such that both is written as stdout. It is also directed to the given output file as of the tee command.\nFurthermore, if you want to append to the log file, use tee -a as:\n\n```\nprogram [arguments...] 2>&1 | tee -a outfile\n\n```"}
{"instruction": "YYYY-MM-DD format date in shell script", "output": "In bash (>=4.2) it is preferable to use printf's built-in date formatter (part of bash) rather than the external date (usually GNU date). Note that invoking a subshell has performance problems in Cygwin due to a slow fork() call on Windows.\nAs such:\n\n```\n# put current date as yyyy-mm-dd in $date\n# -1 -> explicit current date, bash >=4.3 defaults to current time if not provided\n# -2 -> start time for shell\nprintf -v date '%(%Y-%m-%d)T\\n' -1\n\n# put current date as yyyy-mm-dd HH:MM:SS in $date\nprintf -v date '%(%Y-%m-%d %H:%M:%S)T\\n' -1\n\n# to print directly remove -v flag, as such:\nprintf '%(%Y-%m-%d)T\\n' -1\n# -> current date printed to terminal\n\n```\n\nIn bash (<4.2):\n\n```\n# put current date as yyyy-mm-dd in $date\ndate=$(date '+%Y-%m-%d')\n\n# put current date as yyyy-mm-dd HH:MM:SS in $date\ndate=$(date '+%Y-%m-%d %H:%M:%S')\n\n# print current date directly\necho $(date '+%Y-%m-%d')\n\n```\n\nOther available date formats can be viewed from the date man pages (for external non-bash specific command):\n\n```\nman date\n\n```"}
{"instruction": "Passing parameters to a Bash function", "output": "There are two typical ways of declaring a function. I prefer the second approach.\n\n```\nfunction function_name {\n   command...\n} \n\n```\n\nor\n\n```\nfunction_name () {\n   command...\n} \n\n```\n\nTo call a function with arguments:\n\n```\nfunction_name \"$arg1\" \"$arg2\"\n\n```\n\nThe function refers to passed arguments by their position (not by name), that is $1, $2, and so forth. $0 is the name of the script itself.\nExample:\n\n```\nfunction_name () {\n   echo \"Parameter #1 is $1\"\n}\n\n```\n\nAlso, you need to call your function after it is declared.\n\n```\n#!/usr/bin/env sh\n\nfoo 1  # this will fail because foo has not been declared yet.\n\nfoo() {\n    echo \"Parameter #1 is $1\"\n}\n\nfoo 2 # this will work.\n\n```\n\nOutput:\n\n```\n./myScript.sh: line 2: foo: command not found\nParameter #1 is 2\n\n```\n\nReference: Advanced Bash-Scripting Guide."}
{"instruction": "How can I declare and use Boolean variables in a shell script?", "output": "Revised Answer (Feb 12, 2014)\n\n```\nthe_world_is_flat=true\n# ...do something interesting...\nif [ \"$the_world_is_flat\" = true ] ; then\n    echo 'Be careful not to fall off!'\nfi\n\n```\n\n\nOriginal Answer\nCaveats: https://stackoverflow.com/a/21210966/89391\n\n```\nthe_world_is_flat=true\n# ...do something interesting...\nif $the_world_is_flat ; then\n    echo 'Be careful not to fall off!'\nfi\n\n```\n\nFrom: Using boolean variables in Bash\nThe reason the original answer is included here is because the comments before the revision on Feb 12, 2014 pertain only to the original answer, and many of the comments are wrong when associated with the revised answer. For example, Dennis Williamson's comment about Bash's builtin true on Jun 2, 2010 only applies to the original answer, not the revised."}
{"instruction": "How to escape single quotes within single quoted strings", "output": "If you really want to use single quotes in the outermost layer, remember that you can glue both kinds of quotation. Example:\n\n```\n alias rxvt='urxvt -fg '\"'\"'#111111'\"'\"' -bg '\"'\"'#111111'\"'\"\n #                     ^^^^^       ^^^^^     ^^^^^       ^^^^\n #                     12345       12345     12345       1234\n\n```\n\nExplanation of how '\"'\"' is interpreted as just ':\n\n' End first quotation which uses single quotes.\n\" Start second quotation, using double-quotes.\n' Quoted character.\n\" End second quotation, using double-quotes.\n' Start third quotation, using single quotes.\n\nIf you do not place any whitespaces between (1) and (2), or between (4) and (5), the shell will interpret that string as a one long word:\n\n```\n$ echo 'abc''123'  \nabc123\n$ echo 'abc'\\''123'\nabc'123\n$ echo 'abc'\"'\"'123'\nabc'123\n\n```\n\nIt will also keep the internal representation with 'to be joined' strings, and will also prefer the shorter escape syntax when possible:\n\n```\n$ alias test='echo '\"'\"'hi'\"'\"\n$ alias test\nalias test='echo '\\''hi'\\'''\n$ test\nhi\n\n```"}
{"instruction": "Assigning default values to shell variables with a single command in bash", "output": "Very close to what you posted, actually. You can use something called Bash parameter expansion to accomplish this.\nTo get the assigned value, or default if it's missing:\n\n```\nFOO=\"${VARIABLE:-default}\"  # FOO will be assigned 'default' value if VARIABLE not set or null.\n# The value of VARIABLE remains untouched.\n\n```\n\nTo do the same, as well as assign default to VARIABLE:\n\n```\nFOO=\"${VARIABLE:=default}\"  # If VARIABLE not set or null, set its value to 'default'. \n# Then that value will be assigned to FOO\n\n```"}
{"instruction": "Count number of lines in a non binary file (Like a CSV or a TXT) file in terminal", "output": "Use wc:\n\n```\nwc -l <filename>\n\n```\n\nThis will output the number of lines in <filename>:\n\n```\n$ wc -l /dir/file.txt\n3272485 /dir/file.txt\n\n```\n\nOr, to omit the <filename> from the result use wc -l < <filename>:\n\n```\n$ wc -l < /dir/file.txt\n3272485\n\n```\n\nYou can also pipe data to wc as well:\n\n```\n$ cat /dir/file.txt | wc -l\n3272485\n$ curl yahoo.com --silent | wc -l\n63\n\n```"}
{"instruction": "Replace one substring for another string in shell script", "output": "To replace the first occurrence of a pattern with a given string, use ${parameter/pattern/string}:\n\n```\n#!/bin/bash\nfirstString=\"I love Suzi and Marry\"\nsecondString=\"Sara\"\necho \"${firstString/Suzi/\"$secondString\"}\"\n# prints 'I love Sara and Marry'\n\n```\n\nTo replace all occurrences, use ${parameter//pattern/string}:\n\n```\nmessage='The secret code is 12345'\necho \"${message//[0-9]/X}\"\n# prints 'The secret code is XXXXX'\n\n```\n\n(This is documented in the Bash Reference Manual, ¬ß3.5.3 \"Shell Parameter Expansion\".)\nNote that this feature is not specified by POSIX ‚Äî it's a Bash extension ‚Äî so not all Unix shells implement it. For the relevant POSIX documentation, see The Open Group Technical Standard Base Specifications, Issue 7, the Shell & Utilities volume, ¬ß2.6.2 \"Parameter Expansion\"."}
{"instruction": "Listing only directories using ls in Bash?", "output": "*/ is a pattern that matches all of the subdirectories in the current directory (* would match all files and subdirectories; the / restricts it to directories). Similarly, to list all subdirectories under /home/alice/Documents, use ls -d /home/alice/Documents/*/"}
{"instruction": "How to echo shell commands as they are executed", "output": "set -x or set -o xtrace expands variables and prints a little + sign before the line.\nset -v or set -o verbose does not expand the variables before printing.\nUse set +x and set +v to turn off the above settings.\nOn the first line of the script, one can put #!/bin/sh -x (or -v) to have the same effect as set -x (or -v) later in the script.\nThe above also works with /bin/sh.\nSee the bash-hackers' wiki on set attributes, and on debugging.\n\n```\n$ cat shl\n#!/bin/bash                                                                     \n\nDIR=/tmp/so\nls $DIR\n\n$ bash -x shl \n+ DIR=/tmp/so\n+ ls /tmp/so\n$\n\n```"}
{"instruction": "How to compare strings in Bash", "output": "Using variables in if statements\n\n```\nif [ \"$x\" = \"valid\" ]; then\n  echo \"x has the value 'valid'\"\nfi\n\n```\n\nIf you want to do something when they don't match, replace = with !=. You can read more about string operations and arithmetic operations in their respective documentation.\nWhy do we use quotes around $x?\nYou want the quotes around $x, because if it is empty, your Bash script encounters a syntax error as seen below:\n\n```\nif [ = \"valid\" ]; then\n\n```\n\n\nNon-standard use of == operator\nNote that Bash allows == to be used for equality with [, but this is not standard.\nUse either the first case wherein the quotes around $x are optional:\n\n```\nif [[ \"$x\" == \"valid\" ]]; then\n\n```\n\nor use the second case:\n\n```\nif [ \"$x\" = \"valid\" ]; then\n\n```"}
{"instruction": "Pipe to/from the clipboard in a Bash script", "output": "2018 answer\nUse clipboard-cli. It works with macOS, Windows, Linux, OpenBSD, FreeBSD, and Android without any real issues.\nInstall it with:\n\n```\nnpm install -g clipboard-cli\n\n```\n\nThen you can do:\n\n```\necho foo | clipboard \n\n```\n\nIf you want, you can alias to cb by putting the following in your .bashrc, .bash_profile, or .zshrc:\n\n```\nalias cb=clipboard\n\n```"}
{"instruction": "How can I pipe stderr, and not stdout?", "output": "First redirect stderr to stdout ‚Äî the pipe; then redirect stdout to /dev/null (without changing where stderr is going):\n\n```\ncommand 2>&1 >/dev/null | grep 'something'\n\n```\n\nFor the details of I/O redirection in all its variety, see the chapter on Redirections in the Bash reference manual.\nNote that the sequence of I/O redirections is interpreted left-to-right, but pipes are set up before the I/O redirections are interpreted.  File descriptors such as 1 and 2 are references to open file descriptions.  The operation 2>&1 makes file descriptor 2 aka stderr refer to the same open file description as file descriptor 1 aka stdout is currently referring to (see dup2() and open()).  The operation >/dev/null then changes file descriptor 1 so that it refers to an open file description for /dev/null, but that doesn't change the fact that file descriptor 2 refers to the open file description which file descriptor 1 was originally pointing to ‚Äî namely, the pipe."}
{"instruction": "Parsing JSON with Unix tools", "output": "There are a number of tools specifically designed for the purpose of manipulating JSON from the command line, and will be a lot easier and more reliable than doing it with Awk, such as jq:\n\n```\ncurl -s 'https://api.github.com/users/lambda' | jq -r '.name'\n\n```\n\nYou can also do this with tools that are likely already installed on your system, like Python using the json module, and so avoid any extra dependencies, while still having the benefit of a proper JSON parser. The following assume you want to use UTF-8, which the original JSON should be encoded in and is what most modern terminals use as well:\nPython 3:\n\n```\ncurl -s 'https://api.github.com/users/lambda' | \\\n    python3 -c \"import sys, json; print(json.load(sys.stdin)['name'])\"\n\n```\n\nPython 2:\n\n```\nexport PYTHONIOENCODING=utf8\ncurl -s 'https://api.github.com/users/lambda' | \\\n    python2 -c \"import sys, json; print json.load(sys.stdin)['name']\"\n\n```\n\nFrequently Asked Questions\nWhy not a pure shell solution?\nThe standard POSIX/Single Unix Specification shell is a very limited language which doesn't contain facilities for representing sequences (list or arrays) or associative arrays (also known as hash tables, maps, dicts, or objects in some other languages). This makes representing the result of parsing JSON somewhat tricky in portable shell scripts. There are somewhat hacky ways to do it, but many of them can break if keys or values contain certain special characters.\nBash 4 and later, zsh, and ksh have support for arrays and associative arrays, but these shells are not universally available (macOS stopped updating Bash at Bash 3, due to a change from GPLv2 to GPLv3, while many Linux systems don't have zsh installed out of the box). It's possible that you could write a script that would work in either Bash 4 or zsh, one of which is available on most macOS, Linux, and BSD systems these days, but it would be tough to write a shebang line that worked for such a polyglot script.\nFinally, writing a full fledged JSON parser in shell would be a significant enough dependency that you might as well just use an existing dependency like jq or Python instead. It's not going to be a one-liner, or even small five-line snippet, to do a good implementation.\nWhy not use awk, sed, or grep?\nIt is possible to use these tools to do some quick extraction from JSON with a known shape and formatted in a known way, such as one key per line. There are several examples of suggestions for this in other answers.\nHowever, these tools are designed for line based or record based formats; they are not designed for recursive parsing of matched delimiters with possible escape characters.\nSo these quick and dirty solutions using awk/sed/grep are likely to be fragile, and break if some aspect of the input format changes, such as collapsing whitespace, or adding additional levels of nesting to the JSON objects, or an escaped quote within a string. A solution that is robust enough to handle all JSON input without breaking will also be fairly large and complex, and so not too much different than adding another dependency on jq or Python.\nI have had to deal with large amounts of customer data being deleted due to poor input parsing in a shell script before, so I never recommend quick and dirty methods that may be fragile in this way. If you're doing some one-off processing, see the other answers for suggestions, but I still highly recommend just using an existing tested JSON parser.\nHistorical notes\nThis answer originally recommended jsawk, which should still work, but is a little more cumbersome to use than jq, and depends on a standalone JavaScript interpreter being installed which is less common than a Python interpreter, so the above answers are probably preferable:\n\n```\ncurl -s 'https://api.github.com/users/lambda' | jsawk -a 'return this.name'\n\n```\n\nThis answer also originally used the Twitter API from the question, but that API no longer works, making it hard to copy the examples to test out, and the new Twitter API requires API keys, so I've switched to using the GitHub API which can be used easily without API keys.  The first answer for the original question would be:\n\n```\ncurl 'http://twitter.com/users/username.json' | jq -r '.text'\n\n```"}
{"instruction": "Propagate all arguments in a Bash shell script", "output": "Use \"$@\" instead of plain $@ if you actually wish your parameters to be passed the same.\nObserve:\n\n```\n$ cat no_quotes.sh\n#!/bin/bash\n./echo_args.sh $@\n\n$ cat quotes.sh\n#!/bin/bash\n./echo_args.sh \"$@\"\n\n$ cat echo_args.sh\n#!/bin/bash\necho Received: $1\necho Received: $2\necho Received: $3\necho Received: $4\n\n$ ./no_quotes.sh first second\nReceived: first\nReceived: second\nReceived:\nReceived:\n\n$ ./no_quotes.sh \"one quoted arg\"\nReceived: one\nReceived: quoted\nReceived: arg\nReceived:\n\n$ ./quotes.sh first second\nReceived: first\nReceived: second\nReceived:\nReceived:\n\n$ ./quotes.sh \"one quoted arg\"\nReceived: one quoted arg\nReceived:\nReceived:\nReceived:\n\n```"}
{"instruction": "Defining a variable with or without export", "output": "export makes the variable available to sub-processes.\nThat is,\n\n```\nexport name=value\n\n```\n\nmeans that the variable name is available to any process you run from that shell process. If you want a process to make use of this variable, use export, and run the process from that shell.\n\n```\nname=value\n\n```\n\nmeans the variable scope is restricted to the shell, and is not available to any other process. You would use this for (say) loop variables, temporary variables etc. An important exception to this rule is that if you define the variable while running a command, that variable will be available to child processes. For example\n\n```\nMY_VAR=yay node my-script.js\n\n```\n\nIn this case MY_VAR will be available to the node process running my-script.\nIt's important to note that exporting a variable doesn't make it available to parent processes. That is, specifying and exporting a variable in a spawned process doesn't make it available in the process that launched it."}
{"instruction": "How to reload .bash_profile from the command line", "output": "Simply type source ~/.bash_profile\nAlternatively, if you like saving keystrokes, you can type . ~/.bash_profile"}
{"instruction": "How do I clear/delete the current line in terminal?", "output": "You can use Ctrl+U to clear up to the beginning.\nYou can use Ctrl+W to delete just a word.\nYou can also use Ctrl+C to cancel.\nIf you want to keep the history, you can use Alt+Shift+# to make it a comment.\n\nBash Emacs Editing Mode Cheat Sheet"}
{"instruction": "Set environment variables from file of key/value pairs", "output": "Problem with your approach is the export in the while loop is happening in a sub shell, and those variable will not be available in current shell (parent shell of while loop).\nAdd export command in the file itself:\n\n```\nexport MINIENTREGA_FECHALIMITE=\"2011-03-31\"\nexport MINIENTREGA_FICHEROS=\"informe.txt programa.c\"\nexport MINIENTREGA_DESTINO=\"./destino/entrega-prac1\"\n\n```\n\nThen you need to source in the file in current shell using:\n\n```\n. ./conf/prac1\n\n```\n\nOR\n\n```\nsource ./conf/prac1\n\n```"}
{"instruction": "In Bash, how can I check if a string begins with some value?", "output": "This snippet on the Advanced Bash Scripting Guide says:\n\n```\n# The == comparison operator behaves differently within a double-brackets\n# test than within single brackets.\n\n[[ $a == z* ]]   # True if $a starts with a \"z\" (wildcard matching).\n[[ $a == \"z*\" ]] # True if $a is equal to z* (literal matching).\n\n```\n\nSo you had it nearly correct; you needed double brackets, not single brackets.\n\nWith regards to your second question, you can write it this way:\n\n```\nHOST=user1\nif  [[ $HOST == user1 ]] || [[ $HOST == node* ]] ;\nthen\n    echo yes1\nfi\n\nHOST=node001\nif [[ $HOST == user1 ]] || [[ $HOST == node* ]] ;\nthen\n    echo yes2\nfi\n\n```\n\nWhich will echo\n\n```\nyes1\nyes2\n\n```\n\nBash's if syntax is hard to get used to (IMO)."}
{"instruction": "How to iterate over arguments in a Bash script", "output": "Use \"$@\" to represent all the arguments:\n\n```\nfor var in \"$@\"\ndo\n    echo \"$var\"\ndone\n\n```\n\nThis will iterate over each argument and print it out on a separate line.  $@ behaves like $* except that, when quoted, the arguments are broken up properly if there are spaces in them:\n\n```\nsh test.sh 1 2 '3 4'\n1\n2\n3 4\n\n```"}
{"instruction": "Extract substring in Bash", "output": "Use cut:\n\n```\necho 'someletters_12345_moreleters.ext' | cut -d'_' -f 2\n\n```\n\nMore generic:\n\n```\nINPUT='someletters_12345_moreleters.ext'\nSUBSTRING=$(echo $INPUT| cut -d'_' -f 2)\necho $SUBSTRING\n\n```"}
{"instruction": "What does &#39;set -e&#39; mean in a Bash script?", "output": "From help set and Bash Reference Documentation: The Set Builtin:\n\n```\n  -e  Exit immediately if a command exits with a non-zero status.\n\n```\n\nBut it's considered bad practice by some (Bash FAQ and IRC Freenode #bash FAQ authors). It's recommended to use:\n\n```\ntrap 'do_something' ERR\n\n```\n\nto run do_something function when errors occur.\nSee Why doesn't set -e (or set -o errexit, or trap ERR) do what I expected?"}
{"instruction": "Get current directory or folder name (without the full path)", "output": "No need for basename, and especially no need for a subshell running pwd (which adds an extra, and expensive, fork operation); the shell can do this internally using parameter expansion:\n\n```\nresult=${PWD##*/}          # to assign to a variable\nresult=${result:-/}        # to correct for the case where PWD is / (root)\n\nprintf '%s\\n' \"${PWD##*/}\" # to print to stdout\n                           # ...more robust than echo for unusual names\n                           #    (consider a directory named -e or -n)\n\nprintf '%q\\n' \"${PWD##*/}\" # to print to stdout, quoted for use as shell input\n                           # ...useful to make hidden characters readable.\n\n```\n\n\nNote that if you're applying this technique in other circumstances (not PWD, but some other variable holding a directory name), you might need to trim any trailing slashes. The below uses bash's extglob support to work even with multiple trailing slashes:\n\n```\ndirname=/path/to/somewhere//\nshopt -s extglob           # enable +(...) glob syntax\nresult=${dirname%%+(/)}    # trim however many trailing slashes exist\nresult=${result##*/}       # remove everything before the last / that still remains\nresult=${result:-/}        # correct for dirname=/ case\nprintf '%s\\n' \"$result\"\n\n```\n\nAlternatively, without extglob:\n\n```\ndirname=\"/path/to/somewhere//\"\nresult=\"${dirname%\"${dirname##*[!/]}\"}\" # extglob-free multi-trailing-/ trim\nresult=\"${result##*/}\"                  # remove everything before the last /\nresult=${result:-/}                     # correct for dirname=/ case\n\n```"}
{"instruction": "How do I use sudo to redirect output to a location I don&#39;t have permission to write to?", "output": "Your command does not work because the redirection is performed by your shell which does not have the permission to write to /root/test.out. The redirection of the output is not performed by sudo.\nThere are multiple solutions:\n\nRun a shell with sudo and give the command to it by using the -c option:\n\n```\nsudo sh -c 'ls -hal /root/ > /root/test.out'\n\n```\n\nCreate a script with your commands and run that script with sudo:\n\n```\n#!/bin/sh\nls -hal /root/ > /root/test.out\n\n```\n\nRun sudo ls.sh. See Steve Bennett's answer if you don't want to create a temporary file.\nLaunch a shell with sudo -s then run your commands:\n\n```\n[nobody@so]$ sudo -s\n[root@so]# ls -hal /root/ > /root/test.out\n[root@so]# ^D\n[nobody@so]$\n\n```\n\nUse sudo tee (if you have to escape a lot when using the -c option):\n\n```\nsudo ls -hal /root/ | sudo tee /root/test.out > /dev/null\n\n```\n\nThe redirect to /dev/null is needed to stop tee from outputting to the screen. To append instead of overwriting the output file \n(>>), use tee -a or tee --append (the last one is specific to GNU coreutils).\n\nThanks go to Jd, Adam J. Forster and Johnathan for the second, third and fourth solutions."}
{"instruction": "How do I execute a program or call a system command?", "output": "Use subprocess.run:\n\n```\nimport subprocess\n\nsubprocess.run([\"ls\", \"-l\"]) \n\n```\n\nAnother common way is os.system but you shouldn't use it because it is unsafe if any parts of the command come from outside your program or can contain spaces or other special characters, also subprocess.run is generally more flexible (you can get the stdout, stderr, the \"real\" status code, better error handling, etc.). Even the documentation for os.system recommends using subprocess instead.\nOn Python 3.4 and earlier, use subprocess.call instead of .run:\n\n```\nsubprocess.call([\"ls\", \"-l\"])\n\n```"}
{"instruction": "Git is not working after macOS update (&quot;xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools&quot;)", "output": "The problem is that Xcode Command-line Tools needs to be updated due to a MacOs update.\n\nDid not run into this on Sonoma.\n\nMaybe Apple fixed the process?\n\n\nUpdated for Ventura\n\nAfter opening the terminal after restarting, I tried to go to my code, and do a git status, and I got an error and prompt for command line software agreement:\nSo press space until you get to the [agree, print, cancel] option, so careful hit space to scroll down to the end, if you blow past It you have to run a command to get it back. Use sudo xcodebuild -license to get to it again.\nJust be careful on scrolling down and enter agree and press return and it will launch into an update.\n\nThen I tried to use git after the install, and it prompted me to install Xcode tools again.\nI followed my own advice from previous years (see below), and went to https://developer.apple.com/download/all and downloaded\n\"Command Line Tools for Xcode 14\" (You have to log in with your Apple ID and enter MFA code, so have all the devices you need for that handy. Then select \"Command Line Tools for Xcode 14\", or if you want to get into the alphas or betas, that's up to you. But stable releases are probably the best choice for software developers.\n\nYou have to either download the tools from CLI or the developer page and before you can use git, you need to reboot!!! Or you will get stuck in a loop of prompt & downloading\nRebooting will break the loop and complete the installation of your CLI tools including git so that you can get back to work\nSolutions for previous years, these may or may not be valid these days as the downloads page has changed significantly:\nPREVIOUS YEARS SOLUTIONS, probably #2 is most helpful.\n*** Solution #1:\nGo back to your terminal and enter:\n\n```\nxcode-select --install\n\n```\n\nYou'll then receive the following output:\n\n```\nxcode-select: note: install requested for command line developer tools\n\n```\n\nYou will then be prompted in a window to update Xcode Command Line tools. (which could take a while)\nOpen a new terminal window and your development tools should be returned.\nAddition: With any major or semi-major update you'll need to update the command line tools in order to get them functioning properly again. Check Xcode with any update. This goes beyond Mojave...\nAfter that restart your terminal\nAlternatively, IF that fails, and it might.... you'll get a pop-up box saying \"Software not found on server\", proceed to solution 2.\n*** Solution #2: (Preferred method)\nIf you hit xcode-select --install and it doesn't find the software, log into Apple Developer, and install it via webpage.\nLog in or sign up here:\nhttps://developer.apple.com/download/more/\nLook for: \"Command Line Tools for Xcode 14.x\" in the list of downloads\nThen click the dmg and download. (See previous image above) either way, you will probably wind up at an apple downloads webpage."}
{"instruction": "How to reload .bashrc settings without logging out and back in again?", "output": "You can enter the long form command:\n\n```\nsource ~/.bashrc\n\n```\n\nor you can use the shorter version of the command:\n\n```\n. ~/.bashrc\n\n```"}
{"instruction": "How do I clear/delete the current line in terminal?", "output": "You can use Ctrl+U to clear up to the beginning.\nYou can use Ctrl+W to delete just a word.\nYou can also use Ctrl+C to cancel.\nIf you want to keep the history, you can use Alt+Shift+# to make it a comment.\n\nBash Emacs Editing Mode Cheat Sheet"}
{"instruction": "How can I get the current date and time in the terminal and set a custom command in the terminal for it?", "output": "The command is date\nTo customise the output there are a myriad of options available, see date --help for a list.\nFor example, date '+%A %W %Y %X' gives Tuesday 34 2013 08:04:22 which is the name of the day of the week, the week number, the year and the time."}
{"instruction": "How to move the cursor word by word in the OS X Terminal", "output": "Out of the box you can use the quite bizarre Esc+F to move to the beginning of the next word and Esc+B to move to the beginning of the current word."}
{"instruction": "How can I copy the output of a command directly into my clipboard?", "output": "One way of doing it follows:\n\nInstall xclip, such as:\nsudo apt-get install xclip\n\nPipe the output into xclip to be copied into the clipboard:\ncat file | xclip\n\nPaste the text you just copied into a X application:\nxclip -o\n\n\nTo paste somewhere else other than an X application, such as a text area of a web page in a browser window, use:\n\n```\ncat file | xclip -selection clipboard\n\n```\n\nConsider creating an alias:\n\n```\nalias \"c=xclip\"\nalias \"v=xclip -o\"\n\n```\n\nTo see how useful this is, imagine I want to open my current path in a new terminal window (there may be other ways of doing it like Ctrl+T on some systems, but this is just for illustration purposes):\n\n```\nTerminal 1:\npwd | c\n\nTerminal 2:\ncd `v`\n\n```\n\nNotice the ` `  around v. This executes v as a command first and then substitutes it in-place for cd to use.\nOnly copy the content to the X clipboard\n\n```\ncat file | xclip\n\n```"}
{"instruction": "Run / Open VSCode from Mac Terminal", "output": "According to the docs on Launching from the command line:\n\nOpen Visual Studio Code\nOpen the command pallette with Command + Shift + P (or F1)\nType Shell in command palette\nSelect Shell Command: Install code in PATH from suggested list\n\n\nThat's it.\nNow open your terminal type.\n\n```\n$ code .\n\n```\n\n\nTo make this change persist after restart on MacOS\nMany Mac users find this is forgotten and needs to be re-applied after any restart. This may happen if MacOS has applied the quarantine attribute to VS Code, which the OS uses for the \"Are you sure?\" notice applied on first using apps downloaded from the internet.\nTo check if this attribute is applied, look for com.apple.quarantine in the list returned by this command (changing the path if that's not where you installed it):\n\n```\nxattr \"/Applications/Visual Studio Code.app\"\n\n```\n\nIf that does return com.apple.quarantine, you can remove the attribute using the same command with the -d flag (alongside -r to recursively remove it from all contained files and sudo to allow the change):\n\n```\nsudo xattr -r -d com.apple.quarantine \"/Applications/Visual Studio Code.app\"\n\n```\n\n...then do Shell Command : Install code in PATH as above after the attribute has been removed, and it should persist after restart.\nCredit: derflounder.wordpress.com article linked to by RicardoVallejo in this comment."}
{"instruction": "Git branch command behaves like &#39;less&#39;", "output": "As mentioned in comments to Mark Adelsberger's answer, this was a default behavior change introduced in Git 2.16.\nYou can turn paged output for git branch back off by default with the pager.branch config setting:\n\n```\ngit config --global pager.branch false\n\n```"}
{"instruction": "Node Version Manager install - nvm command not found", "output": "Check your .bash_profile,  .zshrc, or .profile file. You most likely had a problem during the installation. \nYou should have the following at the end of one of those files.\n\n```\n[[ -s $HOME/.nvm/nvm.sh ]] && . $HOME/.nvm/nvm.sh  # This loads NVM\n\n```\n\nThe . $HOME/.nvm/nvm.sh is the same as source $HOME/.nvm/nvm.sh\nSee: Sourcing a File\nYou can also check to see if you have a .nvm folder.\n\n```\nls -a | grep .nvm\n\n```\n\nIf you're missing that folder then the installation failed to run the git command. This could be due to being behind a proxy. Try running the following instead.\n\n```\ngit clone http://github.com/creationix/nvm.git .nvm\n\n```"}
{"instruction": "Find and kill a process in one line using bash and regex", "output": "In bash, using only the basic tools listed in your question(1), you should be able to do:\n\n```\nkill $(ps aux | grep '[p]ython csp_build.py' | awk '{print $2}')\n\n```\n\nDetails on its workings are as follows:\n\nThe ps gives you the list of all the processes.\nThe grep filters that based on your search string, [p] is a trick to stop you picking up the actual grep process itself.\nThe awk just gives you the second field of each line, which is the PID.\nThe $(x) construct means to execute x then take its output and put it on the command line. The output of that ps pipeline inside that construct above is the list of process IDs so you end up with a command like kill 1234 1122 7654.\n\nHere's a transcript showing it in action:\n\n```\npax> sleep 3600 &\n[1] 2225\npax> sleep 3600 &\n[2] 2226\npax> sleep 3600 &\n[3] 2227\npax> sleep 3600 &\n[4] 2228\npax> sleep 3600 &\n[5] 2229\npax> kill $(ps aux | grep '[s]leep' | awk '{print $2}')\n[5]+  Terminated              sleep 3600\n[1]   Terminated              sleep 3600\n[2]   Terminated              sleep 3600\n[3]-  Terminated              sleep 3600\n[4]+  Terminated              sleep 3600\n\n```\n\nand you can see it terminating all the sleepers.\nExplaining the grep '[p]ython csp_build.py' bit in a bit more detail: when you do sleep 3600 & followed by ps -ef | grep sleep, you tend to get two processes with sleep in it, the sleep 3600 and the grep sleep (because they both have sleep in them, that's not rocket science).\nHowever, ps -ef | grep '[s]leep' won't create a grep process with sleep in it, it instead creates one with the command grep '[s]leep' and here's the tricky bit: the grep doesn't find that one, because it's looking for the regular expression \"any character from the character class [s] (which is basically just s) followed by leep.\nIn other words, it's looking for sleep but the grep process is grep '[s]leep' which doesn't have the text sleep in it.\nWhen I was shown this (by someone here on SO), I immediately started using it because\n\nit's one less process than adding | grep -v grep; and\nit's elegant and sneaky, a rare combination :-)\n\n\n(1) If you're not limited to using those basic tools, there's a nifty pgrep command which will find processes based on certain criteria (assuming you have it available on your system, of course).\nFor example, you can use pgrep sleep to output the process IDs for all sleep commands (by default, it matches the process name). If you want to match the entire command line as shown in ps, you can do something like pgrep -f 'sleep 9999'.\nAs an aside, it doesn't list itself if you do pgrep pgrep, so the tricky filter method shown above is not necessary in this case.\nYou can check that the processes are the ones you're interested in by using -a to show the full process names. You can also limit the scope to your own processes (or a specific set of users) with -u or -U. See the man page for pgrep/pkill for more options.\nOnce you're satisfied it will only show the processes you're interested in, you can then use pkill with the same parameters to send a signal to all those processes."}
{"instruction": "psql: FATAL: role &quot;postgres&quot; does not exist", "output": "NOTE: If you installed postgres using homebrew, see the comments from @user3402754 and @originalhat below.\nNote that the error message does NOT talk about a missing database, it talks about a missing role. Later in the login process it might also stumble over the missing database.\nBut the first step is to check the missing role: What is the output within psql of the command \\du ? On my Ubuntu system the relevant line looks like this:\n\n```\n                              List of roles\n Role name |            Attributes             | Member of \n-----------+-----------------------------------+-----------\n postgres  | Superuser, Create role, Create DB | {}        \n\n```\n\nIf there is not at least one role with superuser, then you have a problem :-)\nIf there is one, you can use that to login. And looking at the output of your \\l command: The permissions for user on the template0 and template1 databases are the same as on my Ubuntu system for the superuser postgres. So I think your setup simple uses user as the superuser. So you could try this command to login:\n\n```\nsudo -u user psql user\n\n```\n\nIf user is really the DB superuser you can create another DB superuser and a private, empty database for him:\n\n```\nCREATE USER postgres SUPERUSER;\nCREATE DATABASE postgres WITH OWNER postgres;\n\n```\n\nBut since your postgres.app setup does not seem to do this, you also should not. Simple adapt the tutorial."}
{"instruction": "How do I see which version of Swift I&#39;m using?", "output": "Project build settings have a block 'Swift Compiler - Languages', which stores information about Swift Language Version in key-value format. It will show you all available (supported) Swift Language Version for your Xcode and active version also by a tick mark.\n\nProject ‚ñ∫ (Select Your Project Target) ‚ñ∫ Build Settings ‚ñ∫ (Type\n'swift_version' in the Search bar) Swift Compiler Language ‚ñ∫ Swift Language\nVersion ‚ñ∫ Click on Language list to open it (and there will be a tick mark on any one of  list-item, that will be current swift version).\n\nLook at this snapshot, for easy understanding:\n\n\nWith help of following code, programmatically you can find Swift version supported by your project.\n\n```\n#if swift(>=5.10)\nprint(\"Hello, Swift 5.10\")\n\n#elseif swift(>=5.9)\nprint(\"Hello, Swift 5.9\")\n\n#elseif swift(>=5.8)\nprint(\"Hello, Swift 5.8\")\n\n#elseif swift(>=5.7)\nprint(\"Hello, Swift 5.7\")\n\n#elseif swift(>=5.6)\nprint(\"Hello, Swift 5.6\")\n\n#elseif swift(>=5.5)\nprint(\"Hello, Swift 5.5\")\n\n#elseif swift(>=5.4)\nprint(\"Hello, Swift 5.4\")\n\n#elseif swift(>=5.3)\nprint(\"Hello, Swift 5.3\")\n\n#elseif swift(>=5.2)\nprint(\"Hello, Swift 5.2\")\n\n#elseif swift(>=5.1)\nprint(\"Hello, Swift 5.1\")\n\n#elseif swift(>=5.0)\nprint(\"Hello, Swift 5.0\")\n\n#elseif swift(>=4.2)\nprint(\"Hello, Swift 4.2\")\n\n#elseif swift(>=4.1)\nprint(\"Hello, Swift 4.1\")\n\n#elseif swift(>=4.0)\nprint(\"Hello, Swift 4.0\")\n\n#elseif swift(>=3.2)\nprint(\"Hello, Swift 3.2\")\n\n#elseif swift(>=3.0)\nprint(\"Hello, Swift 3.0\")\n\n#elseif swift(>=2.2)\nprint(\"Hello, Swift 2.2\")\n\n#elseif swift(>=2.1)\nprint(\"Hello, Swift 2.1\")\n\n#elseif swift(>=2.0)\nprint(\"Hello, Swift 2.0\")\n\n#elseif swift(>=1.2)\nprint(\"Hello, Swift 1.2\")\n\n#elseif swift(>=1.1)\nprint(\"Hello, Swift 1.1\")\n\n#elseif swift(>=1.0)\nprint(\"Hello, Swift 1.0\")\n\n#endif\n\n```\n\nHere is result using Playground (with Xcode 11.x)\nPLEASE NOTE:\nBuilding your code as Swift 4 in Xcode doesn't actually switch to a Swift 4 compiler, it just tells the compiler to try to apply the same language rules that the Swift 4 compiler would have used."}
{"instruction": "How to use &#39;cp&#39; command to exclude a specific directory?", "output": "rsync is fast and easy:\n\n```\nrsync -av --progress sourcefolder /destinationfolder --exclude thefoldertoexclude\n\n```\n\nYou can use --exclude multiples times.\n\n```\nrsync -av --progress sourcefolder /destinationfolder --exclude thefoldertoexclude --exclude anotherfoldertoexclude\n\n```\n\nNote that the dir thefoldertoexclude after --exclude option is relative to the sourcefolder, i.e., sourcefolder/thefoldertoexclude.\nAlso you can add -n for dry run to see what will be copied before performing  real operation, and if everything is ok, remove -n from command line."}
{"instruction": "Open terminal here in Mac OS finder", "output": "As of Mac OS X Lion 10.7, Terminal includes exactly this functionality as a Service. As with most Services, these are disabled by default, so you'll need to enable this to make it appear in the Services menu.\n\nSystem Preferences > Keyboard > Shortcuts > Services\n\nEnable New Terminal at Folder. There's also New Terminal Tab at Folder, which will create a tab in the frontmost Terminal window (if any, else it will create a new window). These Services work in all applications, not just Finder, and they operate on folders as well as absolute pathnames selected in text.\nYou can even assign command keys to them.\nServices appear in the Services submenu of each application menu, and within the contextual menu (Control-Click or Right-Click on a folder or pathname).\nThe New Terminal at Folder service will become active when you select a folder in Finder. You cannot simply have the folder open and run the service \"in place\". Go back to the parent folder, select the relevant folder, then activate the service via the Services menu or context menu.\nIn addition, Lion Terminal will open a new terminal window if you drag a folder (or pathname) onto the Terminal application icon, and you can also drag to the tab bar of an existing window to create a new tab.\nFinally, if you drag a folder or pathname onto a tab (in the tab bar) and the foreground process is the shell, it will automatically execute a \"cd\" command. (Dragging into the terminal view within the tab merely inserts the pathname on its own, as in older versions of Terminal.)\nYou can also do this from the command line or a shell script:\n\n```\nopen -a Terminal /path/to/folder\n\n```\n\nThis is the command-line equivalent of dragging a folder/pathname onto the Terminal application icon.\nOn a related note, Lion Terminal also has new Services for looking up man pages: Open man page in Terminal displays the selected man page topic in a new terminal window, and Search man Pages in Terminal performs \"apropos\" on the selected text. The former also understands man page references (\"open(2)\"), man page command line arguments (\"2 open\") and man page URLs (\"x-man-page://2/open\")."}
{"instruction": "List Git aliases", "output": "Just adding this because it's so simple and I didn't see it in previous answers (sorry if I missed it).\n\n```\ngit help -a\n\n```\n\nYou'll have to scroll to the bottom (use > as ma11hew28 pointed out)  to see the list, e.g.:\n\n```\nCommand aliases\n   restore-deleted      !git restore $(git ls-files -d)\n\n```\n\nIf you forget even this switch, a simple git help will help you remember:\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help ' or 'git help ' to\nread about a specific subcommand or concept."}
{"instruction": "List of ANSI color escape sequences", "output": "The ANSI escape sequences you're looking for are the Select Graphic Rendition subset. All of these have the form\n\n```\n\\033[XXXm\n\n```\n\nwhere XXX is a series of semicolon-separated parameters.\nTo say, make text red, bold, and underlined (we'll discuss many other options below) in C you might write:\n\n```\nprintf(\"\\033[31;1;4mHello\\033[0m\");\n\n```\n\nIn C++ you'd use\n\n```\nstd::cout<<\"\\033[31;1;4mHello\\033[0m\";\n\n```\n\nIn Python3 you'd use\n\n```\nprint(\"\\033[31;1;4mHello\\033[0m\")\n\n```\n\nand in Bash you'd use\n\n```\necho -e \"\\033[31;1;4mHello\\033[0m\"\n\n```\n\nwhere the first part makes the text red (31), bold (1), underlined (4) and the last part clears all this (0).\nAs described in the table below, there are a large number of text properties you can set, such as boldness, font, underlining, &c.\nFont Effects\n\n\n\nCode\nEffect\nNote\n\n\n\n\n0\nReset / Normal\nall attributes off\n\n\n1\nBold or increased intensity\n\n\n\n2\nFaint (decreased intensity)\nNot widely supported.\n\n\n3\nItalic\nNot widely supported. Sometimes treated as inverse.\n\n\n4\nUnderline\n\n\n\n5\nSlow Blink\nless than 150 per minute\n\n\n6\nRapid Blink\nMS-DOS ANSI.SYS; 150+ per minute; not widely supported\n\n\n7\n[[reverse video]]\nswap foreground and background colors\n\n\n8\nConceal\nNot widely supported.\n\n\n9\nCrossed-out\nCharacters legible, but marked for deletion.  Not widely supported.\n\n\n10\nPrimary(default) font\n\n\n\n11‚Äì19\nAlternate font\nSelect alternate font n-10\n\n\n20\nFraktur\nhardly ever supported\n\n\n21\nBold off or Double Underline\nBold off not widely supported; double underline hardly ever supported.\n\n\n22\nNormal color or intensity\nNeither bold nor faint\n\n\n23\nNot italic, not Fraktur\n\n\n\n24\nUnderline off\nNot singly or doubly underlined\n\n\n25\nBlink off\n\n\n\n27\nInverse off\n\n\n\n28\nReveal\nconceal off\n\n\n29\nNot crossed out\n\n\n\n30‚Äì37\nSet foreground color\nSee color table below\n\n\n38\nSet foreground color\nNext arguments are 5;<n> or 2;<r>;<g>;<b>, see below\n\n\n39\nDefault foreground color\nimplementation defined (according to standard)\n\n\n40‚Äì47\nSet background color\nSee color table below\n\n\n48\nSet background color\nNext arguments are 5;<n> or 2;<r>;<g>;<b>, see below\n\n\n49\nDefault background color\nimplementation defined (according to standard)\n\n\n51\nFramed\n\n\n\n52\nEncircled\n\n\n\n53\nOverlined\n\n\n\n54\nNot framed or encircled\n\n\n\n55\nNot overlined\n\n\n\n60\nideogram underline\nhardly ever supported\n\n\n61\nideogram double underline\nhardly ever supported\n\n\n62\nideogram overline\nhardly ever supported\n\n\n63\nideogram double overline\nhardly ever supported\n\n\n64\nideogram stress marking\nhardly ever supported\n\n\n65\nideogram attributes off\nreset the effects of all of 60-64\n\n\n90‚Äì97\nSet bright foreground color\naixterm (not in standard)\n\n\n100‚Äì107\nSet bright background color\naixterm (not in standard)\n\n\n\n2-bit Colours\nYou've got this already!\n4-bit Colours\nThe standards implementing terminal colours began with limited (4-bit) options. The table below lists the RGB values of the background and foreground colours used for these by a variety of terminal emulators:\n\nUsing the above, you can make red text on a green background (but why?) using:\n\n```\n\\033[31;42m\n\n```\n\n11 Colours (An Interlude)\nIn their book \"Basic Color Terms: Their Universality and Evolution\", Brent Berlin and Paul Kay used data collected from twenty different languages from a range of language families to identify eleven possible basic color categories: white, black, red, green, yellow, blue, brown, purple, pink, orange, and gray.\nBerlin and Kay found that, in languages with fewer than the maximum eleven color categories, the colors followed a specific evolutionary pattern. This pattern is as follows:\n\nAll languages contain terms for black (cool colours) and white (bright colours).\nIf a language contains three terms, then it contains a term for red.\nIf a language contains four terms, then it contains a term for either green or yellow (but not both).\nIf a language contains five terms, then it contains terms for both green and yellow.\nIf a language contains six terms, then it contains a term for blue.\nIf a language contains seven terms, then it contains a term for brown.\nIf a language contains eight or more terms, then it contains terms for purple, pink, orange or gray.\n\nThis may be why story Beowulf only contains the colours black, white, and red. Homer's Odyssey contains black almost 200 times and white about 100 times. Red appears 15 times, while yellow and green appear only 10 times. (More information here)\nDifferences between languages are also interesting: note the profusion of distinct colour words used by English vs. Chinese. However, digging deeper into these languages shows that each uses colour in distinct ways. (More information)\n\nGenerally speaking, the naming, use, and grouping of colours in human languages is fascinating. Now, back to the show.\n8-bit (256) colours\nTechnology advanced, and tables of 256 pre-selected colours became available, as shown below.\n\nUsing these above, you can make pink text like so:\n\n```\n\\033[38;5;206m     #That is, \\033[38;5;<FG COLOR>m\n\n```\n\nAnd make an early-morning blue background using\n\n```\n\\033[48;5;57m      #That is, \\033[48;5;<BG COLOR>m\n\n```\n\nAnd, of course, you can combine these:\n\n```\n\\033[38;5;206;48;5;57m\n\n```\n\nThe 8-bit colours are arranged like so:\n\n\n\nRange\nDescription\n\n\n\n\n0x00-0x07\nstandard colors (same as the 4-bit colours)\n\n\n0x08-0x0F\nhigh intensity colors\n\n\n0x10-0xE7\n6 √ó 6 √ó 6 cube (216 colors): 16 + 36 √ó r + 6 √ó g + b (0 ‚â§ r, g, b ‚â§ 5)\n\n\n0xE8-0xFF\ngrayscale from black to white in 24 steps\n\n\n\nALL THE COLOURS\nNow we are living in the future, and the full RGB spectrum is available using:\n\n```\n\\033[38;2;<r>;<g>;<b>m     #Select RGB foreground color\n\\033[48;2;<r>;<g>;<b>m     #Select RGB background color\n\n```\n\nSo you can put pinkish text on a brownish background using\n\n```\n\\033[38;2;255;82;197;48;2;155;106;0mHello\n\n```\n\nSupport for \"true color\" terminals is listed here.\nMuch of the above is drawn from the Wikipedia page \"ANSI escape code\".\nA Handy Script to Remind Yourself\nSince I'm often in the position of trying to remember what colours are what, I have a handy script called: ~/bin/ansi_colours:\n\n```\n#!/usr/bin/env python3\n\nfor i in range(30, 37 + 1):\n    print(\"\\033[%dm%d\\t\\t\\033[%dm%d\" % (i, i, i + 60, i + 60))\n\nprint(\"\\033[39m\\\\033[49m                 - Reset color\")\nprint(\"\\\\033[2K                          - Clear Line\")\nprint(\"\\\\033[<L>;<C>H or \\\\033[<L>;<C>f  - Put the cursor at line L and column C.\")\nprint(\"\\\\033[<N>A                        - Move the cursor up N lines\")\nprint(\"\\\\033[<N>B                        - Move the cursor down N lines\")\nprint(\"\\\\033[<N>C                        - Move the cursor forward N columns\")\nprint(\"\\\\033[<N>D                        - Move the cursor backward N columns\\n\")\nprint(\"\\\\033[2J                          - Clear the screen, move to (0,0)\")\nprint(\"\\\\033[K                           - Erase to end of line\")\nprint(\"\\\\033[s                           - Save cursor position\")\nprint(\"\\\\033[u                           - Restore cursor position\\n\")\nprint(\"\\\\033[4m                          - Underline on\")\nprint(\"\\\\033[24m                         - Underline off\\n\")\nprint(\"\\\\033[1m                          - Bold on\")\nprint(\"\\\\033[21m                         - Bold off\")\n\n```\n\nThis prints"}
{"instruction": "Unable to show a Git tree in terminal", "output": "How can you get the tree-like view of commits in terminal?\n\n```\ngit log --graph --oneline --all\n\n```\n\nis a good start.\nYou may get some strange letters. They are ASCII codes for colors and structure. To solve this problem add the following to your .bashrc:\n\n```\nexport LESS=\"-R\"\n\n```\n\nsuch that you do not need use Tig's ASCII filter by\n\n```\ngit log --graph --pretty=oneline --abbrev-commit | tig   // Masi needed this \n\n```\n\nThe article text-based graph from Git-ready contains other options:\n\n```\ngit log --graph --pretty=oneline --abbrev-commit\n\n```\n\n\nRegarding the article you mention, I would go with Pod's answer: ad-hoc hand-made output.\n\nJakub Narƒôbski mentions in the comments tig, a ncurses-based text-mode interface for git. See their releases.\nIt added a --graph option back in 2007."}
{"instruction": "OS X Terminal Colors", "output": "Here is a solution I've found to enable the global\nterminal colors.\nEdit your .bash_profile (since OS X 10.8) ‚Äî or (for 10.7 and earlier): .profile or .bashrc or /etc/profile (depending on availability) ‚Äî in your home directory and add following code:\n\n```\nexport CLICOLOR=1\nexport LSCOLORS=GxFxCxDxBxegedabagaced\n\n```\n\nCLICOLOR=1 simply enables coloring of your terminal.\nLSCOLORS=... specifies how to color specific items. \nAfter editing .bash_profile, start a Terminal and force the changes to take place by executing:\nsource ~/.bash_profile\nThen go to Terminal > Preferences, click on the Profiles tab and then the Text subtab and check Display ANSI Colors.\nVerified on Sierra (May 2017)."}
{"instruction": "Keep SSH session alive", "output": "The ssh daemon (sshd), which runs server-side, closes the connection from the server-side if the client goes silent (i.e., does not send information). To prevent connection loss, instruct the ssh client to send a sign-of-life signal to the server once in a while.\nThe configuration for this is in the file $HOME/.ssh/config, create the file if it does not exist (the config file must not be world-readable, so run chmod 600 ~/.ssh/config after creating the file). To send the signal every e.g. four minutes (240 seconds) to the remote host, put the following in that configuration file:\n\n```\nHost remotehost\n    HostName remotehost.com\n    ServerAliveInterval 240\n\n```\n\nTo enable sending a keep-alive signal for all hosts, place the following contents in the configuration file:\n\n```\nHost *\n    ServerAliveInterval 240\n\n```"}
{"instruction": "Change the default terminal in Visual Studio Code", "output": "You can also select your default terminal by pressing F1 in Visual Studio Code and typing/selecting Terminal: Select Default Profile (or Terminal: Select Default Shell in older Visual Studio Code versions).\n\nOlder:"}
{"instruction": "How can I turn off &quot;scrolling the history&quot; in iTerm2", "output": "A few terminals, including iTerm2, have a feature where they change the behavior of the wheel mouse when a full-screen program such as vi, or screen or tmux is running.  This happens when those programs use the alternate screen, to provide a useful function.  Normally, when using the alternate screen in iTerm2, the wheel mouse acts like the scrollbar, scrolling the entire screen up/down.  But when this feature is enabled, iTerm2 sends cursor up/down keys, making your command-history change.\nAs suggested in another comment, select the Preferences menu:\n\nand in that, select the Advanced tab.  Scroll down to the Mouse section,\n\nand toggle the entry for\nScroll wheel sends arrow keys when in alternate screen mode\nfrom Yes to No.  You will have to restart iTerm2 for the change to take effect.  (With iTerm2 v3.1.5 changes take effect without restarting.)"}
{"instruction": "Clear a terminal screen for real", "output": "Use the following command to do a clear screen instead of merely adding new lines ...\n\n```\nprintf \"\\033c\"\n\n```\n\nyes that's a 'printf' on the bash prompt.\nYou will probably want to define an alias though...\n\n```\nalias cls='printf \"\\033c\"'\n\n```\n\nExplanation\n\n```\n\\033 == \\x1B == 27 == ESC\n\n```\n\nSo this becomes <ESC>c which is the VT100 escape code for resetting the terminal. Here is some more information on terminal escape codes.\nEdit\nHere are a few other ways of doing it...\n\n```\nprintf \"\\ec\" #\\e is ESC in bash\necho -en \"\\ec\" #thanks @Jonathon Reinhart.\n# -e    Enable interpretation of of backslash escapes\n# -n    Do not output a new line\n\n```\n\nKDE\nThe above does not work on the KDE console (called Konsole) but there is hope! Use the following sequence of commands to clear the screen and the scroll-back buffer...\n\n```\nclear && echo -en \"\\e[3J\"\n\n```\n\nOr perhaps use the following alias on KDE...\n\n```\nalias cls='clear && echo -en \"\\e[3J\"'\n\n```\n\nI got the scroll-back clearing command from here."}
{"instruction": "How to remove files and directories quickly via terminal (bash shell)", "output": "```\nrm -rf some_dir\n\n```\n\n-r \"recursive\"\n-f \"force\" (suppress confirmation messages)\nBe careful!"}
{"instruction": "How do I find the width &amp; height of a terminal window?", "output": "tput cols tells you the number of columns.\ntput lines tells you the number of rows."}
{"instruction": "How do I output coloured text to a Linux terminal?", "output": "You need to output ANSI colour codes. Note that not all terminals support this; if colour sequences are not supported, garbage will show up.\nExample:\n\n```\n cout << \"\\033[1;31mbold red text\\033[0m\\n\";\n\n```\n\nHere, \\033 is the ESC character, ASCII 27. It is followed by [, then zero or more numbers separated by ;, and finally the letter m. The numbers describe the colour and format to switch to from that point onwards.\nThe codes for foreground and background colours are:\n\n```\n         foreground background\nblack        30         40\nred          31         41\ngreen        32         42\nyellow       33         43\nblue         34         44\nmagenta      35         45\ncyan         36         46\nwhite        37         47\n\n```\n\nAdditionally, you can use these:\n\n```\nreset             0  (everything back to normal)\nbold/bright       1  (often a brighter shade of the same colour)\nunderline         4\ninverse           7  (swap foreground and background colours)\nbold/bright off  21\nunderline off    24\ninverse off      27\n\n```\n\nSee the table on Wikipedia for other, less widely supported codes.\n\nTo determine whether your terminal supports colour sequences, read the value of the TERM environment variable. It should specify the particular terminal type used (e.g. vt100, gnome-terminal, xterm, screen, ...). Then look that up in the terminfo database; check the colors capability."}
{"instruction": "`npm install` fails on node-gyp rebuild with `gyp: No Xcode or CLT version detected!`", "output": "Even though you have them installed (my case), but by upgrading to Catalina (10.15.*) you can get this error (my case :) ).\nTherefore, simply installing wouldn't help as you will get an error that they are already installed.\nTherefore you need to hopefully just (I) reset the tool or in worse case (II) uninstall and install (requires large redownload) it again (this is based on the @Dane_duPlessis's answer).\n\n```\n# just for –∞ preview (not necessary)\nxcode-select --print-path\n# in my case it printed `/Library/Developer/CommandLineTools`\n\n# could be useful if the path is making a problem\nsudo xcode-select --switch /Library/Developer/CommandLineTools\n\n# only for the (I) resetting case\nsudo xcode-select --reset\n\n# only for the (II) uninstalling case - the next line deletes folder returned by the `xcode-select --print-path` command\nsudo rm -rf $(xcode-select --print-path)\n\n\n# only for the (II) uninstalling case - install tools (again) if you don't get a default installation prompt\nxcode-select --install\n\n```\n\nNOTE:\n\nYou do not need to do it per project, but only once\nThere was also discussion that you have to do it on every OSX update, in my case, later updating the OSX system didn't trigger this issue again\n\nCredits to: gyp: No Xcode or CLT version detected macOS Catalina"}
{"instruction": "count number of lines in terminal output", "output": "Pipe the result to wc using the -l (line count) switch:\n\n```\ngrep -Rl \"curl\" ./ | wc -l\n\n```"}
{"instruction": "Setting the correct encoding when piping stdout in Python", "output": "Your code works when run in an script because Python encodes the output to whatever encoding your terminal application is using. If you are piping you must encode it yourself.\nA rule of thumb is: Always use Unicode internally. Decode what you receive, and encode what you send.\n\n```\n# -*- coding: utf-8 -*-\nprint u\"√•√§√∂\".encode('utf-8')\n\n```\n\nAnother didactic example is a Python program to convert between ISO-8859-1 and UTF-8, making everything uppercase in between.\n\n```\nimport sys\nfor line in sys.stdin:\n    # Decode what you receive:\n    line = line.decode('iso8859-1')\n\n    # Work with Unicode internally:\n    line = line.upper()\n\n    # Encode what you send:\n    line = line.encode('utf-8')\n    sys.stdout.write(line)\n\n```\n\nSetting the system default encoding is a bad idea, because some modules and libraries you use can rely on the fact it is ASCII. Don't do it."}
{"instruction": "Mac OS X Terminal: Map option+delete to &quot;backward delete word&quot;", "output": "Enable option key as meta key\n\nGo to Terminal > Settings > Profiles > Keyboard\nCheck Use option key as meta key.\n\nImage\nOn macOS High Sierra 10.13.6, captured on October 23, 2018.\n\nNotes\nMany applications (including bash and tcsh) treat Meta-Delete as \"backward delete word.\""}
{"instruction": "iTerm2 keyboard shortcut - split pane navigation", "output": "From the documentation:\n\nCmd] and Cmd[  navigates among split panes in order of use."}
{"instruction": "Switching from zsh to bash on OS X, and back again?", "output": "You can just use exec to replace your current shell with a new shell:\nSwitch to bash:\n\n```\nexec bash\n\n```\n\nSwitch to zsh:\n\n```\nexec zsh\n\n```\n\nThis won't affect new terminal windows or anything, but it's convenient."}
{"instruction": "Linux command to list all available commands and aliases", "output": "You can use the bash(1) built-in compgen\n\ncompgen -c will list all the commands you could run.\ncompgen -a will list all the aliases you could run.\ncompgen -b will list all the built-ins you could run.\ncompgen -k will list all the keywords you could run.\ncompgen -A function will list all the functions you could run.\ncompgen -A function -abck will list all the above in one go.\n\nCheck the man page for other completions you can generate.\nTo directly answer your question:\n\n```\ncompgen -ac | grep searchstr\n\n```\n\nshould do what you want."}
{"instruction": "How to get Linux console window width in Python", "output": "Not sure why it is in the module shutil, but it landed there in Python 3.3.  See:\nQuerying the size of the output terminal\n\n```\n>>> import shutil\n>>> shutil.get_terminal_size((80, 20))  # pass fallback\nos.terminal_size(columns=87, lines=23)  # returns a named-tuple\n\n```\n\nA low-level implementation is in the os module.  Cross-platform‚Äîworks under Linux, Mac OS, and Windows, probably other Unix-likes.  There's a backport as well, though no longer relevant."}
{"instruction": "How do you run JavaScript script through the Terminal?", "output": "You would need a JavaScript engine (such as Mozilla's Rhino) in order to evaluate the script - exactly as you do for Python, though the latter ships with the standard distribution.\nIf you have Rhino (or alternative) installed and on your path, then running JS can indeed be as simple as\n\n```\n> rhino filename.js\n\n```\n\nIt's worth noting though that while JavaScript is simply a language in its own right, a lot of particular scripts assume that they'll be executing in a browser-like environment - and so try to access global variables such as location.href, and create output by appending DOM objects rather than calling print.\nIf you've got hold of a script which was written for a web page, you may need to wrap or modify it somewhat to allow it to accept arguments from stdin and write to stdout.  (I believe Rhino has a mode to emulate standard browser global vars which helps a lot, though I can't find the docs for this now.)"}
{"instruction": "adb command not found", "output": "Make sure adb is in your user's $PATH variable.\nor\nYou can try to locate it with whereis and run it with ./adb"}
{"instruction": "How do I update zsh to the latest version?", "output": "If you have Homebrew installed, you can do this.\n\n```\n# check the zsh info\nbrew info zsh\n\n# install zsh\nbrew install --without-etcdir zsh\n\n# add shell path\nsudo vim /etc/shells\n\n# add the following line into the very end of the file(/etc/shells)\n/usr/local/bin/zsh\n\n# change default shell\nchsh -s /usr/local/bin/zsh\n\n```"}
{"instruction": "Is there a better Windows Console Window?", "output": "Sorry for the self-promotion, I'm the author of another Console Emulator, not mentioned here.\nConEmu is opensource console emulator with tabs, which represents multiple consoles and simple GUI applications as one customizable GUI window.\nInitially, the program was designed to work with Far Manager (my favorite shell replacement - file and archive management, command history and completion, powerful editor). But ConEmu can be used with any other console application or simple GUI tools (like PuTTY for example). ConEmu is a live project, open to suggestions.\nA brief excerpt from the long list of options:\n\nLatest versions of ConEmu may set up itself as default terminal for Windows\nUse any font installed in the system, or copied to a folder of the program (ttf, otf, fon, bdf)\nRun selected tabs as Administrator (Vista+) or as selected user\nWindows 7 Jump lists and Progress on taskbar\nIntegration with DosBox (useful in 64bit systems to run DOS applications)\nSmooth resize, maximized and fullscreen window modes\nScrollbar initially hidden, may be revealed by mouseover or checkbox in settings\nOptional settings (e.g. pallette) for selected applications\nUser friendly text and block selection (from keyboard or mouse), copy, paste, text search in console\nANSI X3.64 and Xterm 256 color\n\nFar Manager users will acquire shell style drag-n-drop, thumbnails and tiles in panles, tabs for editors and viewers, true colors and font styles (italic/bold/underline).\nPS. Far Manager supports UNC paths (\\\\server\\share\\...)"}
{"instruction": "Clear the terminal in Python", "output": "Use escape sequences:\n\n```\nprint(chr(27) + \"[2J\")\n\n```"}
{"instruction": "How do I edit $PATH (.bash_profile) on OS X?", "output": "You have to open that file with a text editor and then save it.\n\n```\ntouch ~/.bash_profile; open ~/.bash_profile\n\n```\n\nIt will open the file with TextEdit, paste your things and then save it. If you open it again you'll find your edits.\nYou can use other editors:\n\n```\nnano ~/.bash_profile\nmate ~/.bash_profile\nvim ~/.bash_profile\n\n```\n\nBut if you don't know how to use them, it's easier to use the open approach.\n\nAlternatively, you can rely on pbpaste. Copy\n\n```\nexport ANDROID_HOME=/<installation location>/android-sdk-macosx\nexport PATH=${PATH}:$ANDROID_HOME/tools:$ANDROID_HOME/platform-tools\n\n```\n\nin the system clipboard and then in a shell run\n\n```\npbpaste > ~/.bash_profile\n\n```\n\n\nOr alternatively you can also use cat\n\n```\ncat > ~/.bash_profile\n\n```\n\n(now cat waits for input: paste the two export definitions and then hit Ctrl + D)."}
{"instruction": "Using scp to copy a file to Amazon EC2 instance?", "output": "Try specifying the user to be ec2-user, e.g.\n\n```\nscp -i myAmazonKey.pem phpMyAdmin-3.4.5-all-languages.tar.gz ec2-user@mec2-50-17-16-67.compute-1.amazonaws.com:~/.\n\n```\n\nSee Connecting to Linux/UNIX Instances Using SSH."}
{"instruction": "Expert R users, what&#39;s in your .Rprofile?", "output": "Here is mine. It won't help you with the coloring but I get that from ESS and Emacs...\n\n```\noptions(\"width\"=160)                # wide display with multiple monitors\noptions(\"digits.secs\"=3)            # show sub-second time stamps\n\nr <- getOption(\"repos\")             # hard code the US repo for CRAN\nr[\"CRAN\"] <- \"http://cran.us.r-project.org\"\noptions(repos = r)\nrm(r)\n\n## put something this is your .Rprofile to customize the defaults\nsetHook(packageEvent(\"grDevices\", \"onLoad\"),\n        function(...) grDevices::X11.options(width=8, height=8, \n                                             xpos=0, pointsize=10, \n                                             #type=\"nbcairo\"))  # Cairo device\n                                             #type=\"cairo\"))    # other Cairo dev\n                                             type=\"xlib\"))      # old default\n\n## from the AER book by Zeileis and Kleiber\noptions(prompt=\"R> \", digits=4, show.signif.stars=FALSE)\n\n\noptions(\"pdfviewer\"=\"okular\")         # on Linux, use okular as the pdf viewer\n\n```"}
{"instruction": "Temporarily change current working directory in bash to run a command", "output": "You can run the cd and the executable in a subshell by enclosing the command line in a pair of parentheses:\n\n```\n(cd SOME_PATH && exec_some_command)\n\n```\n\nDemo:\n\n```\n$ pwd\n/home/abhijit\n$ (cd /tmp && pwd)  # directory changed in the subshell\n/tmp \n$ pwd               # parent shell's pwd is still the same\n/home/abhijit\n\n```"}
{"instruction": "How do I clear the scrollback buffer in tmux?", "output": "This same question has been plaguing me for quite some time.  Here's the best I've come up with.  Put this into your .tmux.conf file:\n\n```\nbind -n C-k clear-history\n\n```\n\nThis binds ctrl-k to the tmux clear-history command.  The -n after bind makes it so you don't have to issue the tmux command prefix (ctrl-b by default).  I use bash, so ctrl-l already does the equivalent of typing \"clear\" at the command line.  With these two keys I get a nice ctrl-l, ctrl-k combo, which moves all the scroll buffer off the screen (the \"clear\") and then deletes all that history (the tmux \"clear-history\" command).\nIt's not quite as nice as Terminal's, iTerm's, or Konsole's 1-key combos for clearing it out, but it's a world better than typing in clear-history all the time."}
{"instruction": "npm install -g less does not work: EACCES: permission denied", "output": "Using sudo is not recommended. It may give you permission issue later.\nWhile the above works, I am not a fan of changing folders owned by root to be writable for users, although it may only be an issue with multiple users.\nTo work around that, you could use a group, with 'npm users' but that is also more administrative overhead.\nSee here for the options to deal with permissions from the documentation:\nhttps://docs.npmjs.com/getting-started/fixing-npm-permissions\nI would go for option 2:\n\nTo minimize the chance of permissions errors, you can configure npm to\n  use a different directory. In this example, it will be a hidden\n  directory on your home folder.\nMake a directory for global installations:\n\n```\n mkdir ~/.npm-global\n\n```\n\nConfigure npm to use the new directory path:\n\n```\n npm config set prefix '~/.npm-global'\n\n```\n\nOpen or create a ~/.profile file and add this line:\n\n```\n export PATH=~/.npm-global/bin:$PATH\n\n```\n\nBack on the command line, update your system variables:\n\n```\n source ~/.profile\n\n```\n\nTest: Download a package globally without using sudo.\n\n```\nnpm install -g jshint\n\n```\n\nIf still show permission error run (mac os):\n\n```\nsudo chown -R $USER ~/.npm-global   \n\n```\n\n\nThis works with the default ubuntu install of:\n\n```\nsudo apt-get install nodejs npm\n\n```\n\nI recommend nvm if you want more flexibility in managing versions:\nhttps://github.com/creationix/nvm\nOn MacOS use brew, it should work without sudo out of the box if you're on a recent npm version. \nEnjoy :)"}
{"instruction": "Appending a line to a file only if it does not already exist", "output": "Just keep it simple :)\ngrep + echo should suffice:\n\n```\ngrep -qxF 'include \"/configs/projectname.conf\"' foo.bar || echo 'include \"/configs/projectname.conf\"' >> foo.bar\n\n```\n\n\n-q be quiet\n-x match the whole line\n-F pattern is a plain string\nhttps://linux.die.net/man/1/grep\n\nEdit:\nincorporated @cerin and @thijs-wouters suggestions."}
{"instruction": "How can I fix the &quot;zsh: command not found: python&quot; error? (macOS Monterey 12.3, Python&amp;nbsp;3.10, Atom IDE, and atom-python-run 0.9.7)", "output": "OK, after a couple of days trying, this is what has worked for me:\n\nI reinstalled Monterey (not sure it was essential, but I just figured I had messed with terminal and $PATH too much).\nI installed python via brew rather than from the official website.\nIt would still return command not found error.\nI ran echo \"alias python=/usr/bin/python3\" >> ~/.zshrc in terminal to alias python with python3.\nRelaunch the shell or run source ~/.zshrc\n\nProblem solved.\nAs far as I get it, there is no more pre-installed python 2.x in macOS as of 12.3 hence the error. I still find it odd though that atom-python-run would call for python instead of python3 despite the settings."}
{"instruction": "How can I increase the cursor speed in terminal?", "output": "The original answer suggested using 0 as the value but no longer works as of 2019 / MacOS 10 onwards, it's now updated as 1\n\nIf by \"cursor speed\", you mean the repeat rate when holding down a key -  then have a look here:\nhttp://hints.macworld.com/article.php?story=20090823193018149\nTo summarize, open up a Terminal window and type the following command:\n\n```\ndefaults write NSGlobalDomain KeyRepeat -int 1\n\n```\n\nMore detail from the article:\n\nEverybody knows that you can get a pretty fast keyboard repeat rate by changing a slider on the Keyboard tab of the Keyboard & Mouse System Preferences panel. But you can make it even faster! In Terminal, run this command:\ndefaults write NSGlobalDomain KeyRepeat -int 0\nThen log out and log in again. The fastest setting obtainable via System Preferences is 2 (lower numbers are faster), so you may also want to try a value of 1 if 0 seems too fast. You can always visit the Keyboard & Mouse System Preferences panel to undo your changes.\nYou may find that a few applications don't handle extremely fast keyboard input very well, but most will do just fine with it.\n\nNote: It requires a system restart"}
{"instruction": "How can I set my default shell on a Mac, e.g. to Fish?", "output": "These are applicable to macOS v10.12.5 (Sierra) (16F73) and probably some other recent and upcoming versions of macOS.\n\nchsh is not enough to change the default shell. Make sure you press Command + , while your terminal is open and change the 'Shells open with' option to 'Default login shell.'\n\nIn case of Bash, make sure that you execute echo $BASH_VERSION to confirm you are running the intended version of Bash. bash --version does not give you correct information."}
{"instruction": "Change all files and folders permissions of a directory to 644/755", "output": "One approach could be using find:\nfor directories\n\n```\nfind /desired_location -type d -print0 | xargs -0 chmod 0755\n\n```\n\nfor files\n\n```\nfind /desired_location -type f -print0 | xargs -0 chmod 0644\n\n```"}
{"instruction": "How do I execute a program or call a system command?", "output": "Use subprocess.run:\n\n```\nimport subprocess\n\nsubprocess.run([\"ls\", \"-l\"]) \n\n```\n\nAnother common way is os.system but you shouldn't use it because it is unsafe if any parts of the command come from outside your program or can contain spaces or other special characters, also subprocess.run is generally more flexible (you can get the stdout, stderr, the \"real\" status code, better error handling, etc.). Even the documentation for os.system recommends using subprocess instead.\nOn Python 3.4 and earlier, use subprocess.call instead of .run:\n\n```\nsubprocess.call([\"ls\", \"-l\"])\n\n```"}
{"instruction": "How do I check if a directory exists or not in a Bash shell script?", "output": "To check if a directory exists:\n\n```\nif [ -d \"$DIRECTORY\" ]; then\n  echo \"$DIRECTORY does exist.\"\nfi\n\n```\n\nTo check if a directory does not exist:\n\n```\nif [ ! -d \"$DIRECTORY\" ]; then\n  echo \"$DIRECTORY does not exist.\"\nfi\n\n```\n\n\nHowever, as Jon Ericson points out, subsequent commands may not work as intended if you do not take into account that a symbolic link to a directory will also pass this check.\nE.g. running this:\n\n```\nln -s \"$ACTUAL_DIR\" \"$SYMLINK\"\nif [ -d \"$SYMLINK\" ]; then \n  rmdir \"$SYMLINK\" \nfi\n\n```\n\nWill produce the error message:\n\n```\nrmdir: failed to remove `symlink': Not a directory\n\n```\n\nSo symbolic links may have to be treated differently, if subsequent commands expect directories:\n\n```\nif [ -d \"$LINK_OR_DIR\" ]; then \n  if [ -L \"$LINK_OR_DIR\" ]; then\n    # It is a symlink!\n    # Symbolic link specific commands go here.\n    rm \"$LINK_OR_DIR\"\n  else\n    # It's a directory!\n    # Directory command goes here.\n    rmdir \"$LINK_OR_DIR\"\n  fi\nfi\n\n```\n\n\nTake particular note of the double-quotes used to wrap the variables. The reason for this is explained by 8jean in another answer.\nIf the variables contain spaces or other unusual characters it will probably cause the script to fail."}
{"instruction": "How to check if a string contains a substring in Bash", "output": "You can use Marcus's answer (* wildcards) outside a case statement, too, if you use double brackets:\n\n```\nstring='My long string'\nif [[ $string == *\"My long\"* ]]; then\n  echo \"It's there!\"\nfi\n\n```\n\nNote that spaces in the needle string need to be placed between double quotes, and the * wildcards should be outside. Also note that a simple comparison operator is used (i.e. ==), not the regex operator =~."}
{"instruction": "How to concatenate string variables in Bash", "output": "```\nfoo=\"Hello\"\nfoo=\"${foo} World\"\necho \"${foo}\"\n> Hello World\n\n```\n\nIn general to concatenate two variables you can just write them one after another:\n\n```\na='Hello'\nb='World'\nc=\"${a} ${b}\"\necho \"${c}\"\n> Hello World\n\n```"}
{"instruction": "How do I copy a folder from remote to local using scp?", "output": "```\nscp -r user@your.server.example.com:/path/to/foo /home/user/Desktop/\n\n```\n\nBy not including the trailing '/' at the end of foo, you will copy the directory itself (including contents), rather than only the contents of the directory.\nFrom man scp (See online manual)\n\n-r Recursively copy entire directories"}
{"instruction": "What does &quot; 2&gt;&amp;1 &quot; mean?", "output": "File descriptor 1 is the standard output (stdout).\nFile descriptor 2 is the standard error (stderr).\nAt first, 2>1 may look like a good way to redirect stderr to stdout. However, it will actually be interpreted as \"redirect stderr to a file named 1\".\n& indicates that what follows and precedes is a file descriptor, and not a filename. Thus, we use 2>&1. Consider >& to be a redirect merger operator."}
{"instruction": "How can I recursively find all files in current and subfolders based on wildcard matching?", "output": "Use find:\n\n```\nfind . -name \"foo*\"\n\n```\n\nfind needs a starting point, so the . (dot) points to the current directory.\nIf you need case insensitive search use :\n\n```\nfind . -iname \"foo*\"\n\n```"}
{"instruction": "How do I split a string on a delimiter in Bash?", "output": "You can set the internal field separator (IFS) variable, and then let it parse into an array. When this happens in a command, then the assignment to IFS only takes place to that single command's environment (to read ). It then parses the input according to the IFS variable value into an array, which we can then iterate over.\nThis example will parse one line of items separated by ;, pushing it into an array:\n\n```\nIFS=';' read -ra ADDR <<< \"$IN\"\nfor i in \"${ADDR[@]}\"; do\n  # process \"$i\"\ndone\n\n```\n\nThis other example is for processing the whole content of $IN, each time one line of input separated by ;:\n\n```\nwhile IFS=';' read -ra ADDR; do\n  for i in \"${ADDR[@]}\"; do\n    # process \"$i\"\n  done\ndone <<< \"$IN\"\n\n```"}
{"instruction": "How to mkdir only if a directory does not already exist?", "output": "Try mkdir -p:\n\n```\nmkdir -p foo\n\n```\n\nNote that this will also create any intermediate directories that don't exist; for instance,\n\n```\nmkdir -p foo/bar/baz\n\n```\n\nwill create directories foo, foo/bar, and foo/bar/baz if they don't exist.\nSome implementation like GNU mkdir include mkdir --parents as a more readable alias, but this is not specified in POSIX/Single Unix Specification and not available on many common platforms like macOS, various BSDs, and various commercial Unixes, so it should be avoided.\nIf you want an error when parent directories don't exist, and want to create the directory if it doesn't exist, then you can test for the existence of the directory first:\n\n```\n[ -d foo ] || mkdir foo\n\n```"}
{"instruction": "How do I set a variable to the output of a command in Bash?", "output": "In addition to backticks `command`, command substitution can be done with $(command) or \"$(command)\", which I find easier to read, and allows for nesting.\n\n```\nOUTPUT=\"$(ls -1)\"\necho \"${OUTPUT}\"\n\nMULTILINE=\"$(ls \\\n   -1)\"\necho \"${MULTILINE}\"\n\n```\n\nQuoting (\") does matter to preserve multi-line variable values and it is safer to use with whitespace and special characters such as (*) and therefore advised; it is, however, optional on the right-hand side of an assignment when word splitting is not performed, so OUTPUT=$(ls -1) would work fine."}
{"instruction": "How to check if a variable is set in Bash", "output": "(Usually) The right way\n\n```\nif [ -z ${var+x} ]; then echo \"var is unset\"; else echo \"var is set to '$var'\"; fi\n\n```\n\nwhere ${var+x} is a parameter expansion which evaluates to nothing if var is unset, and substitutes the string x otherwise.\nQuotes Digression\nQuotes can be omitted (so we can say ${var+x} instead of \"${var+x}\") because this syntax & usage guarantees this will only expand to something that does not require quotes (since it either expands to x (which contains no word breaks so it needs no quotes), or to nothing (which results in [ -z  ], which conveniently evaluates to the same value (true) that [ -z \"\" ] does as well)).\nHowever, while quotes can be safely omitted, and it was not immediately obvious to all (it wasn't even apparent to the first author of this quotes explanation who is also a major Bash coder), it would sometimes be better to write the solution with quotes as [ -z \"${var+x}\" ], at the very small possible cost of an O(1) speed penalty.  The first author also added this as a comment next to the code using this solution giving the URL to this answer, which now also includes the explanation for why the quotes can be safely omitted.\n(Often) The wrong way\n\n```\nif [ -z \"$var\" ]; then echo \"var is blank\"; else echo \"var is set to '$var'\"; fi\n\n```\n\nThis is often wrong because it doesn't distinguish between a variable that is unset and a variable that is set to the empty string. That is to say, if var='', then the above solution will output \"var is blank\".\nThe distinction between unset and \"set to the empty string\" is essential in situations where the user has to specify an extension, or additional list of properties, and that not specifying them defaults to a non-empty value, whereas specifying the empty string should make the script use an empty extension or list of additional properties.\nThe distinction may not be essential in every scenario though. In those cases  [ -z \"$var\" ] will be just fine."}
{"instruction": "How to delete from a text file, all lines that contain a specific string?", "output": "To remove the line and print the output to standard out:\n\n```\nsed '/pattern to match/d' ./infile\n\n```\n\nTo directly modify the file ‚Äì does not work with BSD sed:\n\n```\nsed -i '/pattern to match/d' ./infile\n\n```\n\nSame, but for BSD sed (Mac OS X and FreeBSD) ‚Äì does not work with GNU sed:\n\n```\nsed -i '' '/pattern to match/d' ./infile\n\n```\n\nTo directly modify the file (and create a backup) ‚Äì works with BSD and GNU sed:\n\n```\nsed -i.bak '/pattern to match/d' ./infile\n\n```"}
{"instruction": "Loop through an array of strings in Bash?", "output": "You can use it like this:\n\n```\n## declare an array variable\ndeclare -a arr=(\"element1\" \"element2\" \"element3\")\n\n## now loop through the above array\nfor i in \"${arr[@]}\"\ndo\n   echo \"$i\"\n   # or do whatever with individual element of the array\ndone\n\n# You can access them using echo \"${arr[0]}\", \"${arr[1]}\" also\n\n```\n\nAlso works for multi-line array declaration\n\n```\ndeclare -a arr=(\"element1\" \n                \"element2\" \"element3\"\n                \"element4\"\n                )\n\n```"}
{"instruction": "How do I exclude a directory when using `find`?", "output": "Use the -prune primary. For example, if you want to exclude ./misc:\n\n```\nfind . -path ./misc -prune -o -name '*.txt' -print\n\n```\n\nNote that the final -print is needed to avoid printing out ./misc. See cycollins's answer for the explanation why.\nTo exclude multiple directories, OR them between parentheses.\n\n```\nfind . -type d \\( -path ./dir1 -o -path ./dir2 -o -path ./dir3 \\) -prune -o -name '*.txt' -print\n\n```\n\nAnd, to exclude directories with a specific name at any level, use the -name primary instead of -path.\n\n```\nfind . -type d -name node_modules -prune -o -name '*.json' -print\n\n```"}
{"instruction": "How to reload .bashrc settings without logging out and back in again?", "output": "You can enter the long form command:\n\n```\nsource ~/.bashrc\n\n```\n\nor you can use the shorter version of the command:\n\n```\n. ~/.bashrc\n\n```"}
{"instruction": "How do I iterate over a range of numbers defined by variables in Bash?", "output": "```\nfor i in $(seq 1 $END); do echo $i; done\n```\n\nedit: I prefer seq over the other methods because I can actually remember it ;)"}
{"instruction": "How can I count all the lines of code in a directory recursively?", "output": "Try:\n\n```\nfind . -name '*.php' | xargs wc -l\n\n```\n\nor (when file names include special characters such as spaces)\n\n```\nfind . -name '*.php' | sed 's/.*/\"&\"/' | xargs  wc -l\n\n```\n\nThe SLOCCount tool may help as well.\nIt will give an accurate source lines of code count for whatever\nhierarchy you point it at, as well as some additional stats.\nSorted output:\nfind . -name '*.php' | xargs wc -l | sort -nr"}
{"instruction": "Check existence of input argument in a Bash shell script", "output": "It is:\n\n```\nif [ $# -eq 0 ]\n  then\n    echo \"No arguments supplied\"\nfi\n\n```\n\nThe $# variable will tell you the number of input arguments the script was passed.\nOr you can check if an argument is an empty string or not like:\n\n```\nif [ -z \"$1\" ]\n  then\n    echo \"No argument supplied\"\nfi\n\n```\n\nThe -z switch will test if the expansion of \"$1\" is a null string or not. If it is a null string then the body is executed."}
{"instruction": "How do I prompt for Yes/No/Cancel input in a Linux shell script?", "output": "A widely available method to get user input at a shell prompt is the read command. Here is a demonstration:\n\n```\nwhile true; do\n    read -p \"Do you wish to install this program? \" yn\n    case $yn in\n        [Yy]* ) make install; break;;\n        [Nn]* ) exit;;\n        * ) echo \"Please answer yes or no.\";;\n    esac\ndone\n\n```\n\n\nAnother method, pointed out by Steven Huwig, is Bash's select command. Here is the same example using select:\n\n```\necho \"Do you wish to install this program?\"\nselect yn in \"Yes\" \"No\"; do\n    case $yn in\n        Yes ) make install; break;;\n        No ) exit;;\n    esac\ndone\n\n```\n\nWith select you don't need to sanitize the input ‚Äì it displays the available choices, and you type a number corresponding to your choice. It also loops automatically, so there's no need for a while true loop to retry if they give invalid input. If you want to allow more flexible input (accepting the words of the options, rather than just their number), you can alter it like this:\n\n```\necho \"Do you wish to install this program?\"\nselect strictreply in \"Yes\" \"No\"; do\n    relaxedreply=${strictreply:-$REPLY}\n    case $relaxedreply in\n        Yes | yes | y ) make install; break;;\n        No  | no  | n ) exit;;\n    esac\ndone\n\n```\n\n\nAlso, L√©a Gris demonstrated a way to make the request language agnostic in her answer. Adapting my first example to better serve multiple languages might look like this:\n\n```\nset -- $(locale LC_MESSAGES)\nyesexpr=\"$1\"; noexpr=\"$2\"; yesword=\"$3\"; noword=\"$4\"\n\nwhile true; do\n    read -p \"Install (${yesword} / ${noword})? \" yn\n    if [[ \"$yn\" =~ $yesexpr ]]; then make install; exit; fi\n    if [[ \"$yn\" =~ $noexpr ]]; then exit; fi\n    echo \"Answer ${yesword} / ${noword}.\"\ndone\n\n```\n\nObviously other communication strings remain untranslated here (Install, Answer) which would need to be addressed in a more fully completed translation, but even a partial translation would be helpful in many cases.\n\nFinally, please check out the excellent answer by F. Hauri."}
{"instruction": "How to specify the private SSH-key to use when executing shell command on Git?", "output": "Something like this should work (suggested by orip):\n\n```\nssh-agent bash -c 'ssh-add /somewhere/yourkey; git clone git@github.com:user/project.git'\n\n```\n\nif you prefer subshells, you could try the following (though it is more fragile):\n\n```\nssh-agent $(ssh-add /somewhere/yourkey; git clone git@github.com:user/project.git)\n\n```\n\nGit will invoke SSH which will find its agent by environment variable; this will, in turn, have the key loaded.\nAlternatively, setting HOME may also do the trick, provided you are willing to setup a directory that contains only a .ssh directory as HOME; this may either contain an identity.pub, or a config file setting IdentityFile."}
{"instruction": "How to convert a string to lower case in Bash", "output": "There are various ways:\nPOSIX standard\ntr\n\n```\n$ echo \"$a\" | tr '[:upper:]' '[:lower:]'\nhi all\n\n```\n\nAWK\n\n```\n$ echo \"$a\" | awk '{print tolower($0)}'\nhi all\n\n```\n\nNon-POSIX\nYou may run into portability issues with the following examples:\nBash 4.0\n\n```\n$ echo \"${a,,}\"\nhi all\n\n```\n\nsed\n\n```\n$ echo \"$a\" | sed -e 's/\\(.*\\)/\\L\\1/'\nhi all\n# this also works:\n$ sed -e 's/\\(.*\\)/\\L\\1/' <<< \"$a\"\nhi all\n\n```\n\nPerl\n\n```\n$ echo \"$a\" | perl -ne 'print lc'\nhi all\n\n```\n\nBash\n\n```\nlc(){\n    case \"$1\" in\n        [A-Z])\n        n=$(printf \"%d\" \"'$1\")\n        n=$((n+32))\n        printf \\\\$(printf \"%o\" \"$n\")\n        ;;\n        *)\n        printf \"%s\" \"$1\"\n        ;;\n    esac\n}\nword=\"I Love Bash\"\nfor((i=0;i<${#word};i++))\ndo\n    ch=\"${word:$i:1}\"\n    lc \"$ch\"\ndone\n\n```\n\nNote: YMMV on this one. Doesn't work for me (GNU bash version 4.2.46 and 4.0.33 (and same behaviour 2.05b.0 but nocasematch  is not implemented)) even with using shopt -u nocasematch;. Unsetting that nocasematch causes [[ \"fooBaR\" == \"FOObar\" ]] to match OK BUT inside case weirdly [b-z] are incorrectly matched by [A-Z]. Bash is confused by the double-negative (\"unsetting nocasematch\")! :-)"}
{"instruction": "YYYY-MM-DD format date in shell script", "output": "In bash (>=4.2) it is preferable to use printf's built-in date formatter (part of bash) rather than the external date (usually GNU date). Note that invoking a subshell has performance problems in Cygwin due to a slow fork() call on Windows.\nAs such:\n\n```\n# put current date as yyyy-mm-dd in $date\n# -1 -> explicit current date, bash >=4.3 defaults to current time if not provided\n# -2 -> start time for shell\nprintf -v date '%(%Y-%m-%d)T\\n' -1\n\n# put current date as yyyy-mm-dd HH:MM:SS in $date\nprintf -v date '%(%Y-%m-%d %H:%M:%S)T\\n' -1\n\n# to print directly remove -v flag, as such:\nprintf '%(%Y-%m-%d)T\\n' -1\n# -> current date printed to terminal\n\n```\n\nIn bash (<4.2):\n\n```\n# put current date as yyyy-mm-dd in $date\ndate=$(date '+%Y-%m-%d')\n\n# put current date as yyyy-mm-dd HH:MM:SS in $date\ndate=$(date '+%Y-%m-%d %H:%M:%S')\n\n# print current date directly\necho $(date '+%Y-%m-%d')\n\n```\n\nOther available date formats can be viewed from the date man pages (for external non-bash specific command):\n\n```\nman date\n\n```"}
{"instruction": "How can I declare and use Boolean variables in a shell script?", "output": "Revised Answer (Feb 12, 2014)\n\n```\nthe_world_is_flat=true\n# ...do something interesting...\nif [ \"$the_world_is_flat\" = true ] ; then\n    echo 'Be careful not to fall off!'\nfi\n\n```\n\n\nOriginal Answer\nCaveats: https://stackoverflow.com/a/21210966/89391\n\n```\nthe_world_is_flat=true\n# ...do something interesting...\nif $the_world_is_flat ; then\n    echo 'Be careful not to fall off!'\nfi\n\n```\n\nFrom: Using boolean variables in Bash\nThe reason the original answer is included here is because the comments before the revision on Feb 12, 2014 pertain only to the original answer, and many of the comments are wrong when associated with the revised answer. For example, Dennis Williamson's comment about Bash's builtin true on Jun 2, 2010 only applies to the original answer, not the revised."}
{"instruction": "Running shell command and capturing the output", "output": "In all officially maintained versions of Python, the simplest approach is to use the subprocess.check_output function:\n\n```\n>>> subprocess.check_output(['ls', '-l'])\nb'total 0\\n-rw-r--r--  1 memyself  staff  0 Mar 14 11:04 files\\n'\n\n```\n\ncheck_output runs a single program that takes only arguments as input.1 It returns the result exactly as printed to stdout. If you need to write input to stdin, skip ahead to the run or Popen sections. If you want to execute complex shell commands, see the note on shell=True at the end of this answer.\nThe check_output function works in all officially maintained versions of Python. But for more recent versions, a more flexible approach is available.\nModern versions of Python (3.5 or higher): run\nIf you're using Python 3.5+, and do not need backwards compatibility, the new run function is recommended by the official documentation for most tasks. It provides a very general, high-level API for the subprocess module. To capture the output of a program, pass the subprocess.PIPE flag to the stdout keyword argument. Then access the stdout attribute of the returned CompletedProcess object:\n\n```\n>>> import subprocess\n>>> result = subprocess.run(['ls', '-l'], stdout=subprocess.PIPE)\n>>> result.stdout\nb'total 0\\n-rw-r--r--  1 memyself  staff  0 Mar 14 11:04 files\\n'\n\n```\n\nThe return value is a bytes object, so if you want a proper string, you'll need to decode it. Assuming the called process returns a UTF-8-encoded string:\n\n```\n>>> result.stdout.decode('utf-8')\n'total 0\\n-rw-r--r--  1 memyself  staff  0 Mar 14 11:04 files\\n'\n\n```\n\nThis can all be compressed to a one-liner if desired:\n\n```\n>>> subprocess.run(['ls', '-l'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n'total 0\\n-rw-r--r--  1 memyself  staff  0 Mar 14 11:04 files\\n'\n\n```\n\nIf you want to pass input to the process's stdin, you can pass a bytes object to the input keyword argument:\n\n```\n>>> cmd = ['awk', 'length($0) > 5']\n>>> ip = 'foo\\nfoofoo\\n'.encode('utf-8')\n>>> result = subprocess.run(cmd, stdout=subprocess.PIPE, input=ip)\n>>> result.stdout.decode('utf-8')\n'foofoo\\n'\n\n```\n\nYou can capture errors by passing stderr=subprocess.PIPE (capture to result.stderr) or stderr=subprocess.STDOUT (capture to result.stdout along with regular output). If you want run to throw an exception when the process returns a nonzero exit code, you can pass check=True. (Or you can check the returncode attribute of result above.) When security is not a concern, you can also run more complex shell commands by passing shell=True as described at the end of this answer.\nLater versions of Python streamline the above further. In Python 3.7+, the above one-liner can be spelled like this:\n\n```\n>>> subprocess.run(['ls', '-l'], capture_output=True, text=True).stdout\n'total 0\\n-rw-r--r--  1 memyself  staff  0 Mar 14 11:04 files\\n'\n\n```\n\nUsing run this way adds just a bit of complexity, compared to the old way of doing things. But now you can do almost anything you need to do with the run function alone.\nOlder versions of Python (3-3.4): more about check_output\nIf you are using an older version of Python, or need modest backwards compatibility, you can use the check_output function as briefly described above. It has been available since Python 2.7.\n\n```\nsubprocess.check_output(*popenargs, **kwargs)  \n\n```\n\nIt takes takes the same arguments as Popen (see below), and returns a string containing the program's output. The beginning of this answer has a more detailed usage example. In Python 3.5+, check_output is equivalent to executing run with check=True and stdout=PIPE, and returning just the stdout attribute.\nYou can pass stderr=subprocess.STDOUT to ensure that error messages are included in the returned output. When security is not a concern, you can also run more complex shell commands by passing shell=True as described at the end of this answer.\nIf you need to pipe from stderr or pass input to the process, check_output won't be up to the task. See the Popen examples below in that case.\nComplex applications and legacy versions of Python (2.6 and below): Popen\nIf you need deep backwards compatibility, or if you need more sophisticated functionality than check_output or run provide, you'll have to work directly with Popen objects, which encapsulate the low-level API for subprocesses.\nThe Popen constructor accepts either a single command without arguments, or a list containing a command as its first item, followed by any number of arguments, each as a separate item in the list. shlex.split can help parse strings into appropriately formatted lists. Popen objects also accept a host of different arguments for process IO management and low-level configuration.\nTo send input and capture output, communicate is almost always the preferred method. As in:\n\n```\noutput = subprocess.Popen([\"mycmd\", \"myarg\"], \n                          stdout=subprocess.PIPE).communicate()[0]\n\n```\n\nOr\n\n```\n>>> import subprocess\n>>> p = subprocess.Popen(['ls', '-a'], stdout=subprocess.PIPE, \n...                                    stderr=subprocess.PIPE)\n>>> out, err = p.communicate()\n>>> print out\n.\n..\nfoo\n\n```\n\nIf you set stdin=PIPE, communicate also allows you to pass data to the process via stdin:\n\n```\n>>> cmd = ['awk', 'length($0) > 5']\n>>> p = subprocess.Popen(cmd, stdout=subprocess.PIPE,\n...                           stderr=subprocess.PIPE,\n...                           stdin=subprocess.PIPE)\n>>> out, err = p.communicate('foo\\nfoofoo\\n')\n>>> print out\nfoofoo\n\n```\n\nNote Aaron Hall's answer, which indicates that on some systems, you may need to set stdout, stderr, and stdin all to PIPE (or DEVNULL) to get communicate to work at all.\nIn some rare cases, you may need complex, real-time output capturing. Vartec's answer suggests a way forward, but methods other than communicate are prone to deadlocks if not used carefully.\nAs with all the above functions, when security is not a concern, you can run more complex shell commands by passing shell=True.\nNotes\n1. Running shell commands: the shell=True argument\nNormally, each call to run, check_output, or the Popen constructor executes a single program. That means no fancy bash-style pipes. If you want to run complex shell commands, you can pass shell=True, which all three functions support. For example:\n\n```\n>>> subprocess.check_output('cat books/* | wc', shell=True, text=True)\n' 1299377 17005208 101299376\\n'\n\n```\n\nHowever, doing this raises security concerns. If you're doing anything more than light scripting, you might be better off calling each process separately, and passing the output from each as an input to the next, via\n\n```\nrun(cmd, [stdout=etc...], input=other_output)\n\n```\n\nOr\n\n```\nPopen(cmd, [stdout=etc...]).communicate(other_output)\n\n```\n\nThe temptation to directly connect pipes is strong; resist it. Otherwise, you'll likely see deadlocks or have to do hacky things like this."}
{"instruction": "Assigning default values to shell variables with a single command in bash", "output": "Very close to what you posted, actually. You can use something called Bash parameter expansion to accomplish this.\nTo get the assigned value, or default if it's missing:\n\n```\nFOO=\"${VARIABLE:-default}\"  # FOO will be assigned 'default' value if VARIABLE not set or null.\n# The value of VARIABLE remains untouched.\n\n```\n\nTo do the same, as well as assign default to VARIABLE:\n\n```\nFOO=\"${VARIABLE:=default}\"  # If VARIABLE not set or null, set its value to 'default'. \n# Then that value will be assigned to FOO\n\n```"}
{"instruction": "Replace one substring for another string in shell script", "output": "To replace the first occurrence of a pattern with a given string, use ${parameter/pattern/string}:\n\n```\n#!/bin/bash\nfirstString=\"I love Suzi and Marry\"\nsecondString=\"Sara\"\necho \"${firstString/Suzi/\"$secondString\"}\"\n# prints 'I love Sara and Marry'\n\n```\n\nTo replace all occurrences, use ${parameter//pattern/string}:\n\n```\nmessage='The secret code is 12345'\necho \"${message//[0-9]/X}\"\n# prints 'The secret code is XXXXX'\n\n```\n\n(This is documented in the Bash Reference Manual, ¬ß3.5.3 \"Shell Parameter Expansion\".)\nNote that this feature is not specified by POSIX ‚Äî it's a Bash extension ‚Äî so not all Unix shells implement it. For the relevant POSIX documentation, see The Open Group Technical Standard Base Specifications, Issue 7, the Shell & Utilities volume, ¬ß2.6.2 \"Parameter Expansion\"."}
{"instruction": "How to echo shell commands as they are executed", "output": "set -x or set -o xtrace expands variables and prints a little + sign before the line.\nset -v or set -o verbose does not expand the variables before printing.\nUse set +x and set +v to turn off the above settings.\nOn the first line of the script, one can put #!/bin/sh -x (or -v) to have the same effect as set -x (or -v) later in the script.\nThe above also works with /bin/sh.\nSee the bash-hackers' wiki on set attributes, and on debugging.\n\n```\n$ cat shl\n#!/bin/bash                                                                     \n\nDIR=/tmp/so\nls $DIR\n\n$ bash -x shl \n+ DIR=/tmp/so\n+ ls /tmp/so\n$\n\n```"}
{"instruction": "Should I put #! (shebang) in Python scripts, and what form should it take?", "output": "The shebang line in any script determines the script's ability to be executed like a standalone executable without typing python beforehand in the terminal or when double clicking it in a file manager (when configured properly). It isn't necessary but generally put there so when someone sees the file opened in an editor, they immediately know what they're looking at. However, which shebang line you use is important.\nCorrect usage for (defaults to version 3.latest) Python 3 scripts is:\n\n```\n#!/usr/bin/env python3\n\n```\n\nCorrect usage for (defaults to version 2.latest) Python 2 scripts is:\n\n```\n#!/usr/bin/env python2\n\n```\n\nThe following should not be used (except for the rare case that you are writing code which is compatible with both Python 2.x and 3.x):\n\n```\n#!/usr/bin/env python\n\n```\n\nThe reason for these recommendations, given in PEP 394, is that python can refer either to python2 or python3 on different systems.\nAlso, do not use:\n\n```\n#!/usr/local/bin/python\n\n```\n\n\n\"python may be installed at /usr/bin/python or /bin/python in those\ncases, the above #! will fail.\"\n\n‚Äï\"#!/usr/bin/env python\" vs \"#!/usr/local/bin/python\""}
{"instruction": "How to call shell commands from Ruby", "output": "This explanation is based on a commented Ruby script from a friend of mine. If you want to improve the script, feel free to update it at the link.\nFirst, note that when Ruby calls out to a shell, it typically calls /bin/sh, not Bash. Some Bash syntax is not supported by /bin/sh on all systems.\nHere are ways to execute a shell script:\n\n```\ncmd = \"echo 'hi'\" # Sample string that can be used\n\n```\n\n\nKernel#` , commonly called backticks ‚Äì `cmd`\nThis is like many other languages, including Bash, PHP, and Perl.\nReturns the result (i.e. standard output) of the shell command.\nDocs: http://ruby-doc.org/core/Kernel.html#method-i-60\n\n```\nvalue = `echo 'hi'`\nvalue = `#{cmd}`\n\n```\n\nBuilt-in syntax, %x( cmd )\nFollowing the x character is a delimiter, which can be any character.\nIf the delimiter is one of the characters (, [, {, or <,\nthe literal consists of the characters up to the matching closing delimiter,\ntaking account of nested delimiter pairs. For all other delimiters, the\nliteral comprises the characters up to the next occurrence of the\ndelimiter character.  String interpolation #{ ... } is allowed.\nReturns the result (i.e. standard output) of the shell command, just like the backticks.\nDocs: https://docs.ruby-lang.org/en/master/syntax/literals_rdoc.html#label-Percent+Strings\n\n```\nvalue = %x( echo 'hi' )\nvalue = %x[ #{cmd} ]\n\n```\n\nKernel#system\nExecutes the given command in a subshell. \nReturns true if the command was found and run successfully, false otherwise.\nDocs: http://ruby-doc.org/core/Kernel.html#method-i-system\n\n```\nwasGood = system( \"echo 'hi'\" )\nwasGood = system( cmd )\n\n```\n\nKernel#exec\nReplaces the current process by running the given external command.\nReturns none, the current process is replaced and never continues.\nDocs: http://ruby-doc.org/core/Kernel.html#method-i-exec\n\n```\nexec( \"echo 'hi'\" )\nexec( cmd ) # Note: this will never be reached because of the line above\n\n```\n\n\nHere's some extra advice:\n$?, which is the same as $CHILD_STATUS, accesses the status of the last system executed command if you use the backticks, system() or %x{}.\nYou can then access the exitstatus and pid properties:\n\n```\n$?.exitstatus\n\n```\n\nFor more reading see:\n\nhttp://www.elctech.com/blog/i-m-in-ur-commandline-executin-ma-commands\nhttp://blog.jayfields.com/2006/06/ruby-kernel-system-exec-and-x.html\nhttp://tech.natemurray.com/2007/03/ruby-shell-commands.html"}
{"instruction": "Defining a variable with or without export", "output": "export makes the variable available to sub-processes.\nThat is,\n\n```\nexport name=value\n\n```\n\nmeans that the variable name is available to any process you run from that shell process. If you want a process to make use of this variable, use export, and run the process from that shell.\n\n```\nname=value\n\n```\n\nmeans the variable scope is restricted to the shell, and is not available to any other process. You would use this for (say) loop variables, temporary variables etc. An important exception to this rule is that if you define the variable while running a command, that variable will be available to child processes. For example\n\n```\nMY_VAR=yay node my-script.js\n\n```\n\nIn this case MY_VAR will be available to the node process running my-script.\nIt's important to note that exporting a variable doesn't make it available to parent processes. That is, specifying and exporting a variable in a spawned process doesn't make it available in the process that launched it."}
{"instruction": "How to reload .bash_profile from the command line", "output": "Simply type source ~/.bash_profile\nAlternatively, if you like saving keystrokes, you can type . ~/.bash_profile"}
{"instruction": "Extract substring in Bash", "output": "Use cut:\n\n```\necho 'someletters_12345_moreleters.ext' | cut -d'_' -f 2\n\n```\n\nMore generic:\n\n```\nINPUT='someletters_12345_moreleters.ext'\nSUBSTRING=$(echo $INPUT| cut -d'_' -f 2)\necho $SUBSTRING\n\n```"}
{"instruction": "What does &#39;set -e&#39; mean in a Bash script?", "output": "From help set and Bash Reference Documentation: The Set Builtin:\n\n```\n  -e  Exit immediately if a command exits with a non-zero status.\n\n```\n\nBut it's considered bad practice by some (Bash FAQ and IRC Freenode #bash FAQ authors). It's recommended to use:\n\n```\ntrap 'do_something' ERR\n\n```\n\nto run do_something function when errors occur.\nSee Why doesn't set -e (or set -o errexit, or trap ERR) do what I expected?"}
{"instruction": "Get current directory or folder name (without the full path)", "output": "No need for basename, and especially no need for a subshell running pwd (which adds an extra, and expensive, fork operation); the shell can do this internally using parameter expansion:\n\n```\nresult=${PWD##*/}          # to assign to a variable\nresult=${result:-/}        # to correct for the case where PWD is / (root)\n\nprintf '%s\\n' \"${PWD##*/}\" # to print to stdout\n                           # ...more robust than echo for unusual names\n                           #    (consider a directory named -e or -n)\n\nprintf '%q\\n' \"${PWD##*/}\" # to print to stdout, quoted for use as shell input\n                           # ...useful to make hidden characters readable.\n\n```\n\n\nNote that if you're applying this technique in other circumstances (not PWD, but some other variable holding a directory name), you might need to trim any trailing slashes. The below uses bash's extglob support to work even with multiple trailing slashes:\n\n```\ndirname=/path/to/somewhere//\nshopt -s extglob           # enable +(...) glob syntax\nresult=${dirname%%+(/)}    # trim however many trailing slashes exist\nresult=${result##*/}       # remove everything before the last / that still remains\nresult=${result:-/}        # correct for dirname=/ case\nprintf '%s\\n' \"$result\"\n\n```\n\nAlternatively, without extglob:\n\n```\ndirname=\"/path/to/somewhere//\"\nresult=\"${dirname%\"${dirname##*[!/]}\"}\" # extglob-free multi-trailing-/ trim\nresult=\"${result##*/}\"                  # remove everything before the last /\nresult=${result:-/}                     # correct for dirname=/ case\n\n```"}
{"instruction": "Shell command to tar directory excluding certain files/folders", "output": "You can have multiple exclude options for tar so\n\n```\n$ tar --exclude='./folder' --exclude='./upload/folder2' -zcvf /backup/filename.tgz .\n\n```\n\netc will work. Make sure to put --exclude before the source and destination items."}
{"instruction": "Shell command to sum integers, one per line?", "output": "Bit of awk should do it? \n\n```\nawk '{s+=$1} END {print s}' mydatafile\n\n```\n\nNote: some versions of awk have some odd behaviours if you are going to be adding anything exceeding 2^31 (2147483647). See comments for more background. One suggestion is to use printf rather than print:\n\n```\nawk '{s+=$1} END {printf \"%.0f\", s}' mydatafile\n\n```"}
{"instruction": "How do I put an already-running process under nohup?", "output": "Using the Job Control of bash to send the process into the background:\n\nCtrl+Z to stop (pause) the program and get back to the shell.\nbg to run it in the background.\ndisown -h [job-spec] where [job-spec] is the job number (like %1 for the first running job; find about your number with the jobs command) so that the job isn't killed when the terminal closes."}
{"instruction": "How to add line break to &#39;git commit -m&#39; from the command line?", "output": "Certainly, how it's done depends on your shell. In Bash, you can use single quotes around the message and can just leave the quote open, which will make Bash prompt for another line, until you close the quote. Like this:\n\n```\ngit commit -m 'Message\n\ngoes\nhere'\n\n```\n\nAlternatively, you can use a \"here document\" (also known as heredoc):\n\n```\ngit commit -F- <<EOF\nMessage\n\ngoes\nhere\nEOF\n\n```"}
{"instruction": "Count number of lines in a git repository", "output": "xargs will let you cat all the files together before passing them to wc, like you asked:\n\n```\ngit ls-files | xargs cat | wc -l\n\n```\n\nBut skipping the intermediate cat gives you more information and is probably better:\n\n```\ngit ls-files | xargs wc -l\n\n```"}
{"instruction": "Given two directory trees, how can I find out which files differ by content?", "output": "Try:\n\n```\ndiff --brief --recursive dir1/ dir2/\n\n```\n\nOr alternatively, with the short flags -qr:\n\n```\ndiff -qr dir1/ dir2/\n\n```\n\nIf you also want to see differences for files that may not exist in either directory:\n\n```\ndiff --brief --recursive --new-file dir1/ dir2/  # with long options\ndiff -qrN dir1/ dir2/                            # with short flag aliases\n\n```"}
{"instruction": "When do we need curly braces around shell variables?", "output": "In your particular example, it makes no difference. However, the {} in ${} are useful if you want to expand the variable foo in the string\n\n```\n\"${foo}bar\"\n\n```\n\nsince \"$foobar\" would instead expand the variable identified by foobar.\nCurly braces are also unconditionally required when:\n\nexpanding array elements, as in ${array[42]}\nusing parameter expansion operations, as in ${filename%.*} (remove extension; strips smallest match)\nexpanding positional parameters beyond 9: \"$8 $9 ${10} ${11}\"\n\nDoing this everywhere, instead of just in potentially ambiguous cases, can be considered good programming practice. This is both for consistency and to avoid surprises like $foo_$bar.jpg, where it's not visually obvious that the underscore becomes part of the variable name."}
{"instruction": "How can I compare numbers in Bash?", "output": "In Bash, you should do your check in an arithmetic context:\n\n```\nif (( a > b )); then\n    ...\nfi\n\n```\n\nFor POSIX shells that don't support (()), you can use -lt and -gt.\n\n```\nif [ \"$a\" -gt \"$b\" ]; then\n    ...\nfi\n\n```\n\nYou can get a full list of comparison operators with help test or man test."}
{"instruction": "Difference between single and double quotes in Bash", "output": "Single quotes won't interpolate anything, but double quotes will. For example: variables, backticks, certain \\ escapes, etc. \nExample:\n\n```\n$ echo \"$(echo \"upg\")\"\nupg\n$ echo '$(echo \"upg\")'\n$(echo \"upg\")\n\n```\n\nThe Bash manual has this to say:\n\n3.1.2.2 Single Quotes\nEnclosing characters in single quotes (') preserves the literal value of each character within the quotes. A single quote may not occur between single quotes, even when preceded by a backslash. \n3.1.2.3 Double Quotes\nEnclosing characters in double quotes (\") preserves the literal value of all characters within the quotes, with the exception of $, `, \\, and, when history expansion is enabled, !. The characters $ and ` retain their special meaning within double quotes (see Shell Expansions). The backslash retains its special meaning only when followed by one of the following characters: $, `, \", \\, or newline. Within double quotes, backslashes that are followed by one of these characters are removed. Backslashes preceding characters without a special meaning are left unmodified. A double quote may be quoted within double quotes by preceding it with a backslash. If enabled, history expansion will be performed unless an ! appearing in double quotes is escaped using a backslash. The backslash preceding the ! is not removed.\nThe special parameters * and @ have special meaning when in double quotes (see Shell Parameter Expansion)."}
{"instruction": "Use grep --exclude/--include syntax to not grep through certain files", "output": "Use the shell globbing syntax:\n\n```\ngrep pattern -r --include=\\*.cpp --include=\\*.h rootdir\n\n```\n\nThe syntax for --exclude is identical.\nNote that the star is escaped with a backslash to prevent it from being expanded by the shell (quoting it, such as --include=\"*.cpp\", would work just as well).  Otherwise, if you had any files in the current working directory that matched the pattern, the command line would expand to something like grep pattern -r --include=foo.cpp --include=bar.cpp rootdir, which would only search files named foo.cpp and bar.cpp, which is quite likely not what you wanted.\nUpdate 2021-03-04\nI've edited the original answer to remove the use of brace expansion, which is a feature provided by several shells such as Bash and zsh to simplify patterns like this; but note that brace expansion is not POSIX shell-compliant.\nThe original example was:\n\n```\ngrep pattern -r --include=\\*.{cpp,h} rootdir\n\n```\n\nto search through all .cpp and .h files rooted in the directory rootdir."}
{"instruction": "Redirect stderr and stdout in Bash", "output": "Take a look here. It should be:\n\n```\nyourcommand &> filename\n\n```\n\nIt redirects both standard output and standard error to file filename."}
{"instruction": "How do I test if a variable is a number in Bash?", "output": "One approach is to use a regular expression, like so:\n\n```\nre='^[0-9]+$'\nif ! [[ $yournumber =~ $re ]] ; then\n   echo \"error: Not a number\" >&2; exit 1\nfi\n\n```\n\nIf the value is not necessarily an integer, consider amending the regex appropriately; for instance:\n\n```\n^[0-9]+([.][0-9]+)?$\n\n```\n\n...or, to handle numbers with a sign:\n\n```\n^[+-]?[0-9]+([.][0-9]+)?$\n\n```"}
{"instruction": "How can I copy the output of a command directly into my clipboard?", "output": "One way of doing it follows:\n\nInstall xclip, such as:\nsudo apt-get install xclip\n\nPipe the output into xclip to be copied into the clipboard:\ncat file | xclip\n\nPaste the text you just copied into a X application:\nxclip -o\n\n\nTo paste somewhere else other than an X application, such as a text area of a web page in a browser window, use:\n\n```\ncat file | xclip -selection clipboard\n\n```\n\nConsider creating an alias:\n\n```\nalias \"c=xclip\"\nalias \"v=xclip -o\"\n\n```\n\nTo see how useful this is, imagine I want to open my current path in a new terminal window (there may be other ways of doing it like Ctrl+T on some systems, but this is just for illustration purposes):\n\n```\nTerminal 1:\npwd | c\n\nTerminal 2:\ncd `v`\n\n```\n\nNotice the ` `  around v. This executes v as a command first and then substitutes it in-place for cd to use.\nOnly copy the content to the X clipboard\n\n```\ncat file | xclip\n\n```"}
{"instruction": "Bash tool to get nth line from a file", "output": "head and pipe with tail will be slow for a huge file. I would suggest sed like this:\n\n```\nsed 'NUMq;d' file\n\n```\n\nWhere NUM is the number of the line you want to print; so, for example, sed '10q;d' file to print the 10th line of file.\nExplanation:\nNUMq will quit immediately when the line number is NUM.\nd will delete the line instead of printing it; this is inhibited on the last line because the q causes the rest of the script to be skipped when quitting.\nIf you have NUM in a variable, you will want to use double quotes instead of single:\n\n```\nsed \"${NUM}q;d\" file\n\n```"}
{"instruction": "How to &#39;grep&#39; a continuous stream?", "output": "Turn on grep's line buffering mode when using BSD grep (FreeBSD, Mac OS X etc.)\n\n```\ntail -f file | grep --line-buffered my_pattern\n\n```\n\nIt looks like a while ago --line-buffered didn't matter for GNU grep (used on pretty much any Linux) as it flushed by default (YMMV for other Unix-likes such as SmartOS, AIX or QNX). However, as of November 2020, --line-buffered is needed (at least with GNU grep 3.5 in openSUSE, but it seems generally needed based on comments below)."}
{"instruction": "How can I reverse the order of lines in a file?", "output": "BSD tail:\n\n```\ntail -r myfile.txt\n\n```\n\nReference: FreeBSD, NetBSD, OpenBSD and OS X manual pages."}
{"instruction": "Bash tool to get nth line from a file", "output": "head and pipe with tail will be slow for a huge file. I would suggest sed like this:\n\n```\nsed 'NUMq;d' file\n\n```\n\nWhere NUM is the number of the line you want to print; so, for example, sed '10q;d' file to print the 10th line of file.\nExplanation:\nNUMq will quit immediately when the line number is NUM.\nd will delete the line instead of printing it; this is inhibited on the last line because the q causes the rest of the script to be skipped when quitting.\nIf you have NUM in a variable, you will want to use double quotes instead of single:\n\n```\nsed \"${NUM}q;d\" file\n\n```"}
{"instruction": "Find and kill a process in one line using bash and regex", "output": "In bash, using only the basic tools listed in your question(1), you should be able to do:\n\n```\nkill $(ps aux | grep '[p]ython csp_build.py' | awk '{print $2}')\n\n```\n\nDetails on its workings are as follows:\n\nThe ps gives you the list of all the processes.\nThe grep filters that based on your search string, [p] is a trick to stop you picking up the actual grep process itself.\nThe awk just gives you the second field of each line, which is the PID.\nThe $(x) construct means to execute x then take its output and put it on the command line. The output of that ps pipeline inside that construct above is the list of process IDs so you end up with a command like kill 1234 1122 7654.\n\nHere's a transcript showing it in action:\n\n```\npax> sleep 3600 &\n[1] 2225\npax> sleep 3600 &\n[2] 2226\npax> sleep 3600 &\n[3] 2227\npax> sleep 3600 &\n[4] 2228\npax> sleep 3600 &\n[5] 2229\npax> kill $(ps aux | grep '[s]leep' | awk '{print $2}')\n[5]+  Terminated              sleep 3600\n[1]   Terminated              sleep 3600\n[2]   Terminated              sleep 3600\n[3]-  Terminated              sleep 3600\n[4]+  Terminated              sleep 3600\n\n```\n\nand you can see it terminating all the sleepers.\nExplaining the grep '[p]ython csp_build.py' bit in a bit more detail: when you do sleep 3600 & followed by ps -ef | grep sleep, you tend to get two processes with sleep in it, the sleep 3600 and the grep sleep (because they both have sleep in them, that's not rocket science).\nHowever, ps -ef | grep '[s]leep' won't create a grep process with sleep in it, it instead creates one with the command grep '[s]leep' and here's the tricky bit: the grep doesn't find that one, because it's looking for the regular expression \"any character from the character class [s] (which is basically just s) followed by leep.\nIn other words, it's looking for sleep but the grep process is grep '[s]leep' which doesn't have the text sleep in it.\nWhen I was shown this (by someone here on SO), I immediately started using it because\n\nit's one less process than adding | grep -v grep; and\nit's elegant and sneaky, a rare combination :-)\n\n\n(1) If you're not limited to using those basic tools, there's a nifty pgrep command which will find processes based on certain criteria (assuming you have it available on your system, of course).\nFor example, you can use pgrep sleep to output the process IDs for all sleep commands (by default, it matches the process name). If you want to match the entire command line as shown in ps, you can do something like pgrep -f 'sleep 9999'.\nAs an aside, it doesn't list itself if you do pgrep pgrep, so the tricky filter method shown above is not necessary in this case.\nYou can check that the processes are the ones you're interested in by using -a to show the full process names. You can also limit the scope to your own processes (or a specific set of users) with -u or -U. See the man page for pgrep/pkill for more options.\nOnce you're satisfied it will only show the processes you're interested in, you can then use pkill with the same parameters to send a signal to all those processes."}
{"instruction": "How to remove double-quotes in jq output for parsing json files in bash?", "output": "Use the -r (or --raw-output) option to emit raw strings as output:\n\n```\njq -r '.name' <json.txt\n\n```"}
{"instruction": "What is the difference between sed and awk?", "output": "sed is a stream editor. It works with streams of characters on a per-line basis. It has a primitive programming language that includes goto-style loops and simple conditionals (in addition to pattern matching and address matching). There are essentially only two \"variables\": pattern space and hold space. Readability of scripts can be difficult. Mathematical operations are extraordinarily awkward at best.\nThere are various versions of sed with different levels of support for command line options and language features.\nawk is oriented toward delimited fields on a per-line basis. It has much more robust programming constructs including if/else, while, do/while and for (C-style and array iteration). There is complete support for variables and single-dimension associative arrays plus (IMO) kludgey multi-dimension arrays. Mathematical operations resemble those in C. It has printf and functions. The \"K\" in \"AWK\" stands for \"Kernighan\" as in \"Kernighan and Ritchie\" of the book \"C Programming Language\" fame (not to forget Aho and Weinberger). One could conceivably write a detector of academic plagiarism using awk.\nGNU awk (gawk) has numerous extensions, including true multidimensional arrays in the latest version. There are other variations of awk including mawk and nawk.\nBoth programs use regular expressions for selecting and processing text.\nI would tend to use sed where there are patterns in the text. For example, you could replace all the negative numbers in some text that are in the form \"minus-sign followed by a sequence of digits\" (e.g. \"-231.45\") with the \"accountant's brackets\" form (e.g. \"(231.45)\") using this (which has room for improvement):\n\n```\nsed 's/-\\([0-9.]\\+\\)/(\\1)/g' inputfile\n\n```\n\nI would use awk when the text looks more like rows and columns or, as awk refers to them \"records\" and \"fields\". If I was going to do a similar operation as above, but only on the third field in a simple comma delimited file I might do something like:\n\n```\nawk -F, 'BEGIN {OFS = \",\"} {gsub(\"-([0-9.]+)\", \"(\" substr($3, 2) \")\", $3); print}' inputfile\n\n```\n\nOf course those are just very simple examples that don't illustrate the full range of capabilities that each has to offer."}
{"instruction": "Using awk to print all columns from the nth to the last", "output": "Print all columns:\n\n```\nawk '{print $0}' somefile\n\n```\n\nPrint all but the first column:\n\n```\nawk '{$1=\"\"; print $0}' somefile\n\n```\n\nPrint all but the first two columns:\n\n```\nawk '{$1=$2=\"\"; print $0}' somefile\n\n```"}
{"instruction": "How do I use shell variables in an awk script?", "output": "#Getting shell variables into awk\nmay be done in several ways. Some are better than others. This should cover most of them.  If you have a comment, please leave below.¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†v1.5\n\n\nUsing -v  (The best way, most portable)\nUse the -v option: (P.S. use a space after -v or it will be less portable. E.g., awk -v var= not awk -vvar=)\n\n```\nvariable=\"line one\\nline two\"\nawk -v var=\"$variable\" 'BEGIN {print var}'\nline one\nline two\n\n```\n\nThis should be compatible with most awk, and the variable is available in the BEGIN block as well:\nIf you have multiple variables:\n\n```\nawk -v a=\"$var1\" -v b=\"$var2\" 'BEGIN {print a,b}'\n\n```\n\nWarning.  As Ed Morton writes and as seen in the above example, the shell variable is expanded by the shell before awk then sees its content as awk -v var='line one\\nline two' and so any escape sequences in the content of that shell variable will be interpreted when using -v, just like they are for every other form of assignment of a string to a variable in awk, e.g. awk 'BEGIN{var=\"line one\\nline two\"} {...}' or awk '{...}' var='line one\\nline two', and so \\n becomes a literal LineFeed character and not the 2-character string \\n. For example, given a variable like:\n\n```\n$ variable='a\\tb\\n$c\\kd'\n\n```\n\nawk would expand the escape sequences in the assignment:\n\n```\n$ awk -v var=\"$variable\" 'BEGIN{ printf \"%s\\n\", var }'\nawk: warning: escape sequence `\\k' treated as plain `k'\na       b\n$ckd\n\n```\n\nIf that's not what you want then, if your shell (e.g. bash) and locale (e.g. LC_ALL=C) support it then you can have backslashes treated literally by using shell parameter substitution to escape any backslashes:\n\n```\n$ awk -v var=\"${variable//\\\\/\\\\\\\\}\" 'BEGIN{ printf \"%s\\n\", var }'\na\\tb\\n$c\\kd\n\n```\n\nor by using ENVIRON[] or access it via ARGV[] (see below).\nYou cannot use -v var=\"$(printf '%q' \"$variable\")\" for this as that would also escape $s, nor can you use -v var=\"${variable@Q}\" as that would just add 's around \"$variable\" and the escape sequences would still be interpreted by awk. That's because those 2 approaches both escape chars according to shell syntax for providing command input, not awk syntax for assigning strings to variables.\nPS If you have vertical bar or other regexp meta characters as separator like |?( etc, they must be double escaped. Example 3 vertical bars ||| becomes -F'\\\\|\\\\|\\\\|'. You can also use -F\"[|][|][|]\".\n\nExample on getting data from a program/function in to awk (here date is used)\n\n\n```\nawk -v time=\"$(date +\"%F %H:%M\" -d '-1 minute')\" 'BEGIN {print time}'\n\n```\n\n\nExample of testing the contents of a shell variable as a regexp:\n\n\n```\nawk -v var=\"$variable\" '$0 ~ var{print \"found it\"}'\n\n```\n\n\nVariable after code block\nHere we get the variable after the awk code. This will work fine as long as you do not need the variable in the BEGIN block:\n\n```\nvariable=\"line one\\nline two\"\necho \"input data\" | awk '{print var}' var=\"${variable}\"\nor\nawk '{print var}' var=\"${variable}\" file\n\n```\n\n\nAdding multiple variables:\n\nawk '{print a,b,$0}' a=\"$var1\" b=\"$var2\" file\n\nIn this way we can also set different Field Separator FS for each file.\n\nawk 'some code' FS=',' file1.txt FS=';' file2.ext\n\nVariable after the code block will not work for the BEGIN block:\n\necho \"input data\" | awk 'BEGIN {print var}' var=\"${variable}\"\n\nHere-string\nVariable can also be added to awk using a here-string from shells that support them (including Bash):\n\n```\nawk '{print $0}' <<< \"$variable\"\ntest\n\n```\n\nThis is the same as:\n\n```\necho \"$variable\" | awk '{print $0}'\nprintf '%s' \"$variable\" | awk '{print $0}'\n\n```\n\nP.S. this treats the variable as a file input.\n\nENVIRON input\nAs TrueY writes, you can use the ENVIRON to print Environment Variables.\nSetting a variable before running AWK, you can print it out like this:\n\n```\nexport X=MyVar\nawk 'BEGIN{print ENVIRON[\"X\"],ENVIRON[\"SHELL\"]}'\nMyVar /bin/bash\n\n```\n\nor for a non-exported variable:\n\n```\nx=MyVar\nx=\"$x\" awk 'BEGIN{print ENVIRON[\"x\"],ENVIRON[\"SHELL\"]}'\nMyVar /bin/bash\n\n```\n\n\nARGV input\nAs Steven Penny writes, you can use ARGV to get the data into awk:\n\n```\nv=\"my data\"\nawk 'BEGIN {print ARGV[1]}' \"$v\"\nmy data\n\n```\n\nTo get the data into the code itself, not just the BEGIN:\n\n```\nv=\"my data\"\necho \"test\" | awk 'BEGIN{var=ARGV[1];ARGV[1]=\"\"} {print var, $0}' \"$v\"\nmy data test\n\n```\n\n\nVariable within the code: USE WITH CAUTION\nYou can use a variable within the awk code, but it's messy and hard to read, and as Charles Duffy points out, this version may also be a victim of code injection.  If someone adds bad stuff to the variable, it will be executed as part of the awk code.\nThis works by extracting the variable within the code, so it becomes a part of it.\nIf you want to make an awk that changes dynamically with use of variables, you can do it this way, but DO NOT use it for normal variables.\n\n```\nvariable=\"line one\\nline two\"\nawk 'BEGIN {print \"'\"$variable\"'\"}'\nline one\nline two\n\n```\n\nHere is an example of code injection:\n\n```\nvariable='line one\\nline two\" ; for (i=1;i<=1000;++i) print i\"'\nawk 'BEGIN {print \"'\"$variable\"'\"}'\nline one\nline two\n1\n2\n3\n.\n.\n1000\n\n```\n\nYou can add lots of commands to awk this way.  Even make it crash with non valid commands.\nOne valid use of this approach, though, is when you want to pass a symbol to awk to be applied to some input, e.g. a simple calculator:\n\n```\n$ calc() { awk -v x=\"$1\" -v z=\"$3\" 'BEGIN{ print x '\"$2\"' z }'; }\n\n$ calc 2.7 '+' 3.4\n6.1\n\n$ calc 2.7 '*' 3.4\n9.18\n\n```\n\nThere is no way to do that using an awk variable populated with the value of a shell variable, you NEED the shell variable to expand to become part of the text of the awk script before awk interprets it. (see comment below by Ed M.)\n\nExtra info:\nUse of double quote\nIt's always good to double quote variable \"$variable\"\nIf not, multiple lines will be added as a long single line.\nExample:\n\n```\nvar=\"Line one\nThis is line two\"\n\necho $var\nLine one This is line two\n\necho \"$var\"\nLine one\nThis is line two\n\n```\n\nOther errors you can get without double quote:\n\n```\nvariable=\"line one\\nline two\"\nawk -v var=$variable 'BEGIN {print var}'\nawk: cmd. line:1: one\\nline\nawk: cmd. line:1:    ^ backslash not last character on line\nawk: cmd. line:1: one\\nline\nawk: cmd. line:1:    ^ syntax error\n\n```\n\nAnd with single quote, it does not expand the value of the variable:\n\n```\nawk -v var='$variable' 'BEGIN {print var}'\n$variable\n\n```\n\nMore info about AWK and variables\nRead this faq."}
{"instruction": "How can I use &quot;:&quot; as an AWK field separator?", "output": "-F is a command line argument, not AWK syntax. Try:\n\n```\necho '1: ' | awk -F  ':' '/1/ {print $1}'\n\n```"}
{"instruction": "How can I shuffle the lines of a text file on the Unix command line or in a shell script?", "output": "You can use shuf. On some systems at least (doesn't appear to be in POSIX).\nAs jleedev pointed out: sort -R might also be an option. On some systems at least; well, you get the picture. It has been pointed out that sort -R doesn't really shuffle but instead sort items according to their hash value.\n[Editor's note: sort -R almost shuffles, except that duplicate lines / sort keys always end up next to each other. In other words: only with unique input lines / keys is it a true shuffle. While it's true that the output order is determined by hash values, the randomness comes from choosing a random hash function - see manual.]"}
{"instruction": "AWK: Access captured group from line pattern", "output": "That was a stroll down memory lane...\nI replaced awk by perl a long time ago.\nApparently the AWK regular expression engine does not capture its groups.\nyou might consider using something like :\n\n```\nperl -n -e'/test(\\d+)/ && print $1'\n\n```\n\nthe -n flag causes perl to loop over every line like awk does."}
{"instruction": "Using multiple delimiters in awk", "output": "The delimiter can be a regular expression.\n\n```\nawk -F'[/=]' '{print $3 \"\\t\" $5 \"\\t\" $8}' file\n\n```\n\nProduces:\n\n```\ntc0001   tomcat7.1    demo.example.com  \ntc0001   tomcat7.2    quest.example.com  \ntc0001   tomcat7.5    www.example.com\n\n```"}
{"instruction": "What are the differences between Perl, Python, AWK and sed?", "output": "In order of appearance, the languages are sed, awk, perl, python.\nThe sed program is a stream editor and is designed to apply the actions from a script to each line (or, more generally, to specified ranges of lines) of the input file or files. Its language is based on ed, the Unix editor, and although it has conditionals and so on, it is hard to work with for complex tasks. You can work minor miracles with it - but at a cost to the hair on your head. However, it is probably the fastest of the programs when attempting tasks within its remit. (It has the least powerful regular expressions of the programs discussed - adequate for many purposes, but certainly not PCRE - Perl-Compatible Regular Expressions)\nThe awk program (name from the initials of its authors - Aho, Weinberger, and Kernighan) is a tool initially for formatting reports. It can be used as a souped-up sed; in its more recent versions, it is computationally complete. It uses an interesting idea - the program is based on 'patterns matched' and 'actions taken when the pattern matches'. The patterns are fairly powerful (Extended Regular Expressions). The language for the actions is similar to C. One of the key features of awk is that it splits the input automatically into records and each record into fields.\nPerl was written in part as an awk-killer and sed-killer. Two of the programs provided with it are a2p and s2p for converting awk scripts and sed scripts into Perl. Perl is one of the earliest of the next generation of scripting languages (Tcl/Tk can probably claim primacy). It has powerful integrated regular expression handling with a vastly more powerful language. It provides access to almost all system calls and has the extensibility of the CPAN modules. (Neither awk nor sed is extensible.) One of Perl's mottos is \"TMTOWTDI - There's more than one way to do it\" (pronounced \"tim-toady\"). Perl has 'objects', but it is more of an add-on than a fundamental part of the language.\nPython was written last, and probably in part as a reaction to Perl. It has some interesting syntactic ideas (indenting to indicate levels - no braces or equivalents). It is more fundamentally object-oriented than Perl; it is just as extensible as Perl.\nOK - when to use each?\n\nSed - when you need to do simple text transforms on files.\nAwk - when you only need simple formatting and summarisation or transformation of data.\nPerl - for almost any task, but especially when the task needs complex regular expressions.\nPython - for the same tasks that you could use Perl for.\n\nI'm not aware of anything that Perl can do that Python can't, nor vice versa. The choice between the two would depend on other factors. I learned Perl before there was a Python, so I tend to use it. Python has less accreted syntax and is generally somewhat simpler to learn. Perl 6, when it becomes available, will be a fascinating development.\n(Note that the 'overviews' of Perl and Python, in particular, are woefully incomplete; whole books could be written on the topic.)"}
{"instruction": "How to show only next line after the matched one?", "output": "you can try with awk:\n\n```\nawk '/blah/{getline; print}' logfile\n\n```"}
{"instruction": "How to split a delimited string into an array in awk?", "output": "Have you tried:\n\n```\necho \"12|23|11\" | awk '{split($0,a,\"|\"); print a[3],a[2],a[1]}'\n\n```"}
{"instruction": "How can I quickly sum all numbers in a file?", "output": "For a Perl one-liner, it's basically the same thing as the awk solution in Ayman Hourieh's answer:\n\n```\n % perl -nle '$sum += $_ } END { print $sum'\n\n```\n\nIf you're curious what Perl one-liners do, you can deparse them:\n\n```\n %  perl -MO=Deparse -nle '$sum += $_ } END { print $sum'\n\n```\n\nThe result is a more verbose version of the program, in a form that no one would ever write on their own:\n\n```\nBEGIN { $/ = \"\\n\"; $\\ = \"\\n\"; }\nLINE: while (defined($_ = <ARGV>)) {\n    chomp $_;\n    $sum += $_;\n}\nsub END {\n    print $sum;\n}\n-e syntax OK\n\n```\n\nJust for giggles, I tried this with a file containing 1,000,000 numbers (in the range 0 - 9,999). On my Mac Pro, it returns virtually instantaneously. That's too bad, because I was hoping using mmap would be really fast, but it's just the same time:\n\n```\nuse 5.010;\nuse File::Map qw(map_file);\n\nmap_file my $map, $ARGV[0];\n\n$sum += $1 while $map =~ m/(\\d+)/g;\n\nsay $sum;\n\n```"}
{"instruction": "How to get the second column from command output?", "output": "Or use sed & regex.\n\n```\n<some_command> | sed 's/^.* \\(\".*\"$\\)/\\1/'\n\n```"}
{"instruction": "How to grep for case insensitive string in a file?", "output": "You can use the -i flag which makes your pattern case insensitive:\n\n```\ngrep -iF \"success...\" file1\n\n```\n\nAlso, there is no need for cat. grep takes a file with the syntax grep <pattern> <file>. I also used the -F flag to search for a fixed string to avoid escaping the ellipsis."}
{"instruction": "awk without printing newline", "output": "awk '{sum+=$3}; END  {printf \"%f\",sum/NR}' ${file}_${f}_v1.xls >> to-plot-p.xls\nprint will insert a newline by default. You dont want that to happen, hence use printf instead."}
{"instruction": "How to merge every two lines into one from the command line?", "output": "awk:\n\n```\nawk 'NR%2{printf \"%s \",$0;next;}1' yourFile\n\n```\n\nnote, there is an empty line at the end of output.\nsed:\n\n```\nsed 'N;s/\\n/ /' yourFile\n\n```"}
{"instruction": "Print second-to-last column/field in `awk`", "output": "```\nawk '{print $(NF-1)}'\n\n```\n\nShould work"}
{"instruction": "Sort a text file by line length including spaces", "output": "Answer\n\n```\n< testfile awk '{ print length, $0 }' | sort -n -s | cut -d\" \" -f2-\n\n```\n\nOr, to do your original (perhaps unintentional) sub-sorting of any equal-length lines:\n\n```\n< testfile awk '{ print length, $0 }' | sort -n | cut -d\" \" -f2-\n\n```\n\nIn both cases, we have solved your stated problem by moving away from awk for your final cut.\nLines of matching length - what to do in the case of a tie:\nThe question did not specify whether or not further sorting was wanted for lines of matching length.  I've assumed that this is unwanted and suggested the use of -s (--stable) to prevent such lines being sorted against each other, and keep them in the relative order in which they occur in the input.\n(Those who want more control of sorting these ties might look at sort's --key option.)\nWhy the question's attempted solution fails (awk line-rebuilding):\nIt is interesting to note the difference between:\n\n```\necho \"hello   awk   world\" | awk '{print}'\necho \"hello   awk   world\" | awk '{$1=\"hello\"; print}'\n\n```\n\nThey yield respectively\n\n```\nhello   awk   world\nhello awk world\n\n```\n\nThe relevant section of (gawk's) manual only mentions as an aside that awk is going to rebuild the whole of $0 (based on the separator, etc) when you change one field.  I guess it's not crazy behaviour.  It has this:\n\"Finally, there are times when it is convenient to force awk to rebuild the entire record, using the current value of the fields and OFS. To do this, use the seemingly innocuous assignment:\"\n\n```\n $1 = $1   # force record to be reconstituted\n print $0  # or whatever else with $0\n\n```\n\n\"This forces awk to rebuild the record.\"\nTest input including some lines of equal length:\n\n```\naa A line   with     MORE    spaces\nbb The very longest line in the file\nccb\n9   dd equal len.  Orig pos = 1\n500 dd equal len.  Orig pos = 2\nccz\ncca\nee A line with  some       spaces\n1   dd equal len.  Orig pos = 3\nff\n5   dd equal len.  Orig pos = 4\ng\n\n```"}
{"instruction": "How to delete duplicate lines in a file without sorting it in Unix", "output": "```\nawk '!seen[$0]++' file.txt\n\n```\n\nseen is an associative array that AWK will pass every line of the file to. If a line isn't in the array then seen[$0] will evaluate to false. The ! is the logical NOT operator and will invert the false to true. AWK will print the lines where the expression evaluates to true.\nThe ++ increments seen so that seen[$0] == 1 after the first time a line is found and then seen[$0] == 2, and so on.\nAWK evaluates everything but 0 and \"\" (empty string) to true. If a duplicate line is placed in seen then !seen[$0] will evaluate to false and the line will not be written to the output."}
{"instruction": "Printing the last column of a line in a file", "output": "You don't see anything, because of buffering. The output is shown, when there are enough lines or end of file is reached. tail -f means wait for more input, but there are no more lines in file and so the pipe to grep is never closed.\nIf you omit -f from tail the output is shown immediately:\n\n```\ntail file | grep A1 | awk '{print $NF}'\n\n```\n\n\n@EdMorton is right of course. Awk can search for A1 as well, which shortens the command line to \n\n```\ntail file | awk '/A1/ {print $NF}'\n\n```\n\nor without tail, showing the last column of all lines containing A1\n\n```\nawk '/A1/ {print $NF}' file\n\n```\n\n\nThanks to @MitchellTracy's comment, tail might miss the record containing A1 and thus you get no output at all. This may be solved by switching tail and awk, searching first through the file and only then show the last line:\n\n```\nawk '/A1/ {print $NF}' file | tail -n1\n\n```"}
{"instruction": "Is there a Unix utility to prepend timestamps to stdin?", "output": "Could try using awk:\n\n```\n<command> | awk '{ print strftime(\"%Y-%m-%d %H:%M:%S\"), $0; fflush(); }'\n\n```\n\nYou may need to make sure that <command> produces line buffered output, i.e. it flushes its output stream after each line; the timestamp awk adds will be the time that the end of the line appeared on its input pipe.\nIf awk shows errors, then try gawk instead."}
{"instruction": "Save modifications in place with awk", "output": "In GNU Awk 4.1.0 (released 2013) and later, it has the option of \"inplace\" file editing:\n\n[...] The \"inplace\" extension, built using the new facility, can be used to simulate the GNU \"sed -i\" feature. [...]\n\nExample usage:\n\n```\n$ gawk -i inplace '{ gsub(/foo/, \"bar\") }; { print }' file1 file2 file3\n\n```\n\nTo keep the backup:\n\n```\n$ gawk -i inplace -v INPLACE_SUFFIX=.bak '{ gsub(/foo/, \"bar\") }\n> { print }' file1 file2 file3\n\n```"}
{"instruction": "How can I delete a newline if it is the last character in a file?", "output": "```\nperl -pe 'chomp if eof' filename >filename2\n\n```\n\nor, to edit the file in place:\n\n```\nperl -pi -e 'chomp if eof' filename\n\n```\n\n[Editor's note: -pi -e was originally -pie, but, as noted by several commenters and explained by @hvd, the latter doesn't work.]\nThis was described as a 'perl blasphemy' on the awk website I saw.\nBut, in a test, it worked."}
{"instruction": "How to select lines between two marker patterns which may occur multiple times with awk/sed", "output": "Use awk with a flag to trigger the print when necessary:\n\n```\n$ awk '/abc/{flag=1;next}/mno/{flag=0}flag' file\ndef1\nghi1\njkl1\ndef2\nghi2\njkl2\n\n```\n\nHow does this work?\n\n/abc/ matches lines having this text, as well as /mno/ does.  \n/abc/{flag=1;next} sets the flag when the text abc is found. Then, it skips the line.  \n/mno/{flag=0} unsets the flag when the text mno is found.\nThe final flag is a pattern with the default action, which is to print $0: if flag is equal 1 the line is printed.\n\nFor a more detailed description and examples, together with cases when the patterns are either shown or not, see How to select lines between two patterns?."}
{"instruction": "How to print matched regex pattern using awk?", "output": "This is the very basic\n\n```\nawk '/pattern/{ print $0 }' file\n\n```\n\nask awk to search for pattern using //, then print out the line, which by default is called a record, denoted by $0. At least read up the documentation.\nIf you only want to get print out the matched word. \n\n```\nawk '{for(i=1;i<=NF;i++){ if($i==\"yyy\"){print $i} } }' file\n\n```"}
{"instruction": "sed or awk: delete n lines following a pattern", "output": "I'll have a go at this.\nTo delete 5 lines after a pattern (including the line with the pattern):\n\n```\nsed -e '/pattern/,+5d' file.txt\n\n```\n\nTo delete 5 lines after a pattern (excluding the line with the pattern):\n\n```\nsed -e '/pattern/{n;N;N;N;N;d}' file.txt\n\n```"}
{"instruction": "Get last field using awk substr", "output": "Use the fact that awk splits the lines in fields based on a field separator, that you can define. Hence, defining the field separator to / you can say:\n\n```\nawk -F \"/\" '{print $NF}' input\n\n```\n\nas NF refers to the number of fields of the current record, printing $NF means printing the last one.\nSo given a file like this:\n\n```\n/home/parent/child1/child2/child3/filename\n/home/parent/child1/child2/filename\n/home/parent/child1/filename\n\n```\n\nThis would be the output:\n\n```\n$ awk -F\"/\" '{print $NF}' file\nfilename\nfilename\nfilename\n\n```"}
{"instruction": "Printing everything except the first field with awk", "output": "Assigning $1 works but it will leave a leading space: awk '{first = $1; $1 = \"\"; print $0, first; }'\nYou can also find the number of columns in NF and use that in a loop.\n\nFrom Thyag: To eliminate the leading space, add sed to the end of the command:\n\n```\nawk {'first = $1; $1=\"\"; print $0'}|sed 's/^ //g'\n\n```"}
{"instruction": "What are the differences among grep, awk &amp; sed?", "output": "Short definition:\ngrep: search for specific terms in a file\n\n```\n#usage\n$ grep This file.txt\nEvery line containing \"This\"\nEvery line containing \"This\"\nEvery line containing \"This\"\nEvery line containing \"This\"\n\n$ cat file.txt\nEvery line containing \"This\"\nEvery line containing \"This\"\nEvery line containing \"That\"\nEvery line containing \"This\"\nEvery line containing \"This\"\n\n```\n\nNow awk and sed are completly different than grep.\nawk and sed are text processors. Not only do they have the ability to find what you are looking for in text, they have the ability to remove, add and modify the text as well (and much more). \nawk is mostly used for data extraction and reporting. sed is a stream editor\nEach one of them has its own functionality and specialties. \nExample \nSed\n\n```\n$ sed -i 's/cat/dog/' file.txt\n# this will replace any occurrence of the characters 'cat' by 'dog'\n\n```\n\nAwk\n\n```\n$ awk '{print $2}' file.txt\n# this will print the second column of file.txt\n\n```\n\nBasic awk usage:\nCompute sum/average/max/min/etc. what ever you may need.\n\n```\n$ cat file.txt\nA 10\nB 20\nC 60\n$ awk 'BEGIN {sum=0; count=0; OFS=\"\\t\"} {sum+=$2; count++} END {print \"Average:\", sum/count}' file.txt\nAverage:    30\n\n```\n\nI recommend that you read this book: Sed & Awk: 2nd Ed.\nIt will help you become a proficient sed/awk user on any unix-like environment."}
{"instruction": "How to escape a single quote inside awk", "output": "This maybe what you're looking for:\n\n```\nawk 'BEGIN {FS=\" \";} {printf \"'\\''%s'\\'' \", $1}'\n\n```\n\nThat is, with '\\'' you close the opening ', then print a literal ' by escaping it and finally open the ' again."}
{"instruction": "How to use multiple arguments for awk with a shebang (i.e. #!)?", "output": "This seems to work for me with (g)awk.\n\n```\n#!/bin/sh\narbitrary_long_name==0 \"exec\" \"/usr/bin/gawk\" \"--re-interval\" \"-f\" \"$0\" \"$@\"\n\n\n# The real awk program starts here\n{ print $0 }\n\n```\n\nNote the #! runs /bin/sh, so this script is first interpreted as a shell script.\nAt first, I simply tried \"exec\" \"/usr/bin/gawk\" \"--re-interval\" \"-f\" \"$0\" \"$@\", but awk treated that as a command and printed out every line of input unconditionally. That is why I put in the arbitrary_long_name==0 - it's supposed to fail all the time. You could replace it with some gibberish string. Basically, I was looking for a false-condition in awk that would not adversely affect the shell script.\nIn the shell script, the arbitrary_long_name==0 defines a variable called arbitrary_long_name and sets it equal to =0."}
{"instruction": "using awk with column value conditions", "output": "If you're looking for a particular string, put quotes around it:\n\n```\nawk '$1 == \"findtext\" {print $3}'\n\n```\n\nOtherwise, awk will assume it's a variable name."}
{"instruction": "Is there still any reason to learn AWK?", "output": "I think it depends on the environment you find yourself in. If you are a *nix person, then knowing awk is a Good Thing. The only other scripting environment that can be found on virtually every *nix is sh. So while grep, sed, etc can surely replace awk on a modern mainstream linux distro, when you move to more exotic systems, knowing a little awk is going to be Real Handy.\nawk can also be used for more than just text processing. For example one of my supervisors writes astronomy code in awk - that is how utterly old school and awesome he is. Back in his days, it was the best tool for the job... and now even though his students like me use python and what not, he sticks to what he knows and works well. \nIn closing, there is a lot of old code kicking around the world, knowing a little awk isn't going to hurt. It will also make you better *nix person :-)"}
{"instruction": "Show filename and line number in grep output", "output": "I think -l is too restrictive as it suppresses the output of -n. I would suggest -H (--with-filename): Print the filename for each match.\n\n```\ngrep -Hn \"search\" *\n\n```\n\nIf that gives too much output, try -o to only print the part that matches.\n\n```\ngrep -nHo \"search\" * \n\n```"}
{"instruction": "How to print last two columns using awk", "output": "You can make use of variable NF which  is set to the total number of fields in the input record:\n\n```\nawk '{print $(NF-1),\"\\t\",$NF}' file\n\n```\n\nthis assumes that you have at least 2 fields."}
{"instruction": "Print all but the first three columns", "output": "A solution that does not add extra leading or trailing whitespace:\n\n```\nawk '{ for(i=4; i<NF; i++) printf \"%s\",$i OFS; if(NF) printf \"%s\",$NF; printf ORS}'\n\n### Example ###\n$ echo '1 2 3 4 5 6 7' |\n  awk '{for(i=4;i<NF;i++)printf\"%s\",$i OFS;if(NF)printf\"%s\",$NF;printf ORS}' |\n  tr ' ' '-'\n4-5-6-7\n\n```\n\nSudo_O proposes an elegant improvement using the ternary operator NF?ORS:OFS\n\n```\n$ echo '1 2 3 4 5 6 7' |\n  awk '{ for(i=4; i<=NF; i++) printf \"%s\",$i (i==NF?ORS:OFS) }' |\n  tr ' ' '-'\n4-5-6-7\n\n```\n\nEdMorton gives a solution preserving original whitespaces between fields:\n\n```\n$ echo '1   2 3 4   5    6 7' |\n  awk '{ sub(/([^ ]+ +){3}/,\"\") }1' |\n  tr ' ' '-'\n4---5----6-7\n\n```\n\nBinaryZebra also provides two awesome solutions:\n(these solutions even preserve trailing spaces from original string)\n\n```\n$ echo -e ' 1   2\\t \\t3     4   5   6 7 \\t 8\\t ' |\n  awk -v n=3 '{ for ( i=1; i<=n; i++) { sub(\"^[\"FS\"]*[^\"FS\"]+[\"FS\"]+\",\"\",$0);} } 1 ' |\n  sed 's/ /./g;s/\\t/->/g;s/^/\"/;s/$/\"/'\n\"4...5...6.7.->.8->.\"\n\n$ echo -e ' 1   2\\t \\t3     4   5   6 7 \\t 8\\t ' |\n  awk -v n=3 '{ print gensub(\"[\"FS\"]*([^\"FS\"]+[\"FS\"]+){\"n\"}\",\"\",1); }' |\n  sed 's/ /./g;s/\\t/->/g;s/^/\"/;s/$/\"/'\n\"4...5...6.7.->.8->.\"\n\n```\n\nThe solution given by larsr in the comments is almost correct:\n\n```\n$ echo '1 2 3 4 5 6 7' | \n  awk '{for (i=3;i<=NF;i++) $(i-2)=$i; NF=NF-2; print $0}' | tr  ' ' '-'\n3-4-5-6-7\n\n```\n\nThis is the fixed and parametrized version of larsr solution:\n\n```\n$ echo '1 2 3 4 5 6 7' | \n  awk '{for(i=n;i<=NF;i++)$(i-(n-1))=$i;NF=NF-(n-1);print $0}' n=4 | tr ' ' '-'\n4-5-6-7\n\n```\n\nAll other answers before Sep-2013 are nice but add extra spaces:\n\nExample of answer adding extra leading spaces:\n\n```\n$ echo '1 2 3 4 5 6 7' | \n  awk '{$1=$2=$3=\"\"}1' | \n  tr  ' ' '-'\n---4-5-6-7\n\n```\n\nExample of answer adding extra trailing space \n\n```\n$ echo '1 2 3 4 5 6 7' | \n  awk '{for(i=4;i<=13;i++)printf \"%s \",$i;printf \"\\n\"}' | \n  tr ' ' '-'\n4-5-6-7-------\n\n```"}
{"instruction": "awk partly string match (if column/word partly matches)", "output": "```\nawk '$3 ~ /snow/ { print }' dummy_file \n\n```"}
{"instruction": "Append file contents to the bottom of existing file in Bash", "output": "This should work:\n\n```\n cat \"$API\" >> \"$CONFIG\"\n\n```\n\nYou need to use the >> operator to append to a file.  Redirecting with > causes the file to be overwritten. (truncated)."}
{"instruction": "Tab separated values in awk", "output": "You need to set the OFS variable (output field separator) to be a tab:\n\n```\necho \"$line\" | \nawk -v var=\"$mycol_new\" -F'\\t' 'BEGIN {OFS = FS} {$3 = var; print}'\n\n```\n\n(make sure you quote the $line variable in the echo statement)"}
{"instruction": "How to print lines between two patterns, inclusive or exclusive (in sed, AWK or Perl)?", "output": "Print lines between PAT1 and PAT2\n\n```\n$ awk '/PAT1/,/PAT2/' file\nPAT1\n3    - first block\n4\nPAT2\nPAT1\n7    - second block\nPAT2\nPAT1\n10    - third block\n\n```\n\nOr, using variables:\n\n```\nawk '/PAT1/{flag=1} flag; /PAT2/{flag=0}' file\n\n```\n\nHow does this work?\n\n/PAT1/ matches lines having this text, as well as /PAT2/ does.  \n/PAT1/{flag=1} sets the flag when the text PAT1 is found in a line.\n/PAT2/{flag=0} unsets the flag when the text PAT2 is found in a line.\nflag is a pattern with the default action, which is to print $0: if flag is equal 1 the line is printed. This way, it will print all those lines occurring from the time PAT1 occurs and up to the next PAT2 is seen. This will also print the lines from the last match of PAT1 up to the end of the file.\n\nPrint lines between PAT1 and PAT2 - not including PAT1 and PAT2\n\n```\n$ awk '/PAT1/{flag=1; next} /PAT2/{flag=0} flag' file\n3    - first block\n4\n7    - second block\n10    - third block\n\n```\n\nThis uses next to skip the line that contains PAT1 in order to avoid this being printed.\nThis call to next can be dropped by reshuffling the blocks: awk '/PAT2/{flag=0} flag; /PAT1/{flag=1}' file.\nPrint lines between PAT1 and PAT2 - including PAT1\n\n```\n$ awk '/PAT1/{flag=1} /PAT2/{flag=0} flag' file\nPAT1\n3    - first block\n4\nPAT1\n7    - second block\nPAT1\n10    - third block\n\n```\n\nBy placing flag at the very end, it triggers the action that was set on either PAT1 or PAT2: to print on PAT1, not to print on PAT2.\nPrint lines between PAT1 and PAT2 - including PAT2\n\n```\n$ awk 'flag; /PAT1/{flag=1} /PAT2/{flag=0}' file\n3    - first block\n4\nPAT2\n7    - second block\nPAT2\n10    - third block\n\n```\n\nBy placing flag at the very beginning, it triggers the action that was set previously and hence print the closing pattern but not the starting one.\nPrint lines between PAT1 and PAT2 - excluding lines from the last PAT1 to the end of file if no other PAT2 occurs\nThis is based on a solution by Ed Morton.\n\n```\nawk 'flag{\n        if (/PAT2/)\n           {printf \"%s\", buf; flag=0; buf=\"\"}\n        else\n            buf = buf $0 ORS\n     }\n     /PAT1/ {flag=1}' file\n\n```\n\nAs a one-liner:\n\n```\n$ awk 'flag{ if (/PAT2/){printf \"%s\", buf; flag=0; buf=\"\"} else buf = buf $0 ORS}; /PAT1/{flag=1}' file\n3    - first block\n4\n7    - second block\n\n# note the lack of third block, since no other PAT2 happens after it\n\n```\n\nThis keeps all the selected lines in a buffer that gets populated from the moment PAT1 is found. Then, it keeps being filled with the following lines until PAT2 is found. In that point, it prints the stored content and empties the buffer."}
{"instruction": "How to print all the columns after a particular number using awk?", "output": "```\nawk '{ s = \"\"; for (i = 9; i <= NF; i++) s = s $i \" \"; print s }'\n\n```"}
{"instruction": "How to delete from a text file, all lines that contain a specific string?", "output": "To remove the line and print the output to standard out:\n\n```\nsed '/pattern to match/d' ./infile\n\n```\n\nTo directly modify the file ‚Äì does not work with BSD sed:\n\n```\nsed -i '/pattern to match/d' ./infile\n\n```\n\nSame, but for BSD sed (Mac OS X and FreeBSD) ‚Äì does not work with GNU sed:\n\n```\nsed -i '' '/pattern to match/d' ./infile\n\n```\n\nTo directly modify the file (and create a backup) ‚Äì works with BSD and GNU sed:\n\n```\nsed -i.bak '/pattern to match/d' ./infile\n\n```"}
{"instruction": "How can I replace each newline (\\n) with a space using sed?", "output": "Use this solution with GNU sed:\n\n```\nsed ':a;N;$!ba;s/\\n/ /g' file\n\n```\n\nThis will read the whole file in a loop (':a;N;$!ba), then replaces the newline(s) with a space (s/\\n/ /g). Additional substitutions can be simply appended if needed.\nExplanation:\n\nsed starts by reading the first line excluding the newline into the pattern space.\nCreate a label via :a.\nAppend a newline and next line to the pattern space via N.\nIf we are before the last line, branch to the created label $!ba ($! means not to do it on the last line. This is necessary to avoid executing N again, which would terminate the script if there is no more input!).\nFinally the substitution replaces every newline with a space on the pattern space (which is the whole file).\n\nHere is cross-platform compatible syntax which works with BSD and OS X's sed (as per @Benjie comment):\n\n```\nsed -e ':a' -e 'N' -e '$!ba' -e 's/\\n/ /g' file\n\n```\n\nAs you can see, using sed for this otherwise simple problem is problematic. For a simpler and adequate solution see this answer."}
{"instruction": "Bash tool to get nth line from a file", "output": "head and pipe with tail will be slow for a huge file. I would suggest sed like this:\n\n```\nsed 'NUMq;d' file\n\n```\n\nWhere NUM is the number of the line you want to print; so, for example, sed '10q;d' file to print the 10th line of file.\nExplanation:\nNUMq will quit immediately when the line number is NUM.\nd will delete the line instead of printing it; this is inhibited on the last line because the q causes the rest of the script to be skipped when quitting.\nIf you have NUM in a variable, you will want to use double quotes instead of single:\n\n```\nsed \"${NUM}q;d\" file\n\n```"}
{"instruction": "How can I remove the first line of a text file using bash/sed script?", "output": "Try tail:\n\n```\ntail -n +2 \"$FILE\"\n\n```\n\n-n x: Just print the last x lines. tail -n 5 would give you the last 5 lines of the input. The + sign kind of inverts the argument and make tail print anything but the first x-1 lines. tail -n +1 would print the whole file, tail -n +2 everything but the first line, etc.\nGNU tail is much faster than sed. tail is also available on BSD and the -n +2 flag is consistent across both tools. Check the FreeBSD or OS X man pages for more. \nThe BSD version can be much slower than sed, though. I wonder how they managed that; tail should just read a file line by line while sed does pretty complex operations involving interpreting a script, applying regular expressions and the like.\nNote: You may be tempted to use\n\n```\n# THIS WILL GIVE YOU AN EMPTY FILE!\ntail -n +2 \"$FILE\" > \"$FILE\"\n\n```\n\nbut this will give you an empty file. The reason is that the redirection (>) happens before tail is invoked by the shell:\n\nShell truncates file $FILE\nShell creates a new process for tail\nShell redirects stdout of the tail process to $FILE\ntail reads from the now empty $FILE\n\nIf you want to remove the first line inside the file, you should use:\n\n```\ntail -n +2 \"$FILE\" > \"$FILE.tmp\" && mv \"$FILE.tmp\" \"$FILE\"\n\n```\n\nThe && will make sure that the file doesn't get overwritten when there is a problem."}
{"instruction": "How to remove double-quotes in jq output for parsing json files in bash?", "output": "Use the -r (or --raw-output) option to emit raw strings as output:\n\n```\njq -r '.name' <json.txt\n\n```"}
{"instruction": "Find and replace in file and overwrite file doesn&#39;t work, it empties the file", "output": "When the shell sees  > index.html in the command line it opens the file index.html for writing, wiping off all its previous contents.\nTo fix this you need to pass the -i option to sed to make the changes inline and create a backup of the original file before it does the changes in-place:\n\n```\nsed -i.bak s/STRING_TO_REPLACE/STRING_TO_REPLACE_IT/g index.html\n\n```\n\nWithout the .bak the command will fail on some platforms, such as Mac OSX."}
{"instruction": "What is the difference between sed and awk?", "output": "sed is a stream editor. It works with streams of characters on a per-line basis. It has a primitive programming language that includes goto-style loops and simple conditionals (in addition to pattern matching and address matching). There are essentially only two \"variables\": pattern space and hold space. Readability of scripts can be difficult. Mathematical operations are extraordinarily awkward at best.\nThere are various versions of sed with different levels of support for command line options and language features.\nawk is oriented toward delimited fields on a per-line basis. It has much more robust programming constructs including if/else, while, do/while and for (C-style and array iteration). There is complete support for variables and single-dimension associative arrays plus (IMO) kludgey multi-dimension arrays. Mathematical operations resemble those in C. It has printf and functions. The \"K\" in \"AWK\" stands for \"Kernighan\" as in \"Kernighan and Ritchie\" of the book \"C Programming Language\" fame (not to forget Aho and Weinberger). One could conceivably write a detector of academic plagiarism using awk.\nGNU awk (gawk) has numerous extensions, including true multidimensional arrays in the latest version. There are other variations of awk including mawk and nawk.\nBoth programs use regular expressions for selecting and processing text.\nI would tend to use sed where there are patterns in the text. For example, you could replace all the negative numbers in some text that are in the form \"minus-sign followed by a sequence of digits\" (e.g. \"-231.45\") with the \"accountant's brackets\" form (e.g. \"(231.45)\") using this (which has room for improvement):\n\n```\nsed 's/-\\([0-9.]\\+\\)/(\\1)/g' inputfile\n\n```\n\nI would use awk when the text looks more like rows and columns or, as awk refers to them \"records\" and \"fields\". If I was going to do a similar operation as above, but only on the third field in a simple comma delimited file I might do something like:\n\n```\nawk -F, 'BEGIN {OFS = \",\"} {gsub(\"-([0-9.]+)\", \"(\" substr($3, 2) \")\", $3); print}' inputfile\n\n```\n\nOf course those are just very simple examples that don't illustrate the full range of capabilities that each has to offer."}
{"instruction": "sed edit file in-place", "output": "The -i option streams the edited content into a new file and then renames it behind the scenes, anyway.\nExample:\n\n```\nsed -i 's/STRING_TO_REPLACE/STRING_TO_REPLACE_IT/g' filename\n\n```\n\nwhile on macOS you need:\n\n```\nsed -i '' 's/STRING_TO_REPLACE/STRING_TO_REPLACE_IT/g' filename\n\n```"}
{"instruction": "Delete empty lines using sed", "output": "You may have spaces or tabs in your \"empty\" line. Use POSIX classes with sed to remove all lines containing only whitespace:\n\n```\nsed '/^[[:space:]]*$/d'\n\n```\n\nA shorter version that uses ERE, for example with gnu sed:\n\n```\nsed -r '/^\\s*$/d'\n\n```\n\n(Note that sed does NOT support PCRE.)"}
{"instruction": "Non greedy (reluctant) regex matching in sed?", "output": "Neither basic nor extended Posix/GNU regex recognizes the non-greedy quantifier; you need a later regex.  Fortunately, Perl regex for this context is pretty easy to get:\n\n```\nperl -pe 's|(http://.*?/).*|\\1|'\n\n```"}
{"instruction": "Remove first and last quote (&quot;) from a variable", "output": "There's a simpler and more efficient way, using the native shell prefix/suffix removal feature:\n\n```\ntemp=\"${opt%\\\"}\"\ntemp=\"${temp#\\\"}\"\necho \"$temp\"\n\n```\n\n${opt%\\\"} will remove the suffix \" (escaped with a backslash to prevent shell interpretation).\n${temp#\\\"} will remove the prefix \" (escaped with a backslash to prevent shell interpretation).\nAnother advantage is that it will remove surrounding quotes only if there are surrounding quotes.\nBTW, your solution always removes the first and last character, whatever they may be (of course, I'm sure you know your data, but it's always better to be sure of what you're removing).\nUsing sed:\n\n```\necho \"$opt\" | sed -e 's/^\"//' -e 's/\"$//'\n\n```\n\n(Improved version, as indicated by jfgagne, getting rid of echo)\n\n```\nsed -e 's/^\"//' -e 's/\"$//' <<<\"$opt\"\n\n```\n\nSo it replaces a leading \" with nothing, and a trailing \" with nothing too. In the same invocation (there isn't any need to pipe and start another sed. Using -e you can have multiple text processing)."}
{"instruction": "sed command with -i option failing on Mac, but works on Linux", "output": "Portable solution below\n\nWhy you get the error\nThe -i option (alternatively, --in-place) means that you want files edited in-place, rather than streaming the change to a new place.\nModifying a file in-place suggests a need for a backup file - and so a user-specified extension is expected after -i, but the parsing of the extension argument is handled differently under GNU sed & Mac (BSD) sed:\n\nGNU : \"If no extension is supplied, the original file is overwritten without making a backup.\" - effectively, you can omit specify a file extension altogether. The extension must be supplied immediately after the -i, with no intervening space.\nMac (BSD) : \"If a zero-length extension is given, no backup will be saved.\" - you must supply an extension, but it can be the empty string '' if you want, to disable the backup.\n\nSo GNU & Mac will interpret this differently:\n\n```\nsed -i 's/hello/bye/g' just_a_file.txt\n\n```\n\n\nGNU : No extension is supplied immediately after the -i, so create no backup, use s/hello/bye/g as the text-editing command, and act on the file just_a_file.txt in-place.\nMac (BSD) : Use s/hello/bye/g as the backup file extension (!) and just_a_file.txt as the editing command (regular expression).\nResult: the command code used is j (not a valid command code for substitution, e.g. s), hence we get the error invalid command code j.\n\n\n```\n# This still create a `my_file.txt-e` backup on macOS Sonoma (14.5)\n# and a `my_file.txt''` on Linux\nsed -i'' -e 's/hello/bye/g' my_file.txt\n\n```\n\nPlacing the extension immediately after the -i (eg -i'' or -i'.bak', without a space) is what GNU sed expects, but macOS expect a space after -i (eg -i '' or -i '.bak').\nand is now accepted by Mac (BSD) sed too, though it wasn't tolerated by earlier versions (eg with Mac OS X v10.6, a space was required after -i, eg -i '.bak').\nThe -e parameter allows us to be explicit about where we're declaring the edit command.\nUntil Mac OS was updated in 2013, there wasn't\nStill there isn't any portable command across GNU and Mac (BSD), as these variants all failed (with an error or unexpected backup files):\n\nsed -i -e ... - works on Linux but does not work on macOS as it creates -e backups\nsed -i '' -e ... - works on macOS but fails on Linux\nsed -i='' -e ... - create = backups files on macOS and Linux\nsed -i'' -e ... - create -e backups files on macOS\n\nPortable solution\nYou have few options to achieve the same result on Linux and macOS, e.g.:\n\nUse Perl: perl -i -pe's/old_link/new_link/g' *.\n\nUse gnu-sed on macOS (Install using Homebrew)\n\n\n\n```\n# Install 'gnu-sed' on macOS using Homebrew\nbrew install gnu-sed\n# Use 'gsed' instead of 'sed' on macOS.\ngsed -i'' -e 's/hello/bye/g' my_file.txt\n\n```\n\n\nNote: On macOS, you could add the bin path of gnu-sed containing the sed command to the PATH environment variable in your shell configuration file (.zshrc). \nIt is best not to do this, since there may be scripts that rely on the macOS built-in version.\nYou can add an alias for gsed as sed using alias sed=gsed (replacing macOS sed with GNU sed) in your ~/.zshrc. This should allow you to use sed \"linux-stile\" in your shell and will have no effects on scripts unless they contain shopt -s expand_aliases.\n\nIf you are using sed in a script, you can try to automate switching to gsed:\n\n```\n#!/usr/bin/env bash\nset -Eeuo pipefail\n\nif [[ \"$OSTYPE\" == \"darwin\"* ]]; then\n  # Require gnu-sed.\n  if ! [ -x \"$(command -v gsed)\" ]; then\n    echo \"Error: 'gsed' is not istalled.\" >&2\n    echo \"If you are using Homebrew, install with 'brew install gnu-sed'.\" >&2\n    exit 1\n  fi\n  SED_CMD=gsed\nelse\n  SED_CMD=sed\nfi\n\n# Use '${SED_CMD}' instead of 'sed'\n${SED_CMD} -i'' -e 's/hello/bye/g' my_file.txt\n\n```\n\nYou can temporarily set PATH to use \"gnu-sed\" sed for a script:\n\n```\n# run a linux script containing sed without changing it\nPATH=\"$(brew --prefix)/opt/gnu-sed/libexec/gnubin:$PATH\" ./linux_script_using_sed.sh\n\n```\n\nIf you are copy/pasting linux scripts, you can alias gsed to sed in the current shell:\n\n```\nalias sed=gsed\nsed -i 's/hello/bye/g' just_a_file.txt\n\n```\n\n\nUse -i '' on macOS and BSD or -i (GNU sed) otherwise\n\n\n```\n#!/usr/bin/env bash\nset -Eeuo pipefail\n\ncase \"$OSTYPE\" in\n  darwin*|bsd*)\n    echo \"Using BSD sed style\"\n    sed_no_backup=( -i '' )\n    ;; \n  *)\n    echo \"Using GNU sed style\"\n    sed_no_backup=( -i )\n    ;;\nesac\n\nsed ${sed_no_backup[@]} -e 's/hello/bye/g' my_file.txt\n\n```"}
{"instruction": "Replace whole line containing a string using Sed", "output": "You can use the change command to replace the entire line, and the -i flag to make the changes in-place. For example, using GNU sed:\n\n```\nsed -i '/TEXT_TO_BE_REPLACED/c\\This line is removed by the admin.' /tmp/foo\n\n```"}
{"instruction": "Escape a string for a sed replace pattern", "output": "Warning: This does not consider newlines. For a more in-depth answer, see this SO-question instead. (Thanks, Ed Morton & Niklas Peter)\nNote that escaping everything is a bad idea. Sed needs many characters to be escaped to get their special meaning. For example, if you escape a digit in the replacement string, it will turn in to a backreference.\nAs Ben Blank said, there are only three characters that need to be escaped in the replacement string (escapes themselves, forward slash for end of statement and & for replace all):\n\n```\nESCAPED_REPLACE=$(printf '%s\\n' \"$REPLACE\" | sed -e 's/[\\/&]/\\\\&/g')\n# Now you can use ESCAPED_REPLACE in the original sed statement\nsed \"s/KEYWORD/$ESCAPED_REPLACE/g\"\n\n```\n\nIf you ever need to escape the KEYWORD string, the following is the one you need:\n\n```\nsed -e 's/[]\\/$*.^[]/\\\\&/g'\n```\n\nAnd can be used by:\n\n```\nKEYWORD=\"The Keyword You Need\";\nESCAPED_KEYWORD=$(printf '%s\\n' \"$KEYWORD\" | sed -e 's/[]\\/$*.^[]/\\\\&/g');\n\n# Now you can use it inside the original sed statement to replace text\nsed \"s/$ESCAPED_KEYWORD/$ESCAPED_REPLACE/g\"\n\n```\n\nRemember, if you use a character other than / as delimiter, you need replace the slash in the expressions above wih the character you are using. See PeterJCLaw's comment for explanation.\nEdited: Due to some corner cases previously not accounted for, the commands above have changed several times. Check the edit history for details."}
{"instruction": "How can I output only captured groups with sed?", "output": "The key to getting this to work is to tell sed to exclude what you don't want to be output as well as specifying what you do want. This technique depends on knowing how many matches you're looking for. The grep command below works for an unspecified number of matches.\n\n```\nstring='This is a sample 123 text and some 987 numbers'\necho \"$string\" | sed -rn 's/[^[:digit:]]*([[:digit:]]+)[^[:digit:]]+([[:digit:]]+)[^[:digit:]]*/\\1 \\2/p'\n\n```\n\nThis says:\n\nuse extended regular expressions (-r)\ndon't default to printing each line (-n)\nexclude zero or more non-digits\ninclude one or more digits\nexclude one or more non-digits\ninclude one or more digits\nexclude zero or more non-digits\nprint the substitution (p) (on one line)\n\nIn general, in sed you capture groups using parentheses and output what you capture using a back reference:\n\n```\necho \"foobarbaz\" | sed 's/^foo\\(.*\\)baz$/\\1/'\n\n```\n\nwill output \"bar\". If you use -r (-E for OS X) for extended regex, you don't need to escape the parentheses:\n\n```\necho \"foobarbaz\" | sed -r 's/^foo(.*)baz$/\\1/'\n\n```\n\nThere can be up to 9 capture groups and their back references. The back references are numbered in the order the groups appear, but they can be used in any order and can be repeated:\n\n```\necho \"foobarbaz\" | sed -r 's/^foo(.*)b(.)z$/\\2 \\1 \\2/'\n\n```\n\noutputs \"a bar a\".\nIf you have GNU grep:\n\n```\necho \"$string\" | grep -Po '\\d+'\n\n```\n\nIt may also work in BSD, including OS X:\n\n```\necho \"$string\" | grep -Eo '\\d+'\n\n```\n\nThese commands will match any number of digit sequences. The output will be on multiple lines.\nor variations such as:\n\n```\necho \"$string\" | grep -Po '(?<=\\D )(\\d+)'\n\n```\n\nThe -P option enables Perl Compatible Regular Expressions. See man 3 pcrepattern or man  3 pcresyntax."}
{"instruction": "Delete specific line number(s) from a text file using sed?", "output": "If you want to delete lines from 5 through 10 and line 12th:\n\n```\nsed -e '5,10d;12d' file\n\n```\n\nThis will print the results to the screen. If you want to save the results to the same file:\n\n```\nsed -i.bak -e '5,10d;12d' file\n\n```\n\nThis will store the unmodified file as file.bak, and delete the given lines.\nNote: Line numbers start at 1. The first line of the file is 1, not 0."}
{"instruction": "Error when using &#39;sed&#39; with &#39;find&#39; command on OS X: &quot;invalid command code .&quot;", "output": "If you are on a OS X, this probably has nothing to do with the sed command. On the OSX version of sed, the -i option expects an extension argument so your command is actually parsed as the extension argument and the file path is interpreted as the command code.\nTry adding the -e argument explicitly and giving '' as argument to -i:\n\n```\nfind ./ -type f -exec sed -i '' -e \"s/192.168.20.1/new.domain.com/\" {} \\;\n\n```\n\nSee this."}
{"instruction": "Insert line after match using sed", "output": "Try doing this using GNU sed:\n\n```\nsed '/CLIENTSCRIPT=\"foo\"/a CLIENTSCRIPT2=\"hello\"' file\n\n```\n\nif you want to substitute in-place, use\n\n```\nsed -i '/CLIENTSCRIPT=\"foo\"/a CLIENTSCRIPT2=\"hello\"' file\n\n```\n\nOutput\n\n```\nCLIENTSCRIPT=\"foo\"\nCLIENTSCRIPT2=\"hello\"\nCLIENTFILE=\"bar\"\n\n```\n\nDoc\n\nsee sed doc and search \\a (append)"}
{"instruction": "Replace comma with newline in sed on MacOS?", "output": "Use tr instead:\n\n```\ntr , '\\n' < file\n\n```"}
{"instruction": "Retrieve last 100 lines logs", "output": "You can use tail command as follows:\n\n```\ntail -100 <log file>   > newLogfile\n\n```\n\nNow last 100 lines will be present in newLogfile\nEDIT:\nMore recent versions of tail as mentioned by twalberg use command:\n\n```\ntail -n 100 <log file>   > newLogfile\n\n```"}
{"instruction": "sed in-place flag that works both on Mac (BSD) and Linux", "output": "If you really want to just use sed -i the 'easy' way, the following DOES work on both GNU and BSD/Mac sed:\n\n```\nsed -i.bak 's/foo/bar/' filename\n\n```\n\nNote the lack of space and the dot.\nProof:\n\n```\n# GNU sed\n% sed --version | head -1\nGNU sed version 4.2.1\n% echo 'foo' > file\n% sed -i.bak 's/foo/bar/' ./file\n% ls\nfile  file.bak\n% cat ./file\nbar\n\n# BSD sed\n% sed --version 2>&1 | head -1\nsed: illegal option -- -\n% echo 'foo' > file\n% sed -i.bak 's/foo/bar/' ./file\n% ls\nfile  file.bak\n% cat ./file\nbar\n\n```\n\nObviously you could then just delete the .bak files."}
{"instruction": "How to swap text based on patterns at once with sed?", "output": "Maybe something like this:\n\n```\nsed 's/ab/~~/g; s/bc/ab/g; s/~~/bc/g'\n\n```\n\nReplace ~ with a character that you know won't be in the string."}
{"instruction": "How to insert a text at the beginning of a file?", "output": "sed can operate on an address:\n\n```\n$ sed -i '1s/^/<added text> /' file\n\n```\n\nWhat is this magical 1s you see on every answer here? Line addressing!.\nWant to add <added text> on the first 10 lines?\n\n```\n$ sed -i '1,10s/^/<added text> /' file\n\n```\n\nOr you can use Command Grouping:\n\n```\n$ { echo -n '<added text> '; cat file; } >file.new\n$ mv file{.new,}\n\n```"}
{"instruction": "How to use sed to replace only the first occurrence in a file?", "output": "```\n # sed script to change \"foo\" to \"bar\" only on the first occurrence\n 1{x;s/^/first/;x;}\n 1,/foo/{x;/first/s///;x;s/foo/bar/;}\n #---end of script---\n\n```\n\nor, if you prefer: Editor's note: works with GNU sed only.\n\n```\nsed '0,/foo/s//bar/' file \n\n```\n\nSource"}
{"instruction": "Environment variable substitution in sed", "output": "Your two examples look identical, which makes problems hard to diagnose.  Potential problems:\n\nYou may need double quotes, as in sed 's/xxx/'\"$PWD\"'/'\n$PWD may contain a slash, in which case you need to find a character not contained in $PWD to use as a delimiter.\n\nTo nail both issues at once, perhaps\n\n```\nsed 's@xxx@'\"$PWD\"'@'\n\n```"}
{"instruction": "Find and replace with sed in directory and sub directories", "output": "Your find should look like that to avoid sending directory names to sed:\n\n```\nfind ./ -type f -exec sed -i -e 's/apple/orange/g' {} \\;\n\n```"}
{"instruction": "Change multiple files", "output": "Better yet:\n\n```\nfor i in xa*; do\n    sed -i 's/asd/dfg/g' $i\ndone\n\n```\n\nbecause nobody knows how many files are there, and it's easy to break command line limits.\nHere's what happens when there are too many files:\n\n```\n# grep -c aaa *\n-bash: /bin/grep: Argument list too long\n# for i in *; do grep -c aaa $i; done\n0\n... (output skipped)\n#\n\n```"}
{"instruction": "What are the differences between Perl, Python, AWK and sed?", "output": "In order of appearance, the languages are sed, awk, perl, python.\nThe sed program is a stream editor and is designed to apply the actions from a script to each line (or, more generally, to specified ranges of lines) of the input file or files. Its language is based on ed, the Unix editor, and although it has conditionals and so on, it is hard to work with for complex tasks. You can work minor miracles with it - but at a cost to the hair on your head. However, it is probably the fastest of the programs when attempting tasks within its remit. (It has the least powerful regular expressions of the programs discussed - adequate for many purposes, but certainly not PCRE - Perl-Compatible Regular Expressions)\nThe awk program (name from the initials of its authors - Aho, Weinberger, and Kernighan) is a tool initially for formatting reports. It can be used as a souped-up sed; in its more recent versions, it is computationally complete. It uses an interesting idea - the program is based on 'patterns matched' and 'actions taken when the pattern matches'. The patterns are fairly powerful (Extended Regular Expressions). The language for the actions is similar to C. One of the key features of awk is that it splits the input automatically into records and each record into fields.\nPerl was written in part as an awk-killer and sed-killer. Two of the programs provided with it are a2p and s2p for converting awk scripts and sed scripts into Perl. Perl is one of the earliest of the next generation of scripting languages (Tcl/Tk can probably claim primacy). It has powerful integrated regular expression handling with a vastly more powerful language. It provides access to almost all system calls and has the extensibility of the CPAN modules. (Neither awk nor sed is extensible.) One of Perl's mottos is \"TMTOWTDI - There's more than one way to do it\" (pronounced \"tim-toady\"). Perl has 'objects', but it is more of an add-on than a fundamental part of the language.\nPython was written last, and probably in part as a reaction to Perl. It has some interesting syntactic ideas (indenting to indicate levels - no braces or equivalents). It is more fundamentally object-oriented than Perl; it is just as extensible as Perl.\nOK - when to use each?\n\nSed - when you need to do simple text transforms on files.\nAwk - when you only need simple formatting and summarisation or transformation of data.\nPerl - for almost any task, but especially when the task needs complex regular expressions.\nPython - for the same tasks that you could use Perl for.\n\nI'm not aware of anything that Perl can do that Python can't, nor vice versa. The choice between the two would depend on other factors. I learned Perl before there was a Python, so I tend to use it. Python has less accreted syntax and is generally somewhat simpler to learn. Perl 6, when it becomes available, will be a fascinating development.\n(Note that the 'overviews' of Perl and Python, in particular, are woefully incomplete; whole books could be written on the topic.)"}
{"instruction": "How to concatenate multiple lines of output to one line?", "output": "Use tr '\\n' ' ' to translate all newline characters to spaces:\n\n```\n$ grep pattern file | tr '\\n' ' '\n\n```\n\nNote: grep reads files, cat concatenates files. Don't cat file | grep!\nEdit:\ntr can only handle single character translations. You could use awk to change the output record separator like:\n\n```\n$ grep pattern file | awk '{print}' ORS='\" '\n\n```\n\nThis would transform:\n\n```\none\ntwo \nthree\n\n```\n\nto:\n\n```\none\" two\" three\" \n\n```"}
{"instruction": "Delete all lines beginning with a # from a file", "output": "This can be done with a sed one-liner:\n\n```\nsed '/^#/d'\n\n```\n\nThis says, \"find all lines that start with # and delete them, leaving everything else.\""}
{"instruction": "How to show only next line after the matched one?", "output": "you can try with awk:\n\n```\nawk '/blah/{getline; print}' logfile\n\n```"}
{"instruction": "RE error: illegal byte sequence on Mac OS X", "output": "A sample command that exhibits the symptom: sed 's/./@/' <<<$'\\xfc' fails, because byte 0xfc is not a valid UTF-8 char.\nNote that, by contrast, GNU sed (Linux, but also installable on macOS) simply passes the invalid byte through, without reporting an error.\nUsing the formerly accepted answer is an option if you don't mind losing support for your true locale (if you're on a US system and you never need to deal with foreign characters, that may be fine.)\nHowever, the same effect can be had ad-hoc for a single command only:\n\n```\nLC_ALL=C sed -i \"\" 's|\"iphoneos-cross\",\"llvm-gcc:-O3|\"iphoneos-cross\",\"clang:-Os|g' Configure\n\n```\n\nNote: What matters is an effective LC_CTYPE setting of C, so LC_CTYPE=C sed ... would normally also work, but if LC_ALL happens to be set (to something other than C), it will override individual LC_*-category variables such as LC_CTYPE. Thus, the most robust approach is to set LC_ALL.\nHowever, (effectively) setting LC_CTYPE to C treats strings as if each byte were its own character (no interpretation based on encoding rules is performed), with no regard for the - multibyte-on-demand - UTF-8 encoding that OS X employs by default, where foreign characters have multibyte encodings.\nIn a nutshell: setting LC_CTYPE to C causes the shell and utilities to only recognize basic English letters as letters (the ones in the 7-bit ASCII range), so that foreign chars. will not be treated as letters, causing, for instance, upper-/lowercase conversions to fail.\nAgain, this may be fine if you needn't match multibyte-encoded characters such as √©, and simply want to pass such characters through.\nIf this is insufficient and/or you want to understand the cause of the original error (including determining what input bytes caused the problem) and perform encoding conversions on demand, read on below.\n\nThe problem is that the input file's encoding does not match the shell's.\nMore specifically, the input file contains characters encoded in a way that is not valid in UTF-8 (as @Klas Lindb√§ck stated in a comment) - that's what the sed error message is trying to say by invalid byte sequence.\nMost likely, your input file uses a single-byte 8-bit encoding such as ISO-8859-1, frequently used to encode \"Western European\" languages.\nExample: \nThe accented letter √† has Unicode codepoint 0xE0 (224) - the same as in ISO-8859-1. However, due to the nature of UTF-8 encoding, this single codepoint is represented as 2 bytes - 0xC3 0xA0, whereas trying to pass the single byte 0xE0 is invalid under UTF-8.\nHere's a demonstration of the problem using the string voil√† encoded as ISO-8859-1, with the √† represented as one byte (via an ANSI-C-quoted bash string ($'...') that uses \\x{e0} to create the byte):\nNote that the sed command is effectively a no-op that simply passes the input through, but we need it to provoke the error:\n\n```\n  # -> 'illegal byte sequence': byte 0xE0 is not a valid char.\nsed 's/.*/&/' <<<$'voil\\x{e0}'\n\n```\n\nTo simply ignore the problem, the above LCTYPE=C approach can be used:\n\n```\n  # No error, bytes are passed through ('√°' will render as '?', though).\nLC_CTYPE=C sed 's/.*/&/' <<<$'voil\\x{e0}'\n\n```\n\nIf you want to determine which parts of the input cause the problem, try the following:\n\n```\n  # Convert bytes in the 8-bit range (high bit set) to hex. representation.\n  # -> 'voil\\x{e0}'\niconv -f ASCII --byte-subst='\\x{%02x}' <<<$'voil\\x{e0}'\n\n```\n\nThe output will show you all bytes that have the high bit set (bytes that exceed the 7-bit ASCII range) in hexadecimal form. (Note, however, that that also includes correctly encoded UTF-8 multibyte sequences - a more sophisticated approach would be needed to specifically identify invalid-in-UTF-8 bytes.)\n\nPerforming encoding conversions on demand:\nStandard utility iconv can be used to convert to (-t) and/or from (-f) encodings; iconv -l lists all supported ones.\nExamples:\nConvert FROM ISO-8859-1 to the encoding in effect in the shell (based on LC_CTYPE, which is UTF-8-based by default), building on the above example:\n\n```\n  # Converts to UTF-8; output renders correctly as 'voil√†'\nsed 's/.*/&/' <<<\"$(iconv -f ISO-8859-1 <<<$'voil\\x{e0}')\"\n\n```\n\nNote that this conversion allows you to properly match foreign characters:\n\n```\n  # Correctly matches '√†' and replaces it with '√º': -> 'voil√º'\nsed 's/√†/√º/' <<<\"$(iconv -f ISO-8859-1 <<<$'voil\\x{e0}')\"\n\n```\n\nTo convert the input BACK to ISO-8859-1 after processing, simply pipe the result to another iconv command:\n\n```\nsed 's/√†/√º/' <<<\"$(iconv -f ISO-8859-1 <<<$'voil\\x{e0}')\" | iconv -t ISO-8859-1\n\n```"}
{"instruction": "Command to get nth line of STDOUT", "output": "Using sed, just for variety:\n\n```\nls -l | sed -n 2p\n\n```\n\nUsing this alternative, which looks more efficient since it stops reading the input when the required line is printed, may generate a SIGPIPE in the feeding process, which may in turn generate an unwanted error message:\n\n```\nls -l | sed -n -e '2{p;q}'\n\n```\n\nI've seen that often enough that I usually use the first (which is easier to type, anyway), though ls is not a command that complains when it gets SIGPIPE.\nFor a range of lines:\n\n```\nls -l | sed -n 2,4p\n\n```\n\nFor several ranges of lines:\n\n```\nls -l | sed -n -e 2,4p -e 20,30p\nls -l | sed -n -e '2,4p;20,30p'\n\n```"}
{"instruction": "In-place edits with sed on OS X", "output": "You can use the -i flag correctly by providing it with a suffix to add to the backed-up file. Extending your example: \n\n```\nsed -i.bu 's/oldword/newword/' file1.txt\n\n```\n\nWill give you two files: one with the name file1.txt that contains the substitution, and one with the name file1.txt.bu that has the original content. \nMildly dangerous\nIf you want to destructively overwrite the original file, use something like: \n\n```\nsed -i '' 's/oldword/newword/' file1.txt\n      ^ note the space\n\n```\n\nBecause of the way the line gets parsed, a space is required between the option flag and its argument because the argument is zero-length. \nOther than possibly trashing your original, I‚Äôm not aware of any further dangers of tricking sed this way. It should be noted, however, that if this invocation of sed is part of a script, The Unix Way‚Ñ¢ would (IMHO) be to use sed non-destructively, test that it exited cleanly, and only then remove the extraneous file."}
{"instruction": "Appending a line to a file only if it does not already exist", "output": "Just keep it simple :)\ngrep + echo should suffice:\n\n```\ngrep -qxF 'include \"/configs/projectname.conf\"' foo.bar || echo 'include \"/configs/projectname.conf\"' >> foo.bar\n\n```\n\n\n-q be quiet\n-x match the whole line\n-F pattern is a plain string\nhttps://linux.die.net/man/1/grep\n\nEdit:\nincorporated @cerin and @thijs-wouters suggestions."}
{"instruction": "How to grep for case insensitive string in a file?", "output": "You can use the -i flag which makes your pattern case insensitive:\n\n```\ngrep -iF \"success...\" file1\n\n```\n\nAlso, there is no need for cat. grep takes a file with the syntax grep <pattern> <file>. I also used the -F flag to search for a fixed string to avoid escaping the ellipsis."}
{"instruction": "sed fails with &quot;unknown option to `s&#39;&quot; error", "output": "The problem is with slashes: your variable contains them and the final command will be something like sed \"s/string/path/to/something/g\", containing way too many slashes.\nSince sed can take any char as delimiter (without having to declare the new delimiter), you can try using another one that doesn't appear in your replacement string:\n\n```\nreplacement=\"/my/path\"\nsed --expression \"s@pattern@$replacement@\"\n\n```\n\nNote that this is not bullet proof: if the replacement string later contains @ it will break for the same reason, and any backslash sequences like \\1 will still be interpreted according to sed rules. Using | as a delimiter is also a nice option as it is similar in readability to /."}
{"instruction": "How to replace a whole line with sed?", "output": "Try this:\n\n```\nsed \"s/aaa=.*/aaa=xxx/g\"\n\n```"}
{"instruction": "How to use sed/grep to extract text between two words?", "output": "```\nsed -e 's/Here\\(.*\\)String/\\1/'\n\n```"}
{"instruction": "How to merge every two lines into one from the command line?", "output": "awk:\n\n```\nawk 'NR%2{printf \"%s \",$0;next;}1' yourFile\n\n```\n\nnote, there is an empty line at the end of output.\nsed:\n\n```\nsed 'N;s/\\n/ /' yourFile\n\n```"}
{"instruction": "How to replace an entire line in a text file by line number", "output": "Not the greatest, but this should work:\n\n```\nsed -i 'Ns/.*/replacement-line/' file.txt\n\n```\n\nwhere N should be replaced by your target line number. This replaces the line in the original file. To save the changed text in a different file, drop the -i option:\n\n```\nsed 'Ns/.*/replacement-line/' file.txt > new_file.txt\n\n```"}
{"instruction": "How to delete duplicate lines in a file without sorting it in Unix", "output": "```\nawk '!seen[$0]++' file.txt\n\n```\n\nseen is an associative array that AWK will pass every line of the file to. If a line isn't in the array then seen[$0] will evaluate to false. The ! is the logical NOT operator and will invert the false to true. AWK will print the lines where the expression evaluates to true.\nThe ++ increments seen so that seen[$0] == 1 after the first time a line is found and then seen[$0] == 2, and so on.\nAWK evaluates everything but 0 and \"\" (empty string) to true. If a duplicate line is placed in seen then !seen[$0] will evaluate to false and the line will not be written to the output."}
{"instruction": "How to remove the lines which appear on file B from another file A?", "output": "If the files are sorted (they are in your example):\n\n```\ncomm -23 file1 file2\n\n```\n\n-23 suppresses the lines that are in both files, or only in file 2. If the files are not sorted, pipe them through sort first...\nSee the man page here"}
{"instruction": "how to parse a JSON String with jq (or other alternatives)?", "output": "jq has the fromjson builtin for this:\n\n```\njq '.c | fromjson | .id' myFile.json\n\n```\n\nfromjson was added in version 1.4."}
{"instruction": "sed one-liner to convert all uppercase to lowercase?", "output": "Here are two methods for doing the conversion using tr and sed:\nUsing tr\nConvert uppercase to lowercase\n\n```\ntr '[:upper:]' '[:lower:]' < input.txt > output.txt\n\n```\n\nConvert lowercase to uppercase\n\n```\ntr '[:lower:]' '[:upper:]' < input.txt > output.txt\n\n```\n\nUsing sed on GNU (but not BSD or Mac)\nConvert uppercase to lowercase\n\n```\nsed -e 's/\\(.*\\)/\\L\\1/' input.txt > output.txt\n\n```\n\nConvert lowercase to uppercase\n\n```\nsed -e 's/\\(.*\\)/\\U\\1/' input.txt > output.txt\n \n\n```\n\nThe reason the sed version doesn't work on BSD or Mac is because those systems don't support the \\L or \\U flags"}
{"instruction": "How do I remove newlines from a text file?", "output": "```\ntr --delete '\\n' < yourfile.txt\ntr -d '\\n' < yourfile.txt\n\n```\n\nIf none of the commands posted here are working, then you have something other than a newline separating your fields. Possibly you have DOS/Windows line endings in the file (although I would expect the Perl solutions to work even in that case)?\nTry:\n\n```\ntr -d \"\\n\\r\" < yourfile.txt\n\n```\n\nIf that doesn't work then you're going to have to inspect your file more closely (e.g., in a hex editor) to find out what characters are actually in there that you want to remove."}
{"instruction": "How to use sed to remove the last n lines of a file", "output": "I don't know about sed, but it can be done with head:\n\n```\nhead -n -2 myfile.txt\n\n```"}
{"instruction": "sed: print only matching group", "output": "Match the whole line, so add a .* at the beginning of your regex. This causes the entire line to be replaced with the contents of the group \n\n```\necho \"foo bar <foo> bla 1 2 3.4\" |\n sed -n  's/.*\\([0-9][0-9]*[\\ \\t][0-9.]*[ \\t]*$\\)/\\1/p'\n2 3.4\n\n```"}
{"instruction": "How to insert strings containing slashes with sed?", "output": "The easiest way would be to use a different delimiter in your search/replace lines, e.g.:\n\n```\ns:?page=one&:pageone:g\n\n```\n\nYou can use any character as a delimiter that's not part of either string. Or, you could escape it with a backslash:\n\n```\ns/\\//foo/\n\n```\n\nWhich would replace / with foo. You'd want to use the escaped backslash in cases where you don't know what characters might occur in the replacement strings (if they are shell variables, for example)."}
{"instruction": "Is there any sed like utility for cmd.exe?", "output": "Today powershell saved me.\nFor grep there is:\n\n```\nget-content somefile.txt | where { $_ -match \"expression\"}\n\n```\n\nor\n\n```\nselect-string somefile.txt -pattern \"expression\"\n\n```\n\nand for sed there is:\n\n```\nget-content somefile.txt | %{$_ -replace \"expression\",\"replace\"}\n\n```\n\nFor more detail about replace PowerShell function see this Microsoft article."}
{"instruction": "Find all files containing a specific text (string) on Linux?", "output": "Do the following:\n\n```\ngrep -rnw '/path/to/somewhere/' -e 'pattern'\n\n```\n\n\n-r or -R is recursive,\n-n is line number, and\n-w stands for match the whole word.\n-l (lower-case L) can be added to just give the file name of matching files.\n-e is the pattern used during the search\n\nAlong with these, --exclude, --include, --exclude-dir flags could be used for efficient searching:\n\nThis will only search through those files which have .c or .h extensions:\n\n```\ngrep --include=\\*.{c,h} -rnw '/path/to/somewhere/' -e \"pattern\"\n\n```\n\n\nThis will exclude searching all the files ending with .o extension:\n\n```\ngrep --exclude=\\*.o -rnw '/path/to/somewhere/' -e \"pattern\"\n\n```\n\n\nFor directories it's possible to exclude one or more directories using the --exclude-dir parameter. For example, this will exclude the dirs dir1/, dir2/ and all of them matching *.dst/:\n\n```\ngrep --exclude-dir={dir1,dir2,*.dst} -rnw '/path/to/search/' -e \"pattern\"\n\n```\n\n\n\nThis works very well for me, to achieve almost the same purpose like yours.\nFor more options, see man grep."}
{"instruction": "grep: show lines surrounding each match", "output": "For BSD or GNU grep you can use -B num to set how many lines before the match and -A num for the number of lines after the match.\n\n```\ngrep -B 3 -A 2 foo README.txt\n\n```\n\nIf you want the same number of lines before and after you can use -C num.\n\n```\ngrep -C 3 foo README.txt\n\n```\n\nThis will show 3 lines before and 3 lines after."}
{"instruction": "How do I recursively grep all directories and subdirectories?", "output": "```\ngrep -r \"texthere\" .\n\n```\n\nThe first parameter represents the regular expression to search for, while the second one represents the directory that should be searched. In this case, . means the current directory.\nNote: This works for GNU grep, and on some platforms like Solaris you must specifically use GNU grep as opposed to legacy implementation.  For Solaris this is the ggrep command."}
{"instruction": "How to grep (search through) committed code in the Git history", "output": "You should use the pickaxe (-S) option of git log.\nTo search for Foo:\n\n```\ngit log -SFoo -- path_containing_change\ngit log -SFoo --since=2009.1.1 --until=2010.1.1 -- path_containing_change\n\n```\n\nSee Git history - find lost line by keyword for more.\n-S (named pickaxe) comes originally from a git diff option (Git v0.99, May 2005).\nThen -S (pickaxe) was ported to git log in May 2006 with Git 1.4.0-rc1.\n\nAs Jakub Narƒôbski commented:\n\nthis looks for differences that introduce or remove an instance of <string>.\nIt usually means \"revisions where you added or removed line with 'Foo'\".\n\nthe --pickaxe-regex option allows you to use extended POSIX regex instead of searching for a string.\nExample (from git log): git log -S\"frotz\\(nitfol\" --pickaxe-regex\n\n\n\nAs Rob commented, this search is case-sensitive - he opened a follow-up question on how to search case-insensitive.\n\nHi Angel notes in the comments:\n\nExecuting a git log -G<regexp> --branches --all (the -G is same as -S but for regexes) does same thing as the accepted one (git grep <regexp> $(git rev-list --all)), but it soooo much faster!\nThe accepted answer was still searching for text after ‚âà10 minutes of me running it, whereas this one gives results after ‚âà4 seconds ü§∑‚Äç‚ôÇÔ∏è.\nThe output here is more useful as well"}
{"instruction": "How can I use grep to show just filenames on Linux?", "output": "The standard option grep -l (that is a lowercase L) could do this.\nFrom the Unix standard:\n\n```\n-l\n    (The letter ell.) Write only the names of files containing selected\n    lines to standard output. Pathnames are written once per file searched.\n    If the standard input is searched, a pathname of (standard input) will\n    be written, in the POSIX locale. In other locales, standard input may be\n    replaced by something more appropriate in those locales.\n\n```\n\nYou also do not need -H in this case."}
{"instruction": "How can I grep recursively, but only in files with certain extensions?", "output": "Just use the --include parameter, like this:\n\n```\ngrep -inr --include \\*.h --include \\*.cpp CP_Image ~/path[12345] | mailx -s GREP email@domain.example\n\n```\n\nThat should do what you want.\nTo take the explanation from HoldOffHunger's answer below:\n\ngrep: command\n\n-r: recursively\n\n-i: ignore-case\n\n-n: each output line is preceded by its relative line number in the file\n\n--include \\*.cpp: all *.cpp: C++ files (escape with \\ just in case you have a directory with asterisks in the filenames)\n\n./: Start at current directory."}
{"instruction": "Negative matching using grep (match lines that do not contain foo)", "output": "grep -v is your friend:\n\n```\ngrep --help | grep invert  \n\n```\n\n\n-v, --invert-match        select non-matching lines\n\nAlso check out the related -L (the complement of -l).\n\n-L, --files-without-match only print FILE names containing no match"}
{"instruction": "How can I pipe stderr, and not stdout?", "output": "First redirect stderr to stdout ‚Äî the pipe; then redirect stdout to /dev/null (without changing where stderr is going):\n\n```\ncommand 2>&1 >/dev/null | grep 'something'\n\n```\n\nFor the details of I/O redirection in all its variety, see the chapter on Redirections in the Bash reference manual.\nNote that the sequence of I/O redirections is interpreted left-to-right, but pipes are set up before the I/O redirections are interpreted.  File descriptors such as 1 and 2 are references to open file descriptions.  The operation 2>&1 makes file descriptor 2 aka stderr refer to the same open file description as file descriptor 1 aka stdout is currently referring to (see dup2() and open()).  The operation >/dev/null then changes file descriptor 1 so that it refers to an open file description for /dev/null, but that doesn't change the fact that file descriptor 2 refers to the open file description which file descriptor 1 was originally pointing to ‚Äî namely, the pipe."}
{"instruction": "How can I exclude directories from grep -R?", "output": "SOLUTION 1 (combine find and grep)\nThe purpose of this solution is not to deal with grep performance but to show a portable solution : should also work with busybox or GNU version older than 2.5.\nUse find, for excluding directories foo and bar :\n\n```\nfind /dir \\( -name foo -prune \\) -o \\( -name bar -prune \\) -o -name \"*.sh\" -print\n\n```\n\nThen combine find and the non-recursive use of grep, as a portable solution :\n\n```\nfind /dir \\( -name node_modules -prune \\) -o -name \"*.sh\" -exec grep --color -Hn \"your text to find\" {} 2>/dev/null \\;\n\n```\n\nSOLUTION 2 (recursive use of grep, using the -R and --exclude-dir option of grep):\nYou know this solution already (recursive search), but I add it since it's the most recent and efficient solution. Note this is a less portable solution but more human-readable.\n\n```\ngrep -R --exclude-dir=node_modules 'some pattern' /path/to/search\n\n```\n\nTo exclude multiple directories, use --exclude-dir as:\n--exclude-dir={node_modules,dir1,dir2,dir3}\nNote the \"-R\" option for a deference recursive file reading, i.e. following ALL sym links, unlike \"-r\".\nSOLUTION 3 (Ag)\nIf you frequently search through code, Ag (The Silver Searcher) is a much faster alternative to grep, that's customized for searching code. For instance, it automatically ignores files and directories listed in .gitignore, so you don't have to keep passing the same cumbersome exclude options to grep or find."}
{"instruction": "Use grep --exclude/--include syntax to not grep through certain files", "output": "Use the shell globbing syntax:\n\n```\ngrep pattern -r --include=\\*.cpp --include=\\*.h rootdir\n\n```\n\nThe syntax for --exclude is identical.\nNote that the star is escaped with a backslash to prevent it from being expanded by the shell (quoting it, such as --include=\"*.cpp\", would work just as well).  Otherwise, if you had any files in the current working directory that matched the pattern, the command line would expand to something like grep pattern -r --include=foo.cpp --include=bar.cpp rootdir, which would only search files named foo.cpp and bar.cpp, which is quite likely not what you wanted.\nUpdate 2021-03-04\nI've edited the original answer to remove the use of brace expansion, which is a feature provided by several shells such as Bash and zsh to simplify patterns like this; but note that brace expansion is not POSIX shell-compliant.\nThe original example was:\n\n```\ngrep pattern -r --include=\\*.{cpp,h} rootdir\n\n```\n\nto search through all .cpp and .h files rooted in the directory rootdir."}
{"instruction": "How to &#39;grep&#39; a continuous stream?", "output": "Turn on grep's line buffering mode when using BSD grep (FreeBSD, Mac OS X etc.)\n\n```\ntail -f file | grep --line-buffered my_pattern\n\n```\n\nIt looks like a while ago --line-buffered didn't matter for GNU grep (used on pretty much any Linux) as it flushed by default (YMMV for other Unix-likes such as SmartOS, AIX or QNX). However, as of November 2020, --line-buffered is needed (at least with GNU grep 3.5 in openSUSE, but it seems generally needed based on comments below)."}
{"instruction": "How can I grep Git commits for a certain word?", "output": "If you want to find all commits where the commit message contains a given word, use\n\n```\ngit log --grep=word\n\n```\n\nIf you want to find all commits where \"word\" was added or removed in the file contents (to be more exact: where the number of occurrences of \"word\" changed), i.e., search the commit contents, use a so-called 'pickaxe' search with\n\n```\ngit log -Sword\n\n```\n\nIn modern Git there is also\n\n```\ngit log -Gword\n\n```\n\nto look for differences whose added or removed line matches \"word\" (also commit contents).\nA few things to note:\n\n-G by default accepts a regex, while -S accepts a string, but it can be modified to accept regexes using the --pickaxe-regex.\n-S finds commits where the number of occurrences of \"word\" changed, while -G finds commits where \"word\" appears in the diff.\nThis means that -S<regex> --pickaxe-regex and -G<regex> do not do exactly the same thing.\n\nThe git diff documentation has a nice explanation of the difference:\n\nTo illustrate the difference between -S<regex> --pickaxe-regex and -G<regex>, consider a commit with the following diff in the same file:\n\n```\n+    return frotz(nitfol, two->ptr, 1, 0);\n...\n-    hit = frotz(nitfol, mf2.ptr, 1, 0);\n\n```\n\nWhile git log -G\"frotz\\(nitfol\" will show this commit, git log -S\"frotz\\(nitfol\" --pickaxe-regex will not (because the number of occurrences of that string did not change).\n\nThis will show the commits containing the search terms, but if you want to see the actual changes in those commits instead you can use --patch:\n\n```\ngit log -G\"searchTerm\" --patch\n\n```\n\nThis can then be piped to grep to isolate the output just to display commit diff lines with that search term.  A common use-case is to display diff lines with that search term in commits since and including a given commit - 3b5ab0f2a1 in this example - like so:\n\n```\ngit log 3b5ab0f2a1^.. -G\"searchTerm\" --patch | grep searchTerm\n\n```"}
{"instruction": "How do I find files that do not contain a given string pattern?", "output": "The following command gives me all the files that do not contain the pattern foo:\n\n```\nfind .  -not  -ipath '.*svn*' -exec  grep  -H -E -o -c  \"foo\"  {} \\; | grep 0\n\n```"}
{"instruction": "Delete all local Git branches", "output": "The 'git branch -d' subcommand can delete more than one branch.  So, simplifying @sblom's answer but adding a critical xargs:\n\n```\ngit branch -D `git branch --merged | grep -v \\* | xargs`\n\n```\n\nor, further simplified to:\n\n```\ngit branch --merged | grep -v \\* | xargs git branch -D \n\n```\n\nImportantly, as noted by @AndrewC, using git branch for scripting is discouraged.  To avoid it use something like:\n\n```\ngit for-each-ref --format '%(refname:short)' refs/heads | grep -v \"master\\|main\" | xargs git branch -D\n\n```\n\nCaution warranted on deletes!\n\n```\n$ mkdir br\n$ cd br; git init\nInitialized empty Git repository in /Users/ebg/test/br/.git/\n$ touch README; git add README; git commit -m 'First commit'\n[master (root-commit) 1d738b5] First commit\n 0 files changed, 0 insertions(+), 0 deletions(-)\n create mode 100644 README\n$ git branch Story-123-a\n$ git branch Story-123-b\n$ git branch Story-123-c\n$ git branch --merged\n  Story-123-a\n  Story-123-b\n  Story-123-c\n* master\n$ git branch --merged | grep -v \\* | xargs\nStory-123-a Story-123-b Story-123-c\n$ git branch --merged | grep -v \\* | xargs git branch -D\nDeleted branch Story-123-a (was 1d738b5).\nDeleted branch Story-123-b (was 1d738b5).\nDeleted branch Story-123-c (was 1d738b5).\n\n```"}
{"instruction": "How can I use grep to find a word inside a folder?", "output": "```\ngrep -nr 'yourString*' .\n\n```\n\nThe dot at the end searches the current directory. Meaning for each parameter:\n\n```\n-n            Show relative line number in the file\n'yourString*' String for search, followed by a wildcard character\n-r            Recursively search subdirectories listed\n.             Directory for search (current directory)\n\n```\n\ngrep -nr 'MobileAppSer*' .  (Would find MobileAppServlet.java or MobileAppServlet.class or MobileAppServlet.txt; 'MobileAppASer*.*' is another way to do the same thing.)\nTo check more parameters use man grep command."}
{"instruction": "Colorized grep -- viewing the entire file with highlighted matches", "output": "Here are some ways to do it:\n\n```\ngrep --color 'pattern\\|$' file\ngrep --color -E 'pattern|$' file\negrep --color 'pattern|$' file\n\n```\n\nThe | symbol is the OR operator. Either escape it using \\ or tell grep that the search text has to be interpreted as regular expressions by adding -E or using the egrep command instead of grep.\nThe search text \"pattern|$\" is actually a trick, it will match lines that have pattern OR lines that have an end. Because all lines have an end, all lines are matched, but the end of a line isn't actually any characters, so it won't be colored.\nTo also pass the colored parts through pipes, e.g. towards less, provide the always parameter to --color:\n\n```\ngrep --color=always 'pattern\\|$' file | less -r\ngrep --color=always -E 'pattern|$' file | less -r\negrep --color=always 'pattern|$' file | less -r\n\n```"}
{"instruction": "How can I exclude one word with grep?", "output": "You can do it using -v (for --invert-match) option of grep as:\n\n```\ngrep -v \"unwanted_word\" file | grep XXXXXXXX\n\n```\n\ngrep -v \"unwanted_word\" file will filter the lines that have the unwanted_word and grep XXXXXXXX will list only lines with pattern XXXXXXXX.\nEDIT:\nFrom your comment it looks like you want to list all lines without the unwanted_word. In that case all you need is:\n\n```\ngrep -v 'unwanted_word' file\n\n```"}
{"instruction": "How can I grep for a string that begins with a dash/hyphen?", "output": "Use:\ngrep -- -X\nDocumentation\nRelated: What does a bare double dash mean? (thanks to nutty about natty)."}
{"instruction": "Capturing Groups From a Grep RegEx", "output": "If you're using Bash, you don't even have to use grep:\n\n```\nfiles=\"*.jpg\"\nregex=\"[0-9]+_([a-z]+)_[0-9a-z]*\" # put the regex in a variable because some patterns won't work if included literally\nfor f in $files    # unquoted in order to allow the glob to expand\ndo\n    if [[ $f =~ $regex ]]\n    then\n        name=\"${BASH_REMATCH[1]}\"\n        echo \"${name}.jpg\"    # concatenate strings\n        name=\"${name}.jpg\"    # same thing stored in a variable\n    else\n        echo \"$f doesn't match\" >&2 # this could get noisy if there are a lot of non-matching files\n    fi\ndone\n\n```\n\nIt's better to put the regex in a variable. Some patterns won't work if included literally.\nThis uses  =~ which is Bash's regex match operator. The results of the match are saved to an array called $BASH_REMATCH. The first capture group is stored in index 1, the second (if any) in index 2, etc. Index zero is the full match.\n\n\n\nside note #1 regarding regex anchors:\nYou should be aware that without anchors, this regex (and the one using grep) will match any of the following examples and more, which may not be what you're looking for:\n\n```\n123_abc_d4e5\nxyz123_abc_d4e5\n123_abc_d4e5.xyz\nxyz123_abc_d4e5.xyz\n\n```\n\nTo eliminate the second and fourth examples, make your regex like this:\n\n```\n^[0-9]+_([a-z]+)_[0-9a-z]*\n\n```\n\nwhich says the string must start with one or more digits. The carat represents the beginning of the string. If you add a dollar sign at the end of the regex, like this:\n\n```\n^[0-9]+_([a-z]+)_[0-9a-z]*$\n\n```\n\nthen the third example will also be eliminated since the dot is not among the characters in the regex and the dollar sign represents the end of the string. Note that the fourth example fails this match as well.\nside note #2 regarding grep and the \\K operator:\nIf you have GNU grep (around 2.5 or later, I think, when the \\K operator was added):\n\n```\nname=$(echo \"$f\" | grep -Po '(?i)[0-9]+_\\K[a-z]+(?=_[0-9a-z]*)').jpg\n\n```\n\nThe \\K operator (variable-length look-behind) causes the preceding pattern to match, but doesn't include the match in the result. The fixed-length equivalent is (?<=) - the pattern would be included before the closing parenthesis. You must use \\K if quantifiers may match strings of different lengths (e.g. +, *, {2,4}).\nThe (?=) operator matches fixed or variable-length patterns and is called \"look-ahead\". It also does not include the matched string in the result.\nIn order to make the match case-insensitive, the (?i) operator is used. It affects the patterns that follow it so its position is significant.\nThe regex might need to be adjusted depending on whether there are other characters in the filename. You'll note that in this case, I show an example of concatenating a string at the same time that the substring is captured."}
{"instruction": "How can I make grep print the lines below and above each matching line?", "output": "grep's -A 1 option will give you one line after; -B 1 will give you one line before; and -C 1 combines both to give you one line both before and after, -1 does the same."}
{"instruction": "Get line number while using grep", "output": "```\ngrep -n SEARCHTERM file1 file2 ...\n\n```"}
{"instruction": "How do I grep for all non-ASCII characters?", "output": "You can use the command:\n\n```\nLC_ALL=C  grep --color='auto' -P -n \"[\\x80-\\xFF]\" file.xml\n\n```\n\nThis will give you the line number, and will highlight non-ascii chars in red.\nIn  some systems, depending on your settings, the above will not work, so you can grep by the inverse\n\n```\nLC_ALL=C  grep --color='auto' -P -n \"[^\\x00-\\x7F]\" file.xml\n\n```\n\nNote also, that the important bit is the -P flag which equates to --perl-regexp: so it will interpret your pattern as a Perl regular expression. It also says that\n\nthis is highly experimental and grep -P may warn of unimplemented\nfeatures."}
{"instruction": "How can I format my grep output to show line numbers at the end of the line, and also the hit count?", "output": "-n returns line number.\n-i is for ignore-case. Only to be used if case matching is not necessary\n\n```\n$ grep -in null myfile.txt\n\n2:example two null,\n4:example four null,\n\n```\n\nCombine with awk to print out the line number after the match:\n\n```\n$ grep -in null myfile.txt | awk -F: '{print $2\" - Line number : \"$1}'\n\nexample two null, - Line number : 2\nexample four null, - Line number : 4\n\n```\n\nUse command substitution to print out the total null count:\n\n```\n$ echo \"Total null count :\" $(grep -ic null myfile.txt)\n\nTotal null count : 2\n\n```"}
{"instruction": "How to check if a file contains a specific string using Bash", "output": "```\nif grep -q SomeString \"$File\"; then\n  Some Actions # SomeString was found\nfi\n\n```\n\nYou don't need [[ ]] here. Just run the command directly. Add -q option when you don't need the string displayed when it was found.\nThe grep command returns 0 or 1 in the exit code depending on\nthe result of search. 0 if something was found; 1 otherwise.\n\n```\n$ echo hello | grep hi ; echo $?\n1\n$ echo hello | grep he ; echo $?\nhello\n0\n$ echo hello | grep -q he ; echo $?\n0\n\n```\n\nYou can specify commands as an condition of if. If the command returns 0 in its exitcode that means that the condition is true; otherwise false.\n\n```\n$ if /bin/true; then echo that is true; fi\nthat is true\n$ if /bin/false; then echo that is true; fi\n$\n\n```\n\nAs you can see you run here the programs directly. No additional [] or [[]]."}
{"instruction": "Fast way of finding lines in one file that are not in another?", "output": "You can achieve this by controlling the formatting of the old/new/unchanged lines in GNU diff output:\n\n```\ndiff --new-line-format=\"\" --unchanged-line-format=\"\"  file1 file2\n\n```\n\nThe input files should be sorted for this to work. With bash (and zsh) you can sort in-place with process substitution <( ):\n\n```\ndiff --new-line-format=\"\" --unchanged-line-format=\"\" <(sort file1) <(sort file2)\n\n```\n\nIn the above new and unchanged lines are suppressed,  so only changed (i.e. removed lines in your case) are output. You may also use a few diff options that other solutions don't offer, such as -i to ignore case, or various whitespace options (-E, -b, -v etc) for less strict matching.\n\nExplanation\nThe options --new-line-format, --old-line-format and --unchanged-line-format let you control the way diff formats the differences, similar to  printf format specifiers. These options format new (added), old (removed) and unchanged lines respectively. Setting one to empty \"\" prevents output of that kind of line.\nIf you are familiar with unified diff format, you can partly recreate it with:\n\n```\ndiff --old-line-format=\"-%L\" --unchanged-line-format=\" %L\" \\\n     --new-line-format=\"+%L\" file1 file2\n\n```\n\nThe %L specifier is the line in question, and we prefix each with \"+\" \"-\" or \" \", like diff -u\n(note that it only outputs differences, it lacks the --- +++ and @@ lines at the top of each grouped change).\nYou can also use this to do other useful things like number each line with %dn.\n\nThe diff method (along with other suggestions comm and join) only produce the expected output with sorted input, though you can use <(sort ...) to sort in place. Here's a simple awk (nawk) script (inspired by the scripts linked-to in Konsolebox's answer) which accepts arbitrarily ordered input files, and outputs the missing lines in the order they occur in file1.\n\n```\n# output lines in file1 that are not in file2\nBEGIN { FS=\"\" }                         # preserve whitespace\n(NR==FNR) { ll1[FNR]=$0; nl1=FNR; }     # file1, index by lineno\n(NR!=FNR) { ss2[$0]++; }                # file2, index by string\nEND {\n    for (ll=1; ll<=nl1; ll++) if (!(ll1[ll] in ss2)) print ll1[ll]\n}\n\n```\n\nThis stores the entire contents of file1 line by line in a line-number indexed array ll1[], and the entire contents of file2 line by line in a line-content indexed associative array ss2[]. After both files are read, iterate over ll1 and use the in operator to determine if the line in file1 is present in file2. (This will have have different output to the diff method if there are duplicates.)\nIn the event that the files are sufficiently large that storing them both causes a memory problem, you can trade CPU for memory by storing only file1 and deleting matches along the way as file2 is read.\n\n```\nBEGIN { FS=\"\" }\n(NR==FNR) {  # file1, index by lineno and string\n  ll1[FNR]=$0; ss1[$0]=FNR; nl1=FNR;\n}\n(NR!=FNR) {  # file2\n  if ($0 in ss1) { delete ll1[ss1[$0]]; delete ss1[$0]; }\n}\nEND {\n  for (ll=1; ll<=nl1; ll++) if (ll in ll1) print ll1[ll]\n}\n\n```\n\nThe above stores the entire contents of file1 in two arrays, one indexed by line number ll1[], one indexed by line content ss1[]. Then as file2 is read, each matching line is deleted from ll1[] and ss1[]. At the end the remaining lines from file1 are output, preserving the original order.\nIn this case, with the problem as stated, you can also divide and conquer using GNU split (filtering is a GNU extension), repeated runs with chunks of file1 and reading file2 completely each time:\n\n```\nsplit -l 20000 --filter='gawk -f linesnotin.awk - file2' < file1\n\n```\n\nNote the use and placement of - meaning stdin on the gawk command line. This is provided by split from file1 in chunks of 20000 line per-invocation.\nFor users on non-GNU systems, there is almost certainly a GNU coreutils package you can obtain, including on OSX as part of the Apple Xcode tools which provides GNU diff, awk, though only a POSIX/BSD split rather than a GNU version."}
{"instruction": "How to suppress binary file matching results in grep", "output": "There are three options, that you can use. -I is to exclude binary files in grep. Other are for line numbers and file names.\n\n```\ngrep -I -n -H \n\n\n-I -- process a binary file as if it did not contain matching data; \n-n -- prefix each line of output with the 1-based line number within its input file\n-H -- print the file name for each match\n\n```\n\nSo this might be a way to run grep:\n\n```\ngrep -InH your-word *\n\n```"}
{"instruction": "Display filename before matching line", "output": "Try this little trick to coax grep into thinking it is dealing with multiple files, so that it displays the filename:\n\n```\ngrep 'pattern' file /dev/null\n\n```\n\nTo also get the line number:\n\n```\ngrep -n 'pattern' file /dev/null\n\n```"}
{"instruction": "How can I find all of the distinct file extensions in a folder hierarchy?", "output": "Try this (not sure if it's the best way, but it works):\n\n```\nfind . -type f | perl -ne 'print $1 if m/\\.([^.\\/]+)$/' | sort -u\n\n```\n\nIt work as following:\n\nFind all files from current folder\nPrints extension of files if any\nMake a unique sorted list"}
{"instruction": "Highlight text similar to grep, but don&#39;t filter out text", "output": "Use ack. Check out its --passthru option here: ack. It has the added benefit of allowing full Perl regular expressions.\n\n```\n    $ ack --passthru 'pattern1' file_name\n\n    $ command_here | ack --passthru 'pattern1'\n\n```\n\nYou can also do it using grep like this:\n\n```\n    $ grep --color -E '^|pattern1|pattern2' file_name\n\n    $ command_here | grep --color -E '^|pattern1|pattern2'\n\n```\n\nThis will match all lines and highlight the patterns. The ^ matches every start of the line but won't get printed/highlighted since it's not a character.\n(Note that most of the setups will use --color by default. You may not need that flag)."}
{"instruction": "Count all occurrences of a string in lots of files with grep", "output": "```\ncat * | grep -c string\n\n```"}
{"instruction": "How to invert a grep expression", "output": "Use command-line option -v or --invert-match,\n\n```\nls -R |grep -v -E .*[\\.exe]$\\|.*[\\.html]$\n\n```"}
{"instruction": "Grep regex NOT containing a string", "output": "grep matches, grep -v does the inverse. If you need to \"match A but not B\" you usually use pipes:\n\n```\ngrep \"${PATT}\" file | grep -v \"${NOTPATT}\"\n\n```"}
{"instruction": "How to get the process ID to kill a nohup process?", "output": "When using nohup and you put the task in the background, the background operator (&) will give you the PID at the command prompt. If your plan is to manually manage the process, you can save that PID and use it later to kill the process if needed, via kill PID or kill -9 PID (if you need to force kill). Alternatively, you can find the PID later on by ps -ef | grep \"command name\" and locate the PID from there. Note that nohup keyword/command itself does not appear in the ps output for the command in question.\nIf you use a script, you could do something like this in the script:\n\n```\nnohup my_command > my.log 2>&1 &\necho $! > save_pid.txt\n\n```\n\nThis will run my_command saving all output into my.log (in a script, $! represents the PID of the last process executed). The 2 is the file descriptor for standard error (stderr) and 2>&1 tells the shell to route standard error output to the standard output (file descriptor 1). It requires &1 so that the shell knows it's a file descriptor in that context instead of just a file named 1. The 2>&1 is needed to capture any error messages that normally are written to standard error into our my.log file (which is coming from standard output). See I/O Redirection for more details on handling I/O redirection with the shell.\nIf the command sends output on a regular basis, you can check the output occasionally with tail my.log, or if you want to follow it \"live\" you can use tail -f my.log. Finally, if you need to kill the process, you can do it via:\n\n```\nkill -9 `cat save_pid.txt`\nrm save_pid.txt\n\n```"}
{"instruction": "What is makeinfo, and how do I get it?", "output": "In (at least) Ubuntu when using bash, it tells you what package you need to install if you type in a command and its not found in your path. My terminal says you need to install 'texinfo' package.\n\n```\nsudo apt-get install texinfo\n\n```"}
{"instruction": "How to search contents of multiple pdf files?", "output": "Your distribution should provide a utility called pdftotext:\n\n```\nfind /path -name '*.pdf' -exec sh -c 'pdftotext \"{}\" - | grep --with-filename --label=\"{}\" --color \"your pattern\"' \\;\n\n```\n\nThe \"-\" is necessary to have pdftotext output to stdout, not to files.\nThe --with-filename and --label= options will put the file name in the output of grep.\nThe optional --color flag is nice and tells grep to output using colors on the terminal.\n(In Ubuntu, pdftotext is provided by the package xpdf-utils or poppler-utils.)\nThis method, using pdftotext and grep, has an advantage over pdfgrep if you want to use features of GNU grep that pdfgrep doesn't support. Note: pdfgrep-1.3.x supports -C option for printing line of context."}
{"instruction": "How can I have grep not print out &#39;No such file or directory&#39; errors?", "output": "You can use the -s or --no-messages flag to suppress errors.\n\n-s, --no-messages         suppress error messages\n\n\n```\ngrep pattern * -s -R -n\n\n```"}
{"instruction": "Match two strings in one line with grep", "output": "You can use\n\n```\ngrep 'string1' filename | grep 'string2'\n\n```\n\nThis searches for string1 followed by string 2 on the same line, or string2 followed by string1 on the same line; it does not answer the question:\n\n```\ngrep 'string1.*string2\\|string2.*string1' filename\n\n```"}
{"instruction": "grep without showing path/file:line", "output": "No need to find. If you are just looking for a pattern within a specific directory, this should suffice:\n\n```\ngrep -h FOO /your/path/*.bar\n\n```\n\nWhere -h is the parameter to hide the filename, as from man grep:\n\n-h, --no-filename\nSuppress  the  prefixing of file names on output.  This is the default\nwhen  there is only one file (or only standard input) to search.\n\nNote that you were using\n\n-H, --with-filename\nPrint the file name for each match.  This is the default when there is\nmore than one file to search."}
{"instruction": "Regex (grep) for multi-line search needed", "output": "Without the need to install the grep variant pcregrep, you can do a multiline search with grep.\n\n```\n$ grep -Pzo \"(?s)^(\\s*)\\N*main.*?{.*?^\\1}\" *.c\n\n```\n\nExplanation:\n-P activate perl-regexp for grep (a powerful extension of regular expressions)\n-z Treat the input as a set of lines, each terminated by a zero byte (the ASCII NUL character) instead of a newline. That is, grep knows where the ends of the lines are, but sees the input as one big line.  Beware this also adds a trailing NUL char if used with -o, see comments.\n-o print only matching. Because we're using -z, the whole file is like a single big line, so if there is a match, the entire file would be printed; this way it won't do that.\nIn regexp:\n(?s) activate PCRE_DOTALL, which means that . finds any character or newline\n\\N find anything except newline, even with PCRE_DOTALL activated\n.*? find . in non-greedy mode, that is, stops as soon as possible.\n^ find start of line\n\\1 backreference to the first group (\\s*). This is a try to find the same indentation of method.\nAs you can imagine, this search prints the main method in a C (*.c) source file."}
{"instruction": "How to find patterns across multiple lines using grep?", "output": "Grep is an awkward tool for this operation.\npcregrep which is found in most of the modern Linux systems can be used as\n\n```\npcregrep -M 'abc.*(\\n|.)*efg' test.txt\n\n```\n\nwhere -M, --multiline  allow patterns to match more than one line\nThere is a newer pcre2grep also. Both are provided by the PCRE project.\npcre2grep is available for Mac OS X via Mac Ports as part of port pcre2:\n\n```\n% sudo port install pcre2 \n\n```\n\nand via Homebrew as:\n\n```\n% brew install pcre\n\n```\n\nor for pcre2\n\n```\n% brew install pcre2\n\n```\n\npcre2grep is also available on Linux (Ubuntu 18.04+)\n\n```\n$ sudo apt install pcre2-utils # PCRE2\n$ sudo apt install pcregrep    # Older PCRE\n\n```"}
{"instruction": "How to concatenate multiple lines of output to one line?", "output": "Use tr '\\n' ' ' to translate all newline characters to spaces:\n\n```\n$ grep pattern file | tr '\\n' ' '\n\n```\n\nNote: grep reads files, cat concatenates files. Don't cat file | grep!\nEdit:\ntr can only handle single character translations. You could use awk to change the output record separator like:\n\n```\n$ grep pattern file | awk '{print}' ORS='\" '\n\n```\n\nThis would transform:\n\n```\none\ntwo \nthree\n\n```\n\nto:\n\n```\none\" two\" three\" \n\n```"}
{"instruction": "What are good grep tools for Windows?", "output": "Based on recommendations in the comments, I've started using grepWin and it's fantastic and free.\n\n(I'm still a fan of PowerGREP, but I don't use it anymore.)\nI know you already mentioned it, but PowerGREP is awesome.\nSome of my favorite features are:\n\nRight-click on a folder to run PowerGREP on it\nUse regular expressions or literal text\nSpecify wildcards for files to include & exclude\nSearch & replace\nPreview mode is nice because you can make sure you're replacing what you intend to.\n\nNow I realize that the other grep tools can do all of the above. It's just that PowerGREP packages all of the functionality into a very easy-to-use GUI.\nFrom the same wonderful folks who brought you RegexBuddy and who I have no affiliation with beyond loving their stuff. (It should be noted that RegexBuddy includes a basic version of grep (for Windows) itself and it costs a lot less than PowerGREP.)\n\nAdditional solutions\nExisting Windows commands\n\nFINDSTR\nSelect-String in PowerShell\n\nLinux command implementations on Windows\n\nCygwin\nCash\n\nGrep tools with a graphical interface\n\nAstroGrep\nBareGrep\nGrepWin\n\nAdditional Grep tools\n\ndnGrep"}
{"instruction": "Using grep to search for a string that has a dot in it", "output": "grep uses regexes; . means \"any character\" in a regex.  If you want a literal string, use grep -F, fgrep, or escape the . to \\..\nDon't forget to wrap your string in double quotes. Or else you should use \\\\.\nSo, your command would need to be:\n\n```\ngrep -r \"0\\.49\" *\n\n```\n\nor \n\n```\ngrep -r 0\\\\.49 *\n\n```\n\nor\n\n```\ngrep -Fr 0.49 *\n\n```"}
{"instruction": "How to show only next line after the matched one?", "output": "you can try with awk:\n\n```\nawk '/blah/{getline; print}' logfile\n\n```"}
{"instruction": "Exploitable PHP functions", "output": "To build this list I used 2 sources.  A Study In Scarlet and RATS.   I have also added some of my own to the mix and people on this thread have helped out.\nEdit: After posting this list I contacted the founder of RIPS and as of now this tools searches PHP code for the use of every function in this list.\nMost of these function calls are classified as Sinks. When a tainted variable (like $_REQUEST) is passed to a sink function, then you have a vulnerability.  Programs like RATS and RIPS use grep like functionality to identify all sinks in an application.  This means that programmers should take extra care when using these functions,  but if they where all banned then you wouldn't be able to get much done.  \n\"With great power comes great responsibility.\"\n--Stan Lee \nCommand Execution\n\n```\nexec           - Returns last line of commands output\npassthru       - Passes commands output directly to the browser\nsystem         - Passes commands output directly to the browser and returns last line\nshell_exec     - Returns commands output\n`` (backticks) - Same as shell_exec()\npopen          - Opens read or write pipe to process of a command\nproc_open      - Similar to popen() but greater degree of control\npcntl_exec     - Executes a program\n\n```\n\nPHP Code Execution\nApart from eval there are other ways to execute PHP code: include/require can be used for remote code execution in the form of Local File Include and Remote File Include vulnerabilities.\n\n```\neval()\nassert()  - identical to eval()\npreg_replace('/.*/e',...) - /e does an eval() on the match\ncreate_function()\ninclude()\ninclude_once()\nrequire()\nrequire_once()\n$_GET['func_name']($_GET['argument']);\n$func = new ReflectionFunction($_GET['func_name']); $func->invoke(); or $func->invokeArgs(array());\n\n```\n\nList of functions which accept callbacks\nThese functions accept a string parameter which could be used to call a function of the attacker's choice.  Depending on the function the attacker may or may not have the ability to pass a parameter.  In that case an Information Disclosure function like phpinfo() could be used.\n\n```\nFunction                     => Position of callback arguments\n'ob_start'                   =>  0,\n'array_diff_uassoc'          => -1,\n'array_diff_ukey'            => -1,\n'array_filter'               =>  1,\n'array_intersect_uassoc'     => -1,\n'array_intersect_ukey'       => -1,\n'array_map'                  =>  0,\n'array_reduce'               =>  1,\n'array_udiff_assoc'          => -1,\n'array_udiff_uassoc'         => array(-1, -2),\n'array_udiff'                => -1,\n'array_uintersect_assoc'     => -1,\n'array_uintersect_uassoc'    => array(-1, -2),\n'array_uintersect'           => -1,\n'array_walk_recursive'       =>  1,\n'array_walk'                 =>  1,\n'assert_options'             =>  1,\n'uasort'                     =>  1,\n'uksort'                     =>  1,\n'usort'                      =>  1,\n'preg_replace_callback'      =>  1,\n'spl_autoload_register'      =>  0,\n'iterator_apply'             =>  1,\n'call_user_func'             =>  0,\n'call_user_func_array'       =>  0,\n'register_shutdown_function' =>  0,\n'register_tick_function'     =>  0,\n'set_error_handler'          =>  0,\n'set_exception_handler'      =>  0,\n'session_set_save_handler'   => array(0, 1, 2, 3, 4, 5),\n'sqlite_create_aggregate'    => array(2, 3),\n'sqlite_create_function'     =>  2,\n\n```\n\nInformation Disclosure\nMost of these function calls are not sinks.   But rather it maybe a vulnerability if any of the data returned is viewable to an attacker.  If an attacker can see phpinfo() it is definitely a vulnerability. \n\n```\nphpinfo\nposix_mkfifo\nposix_getlogin\nposix_ttyname\ngetenv\nget_current_user\nproc_get_status\nget_cfg_var\ndisk_free_space\ndisk_total_space\ndiskfreespace\ngetcwd\ngetlastmo\ngetmygid\ngetmyinode\ngetmypid\ngetmyuid\n\n```\n\nOther\n\n```\nextract - Opens the door for register_globals attacks (see study in scarlet).\nparse_str -  works like extract if only one argument is given.  \nputenv\nini_set\nmail - has CRLF injection in the 3rd parameter, opens the door for spam. \nheader - on old systems CRLF injection could be used for xss or other purposes, now it is still a problem if they do a header(\"location: ...\"); and they do not die();. The script keeps executing after a call to header(), and will still print output normally. This is nasty if you are trying to protect an administrative area. \nproc_nice\nproc_terminate\nproc_close\npfsockopen\nfsockopen\napache_child_terminate\nposix_kill\nposix_mkfifo\nposix_setpgid\nposix_setsid\nposix_setuid\n\n```\n\nFilesystem Functions\nAccording to RATS all filesystem functions in php are nasty. Some of these don't seem very useful to the attacker. Others are more useful than you might think. For instance if allow_url_fopen=On then a url can be used as a file path, so a call to copy($_GET['s'], $_GET['d']); can be used to upload a PHP script anywhere on the system. \nAlso if a site is vulnerable to a request send via GET everyone of those file system functions can be abused to channel and attack to another host through your server.\n\n```\n// open filesystem handler\nfopen\ntmpfile\nbzopen\ngzopen\nSplFileObject->__construct\n// write to filesystem (partially in combination with reading)\nchgrp\nchmod\nchown\ncopy\nfile_put_contents\nlchgrp\nlchown\nlink\nmkdir\nmove_uploaded_file\nrename\nrmdir\nsymlink\ntempnam\ntouch\nunlink\nimagepng   - 2nd parameter is a path.\nimagewbmp  - 2nd parameter is a path. \nimage2wbmp - 2nd parameter is a path. \nimagejpeg  - 2nd parameter is a path.\nimagexbm   - 2nd parameter is a path.\nimagegif   - 2nd parameter is a path.\nimagegd    - 2nd parameter is a path.\nimagegd2   - 2nd parameter is a path.\niptcembed\nftp_get\nftp_nb_get\n// read from filesystem\nfile_exists\nfile_get_contents\nfile\nfileatime\nfilectime\nfilegroup\nfileinode\nfilemtime\nfileowner\nfileperms\nfilesize\nfiletype\nglob\nis_dir\nis_executable\nis_file\nis_link\nis_readable\nis_uploaded_file\nis_writable\nis_writeable\nlinkinfo\nlstat\nparse_ini_file\npathinfo\nreadfile\nreadlink\nrealpath\nstat\ngzfile\nreadgzfile\ngetimagesize\nimagecreatefromgif\nimagecreatefromjpeg\nimagecreatefrompng\nimagecreatefromwbmp\nimagecreatefromxbm\nimagecreatefromxpm\nftp_put\nftp_nb_put\nexif_read_data\nread_exif_data\nexif_thumbnail\nexif_imagetype\nhash_file\nhash_hmac_file\nhash_update_file\nmd5_file\nsha1_file\nhighlight_file\nshow_source\nphp_strip_whitespace\nget_meta_tags\n\n```"}
{"instruction": "How to grep a string in a directory and all its subdirectories?", "output": "If your grep supports -R, do:\n\n```\ngrep -R 'string' dir/\n\n```\n\nIf not, then use find:\n\n```\nfind dir/ -type f -exec grep -H 'string' {} +\n\n```"}
{"instruction": "PowerShell equivalent to grep -f", "output": "The -Pattern parameter in Select-String supports an array of patterns. So the one you're looking for is:\n\n```\nGet-Content .\\doc.txt | Select-String -Pattern (Get-Content .\\regex.txt)\n\n```\n\nThis searches through the textfile doc.txt by using every regex(one per line) in regex.txt"}
{"instruction": "Is there a Pattern Matching Utility like GREP in Windows?", "output": "I also found one more way of utilizing GREP like functionality in Windows 7 and above without any extra application to install and on older systems you can use install Powershell.\nIn Powershell, User can use Where-Object it has quite comprehensive set of feature that provides all the functionality of GREP plus more.\nHope It helps."}
{"instruction": "grep output to show only matching file", "output": "```\ngrep -l \n\n```\n\n(That's a lowercase L)"}
{"instruction": "How to grep for case insensitive string in a file?", "output": "You can use the -i flag which makes your pattern case insensitive:\n\n```\ngrep -iF \"success...\" file1\n\n```\n\nAlso, there is no need for cat. grep takes a file with the syntax grep <pattern> <file>. I also used the -F flag to search for a fixed string to avoid escaping the ellipsis."}
{"instruction": "Find all files containing a specific text (string) on Linux?", "output": "Do the following:\n\n```\ngrep -rnw '/path/to/somewhere/' -e 'pattern'\n\n```\n\n\n-r or -R is recursive,\n-n is line number, and\n-w stands for match the whole word.\n-l (lower-case L) can be added to just give the file name of matching files.\n-e is the pattern used during the search\n\nAlong with these, --exclude, --include, --exclude-dir flags could be used for efficient searching:\n\nThis will only search through those files which have .c or .h extensions:\n\n```\ngrep --include=\\*.{c,h} -rnw '/path/to/somewhere/' -e \"pattern\"\n\n```\n\n\nThis will exclude searching all the files ending with .o extension:\n\n```\ngrep --exclude=\\*.o -rnw '/path/to/somewhere/' -e \"pattern\"\n\n```\n\n\nFor directories it's possible to exclude one or more directories using the --exclude-dir parameter. For example, this will exclude the dirs dir1/, dir2/ and all of them matching *.dst/:\n\n```\ngrep --exclude-dir={dir1,dir2,*.dst} -rnw '/path/to/search/' -e \"pattern\"\n\n```\n\n\n\nThis works very well for me, to achieve almost the same purpose like yours.\nFor more options, see man grep."}
{"instruction": "How do I exclude a directory when using `find`?", "output": "Use the -prune primary. For example, if you want to exclude ./misc:\n\n```\nfind . -path ./misc -prune -o -name '*.txt' -print\n\n```\n\nNote that the final -print is needed to avoid printing out ./misc. See cycollins's answer for the explanation why.\nTo exclude multiple directories, OR them between parentheses.\n\n```\nfind . -type d \\( -path ./dir1 -o -path ./dir2 -o -path ./dir3 \\) -prune -o -name '*.txt' -print\n\n```\n\nAnd, to exclude directories with a specific name at any level, use the -name primary instead of -path.\n\n```\nfind . -type d -name node_modules -prune -o -name '*.json' -print\n\n```"}
{"instruction": "How can I exclude all &quot;permission denied&quot; messages from &quot;find&quot;?", "output": "Note:\n\nThis answer probably goes deeper than the use case warrants, and find 2>/dev/null may be good enough in many situations. It may still be of interest for a cross-platform perspective and for its discussion of some advanced shell techniques in the interest of finding a solution that is as robust as possible, even though the cases guarded against may be largely hypothetical.\n\n\nIf your shell is bash or zsh, there's a solution that is robust while being reasonably simple, using only POSIX-compliant find features; while bash itself is not part of POSIX, most modern Unix platforms come with it, making this solution widely portable:\n\n```\nfind . > files_and_folders 2> >(grep -v 'Permission denied$' >&2)\n\n```\n\nNote:\n\nIf your system is configured to show localized error messages, prefix the find calls below with LC_ALL=C  (LC_ALL=C find ...) to ensure that English messages are reported, so that grep -v 'Permission denied$' works as intended. Invariably, however, any error messages that do get displayed will then be in English as well.\n\n>(...) is a (rarely used) output process substitution that allows redirecting output (in this case, stderr output (2>) to the stdin of the command inside >(...).\nIn addition to bash and zsh, ksh supports them as well in principle, but trying to combine them with redirection from stderr, as is done here (2> >(...)), appears to be silently ignored (in ksh 93u+).\n\ngrep -v 'Permission denied$' filters out (-v) all lines (from the find command's stderr stream) that end with the phrase Permission denied and outputs the remaining lines to stderr (>&2).\n\nNote: There's a small chance that some of grep's output may arrive after find completes, because the overall command doesn't wait for the command inside >(...) to finish. In bash, you can prevent this by appending | cat to the command.\n\n\n\n\nThis approach is:\n\nrobust: grep is only applied to error messages (and not to a combination of file paths and error messages, potentially leading to false positives), and error messages other than permission-denied ones are passed through, to stderr.\n\nside-effect free: find's exit code is preserved: the inability to access at least one of the filesystem items encountered results in exit code 1 (although that won't tell you whether errors other than permission-denied ones occurred (too)).\n\n\n\nPOSIX-compliant solutions:\nFully POSIX-compliant solutions either have limitations or require additional work.\nIf find's output is to be captured in a file anyway (or suppressed altogether), then the pipeline-based solution from Jonathan Leffler's answer is simple, robust, and POSIX-compliant:\n\n```\nfind . 2>&1 >files_and_folders | grep -v 'Permission denied$' >&2\n\n```\n\nNote that the order of the redirections matters: 2>&1 must come first.\nCapturing stdout output in a file up front allows 2>&1 to send only error messages through the pipeline, which grep can then unambiguously operate on.\nThe only downside is that the overall exit code will be the grep command's, not find's, which in this case means: if there are no errors at all or only permission-denied errors, the exit code will be 1 (signaling failure), otherwise (errors other than permission-denied ones) 0 - which is the opposite of the intent.\nThat said, find's exit code is rarely used anyway, as it often conveys little information beyond fundamental failure such as passing a non-existent path.\nHowever, the specific case of even only some of the input paths being inaccessible due to lack of permissions is reflected in find's exit code (in both GNU and BSD find): if a permissions-denied error occurs for any of the files processed, the exit code is set to 1.\nThe following variation addresses that:\n\n```\nfind . 2>&1 >files_and_folders | { grep -v 'Permission denied$' >&2; [ $? -eq 1 ]; }\n\n```\n\nNow, the exit code indicates whether any errors other than Permission denied occurred: 1 if so, 0 otherwise.\nIn other words: the exit code now reflects the true intent of the command: success (0) is reported, if no errors at all or only permission-denied errors occurred.\nThis is arguably even better than just passing find's exit code through, as in the solution at the top.\n\ngniourf_gniourf in the comments proposes a (still POSIX-compliant) generalization of this solution using sophisticated redirections, which works even with the default behavior of printing the file paths to stdout:\n\n```\n{ find . 3>&2 2>&1 1>&3 | grep -v 'Permission denied$' >&3; } 3>&2 2>&1\n\n```\n\nIn short: Custom file descriptor 3 is used to temporarily swap stdout (1) and stderr (2), so that error messages alone can be piped to grep via stdout.\nWithout these redirections, both data (file paths) and error messages would be piped to grep via stdout, and grep would then not be able to distinguish between error message Permission denied and a (hypothetical) file whose name happens to end with the phrase Permission denied.\nAs in the first solution, however, the the exit code reported will be grep's, not find's, but the same fix as above can be applied.\n\nNotes on the existing answers:\n\nThere are several points to note about Michael Brux's answer, find . ! -readable -prune -o -print:\n\nIt requires GNU find; notably, it won't work on macOS. Of course, if you only ever need the command to work with GNU find, this won't be a problem for you.\n\nSome Permission denied errors may still surface: find ! -readable -prune reports such errors for the child items of directories for which the current user does have r permission, but lacks x (executable) permission. The reason is that because the directory itself is readable, -prune is not executed, and the attempt to descend into that directory then triggers the error messages. That said, the typical case is for the r permission to be missing.\n\nNote: The following point is a matter of philosophy and/or specific use case, and you may decide it is not relevant to you and that the command fits your needs well, especially if simply printing the paths is all you do:\n\nIf you conceptualize the filtering of the permission-denied error messages a separate task that you want to be able to apply to any find command, then the opposite approach of proactively preventing permission-denied errors requires introducing \"noise\" into the find command, which also introduces complexity and logical pitfalls.\nFor instance, the most up-voted comment on Michael's answer (as of this writing) attempts to show how to extend the command by including a -name filter, as follows:\nfind . ! -readable -prune -o -name '*.txt'\nThis, however, does not work as intended, because the trailing -print action is required (an explanation can be found in this answer). Such subtleties can introduce bugs.\n\n\n\n\nThe first solution in Jonathan Leffler's answer, find . 2>/dev/null > files_and_folders, as he himself states,  blindly silences all error messages (and the workaround is cumbersome and not fully robust, as he also explains). Pragmatically speaking, however, it is the simplest solution, as you may be content to assume that any and all errors would be permission-related.\n\nmist's answer, sudo find . > files_and_folders, is concise and pragmatic, but ill-advised for anything other than merely printing filenames, for security reasons: because you're running as the root user, \"you risk having your whole system being messed up by a bug in find or a malicious version, or an incorrect invocation which writes something unexpectedly, which could not happen if you ran this with normal privileges\" (from a comment on mist's answer by tripleee).\n\nThe 2nd solution in viraptor's answer, find . 2>&1 | grep -v 'Permission denied$' > some_file runs the risk of false positives (due to sending a mix of stdout and stderr through the pipeline), and, potentially, instead of reporting non-permission-denied errors via stderr, captures them alongside the output paths in the output file."}
{"instruction": "Find a value in a list", "output": "As for your first question: \"if item is in my_list:\" is perfectly fine and should work if item equals one of the elements inside my_list. The item must exactly match an item in the list. For instance, \"abc\" and \"ABC\" do not match. Floating point values in particular may suffer from inaccuracy. For instance, 1 - 1/3 != 2/3.\nAs for your second question: There's actually several possible ways if \"finding\" things in lists.\nChecking if something is inside\nThis is the use case you describe: Checking whether something is inside a list or not. As you know, you can use the in operator for that:\n\n```\n3 in [1, 2, 3] # => True\n\n```\n\nFiltering a collection\nThat is, finding all elements in a sequence that meet a certain condition. You can use list comprehension or generator expressions for that:\n\n```\nmatches = [x for x in lst if fulfills_some_condition(x)]\nmatches = (x for x in lst if x > 6)\n\n```\n\nThe latter will return a generator which you can imagine as a sort of lazy list that will only be built as soon as you iterate through it. By the way, the first one is exactly equivalent to\n\n```\nmatches = filter(fulfills_some_condition, lst)\n\n```\n\nin Python 2. Here you can see higher-order functions at work. In Python 3, filter doesn't return a list, but a generator-like object.\nFinding the first occurrence\nIf you only want the first thing that matches a condition (but you don't know what it is yet), it's fine to use a for loop (possibly using the else clause as well, which is not really well-known). You can also use\n\n```\nnext(x for x in lst if ...)\n\n```\n\nwhich will return the first match or raise a StopIteration if none is found. Alternatively, you can use\n\n```\nnext((x for x in lst if ...), [default value])\n\n```\n\nFinding the location of an item\nFor lists, there's also the index method that can sometimes be useful if you want to know where a certain element is in the list:\n\n```\n[1,2,3].index(2) # => 1\n[1,2,3].index(4) # => ValueError\n\n```\n\nHowever, note that if you have duplicates, .index always returns the lowest index:......\n\n```\n[1,2,3,2].index(2) # => 1\n\n```\n\nIf there are duplicates and you want all the indexes then you can use enumerate() instead:\n\n```\n[i for i,x in enumerate([1,2,3,2]) if x==2] # => [1, 3]\n\n```"}
{"instruction": "How do I find files that do not contain a given string pattern?", "output": "The following command gives me all the files that do not contain the pattern foo:\n\n```\nfind .  -not  -ipath '.*svn*' -exec  grep  -H -E -o -c  \"foo\"  {} \\; | grep 0\n\n```"}
{"instruction": "Does a &quot;Find in project...&quot; feature exist in Eclipse IDE?", "output": "1. Ctrl + H \n2. Choose File Search for plain text search in workspace/selected projects \nFor specific expression searches, choose the relevant tab (such as Java Search which allows you to search for specific identifiers)\nFor whole project search:\n3. Scope (in the form section) > Enclosing project (Radio button selection)."}
{"instruction": "Find duplicate lines in a file and count how many time each line was duplicated?", "output": "Assuming there is one number per line:\n\n```\nsort <file> | uniq -c\n\n```\n\nYou can use the more verbose --count flag too with the GNU version, e.g., on Linux:\n\n```\nsort <file> | uniq --count\n\n```"}
{"instruction": "How can I get a recursive full-path listing, one line per file?", "output": "If you really want to use ls, then format its output using awk:\n\n```\nls -R /path | awk '\n/:$/&&f{s=$0;f=0}\n/:$/&&!f{sub(/:$/,\"\");s=$0;f=1;next}\nNF&&f{ print s\"/\"$0 }'\n\n```"}
{"instruction": "Find a file by name in Visual Studio Code", "output": "When you have opened a folder in a workspace you can do Ctrl+P (Cmd+P on Mac) and start typing the filename, or extension to filter the list of filenames\nif you have:\n\nplugin.ts\npage.css\nplugger.ts\n\nYou can type css and press enter and it will open the page.css. If you type .ts the list is filtered and contains two items."}
{"instruction": "Using find to locate files that match one of multiple patterns", "output": "Use -o, which means \"or\":\n\n```\nfind Documents \\( -name \"*.py\" -o -name \"*.html\" \\)\n\n```\n\n\nYou'd need to build that command line programmatically, which isn't that easy.\nAre you using bash (or Cygwin on Windows)?  If you are, you should be able to do this:\n\n```\nls **/*.py **/*.html\n\n```\n\nwhich might be easier to build programmatically."}
{"instruction": "How to use regex with find command?", "output": "```\nfind . -regextype sed -regex \".*/[a-f0-9\\-]\\{36\\}\\.jpg\"\n\n```\n\nNote that you need to specify .*/ in the beginning because find matches the whole path.\nExample:\n\n```\nsusam@nifty:~/so$ find . -name \"*.jpg\"\n./foo-111.jpg\n./test/81397018-b84a-11e0-9d2a-001b77dc0bed.jpg\n./81397018-b84a-11e0-9d2a-001b77dc0bed.jpg\nsusam@nifty:~/so$ \nsusam@nifty:~/so$ find . -regextype sed -regex \".*/[a-f0-9\\-]\\{36\\}\\.jpg\"\n./test/81397018-b84a-11e0-9d2a-001b77dc0bed.jpg\n./81397018-b84a-11e0-9d2a-001b77dc0bed.jpg\n\n```\n\nMy version of find:\n\n```\n$ find --version\nfind (GNU findutils) 4.4.2\nCopyright (C) 2007 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.\n\nWritten by Eric B. Decker, James Youngman, and Kevin Dalley.\nBuilt using GNU gnulib version e5573b1bad88bfabcda181b9e0125fb0c52b7d3b\nFeatures enabled: D_TYPE O_NOFOLLOW(enabled) LEAF_OPTIMISATION FTS() CBO(level=0) \nsusam@nifty:~/so$ \nsusam@nifty:~/so$ find . -regextype foo -regex \".*/[a-f0-9\\-]\\{36\\}\\.jpg\"\nfind: Unknown regular expression type `foo'; valid types are `findutils-default', `awk', `egrep', `ed', `emacs', `gnu-awk', `grep', `posix-awk', `posix-basic', `posix-egrep', `posix-extended', `posix-minimal-basic', `sed'.\n\n```"}
{"instruction": "Make xargs handle filenames that contain spaces", "output": "The xargs command takes white space characters (tabs, spaces, new lines) as delimiters.\nYou can narrow it down only for the new line characters ('\\n') with -d option like this:\n\n```\nls *.mp3 | xargs -d '\\n' mplayer\n\n```\n\nIt works only with GNU xargs.\nFor MacOS:\n\n```\nls *.mp3 | tr \\\\n \\\\0 | xargs -0 mplayer\n\n```\n\nThe more simplistic and practically useful approach (when don't need to process the filenames further):\n\n```\nmplayer *.mp3\n\n```"}
{"instruction": "Fast way of finding lines in one file that are not in another?", "output": "You can achieve this by controlling the formatting of the old/new/unchanged lines in GNU diff output:\n\n```\ndiff --new-line-format=\"\" --unchanged-line-format=\"\"  file1 file2\n\n```\n\nThe input files should be sorted for this to work. With bash (and zsh) you can sort in-place with process substitution <( ):\n\n```\ndiff --new-line-format=\"\" --unchanged-line-format=\"\" <(sort file1) <(sort file2)\n\n```\n\nIn the above new and unchanged lines are suppressed,  so only changed (i.e. removed lines in your case) are output. You may also use a few diff options that other solutions don't offer, such as -i to ignore case, or various whitespace options (-E, -b, -v etc) for less strict matching.\n\nExplanation\nThe options --new-line-format, --old-line-format and --unchanged-line-format let you control the way diff formats the differences, similar to  printf format specifiers. These options format new (added), old (removed) and unchanged lines respectively. Setting one to empty \"\" prevents output of that kind of line.\nIf you are familiar with unified diff format, you can partly recreate it with:\n\n```\ndiff --old-line-format=\"-%L\" --unchanged-line-format=\" %L\" \\\n     --new-line-format=\"+%L\" file1 file2\n\n```\n\nThe %L specifier is the line in question, and we prefix each with \"+\" \"-\" or \" \", like diff -u\n(note that it only outputs differences, it lacks the --- +++ and @@ lines at the top of each grouped change).\nYou can also use this to do other useful things like number each line with %dn.\n\nThe diff method (along with other suggestions comm and join) only produce the expected output with sorted input, though you can use <(sort ...) to sort in place. Here's a simple awk (nawk) script (inspired by the scripts linked-to in Konsolebox's answer) which accepts arbitrarily ordered input files, and outputs the missing lines in the order they occur in file1.\n\n```\n# output lines in file1 that are not in file2\nBEGIN { FS=\"\" }                         # preserve whitespace\n(NR==FNR) { ll1[FNR]=$0; nl1=FNR; }     # file1, index by lineno\n(NR!=FNR) { ss2[$0]++; }                # file2, index by string\nEND {\n    for (ll=1; ll<=nl1; ll++) if (!(ll1[ll] in ss2)) print ll1[ll]\n}\n\n```\n\nThis stores the entire contents of file1 line by line in a line-number indexed array ll1[], and the entire contents of file2 line by line in a line-content indexed associative array ss2[]. After both files are read, iterate over ll1 and use the in operator to determine if the line in file1 is present in file2. (This will have have different output to the diff method if there are duplicates.)\nIn the event that the files are sufficiently large that storing them both causes a memory problem, you can trade CPU for memory by storing only file1 and deleting matches along the way as file2 is read.\n\n```\nBEGIN { FS=\"\" }\n(NR==FNR) {  # file1, index by lineno and string\n  ll1[FNR]=$0; ss1[$0]=FNR; nl1=FNR;\n}\n(NR!=FNR) {  # file2\n  if ($0 in ss1) { delete ll1[ss1[$0]]; delete ss1[$0]; }\n}\nEND {\n  for (ll=1; ll<=nl1; ll++) if (ll in ll1) print ll1[ll]\n}\n\n```\n\nThe above stores the entire contents of file1 in two arrays, one indexed by line number ll1[], one indexed by line content ss1[]. Then as file2 is read, each matching line is deleted from ll1[] and ss1[]. At the end the remaining lines from file1 are output, preserving the original order.\nIn this case, with the problem as stated, you can also divide and conquer using GNU split (filtering is a GNU extension), repeated runs with chunks of file1 and reading file2 completely each time:\n\n```\nsplit -l 20000 --filter='gawk -f linesnotin.awk - file2' < file1\n\n```\n\nNote the use and placement of - meaning stdin on the gawk command line. This is provided by split from file1 in chunks of 20000 line per-invocation.\nFor users on non-GNU systems, there is almost certainly a GNU coreutils package you can obtain, including on OSX as part of the Apple Xcode tools which provides GNU diff, awk, though only a POSIX/BSD split rather than a GNU version."}
{"instruction": "How can I find elements by text content with jQuery?", "output": "You can use the :contains selector to get elements based on their content.\nDemo here\n\n\n\n```\n$('div:contains(\"test\")').css('background-color', 'red');\n```\n\n\n```\n<div>This is a test</div>\r\n<div>Another Div</div>\r\n\r\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/1.12.4/jquery.min.js\"></script>\n```"}
{"instruction": "How do I find all of the symlinks in a directory tree?", "output": "This will recursively traverse the /path/to/folder directory and list only the symbolic links:\n\n```\nls -lR /path/to/folder | grep '^l'\n\n```\n\nIf your intention is to follow the symbolic links too, you should use your find command but you should include the -L option; in fact the find man page says:\n\n```\n   -L     Follow symbolic links.  When find examines or prints information\n          about files, the information used shall be taken from the  prop‚Äê\n          erties  of  the file to which the link points, not from the link\n          itself (unless it is a broken symbolic link or find is unable to\n          examine  the file to which the link points).  Use of this option\n          implies -noleaf.  If you later use the -P option,  -noleaf  will\n          still  be  in  effect.   If -L is in effect and find discovers a\n          symbolic link to a subdirectory during its search, the subdirec‚Äê\n          tory pointed to by the symbolic link will be searched.\n\n          When the -L option is in effect, the -type predicate will always\n          match against the type of the file that a symbolic  link  points\n          to rather than the link itself (unless the symbolic link is bro‚Äê\n          ken).  Using -L causes the -lname and -ilname predicates  always\n          to return false.\n\n```\n\nThen try this:\n\n```\nfind -L /var/www/ -type l\n\n```\n\nThis will probably work: I found in the find man page this diamond: if you are using the -type option you have to change it to the -xtype option:\n\n```\n          l      symbolic link; this is never true if the -L option or the\n                 -follow option is in effect, unless the symbolic link  is\n                 broken.  If you want to search for symbolic links when -L\n                 is in effect, use -xtype.\n\n```\n\nThen:\n\n```\nfind -L /var/www/ -xtype l\n\n```"}
{"instruction": "How to only get file name with Linux &#39;find&#39;?", "output": "In GNU find you can use -printf parameter for that, e.g.:\n\n```\nfind /dir1 -type f -printf \"%f\\n\"\n\n```"}
{"instruction": "Find the files that have been changed in last 24 hours", "output": "To find all files modified in the last 24 hours (last full day) in a particular specific directory and its sub-directories:\n\n```\nfind /directory_path -mtime -1 -ls\n\n```\n\nShould be to your liking\nThe - before 1 is important - it means anything changed one day or less ago.\nA + before 1 would instead mean anything changed at least one day ago, while having nothing before the 1 would have meant it was changed exacted one day ago, no more, no less."}
{"instruction": "Error when using &#39;sed&#39; with &#39;find&#39; command on OS X: &quot;invalid command code .&quot;", "output": "If you are on a OS X, this probably has nothing to do with the sed command. On the OSX version of sed, the -i option expects an extension argument so your command is actually parsed as the extension argument and the file path is interpreted as the command code.\nTry adding the -e argument explicitly and giving '' as argument to -i:\n\n```\nfind ./ -type f -exec sed -i '' -e \"s/192.168.20.1/new.domain.com/\" {} \\;\n\n```\n\nSee this."}
{"instruction": "How to check if a value exists in a dictionary?", "output": "```\n>>> d = {'1': 'one', '3': 'three', '2': 'two', '5': 'five', '4': 'four'}\n>>> 'one' in d.values()\nTrue\n\n```\n\nOut of curiosity, some comparative timing:\n\n```\n>>> T(lambda : 'one' in d.itervalues()).repeat()\n[0.28107285499572754, 0.29107213020324707, 0.27941107749938965]\n>>> T(lambda : 'one' in d.values()).repeat()\n[0.38303399085998535, 0.37257885932922363, 0.37096405029296875]\n>>> T(lambda : 'one' in d.viewvalues()).repeat()\n[0.32004380226135254, 0.31716084480285645, 0.3171098232269287]\n>>> T(lambda : 'four' in d.itervalues()).repeat()\n[0.41178202629089355, 0.3959040641784668, 0.3970959186553955]\n>>> T(lambda : 'four' in d.values()).repeat()\n[0.4631338119506836, 0.43541407585144043, 0.4359898567199707]\n>>> T(lambda : 'four' in d.viewvalues()).repeat()\n[0.43414998054504395, 0.4213531017303467, 0.41684913635253906]\n\n```\n\nThe reason is that each of the above returns a different type of object, which may or may not be well suited for lookup operations:\n\n```\n>>> type(d.viewvalues())\n<type 'dict_values'>\n>>> type(d.values())\n<type 'list'>\n>>> type(d.itervalues())\n<type 'dictionary-valueiterator'>\n\n```"}
{"instruction": "find: missing argument to -exec", "output": "I figured it out now. When you need to run two commands in exec in a find you need to actually have two separate execs. This finally worked for me.\n\n```\nfind . -type f -name \"*.rm\" -exec ffmpeg -i {} -sameq {}.mp3 \\; -exec rm {} \\;\n\n```"}
{"instruction": "How to use &#39;find&#39; to search for files created on a specific date?", "output": "As pointed out by Max, you can't, but checking files modified or accessed is not all that hard.  I wrote a tutorial about this, as late as today. The essence of which is to use -newerXY and ! -newerXY:\nExample: To find all files modified on the 7th of June, 2007:\n\n```\n$ find . -type f -newermt 2007-06-07 ! -newermt 2007-06-08\n\n```\n\nTo find all files accessed on the 29th of september, 2008:\n\n```\n$ find . -type f -newerat 2008-09-29 ! -newerat 2008-09-30\n\n```\n\nOr, files which had their permission changed on the same day:\n\n```\n$ find . -type f -newerct 2008-09-29 ! -newerct 2008-09-30\n\n```\n\nIf you don't change permissions on the file, 'c' would normally correspond to the creation date, though."}
{"instruction": "&quot;find: paths must precede expression:&quot; How do I specify a recursive search that also finds files in the current directory?", "output": "Try putting it in quotes -- you're running into the shell's wildcard expansion, so what you're acually passing to find will look like:\n\n```\nfind . -name bobtest.c cattest.c snowtest.c\n\n```\n\n...causing the syntax error. So try this instead:\n\n```\nfind . -name '*test.c'\n\n```\n\nNote the single quotes around your file expression -- these will stop the shell (bash) expanding your wildcards."}
{"instruction": "How can I find WPF controls by name or type?", "output": "I combined the template format used by John Myczek and Tri Q's algorithm above to create a findChild Algorithm that can be used on any parent. Keep in mind that recursively searching a tree downwards could be a lengthy process. I've only spot-checked this on a WPF application, please comment on any errors you might find and I'll correct my code.\nWPF Snoop is a useful tool in looking at the visual tree - I'd strongly recommend using it while testing or using this algorithm to check your work.\nThere is a small error in Tri Q's Algorithm. After the child is found, if childrenCount is > 1 and we iterate again we can overwrite the properly found child. Therefore I added a if (foundChild != null) break; into my code to deal with this condition.\n\n```\n/// <summary>\n/// Finds a Child of a given item in the visual tree. \n/// </summary>\n/// <param name=\"parent\">A direct parent of the queried item.</param>\n/// <typeparam name=\"T\">The type of the queried item.</typeparam>\n/// <param name=\"childName\">x:Name or Name of child. </param>\n/// <returns>The first parent item that matches the submitted type parameter or null if not found</returns> \npublic static T FindChild<T>(DependencyObject parent, string childName)\n   where T : DependencyObject\n{    \n  // Confirm parent and childName are valid. \n  if (parent == null) return null;\n\n  T foundChild = null;\n\n  int childrenCount = VisualTreeHelper.GetChildrenCount(parent);\n  for (int i = 0; i < childrenCount; i++)\n  {\n    var child = VisualTreeHelper.GetChild(parent, i);\n    // If the child is not of the request child type child\n    T childType = child as T;\n    if (childType == null)\n    {\n      // recursively drill down the tree\n      foundChild = FindChild<T>(child, childName);\n      \n      // If the child is found, break so we do not overwrite the found child. \n      if (foundChild != null) break;\n    }\n    else if (!string.IsNullOrEmpty(childName))\n    {\n      var frameworkElement = child as FrameworkElement;\n      // If the child's name is set for search\n      if (frameworkElement != null && frameworkElement.Name == childName)\n      {\n        // if the child's name is of the request name\n        foundChild = (T)child;\n        break;\n      }\n    }\n    else\n    {\n      // child element found.\n      foundChild = (T)child;\n      break;\n    }\n  }\n  \n  return foundChild;\n}\n\n```\n\nCall it like this:\n\n```\nTextBox foundTextBox = \n   UIHelper.FindChild<TextBox>(Application.Current.MainWindow, \"myTextBoxName\");\n\n```\n\nNote Application.Current.MainWindow can be any parent window."}
{"instruction": "grep without showing path/file:line", "output": "No need to find. If you are just looking for a pattern within a specific directory, this should suffice:\n\n```\ngrep -h FOO /your/path/*.bar\n\n```\n\nWhere -h is the parameter to hide the filename, as from man grep:\n\n-h, --no-filename\nSuppress  the  prefixing of file names on output.  This is the default\nwhen  there is only one file (or only standard input) to search.\n\nNote that you were using\n\n-H, --with-filename\nPrint the file name for each match.  This is the default when there is\nmore than one file to search."}
{"instruction": "find without recursion", "output": "I think you'll get what you want with the -maxdepth 1 option, based on your current command structure. If not, you can try looking at the man page for find.\nRelevant entry (for convenience's sake):\n\n```\n-maxdepth levels\n          Descend at most levels (a non-negative integer) levels of direc-\n          tories below the command line arguments.   `-maxdepth  0'  means\n          only  apply the tests and actions to the command line arguments.\n\n```\n\nYour options basically are:\n\n```\n# Do NOT show hidden files (beginning with \".\", i.e., .*):\nfind DirsRoot/* -maxdepth 0 -type f\n\n```\n\nOr:\n\n```\n#  DO show hidden files:\nfind DirsRoot/ -maxdepth 1 -type f\n\n```"}
{"instruction": "find filenames NOT ending in specific extensions on Unix?", "output": "Or without ( and the need to escape it:\n\n```\nfind . -not -name \"*.exe\" -not -name \"*.dll\"\n\n```\n\nand to also exclude the listing of directories\n\n```\nfind . -not -name \"*.exe\" -not -name \"*.dll\" -not -type d\n\n```\n\nor in positive logic ;-)\n\n```\nfind . -not -name \"*.exe\" -not -name \"*.dll\" -type f\n\n```"}
{"instruction": "How to recursively find the latest modified file in a directory?", "output": "```\nfind . -type f -printf '%T@ %p\\n' \\\n| sort -n | tail -1 | cut -f2- -d\" \"\n\n```\n\nFor a huge tree, it might be hard for sort to keep everything in memory.\n%T@ gives you the modification time like a unix timestamp, sort -n sorts numerically, tail -1 takes the last line (highest timestamp), cut -f2 -d\" \" cuts away the first field (the timestamp) from the output.\nEdit: Just as -printf is probably GNU-only, ajreals usage of stat -c is too. Although it is possible to do the same on BSD, the options for formatting is different (-f \"%m %N\" it would seem)\nAnd I missed the part of plural; if you want more then the latest file, just bump up the tail argument."}
{"instruction": "How to use &#39;-prune&#39; option of &#39;find&#39; in sh?", "output": "The thing I'd found confusing about -prune is that it's an action (like -print), not a test (like -name). It alters the \"to-do\" list, but always returns true.\nThe general pattern for using -prune is this:\n\n```\nfind [path] [conditions to prune] -prune -o \\\n            [your usual conditions] [actions to perform]\n\n```\n\nYou pretty much always want the -o (logical OR) immediately after -prune, because that first part of the test (up to and including -prune) will return false for the stuff you actually want (ie: the stuff you don't want to prune out).\nHere's an example:\n\n```\nfind . -name .snapshot -prune -o -name '*.foo' -print\n\n```\n\nThis will find the \"*.foo\" files that aren't under \".snapshot\" directories. In this example, -name .snapshot makes up the [conditions to prune], and -name '*.foo' -print is [your usual conditions] and [actions to perform].\nImportant notes:\n\nIf all you want to do is print the results you might be used to leaving out the -print action. You generally don't want to do that when using -prune.\nThe default behavior of find is to \"and\" the entire expression with the -print action if there are no actions other than -prune (ironically) at the end. That means that writing this:\n\n```\n find . -name .snapshot -prune -o -name '*.foo'              # DON'T DO THIS\n\n```\n\nis equivalent to writing this:\n\n```\n find . \\( -name .snapshot -prune -o -name '*.foo' \\) -print # DON'T DO THIS\n\n```\n\nwhich means that it'll also print out the name of the directory you're pruning, which usually isn't what you want. Instead it's better to explicitly specify the -print action if that's what you want:\n\n```\n find . -name .snapshot -prune -o -name '*.foo' -print       # DO THIS\n\n```\n\n\nIf your \"usual condition\" happens to match files that also match your prune condition, those files will not be included in the output. The way to fix this is to add a -type d predicate to your prune condition.\nFor example, suppose we wanted to prune out any directory that started with .git (this is admittedly somewhat contrived -- normally you only need to remove the thing named exactly .git), but other than that wanted to see all files, including files like .gitignore. You might try this:\n\n```\nfind . -name '.git*' -prune -o -type f -print               # DON'T DO THIS\n\n```\n\nThis would not include .gitignore in the output. Here's the fixed version:\n\n```\nfind . -name '.git*' -type d -prune -o -type f -print       # DO THIS\n\n```\n\n\n\nExtra tip: if you're using the GNU version of find, the texinfo page for find has a more detailed explanation than its manpage (as is true for most GNU utilities)."}
{"instruction": "How to exclude this / current / dot folder from find &quot;type d&quot;", "output": "POSIX 7 solution:\n\n```\nfind . ! -path . -type d\n\n```\n\nFor this particular case (.), golfs better than the mindepth solution (24 vs 26 chars), although this is probably slightly harder to type because of the !.\nTo exclude other directories, this will golf less well and requires a variable for DRYness:\n\n```\nD=\"long_name\"\nfind \"$D\" ! -path \"$D\" -type d\n\n```\n\nMy decision tree between ! and -mindepth:\n\nscript? Use ! for portability.\ninteractive session on GNU?\n\n\nexclude .? Throw a coin.\nexclude long_name? Use -mindepth."}
{"instruction": "&#39;find -exec&#39; a shell function in Linux", "output": "Since only the shell knows how to run shell functions, you have to run a shell to run a function.  You also need to mark your function for export with export -f, otherwise the subshell won't inherit them:\n\n```\nexport -f dosomething\nfind . -exec bash -c 'dosomething \"$0\"' {} \\;\n\n```"}
{"instruction": "How to pipe list of files returned by find command to cat to view all the files", "output": "Piping to another process (although this won't accomplish what you said you are trying to do):\n\n```\n command1 | command2\n\n```\n\nThis will send the output of command1 as the input of command2.\n\n-exec on a find (this will do what you want to do, but it's specific to find):\n\n```\n find . -name '*.foo' -exec cat {} \\;\n\n```\n\nEverything between find and -exec are the find predicates you were already using. {} will substitute the particular file you found into the command (cat {} in this case); the \\; is to end the -exec command.\n\nSend output of one process as command line arguments to another process:\n\n```\n command2 `command1`\n\n```\n\nFor example:\n\n```\n cat `find . -name '*.foo' -print`\n\n```\n\nNote these are backquotes not regular quotes (they are under the tilde ~ on my keyboard).\nThis will send the output of command1 into command2 as command line arguments. It's called command substitution. Note that file names containing spaces (newlines, etc) will be broken into separate arguments, though."}
{"instruction": "Chmod recursively", "output": "You need read access, in addition to execute access, to list a directory. If you only have execute access, then you can find out the names of entries in the directory, but no other information (not even types, so you don't know which of the entries are subdirectories). This works for me:\n\n```\nfind . -type d -exec chmod +rx {} \\;\n\n```"}
{"instruction": "Find all files with a filename beginning with a specified string?", "output": "Use find with a wildcard:\n\n```\nfind . -name 'mystring*'\n\n```"}
{"instruction": "Using semicolon (;) vs plus (+) with exec in find", "output": "This might be best illustrated with an example. Let's say that find turns up these files:\n\n```\nfile1\nfile2\nfile3\n\n```\n\n\nUsing -exec with a semicolon (find . -exec ls '{}' \\;), will execute\n\n```\nls file1\nls file2\nls file3\n\n```\n\nBut if you use a plus sign instead (find . -exec ls '{}' \\+), as many  filenames as possible are passed as arguments to a single command:\n\n```\nls file1 file2 file3\n\n```\n\nThe number of filenames is only limited by the system's maximum command line length. If the command exceeds this length, the command will be called multiple times."}
{"instruction": "How can I get `find` to ignore .svn directories?", "output": "For searching, can I suggest you look at ack ? It's a source-code aware find, and as such will automatically ignore many file types, including source code repository info such as the above."}
{"instruction": "How to go to each directory and execute a command?", "output": "You can do the following, when your current directory is parent_directory:\n\n```\nfor d in [0-9][0-9][0-9]\ndo\n    ( cd \"$d\" && your-command-here )\ndone\n\n```\n\nThe ( and ) create a subshell, so the current directory isn't changed in the main script."}
{"instruction": "How can I check if character in a string is a letter? (Python)", "output": "You can use str.isalpha().\nFor example:\n\n```\ns = 'a123b'\n\nfor char in s:\n    print(char, char.isalpha())\n\n```\n\nOutput:\n\n```\na True\n1 False\n2 False\n3 False\nb True\n\n```"}
{"instruction": "Search for executable files using find command", "output": "On GNU versions of find you can use -executable:\n\n```\nfind . -type f -executable -print\n\n```\n\nFor BSD (and MacOS) versions of find, you can use -perm with + and an octal mask:\n\n```\nfind . -type f -perm +111 -print\n\n```\n\nIn this context \"+\" means \"any of these bits are set\" and 111 is the execute bits.\nNote that this is not identical to the -executable predicate in GNU find. In particular, -executable tests that the file can be executed by the current user, while -perm +111 just tests if any execute permissions are set.\nOlder versions of GNU find also support the -perm +111 syntax, but as of 4.5.12 this syntax is no longer supported. Instead, you can use -perm /111 to get this behavior."}
{"instruction": "How to use find command to find all files with extensions from list?", "output": "```\nfind /path/to -regex \".*\\.\\(jpg\\|gif\\|png\\|jpeg\\)\" > log\n\n```"}
{"instruction": "Find and copy files", "output": "If your intent is to copy the found files into /home/shantanu/tosend, you have the order of the arguments to cp reversed:\n\n```\nfind /home/shantanu/processed/ -name '*2011*.xml' -exec cp \"{}\" /home/shantanu/tosend  \\;\n\n```\n\nPlease note: the find command uses {} as placeholder for the matched file."}
{"instruction": "Find files containing a given text", "output": "```\negrep -ir --include=*.{php,html,js} \"(document.cookie|setcookie)\" .\n\n```\n\nThe r flag means to search recursively (search subdirectories). The i flag means case insensitive.\nIf you just want file names add the l (lowercase L) flag:\n\n```\negrep -lir --include=*.{php,html,js} \"(document.cookie|setcookie)\" .\n\n```"}
{"instruction": "Command line: piping find results to rm", "output": "You are actually piping rm's output to the input of find. What you want is to use the output of find as arguments to rm:\n\n```\nfind -type f -name '*.sql' -mtime +15 | xargs rm\n\n```\n\nxargs is the command that \"converts\" its standard input into arguments of another program, or, as they more accurately put it on the man page,\n\nbuild and execute command lines from standard input\n\nNote that if file names can contain whitespace characters, you should correct for that:\n\n```\nfind -type f -name '*.sql' -mtime +15 -print0 | xargs -0 rm\n\n```\n\nBut actually, find has a shortcut for this: the -delete option:\n\n```\nfind -type f -name '*.sql' -mtime +15 -delete\n\n```\n\nPlease be aware of the following warnings in man find:\n\n\n```\n  Warnings:  Don't  forget that the find command line is evaluated\n  as an expression, so putting -delete first will make find try to\n  delete everything below the starting points you specified.  When\n  testing a find command line that you later intend  to  use  with\n  -delete,  you should explicitly specify -depth in order to avoid\n  later surprises.  Because -delete  implies  -depth,  you  cannot\n  usefully use -prune and -delete together.\n\n```\n\nP.S. Note that piping directly to rm isn't an option, because rm doesn't expect filenames on standard input. What you are currently doing is piping them backwards."}
{"instruction": "Check if one list contains element from the other", "output": "If you just need to test basic equality, this can be done with the basic JDK without modifying the input lists in the one line\n\n```\n!Collections.disjoint(list1, list2);\n\n```\n\nIf you need to test a specific property, that's harder.  I would recommend, by default,\n\n```\nlist1.stream()\n   .map(Object1::getProperty)\n   .anyMatch(\n     list2.stream()\n       .map(Object2::getProperty)\n       .collect(toSet())\n       ::contains)\n\n```\n\n...which collects the distinct values in list2 and tests each value in list1 for presence."}
{"instruction": "How can I store the &quot;find&quot; command results as an array in Bash", "output": "Update 2020 for Linux Users:\nIf you have an up-to-date version of bash (4.4-alpha or better), as you probably do if you are on Linux, then you should be using Benjamin W.'s answer.\nIf you are on Mac OS, which ‚Äîlast I checked‚Äî still used bash 3.2, or are otherwise using an older bash, then continue on to the next section.\nAnswer for bash 4.3 or earlier\nHere is one solution for getting the output of find into a bash array:\n\n```\narray=()\nwhile IFS=  read -r -d $'\\0'; do\n    array+=(\"$REPLY\")\ndone < <(find . -name \"${input}\" -print0)\n\n```\n\nThis is tricky because, in general, file names can have spaces, new lines, and other script-hostile characters.  The only way to use find and have the file names safely separated from each other is to use -print0 which prints the file names separated with a null character.  This would not be much of an inconvenience if bash's readarray/mapfile functions supported null-separated strings but they don't.  Bash's read does and that leads us to the loop above.\n[This answer was originally written in 2014.  If you have a recent version of bash, please see the update below.]\nHow it works\n\nThe first line creates an empty array: array=()\nEvery time that the read statement is executed, a null-separated file name is read from standard input.  The -r option tells read to leave backslash characters alone.  The -d $'\\0' tells read that the input will be null-separated.  Since we omit the name to read, the shell puts the input into the default name: REPLY.\nThe array+=(\"$REPLY\") statement appends the new file name to the array array.\nThe final line combines redirection and command substitution to provide the output of find to the standard input of the while loop. \n\nWhy use process substitution?\nIf we didn't use process substitution, the loop could be written as:\n\n```\narray=()\nfind . -name \"${input}\" -print0 >tmpfile\nwhile IFS=  read -r -d $'\\0'; do\n    array+=(\"$REPLY\")\ndone <tmpfile\nrm -f tmpfile\n\n```\n\nIn the above the output of find is stored in a temporary file and that file is used as standard input to the while loop.  The idea of process substitution is to make such temporary files unnecessary.  So, instead of having the while loop get its stdin from tmpfile, we can have it get its stdin from <(find . -name ${input} -print0).\nProcess substitution is widely useful.  In many places where a command wants to read from a file, you can specify process substitution, <(...), instead of a file name.  There is an analogous form, >(...), that can be used in place of a file name where the command wants to write to the file.\nLike arrays, process substitution is a feature of bash and other advanced shells.  It is not part of the POSIX standard.\nAlternative: lastpipe\nIf desired, lastpipe can be used instead of process substitution (hat tip: Caesar):\n\n```\nset +m\nshopt -s lastpipe\narray=()\nfind . -name \"${input}\" -print0 | while IFS=  read -r -d $'\\0'; do array+=(\"$REPLY\"); done; declare -p array\n\n```\n\nshopt -s lastpipe tells bash to run the last command in the pipeline in the current shell (not the background).  This way, the array remains in existence after the pipeline completes.  Because lastpipe only takes effect if job control is turned off, we run set +m.  (In a script, as opposed to the command line, job control is off by default.)\nAdditional notes\nThe following command creates a shell variable, not a shell array:\n\n```\narray=`find . -name \"${input}\"`\n\n```\n\nIf you wanted to create an array, you would need to put parens around the output of find.  So, naively, one could:\n\n```\narray=(`find . -name \"${input}\"`)  # don't do this\n\n```\n\nThe problem is that the shell performs word splitting on the results of find so that the elements of the array are not guaranteed to be what you want.\nUpdate 2019\nStarting with version 4.4-alpha, bash now supports a -d option so that the above loop is no longer necessary.  Instead, one can use:\n\n```\nmapfile -d $'\\0' array < <(find . -name \"${input}\" -print0)\n\n```\n\nFor more information on this, please see (and upvote) Benjamin W.'s answer."}
{"instruction": "How do I get the find command to print out the file size with the file name?", "output": "```\nfind . -name '*.ear' -print0 | xargs -0 ls -lhS\n\n```\n\njust the h extra from jer.drab.org's reply. saves time converting to MB mentally ;)"}
{"instruction": "How to strip leading &quot;./&quot; in unix &quot;find&quot;?", "output": "If they're only in the current directory\nfind * -type f -print\nIs that what you want?"}
{"instruction": "How can I search for a multiline pattern in a file?", "output": "So I discovered pcregrep which stands for Perl Compatible Regular Expressions GREP.\n\nthe -M option makes it possible to search for patterns that span line boundaries.\n\nFor example, you need to find files where the '_name' variable is followed on the next line by the '_description' variable:\n\n```\nfind . -iname '*.py' | xargs pcregrep -M '_name.*\\n.*_description'\n\n```\n\nTip: you need to include the line break character in your pattern. Depending on your platform, it could be '\\n', \\r', '\\r\\n', ..."}
{"instruction": "Ruby Array find_first object?", "output": "Either I don't understand your question, or Enumerable#find is the thing you were looking for."}
{"instruction": "Delete files older than 10 days using shell script in Unix", "output": "find is the common tool for this kind of task :\n\n```\nfind ./my_dir -mtime +10 -type f -delete\n\n```\n\nEXPLANATIONS\n\n./my_dir your directory (replace with your own)\n-mtime +10 older than 10 days\n-type f only files\n-delete no surprise. Remove it to test your find filter before executing the whole command\n\nAnd take care that ./my_dir exists to avoid bad surprises !"}
{"instruction": "Find all elements on a page whose element ID contains a certain text using jQuery", "output": "```\n$('*[id*=mytext]:visible').each(function() {\n    $(this).doStuff();\n});\n\n```\n\nNote the asterisk '*' at the beginning of the selector matches all elements.\nSee the Attribute Contains Selectors, as well as the :visible and :hidden selectors."}
{"instruction": "How do I copy a folder from remote to local using scp?", "output": "```\nscp -r user@your.server.example.com:/path/to/foo /home/user/Desktop/\n\n```\n\nBy not including the trailing '/' at the end of foo, you will copy the directory itself (including contents), rather than only the contents of the directory.\nFrom man scp (See online manual)\n\n-r Recursively copy entire directories"}
{"instruction": "ssh &quot;permissions are too open&quot;", "output": "The keys need to be read-writable only by you:\n\n```\nchmod 600 ~/.ssh/id_rsa\n\n```\n\nAlternatively, the keys can be only readable by you (this also blocks your write access):\n\n```\nchmod 400 ~/.ssh/id_rsa\n\n```\n\n600 appears to be better in most cases, because you don't need to change file permissions later to edit it. (See the comments for more nuances)\nThe relevant portion from the manpage (man ssh)\n\n\n```\n ~/.ssh/id_rsa\n         Contains the private key for authentication.  These files contain sensitive \n         data and should be readable by the user but not\n         accessible by others (read/write/execute).  ssh will simply ignore a private \n         key file if it is              \n         accessible by others.  It is possible to specify a\n         passphrase when generating the key which will be used to encrypt the sensitive \n         part of this file using 3DES.\n\n ~/.ssh/identity.pub\n ~/.ssh/id_dsa.pub\n ~/.ssh/id_ecdsa.pub\n ~/.ssh/id_rsa.pub\n         Contains the public key for authentication.  These files are not sensitive and \n         can (but need not) be readable by anyone.\n\n```"}
{"instruction": "How to specify the private SSH-key to use when executing shell command on Git?", "output": "Something like this should work (suggested by orip):\n\n```\nssh-agent bash -c 'ssh-add /somewhere/yourkey; git clone git@github.com:user/project.git'\n\n```\n\nif you prefer subshells, you could try the following (though it is more fragile):\n\n```\nssh-agent $(ssh-add /somewhere/yourkey; git clone git@github.com:user/project.git)\n\n```\n\nGit will invoke SSH which will find its agent by environment variable; this will, in turn, have the key loaded.\nAlternatively, setting HOME may also do the trick, provided you are willing to setup a directory that contains only a .ssh directory as HOME; this may either contain an identity.pub, or a config file setting IdentityFile."}
{"instruction": "How do I remove the passphrase for the SSH key without having to create a new key?", "output": "Short answer:\n\n```\n$ ssh-keygen -p\n\n```\n\nThis will then prompt you to enter the keyfile location, the old passphrase, and the new passphrase (which can be left blank to have no passphrase).\n\nIf you would like to do it all on one line without prompts do:\n\n```\n$ ssh-keygen -p [-P old_passphrase] [-N new_passphrase] [-f keyfile]\n\n```\n\nImportant: Beware that when executing commands they will typically be logged in your ~/.bash_history file (or similar) in plain text including all arguments provided (i.e. the passphrases in this case). It is, therefore, is recommended that you use the first option unless you have a specific reason to do otherwise.   \nNotice though that you can still use -f keyfile without having to specify -P nor -N, and that the keyfile defaults to ~/.ssh/id_rsa, so in many cases, it's not even needed.\nYou might want to consider using ssh-agent, which can cache the passphrase for a time. The latest versions of gpg-agent also support the protocol that is used by ssh-agent."}
{"instruction": "How to Configure Multiple SSH Private Keys for Different Servers Efficiently?", "output": "From my .ssh/config:\n\n```\nHost myshortname realname.example.com\n    HostName realname.example.com\n    IdentityFile ~/.ssh/realname_rsa # private key for realname\n    User remoteusername\n\nHost myother realname2.example.org\n    HostName realname2.example.org\n    IdentityFile ~/.ssh/realname2_rsa  # different private key for realname2\n    User remoteusername\n\n```\n\nThen you can use the following to connect:\nssh myshortname\nssh myother\nAnd so on."}
{"instruction": "How to solve Permission denied (publickey) error when using Git?", "output": "If the user has not generated a ssh public/private key pair set before\nThis info is working on theChaw but can be applied to all other git repositories which support SSH pubkey authentications. (See [gitolite][1], gitlab or github for example.)\n\nFirst start by setting up your own public/private key pair set. This\ncan use either DSA or RSA, so basically any key you setup will work.\nOn most systems you can use ssh-keygen.\n\n\n\nFirst you'll want to cd into your .ssh directory. Open up the terminal and run:\n\n\ncd ~/.ssh && ssh-keygen\n\n\nNext you need to copy this to your clipboard.\nOn OS X run: cat id_rsa.pub | pbcopy\nOn Linux run: cat id_rsa.pub | xclip\nOn Windows (via Cygwin/Git Bash) run: cat id_rsa.pub | clip\nOn Windows (Powershell) run: Get-Content id_rsa.pub | Set-Clipboard (Thx to @orion elenzil)\nAdd your key to your account via the website.\nFinally setup your .gitconfig.\ngit config --global user.name \"bob\"\ngit config --global user.email bob@...\n(don't forget to restart your command line to make sure the config is reloaded)\n\nThat's it you should be good to clone and checkout.\n\nFurther information can be found at https://help.github.com/articles/generating-ssh-keys (thanks to @Lee Whitney)\n[1]: https://github.com/sitaramc/gitolite\n-\nIf the user has generated a ssh public/private key pair set before\n\ncheck which key have been authorized on your github or gitlab account settings\ndetermine which corresponding private key must be associated from your local computer\n\neval $(ssh-agent -s)\n\ndefine where the keys are located\n\nssh-add ~/.ssh/id_rsa"}
{"instruction": "Calculate RSA key fingerprint", "output": "Run the following command to retrieve the SHA256 fingerprint of your SSH key (-l means \"list\" instead of create a new key, -f means \"filename\"):\n\n```\n$ ssh-keygen -lf /path/to/ssh/key\n\n```\n\nSo for example, on my machine the command I ran was (using RSA public key):\n\n```\n$ ssh-keygen -lf ~/.ssh/id_rsa.pub\n2048 00:11:22:33:44:55:66:77:88:99:aa:bb:cc:dd:ee:ff /Users/username/.ssh/id_rsa.pub (RSA)\n\n```\n\nTo get the GitHub (MD5) fingerprint format with newer versions of ssh-keygen, run:\n\n```\n$ ssh-keygen -E md5 -lf <fileName>\n\n```\n\nBonus information:\nssh-keygen -lf also works on known_hosts and authorized_keys files.\nTo find most public keys on Linux/Unix/OS¬†X systems, run\n\n```\n$ find /etc/ssh /home/*/.ssh /Users/*/.ssh -name '*.pub' -o -name 'authorized_keys' -o -name 'known_hosts'\n\n```\n\n(If you want to see inside other users' homedirs, you'll have to be root or sudo.)\nThe ssh-add -l is very similar, but lists the fingerprints of keys added to your agent. (OS¬†X users take note that magic passwordless SSH via Keychain is not the same as using ssh-agent.)"}
{"instruction": "&quot;UNPROTECTED PRIVATE KEY FILE!&quot; Error using SSH into Amazon EC2 Instance (AWS)", "output": "The problem is wrong set of permissions on the file.\nEasily solved by executing -\nchmod 400 mykey.pem\nTaken from AWS instructions -\n\nYour key file must not be publicly viewable for SSH to work. Use this\ncommand if needed:  chmod 400 mykey.pem\n\n400 protects it by making it read only and only for the owner."}
{"instruction": "How do I access my SSH public key?", "output": "cat ~/.ssh/id_rsa.pub or cat ~/.ssh/id_dsa.pub or cat ~/.ssh/id_ed25519.pub\nYou can list all the public keys you have by doing:\n$ ls ~/.ssh/*.pub"}
{"instruction": "SSH Key - Still asking for password and passphrase", "output": "If you work with HTTPs urls, it'll always ask for your username / password. This could be solved using @Manavalan Gajapathy's comment (copying here):\nSee this github doc to convert remote's URL from https to ssh. To check if remote's URL is ssh or https, use git remote -v. To switch from https to ssh:\n\n```\ngit remote set-url origin git@github.com:USERNAME/REPOSITORY.git\n\n```\n\nIf you're correctly using SSH when cloning / setting remotes: make sure you have a ssh-agent to remember your password (see this answer by @Komu). That way, you'll only enter your passphrase once by terminal session.\nIf it is still too annoying, then simply set a ssh-key without passphrase."}
{"instruction": "How to download a file from server using SSH?", "output": "In your terminal, type:\n\n```\nscp your_username@remotehost.edu:foobar.txt /local/dir\n\n```\n\nreplacing the username, host, remote filename, and local directory as appropriate.\nIf you want to access EC2 (or other service that requires authenticating with a private key), use the -i option:\n\n```\nscp -i key_file.pem your_username@remotehost.edu:/remote/dir/foobar.txt /local/dir\n\n```\n\nFrom: http://www.hypexr.org/linux_scp_help.php"}
{"instruction": "SSH Key: ‚ÄúPermissions 0644 for &#39;id_rsa.pub&#39; are too open.‚Äù on mac", "output": "```\ndebug1: identity file /Users/tudouya/.ssh/vm/vm_id_rsa.pub type 1\n\n```\n\nIt appears that you're trying to use the wrong key file. The file with the \".pub\" extension contains the public portion of the key. The corresponding file without the \".pub\" extension contains the private part of the key. When you run an ssh client to connect to a remote server, you have to provide the private key file to the ssh client.\nYou probably have a line in the your .ssh/config file (or /etc/ssh_config) which looks like this:\n\n```\nIdentityFile .../.ssh/vm/vm_id_rsa.pub\n\n```\n\nYou need to remove the \".pub\" extension from the filename:\n\n```\nIdentityFile .../.ssh/vm/vm_id_rsa\n\n```"}
{"instruction": "How to permanently add a private key with ssh-add on Ubuntu?", "output": "A solution would be to force the key files to be kept permanently, by adding them in your ~/.ssh/config file:\n\n```\nHost *\n    IdentityFile ~/.ssh/gitHubKey\n    IdentityFile ~/.ssh/id_rsa_buhlServer\n\n```\n\nIf you do not have a 'config' file in the ~/.ssh directory, then you should create one. It does not need root rights, so simply:\n\n```\nnano ~/.ssh/config\n\n```\n\n...and enter the lines above as per your requirements.\nFor this to work the file needs to have chmod 600. You can use the command chmod 600 ~/.ssh/config.\nIf you want all users on the computer to use the key put these lines into /etc/ssh/ssh_config and the key in a folder accessible to all.\nAdditionally if you want to set the key specific to one host, you can do the following in your ~/.ssh/config :\n\n```\nHost github.com\n    User git\n    IdentityFile ~/.ssh/githubKey\n\n```\n\nThis has the advantage when you have many identities that a server doesn't reject you because you tried the wrong identities first. Only the specific identity will be tried."}
{"instruction": "Why are connections to GitHub over SSH throwing an error &quot;Warning: Remote Host Identification Has Changed&quot;?", "output": "This happened because on the 24th of March 2023, GitHub updated their RSA SSH host key used to secure Git operations for GitHub.com because the private key was briefly exposed in a public GitHub repository.  You will get that message if you had remembered GitHub‚Äôs previous key fingerprint in your SSH client before that date.\nAs per per the linked blog post, the solution is to remove the old key by running this command:\n\n```\n$ ssh-keygen -R github.com\n\n```\n\nNow the next git connection (pull, push or clone) should ask if you trust the new SSH key.  Before entering yes, ensure the shown new key is valid, using the list:\nhttps://docs.github.com/en/authentication/keeping-your-account-and-data-secure/githubs-ssh-key-fingerprints\nRefer to the blog post for other ways to fix the issue."}
{"instruction": "How to SSH to a VirtualBox guest externally through a host?", "output": "The best way to login to a guest Linux VirtualBox VM is port forwarding. By default, you should have one interface already which is using NAT. Then go to the Network settings and click the Port Forwarding button.  Add a new Rule. As the rule name, insert \"ssh\". As \"Host port\", insert 3022. As \"Guest port\", insert 22. Everything else of the rule can be left blank.\nor from the command line\n\n```\nVBoxManage modifyvm myserver --natpf1 \"ssh,tcp,,3022,,22\"\n\n```\n\nwhere 'myserver' is the name of the created VM. Check the added rules:\n\n```\nVBoxManage showvminfo myserver | grep 'Rule'\n\n```\n\nThat's all! Please be sure you don't forget to install an SSH server in the VM:\n\n```\nsudo apt-get install openssh-server\n\n```\n\nTo SSH into the guest VM, write:\n\n```\nssh -p 3022 user@127.0.0.1\n\n```\n\nWhere user is your username within the VM."}
{"instruction": "Git error: &quot;Host Key Verification Failed&quot; when connecting to remote repository", "output": "You are connecting via the SSH protocol, as indicated by the ssh:// prefix on your clone URL. Using SSH, every host has a key. Clients remember the host key associated with a particular address and refuse to connect if a host key appears to change. This prevents man in the middle attacks.\nThe host key for domain.example has changed. If this does not seem fishy to you, remove the old key from your local cache by editing ${HOME}/.ssh/known_hosts to remove the line for domain.example or letting an SSH utility do it for you with\n\n```\nssh-keygen -R domain.example\n\n```\n\nFrom here, record the updated key either by doing it yourself with\n\n```\nssh-keyscan -t rsa domain.example >> ~/.ssh/known_hosts\n\n```\n\nor, equivalently, let ssh do it for you next time you connect with git fetch, git pull, or git push (or even a plain ol‚Äô ssh domain.example) by answering yes when prompted\n\n```\nThe authenticity of host 'domain.example (a.b.c.d)' can't be established.\nRSA key fingerprint is XX:XX:...:XX.\nAre you sure you want to continue connecting (yes/no)?\n```\n\nThe reason for this prompt is domain.example is no longer in your known_hosts after deleting it and presumably not in the system‚Äôs /etc/ssh/ssh_known_hosts, so ssh has no way to know whether the host on the other end of the connection is really domain.example. (If the wrong key is in /etc, someone with administrative privileges will have to update the system-wide file.)\nI strongly encourage you to consider having users authenticate with keys as well. That way, ssh-agent can store key material for convenience (rather than everyone having to enter her password for each connection to the server), and passwords do not go over the network."}
{"instruction": "Is it possible to specify a different ssh port when using rsync?", "output": "Another option, in the host you run rsync from, set the port in the ssh config file, ie:\n\n```\ncat ~/.ssh/config\nHost host\n    Port 2222\n\n```\n\nThen rsync over ssh will talk to port 2222:\n\n```\nrsync -rvz --progress --remove-sent-files ./dir user@host:/path\n\n```"}
{"instruction": "Specify an SSH key for git push for a given domain", "output": "Even if the user and host are the same, they can still be distinguished in ~/.ssh/config.  For example, if your configuration looks like this:\n\n```\nHost gitolite-as-alice\n  HostName git.company.com\n  User git\n  IdentityFile /home/whoever/.ssh/id_rsa.alice\n  IdentitiesOnly yes\n\nHost gitolite-as-bob\n  HostName git.company.com\n  User git\n  IdentityFile /home/whoever/.ssh/id_dsa.bob\n  IdentitiesOnly yes\n\n```\n\nThen you just use gitolite-as-alice and gitolite-as-bob instead of the hostname in your URL:\n\n```\ngit remote add alice git@gitolite-as-alice:whatever.git\ngit remote add bob git@gitolite-as-bob:whatever.git\n\n```\n\nNote\nYou want to include the option IdentitiesOnly yes to prevent the use of default ids. Otherwise, if you also have id files matching the default names, they will get tried first because unlike other config options (which abide by \"first in wins\") the IdentityFile option appends to the list of identities to try. See: https://serverfault.com/questions/450796/how-could-i-stop-ssh-offering-a-wrong-key/450807#450807"}
{"instruction": "Adding a public key to ~/.ssh/authorized_keys does not log me in automatically", "output": "You need to verify the permissions of the authorized_keys file and the folder / parent folders in which it is located.\n\n```\nchmod 700 ~/.ssh\nchmod 600 ~/.ssh/authorized_keys\n\n```\n\nFor more information see this page.\nYou may also need to change/verify the permissions of your home directory to remove write access for the group and others.\n\n```\nchmod go-w ~\n\n```"}
{"instruction": "Pseudo-terminal will not be allocated because stdin is not a terminal", "output": "Try ssh -t -t(or ssh -tt for short) to force pseudo-tty allocation even if stdin isn't a terminal.\nSee also: Terminating SSH session executed by bash script\nFrom ssh manpage:\n\n```\n-T      Disable pseudo-tty allocation.\n\n-t      Force pseudo-tty allocation.  This can be used to execute arbitrary \n        screen-based programs on a remote machine, which can be very useful,\n        e.g. when implementing menu services.  Multiple -t options force tty\n        allocation, even if ssh has no local tty.\n\n```"}
{"instruction": "Starting ssh-agent on Windows 10 fails: &quot;unable to start ssh-agent service, error :1058&quot;", "output": "I solved the problem by changing the StartupType of the ssh-agent to Manual via Set-Service ssh-agent -StartupType Manual.\nThen I was able to start the service via Start-Service ssh-agent or just ssh-agent.exe."}
{"instruction": "scp (secure copy) to ec2 instance without password", "output": "I figured it out.  I had the arguments in the wrong order.  This works:\n\n```\nscp -i mykey.pem somefile.txt root@my.ec2.id.amazonaws.com:/\n\n```"}
{"instruction": "Keep SSH session alive", "output": "The ssh daemon (sshd), which runs server-side, closes the connection from the server-side if the client goes silent (i.e., does not send information). To prevent connection loss, instruct the ssh client to send a sign-of-life signal to the server once in a while.\nThe configuration for this is in the file $HOME/.ssh/config, create the file if it does not exist (the config file must not be world-readable, so run chmod 600 ~/.ssh/config after creating the file). To send the signal every e.g. four minutes (240 seconds) to the remote host, put the following in that configuration file:\n\n```\nHost remotehost\n    HostName remotehost.com\n    ServerAliveInterval 240\n\n```\n\nTo enable sending a keep-alive signal for all hosts, place the following contents in the configuration file:\n\n```\nHost *\n    ServerAliveInterval 240\n\n```"}
{"instruction": ".bashrc at ssh login", "output": ".bashrc is not sourced when you log in using SSH. You need to source it in your .bash_profile like this:\n\n```\nif [ -f ~/.bashrc ]; then\n  . ~/.bashrc\nfi\n\n```"}
{"instruction": "What is the cleanest way to ssh and run multiple commands in Bash?", "output": "How about a Bash Here Document:\n\n```\nssh otherhost << EOF\n  ls some_folder; \n  ./someaction.sh 'some params'\n  pwd\n  ./some_other_action 'other params'\nEOF\n\n```\n\nTo avoid the problems mentioned by @Globalz in the comments, you may be able to (depending what you're doing on the remote site) get away with replacing the first line with\n\n```\nssh otherhost /bin/bash << EOF\n\n```\n\nNote that you can do variable substitution in the Here document, but you may have to deal with quoting issues. For instance, if you quote the \"limit string\" (ie. EOF in the above), then you can't do variable substitutions. But without quoting the limit string, variables are substituted. For example, if you have defined $NAME above in your shell script, you could do\n\n```\nssh otherhost /bin/bash << EOF\ntouch \"/tmp/${NAME}\"\nEOF\n\n```\n\nand it would create a file on the destination otherhost with the name of whatever you'd assigned to $NAME. Other rules about shell script quoting also apply, but are too complicated to go into here."}
{"instruction": "git clone with HTTPS or SSH remote?", "output": "GitHub have changed their recommendation several times (example).\nIt appears that they currently recommend HTTPS because it is the easiest to set up on the widest range of networks and platforms, and by users who are new to all this.\nThere is no inherent flaw in SSH (if there was they would disable it) -- in the links below, you will see that they still provide details about SSH connections too:\n\nHTTPS is less likely to be blocked by a firewall.\nhttps://docs.github.com/en/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls\n\nThe https:// clone URLs are available on all repositories, regardless of visibility. https:// clone URLs work even if you are behind a firewall or proxy.\n\n\nAn HTTPS connection allows credential.helper to cache your password.\nhttps://docs.github.com/en/get-started/quickstart/set-up-git#connecting-over-https-recommended\n\nIf you clone with HTTPS, you can cache your GitHub credentials in Git using a credential helper. For more information, see \"Cloning with HTTPS urls\" and \"Caching your GitHub credentials in Git.\""}
{"instruction": "How do I verify/check/test/validate my SSH passphrase?", "output": "You can verify your SSH key passphrase by attempting to load it into your SSH agent. With OpenSSH this is done via ssh-add.\nOnce you're done, remember to unload your SSH passphrase from the terminal by running ssh-add -d."}
{"instruction": "Copying files using rsync from remote server to local machine", "output": "From your local machine:\n\n```\nrsync -chavzP --stats user@remote.host:/path/to/copy /path/to/local/storage\n\n```\n\nFrom your local machine with a non standard ssh port:\n\n```\nrsync -chavzP -e \"ssh -p $portNumber\" user@remote.host:/path/to/copy /local/path\n\n```\n\nOr from the remote host, assuming you really want to work this way and your local machine is listening on SSH:\n\n```\nrsync -chavzP --stats /path/to/copy user@host.remoted.from:/path/to/local/storage\n\n```\n\nSee man rsync for an explanation of my usual switches."}
{"instruction": "How to fix &quot;ssh: connect to host github.com port 22: Connection timed out&quot; for git push/pull/... commands?", "output": "The reason could be the firewall modification since you are on a network, in which case they may deliberately block some ports.\nTo double check if this is the reason, run:\n\n```\nssh -T git@github.com\n\n```\n\nThis should timeout.  If that's the case, then use the http protocol instead of ssh.  Change your url in the config file to http:\n\n```\ngit config --local -e\n\n```\n\nand change:\n\n```\n url = git@github.com:username/repo.git\n\n```\n\nto\n\n```\nurl = https://github.com/username/repo.git\n\n```"}
{"instruction": "Start ssh-agent on login", "output": "Please go through this article. You may find this very useful:\nhttps://web.archive.org/web/20210506080335/https://mah.everybody.org/docs/ssh\nJust in case the above link vanishes some day, I am capturing the main piece of the solution below:\n\nThis solution from Joseph M. Reagle by way of Daniel Starin:\nAdd this following to your .bash_profile\n\n```\nSSH_ENV=\"$HOME/.ssh/agent-environment\"\n\nfunction start_agent {\n    echo \"Initialising new SSH agent...\"\n    /usr/bin/ssh-agent | sed 's/^echo/#echo/' >\"$SSH_ENV\"\n    echo succeeded\n    chmod 600 \"$SSH_ENV\"\n    . \"$SSH_ENV\" >/dev/null\n    /usr/bin/ssh-add; \n}\n\n# Source SSH settings, if applicable\n\nif [ -f \"$SSH_ENV\" ]; then\n    . \"$SSH_ENV\" >/dev/null\n    #ps $SSH_AGENT_PID doesn't work under Cygwin\n    ps -ef | grep $SSH_AGENT_PID | grep ssh-agent$ >/dev/null || {\n        start_agent\n    } \nelse\n    start_agent \nfi \n\n```\n\nThis version is especially nice since it will see if you've already started ssh-agent  and, if it can't find it, will start it up and store the settings so that they'll be usable the next time you start up a shell."}
{"instruction": "git remote add with other SSH port", "output": "You can just do this:\n\n```\ngit remote add origin ssh://user@host:1234/srv/git/example\n\n```\n\n1234 is the ssh port being used"}
{"instruction": "Vagrant stuck connection timeout retrying", "output": "I solved this problem, and will answer in case anyone else has a similar issue.\nWhat I did was: I enabled the GUI of Virtual box to see that it was waiting for input on startup to select whether I wanted to boot directly to ubuntu or safemode etc.\nTo turn on the GUI you have to put this in your vagrant config Vagrantfile:\n\n```\nconfig.vm.provider :virtualbox do |vb|\n  vb.gui = true\nend\n\n```"}
{"instruction": "Is it possible to create a remote repo on GitHub from the CLI without opening browser?", "output": "With Github's official new command line interface:\n\n```\ngh repo create\n\n```\n\nSee additional details and options and installation instructions.\n\nFor instance, to complete your git workflow:\n\n```\nmkdir project\ncd project\ngit init\ntouch file\ngit add file\ngit commit -m 'Initial commit'\ngh repo create\ngit push -u origin master\n\n```"}
{"instruction": "How do I get git to default to ssh and not https for new repositories", "output": "Set up a repository's origin branch to be SSH\nThe GitHub repository setup page is just a suggested list of commands (and GitHub now suggests using the HTTPS protocol). Unless you have administrative access to GitHub's site, I don't know of any way to change their suggested commands.\nIf you'd rather use the SSH protocol, simply add a remote branch like so (i.e. use this command in place of GitHub's suggested command). To modify an existing branch, see the next section.\n\n```\n$ git remote add origin git@github.com:nikhilbhardwaj/abc.git\n\n```\n\n\nModify a pre-existing repository\nAs you already know,  to switch a pre-existing repository to use SSH instead of HTTPS, you can change the remote url within your .git/config file.\n\n```\n[remote \"origin\"]\n    fetch = +refs/heads/*:refs/remotes/origin/*\n    -url = https://github.com/nikhilbhardwaj/abc.git\n    +url = git@github.com:nikhilbhardwaj/abc.git\n\n```\n\nA shortcut is to use the set-url command:\n\n```\n$ git remote set-url origin git@github.com:nikhilbhardwaj/abc.git\n\n```\n\n\nMore information about the SSH-HTTPS switch\n\n\"Why is Git always asking for my password?\" - GitHub help page.\nGitHub's switch to Smart HTTP - relevant StackOverflow question\nCredential Caching for Wrist-Friendly Git Usage - GitHub blog post about HTTPS, and how to avoid re-entering your password"}
{"instruction": "Getting ssh to execute a command in the background on target machine", "output": "This should solve your problem:\n\n```\nnohup myprogram > foo.log 2> foo.err < /dev/null &\n\n```\n\n\nThe syntax and unusual use of < /dev/null are explained especially well in this answer, quoted here for your convenience.\n\n< /dev/null is used to instantly send EOF to the program, so that it\ndoesn't wait for input (/dev/null, the null device, is a special file\nthat discards all data written to it, but reports that the write\noperation succeeded, and provides no data to any process that reads\nfrom it, yielding EOF immediately).\nSo the command:\n\n```\nnohup myscript.sh >myscript.log 2>&1 </dev/null &\n#\\__/             \\___________/ \\__/ \\________/ ^\n# |                    |          |      |      |\n# |                    |          |      |  run in background\n# |                    |          |      |\n# |                    |          |   don't expect input\n# |                    |          |   \n# |                    |        redirect stderr to stdout\n# |                    |           \n# |                    redirect stdout to myscript.log\n# |\n# keep the command running \n# no matter whether the connection is lost or you logout\n\n```\n\nwill move to background the command, outputting both stdout and\nstderr to myscript.log without waiting for any input.\n\n\nSee also the wikipedia artcle on nohup, also quoted here for your convenience.\n\nNohuping backgrounded jobs is for\nexample useful when logged in via SSH,\nsince backgrounded jobs can cause the\nshell to hang on logout due to a race\ncondition. This problem can also\nbe overcome by redirecting all three\nI/O streams."}
{"instruction": "Multiple GitHub Accounts &amp; SSH Config", "output": "Andy Lester's response is accurate but I found an important extra step I needed to make to get this to work. In trying to get two profiles set up, one for personal and one for work, my ~/.ssh/config was roughly as follows:\n\n```\nHost me.github.com\n    HostName github.com\n    PreferredAuthentications publickey\n    IdentityFile ~/.ssh/me_rsa\n\nHost work.github.com\n    HostName github.com\n    PreferredAuthentications publickey\n    IdentityFile ~/.ssh/work_rsa\n\n```\n\nMy work profile didn't take until I did a ssh-add ~/.ssh/work_rsa. After that connections to github used the correct profile. Previously they defaulted to the first public key.\n\nIf you encounter problems, they are likely from using the wrong account. You can troubleshoot things from the command line with a command like this (use ssh -vvv for max verbosity):\n\n```\n$ GIT_SSH_COMMAND='ssh -v' git pull --verbose\n\n```\n\nFor Could not open a connection to your authentication agent when using ssh-add,check https://stackoverflow.com/a/17695338/1760313\nFor  defaulted to the first public key, add IdentitiesOnly to the Host * section at the bottom of your ~/.ssh/config file. This tells ssh to only use specified identities rather than all unlocked keys in your agent.\nIf your Host * section has an IdentityFile line, this will not work. Remove that line and add it to a new Host !work.github.com * secion.\nIf you use ControlMaster and you try to use different Github accounts before the ControlPersist timeout expires, it will use the persisted connection, which may be for the other account. Change your ControlPath to include %k (the host key alias if specified, otherwise this is identical to %h, the hostname). If you prefer, you can limit this to just one Github account's path with e.g. ControlPath ~/.ssh/control-%C-2 solely in the Host work.github.com stanza."}
{"instruction": "How can I ssh directly to a particular directory?", "output": "You can do the following:\n\n```\nssh -t xxx.xxx.xxx.xxx \"cd /directory_wanted ; bash --login\"\n\n```\n\nThis way, you will get a login shell right on the directory_wanted.\n\nExplanation\n\n-t Force pseudo-terminal allocation.  This can be used to execute arbitrary screen-based programs on a remote machine, which can be very useful, e.g. when implementing menu services.\nMultiple -t options force tty allocation, even if ssh has no local tty.\n\n\nIf you don't use -t then no prompt will appear.\nIf you don't add ; bash then the connection will get closed and return control to your local machine\nIf you don't add bash --login then it will not use your configs because it's not a login shell"}
{"instruction": "How to solve &quot;sign_and_send_pubkey: signing failed: agent refused operation&quot;?", "output": "Run ssh-add on the client machine, that will add the SSH key to the agent. \nConfirm with ssh-add -l (again on the client) that it was indeed added."}
{"instruction": "Permission denied (publickey) when SSH Access to Amazon EC2 instance", "output": "This error message means you failed to authenticate. \nThese are common reasons that can cause that:\n\nTrying to connect with the wrong key. Are you sure this instance is using this keypair?\nTrying to connect with the wrong username. ubuntu is the username for the ubuntu based AWS distribution, but on some others it's ec2-user (or admin on some Debians, according to Bogdan Kulbida's answer)(can also be root, fedora, see below) \nTrying to connect the wrong host. Is that the right host you are trying to log in to?\n\nNote that 1. will also happen if you have messed up the /home/<username>/.ssh/authorized_keys file on your EC2 instance. \nAbout 2., the information about which username you should use is often lacking from the AMI Image description. But you can find some in AWS EC2 documentation, bullet point 4. : \nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AccessingInstancesLinux.html\n\nUse the ssh command to connect to the instance. You'll specify the private key (.pem) file and user_name@public_dns_name. For Amazon Linux, the user name is ec2-user. For RHEL5, the user name is either root or ec2-user. For Ubuntu, the user name is ubuntu. For Fedora, the user name is either fedora or ec2-user. For SUSE Linux, the user name is root. Otherwise, if ec2-user and root don't work, check with your AMI provider.\n\nFinally, be aware that there are many other reasons why authentication would fail. SSH is usually pretty explicit about what went wrong if you care to add the -v option to your SSH command and read the output, as explained in many other answers to this question."}
{"instruction": "vim: how to delete a newline/linefeed character(s)?", "output": "If you are on the first line, pressing (upper case) J will join that line and the next line together, removing the newline. You can also combine this with a count, so pressing 3J will combine all 3 lines together."}
{"instruction": "How to prevent a background process from being stopped after closing SSH client in Linux", "output": "Check out the \"nohup\" program."}
{"instruction": "Repository access denied. access via a deployment key is read-only", "output": "First confusion on my side was about where exactly to set SSH Keys in BitBucket.\nI am new to BitBucket and I was setting a Deployment Key which gives read-access only.\nSo make sure you are setting your rsa pub key in your BitBucket Account Settings. \nClick your BitBucket avatar and select Bitbucket Settings(Manage account). There you'll be able to set SSH Keys.\nI simply deleted the Deployment Key, I don't need any for now. And it worked"}
{"instruction": "Getting permission denied (public key) on gitlab", "output": "I found this after searching a lot. It will work perfectly fine for me.\n\nGo to \"Git Bash\" just like cmd. Right click and \"Run as Administrator\".\nType ssh-keygen\nPress enter.\nIt will ask you to save the key to the specific directory.\nPress enter. It will prompt you to type password or enter without password.\nThe public key will be created to the specific directory.\nNow go to the directory and open .ssh folder.\nYou'll see a file id_rsa.pub. Open it on notepad. Copy all text from it.\nGo to https://gitlab.com/-/profile/keys or\nPaste here in the \"key\" textfield.\nNow click on the \"Title\" below. It will automatically get filled.\nThen click \"Add key\".\n\nNow give it a shot and it will work for sure."}
{"instruction": "&#39;heroku&#39; does not appear to be a git repository", "output": "To add a Heroku app as a Git remote, you need to execute heroku git:remote -a yourapp. \nSource: Deploying with Git"}
{"instruction": "How to set ssh timeout?", "output": "```\nssh -o ConnectTimeout=10  <hostName>\n\n```\n\nWhere 10 is time in seconds.  This Timeout applies only to the creation of the connection."}
{"instruction": "Add Keypair to existing EC2 instance", "output": "You can't apply a keypair to a running instance. You can only use the new keypair to launch a new instance.\nFor recovery, if it's an EBS boot AMI, you can stop it, make a snapshot of the volume. Create a new volume based on it. And be able to use it back to start the old instance, create a new image, or recover data. \nThough data at ephemeral storage will be lost.\n\nDue to the popularity of this question and answer, I wanted to capture the information in the link that Rodney posted on his comment.\nCredit goes to Eric Hammond for this information.\nFixing Files on the Root EBS Volume of an EC2 Instance\nYou can examine and edit files on the root EBS volume on an EC2 instance even if you are in what you considered a disastrous situation like:\n\nYou lost your ssh key or forgot your password\nYou made a mistake editing the /etc/sudoers file and can no longer\ngain root access with sudo to fix it\nYour long running instance is hung for some reason, cannot be\ncontacted, and fails to boot properly\nYou need to recover files off of the instance but cannot get to it\n\nOn a physical computer sitting at your desk, you could simply boot the system with a CD or USB stick, mount the hard drive, check out and fix the files, then reboot the computer to be back in business.\nA remote EC2 instance, however, seems distant and inaccessible when you are in one of these situations. Fortunately, AWS provides us with the power and flexibility to be able to recover a system like this, provided that we are running EBS boot instances and not instance-store.\nThe approach on EC2 is somewhat similar to the physical solution, but we‚Äôre going to move and mount the faulty ‚Äúhard drive‚Äù (root EBS volume) to a different instance, fix it, then move it back.\nIn some situations, it might simply be easier to start a new EC2 instance and throw away the bad one, but if you really want to fix your files, here is the approach that has worked for many:\nSetup\nIdentify the original instance (A) and volume that contains the broken root EBS volume with the files you want to view and edit.\n\n```\ninstance_a=i-XXXXXXXX\n\nvolume=$(ec2-describe-instances $instance_a |\n  egrep '^BLOCKDEVICE./dev/sda1' | cut -f3)\n\n```\n\nIdentify the second EC2 instance (B) that you will use to fix the files on the original EBS volume. This instance must be running in the same availability zone as instance A so that it can have the EBS volume attached to it. If you don‚Äôt have an instance already running, start a temporary one.\n\n```\ninstance_b=i-YYYYYYYY\n\n```\n\nStop the broken instance A (waiting for it to come to a complete stop), detach the root EBS volume from the instance (waiting for it to be detached), then attach the volume to instance B on an unused device.\n\n```\nec2-stop-instances $instance_a\nec2-detach-volume $volume\nec2-attach-volume --instance $instance_b --device /dev/sdj $volume\n\n```\n\nssh to instance B and mount the volume so that you can access its file system.\n\n```\nssh ...instance b...\n\nsudo mkdir -p 000 /vol-a\nsudo mount /dev/sdj /vol-a\n\n```\n\nFix It\nAt this point your entire root file system from instance A is available for viewing and editing under /vol-a on instance B. For example, you may want to:\n\nPut the correct ssh keys in /vol-a/home/ubuntu/.ssh/authorized_keys\nEdit and fix /vol-a/etc/sudoers\nLook for error messages in /vol-a/var/log/syslog\nCopy important files out of /vol-a/‚Ä¶\n\nNote: The uids on the two instances may not be identical, so take care if you are creating, editing, or copying files that belong to non-root users. For example, your mysql user on instance A may have the same UID as your postfix user on instance B which could cause problems if you chown files with one name and then move the volume back to A.\nWrap Up\nAfter you are done and you are happy with the files under /vol-a, unmount the file system (still on instance-B):\n\n```\nsudo umount /vol-a\nsudo rmdir /vol-a\n\n```\n\nNow, back on your system with ec2-api-tools, continue moving the EBS volume back to it‚Äôs home on the original instance A and start the instance again:\n\n```\nec2-detach-volume $volume\nec2-attach-volume --instance $instance_a --device /dev/sda1 $volume\nec2-start-instances $instance_a\n\n```\n\nHopefully, you fixed the problem, instance A comes up just fine, and you can accomplish what you originally set out to do. If not, you may need to continue repeating these steps until you have it working.\nNote: If you had an Elastic IP address assigned to instance A when you stopped it, you‚Äôll need to reassociate it after starting it up again.\nRemember! If your instance B was temporarily started just for this process, don‚Äôt forget to terminate it now."}
{"instruction": "How can I remove an SSH key?", "output": "Note that there are at least two bug reports for ssh-add -d/-D not removing keys:\n\n\"Debian Bug report #472477: ssh-add -D does not remove SSH key from gnome-keyring-daemon memory\"\n\"Ubuntu: ssh-add -D deleting all identities does not work. Also, why are all identities auto-added?\"\n\nThe exact issue is:\n\nssh-add -d/-D deletes only manually added keys from gnome-keyring.\nThere is no way to delete automatically added keys.\nThis is the original bug, and it's still definitely present.\nSo, for example, if you have two different automatically-loaded ssh identities associated with two different GitHub accounts -- say for work and for home -- there's no way to switch between them. GitHubtakes the first one which matches, so you always appear as your 'home' user to GitHub, with no way to upload things to work projects.\nAllowing ssh-add -d to apply to automatically-loaded keys (and ssh-add -t X to change the lifetime of automatically-loaded keys), would restore the behavior most users expect.\n\n\nMore precisely, about the issue:\n\nThe culprit is gpg-keyring-daemon:\n\nIt subverts the normal operation of ssh-agent, mostly just so that it can pop up a pretty box into which you can type the passphrase for an encrypted ssh key.\nAnd it paws through your .ssh directory, and automatically adds any keys it finds to your agent.\nAnd it won't let you delete those keys.\n\nHow do we hate this? Let's not count the ways -- life's too short.\nThe failure is compounded because newer ssh clients automatically try all the keys in your ssh-agent when connecting to a host.\nIf there are too many, the server will reject the connection.\nAnd since gnome-keyring-daemon has decided for itself how many keys you want your ssh-agent to have, and has autoloaded them, AND WON'T LET YOU DELETE THEM, you're toast.\n\nThis bug is still confirmed in Ubuntu 14.04.4, as recently as two days ago (August 21st, 2014)\n\nA possible workaround:\n\n\nDo ssh-add -D to delete all your manually added keys. This also locks\nthe automatically added keys, but is not much use since gnome-keyring will ask you to unlock them anyways when you try doing a git push.\nNavigate to your ~/.ssh folder and move all your key files except the one you want to identify with into a separate folder called backup. If necessary you can also open seahorse and delete the keys from there.\nNow you should be able to do git push without a problem.\n\n\n\nAnother workaround:\n\nWhat you really want to do is to turn off gpg-keyring-daemon altogether.\nGo to System --> Preferences --> Startup Applications, and unselect the \"SSH Key Agent (Gnome Keyring SSH Agent)\" box -- you'll need to scroll down to find it.\nYou'll still get an ssh-agent, only now it will behave sanely: no keys autoloaded, you run ssh-add to add them, and if you want to delete keys, you can. Imagine that.\n\nThis comments actually suggests:\n\nThe solution is to keep gnome-keyring-manager from ever starting up, which was strangely difficult by finally achieved by removing the program file's execute permission.\n\n\nRyan Lue adds another interesting corner case in the comments:\n\nIn case this helps anyone: I even tried deleting the id_rsa and id_rsa.pub files altogether, and the key was still showing up.\nTurns out gpg-agent was caching them in a ~/.gnupg/sshcontrol file; I had to manually delete them from there.\n\nThat is the case when the keygrip has been added as in here."}
{"instruction": "Why does an SSH remote command get fewer environment variables then when run manually?", "output": "There are different types of shells. The SSH command execution shell is a non-interactive shell, whereas your normal shell is either a login shell or an interactive shell. Description follows, from man bash:\n\n```\n\n       A  login  shell  is  one whose first character of argument\n       zero is a -, or one started with the --login option.\n\n       An interactive shell is  one  started  without  non-option\n       arguments  and  without the -c option whose standard input\n       and error are both connected to terminals  (as  determined\n       by  isatty(3)), or one started with the -i option.  PS1 is\n       set and $- includes i if bash is interactive,  allowing  a\n       shell script or a startup file to test this state.\n\n       The  following  paragraphs  describe how bash executes its\n       startup files.  If any of the files exist  but  cannot  be\n       read,  bash reports an error.  Tildes are expanded in file\n       names as described below  under  Tilde  Expansion  in  the\n       EXPANSION section.\n\n       When  bash is invoked as an interactive login shell, or as\n       a non-interactive shell with the --login option, it  first\n       reads and executes commands from the file /etc/profile, if\n       that file exists.  After reading that file, it  looks  for\n       ~/.bash_profile,  ~/.bash_login,  and  ~/.profile, in that\n       order, and reads and executes commands from the first  one\n       that  exists  and is readable.  The --noprofile option may\n       be used when the shell is started to inhibit  this  behav¬≠\n       ior.\n\n       When a login shell exits, bash reads and executes commands\n       from the file ~/.bash_logout, if it exists.\n\n       When an interactive shell that is not  a  login  shell  is\n       started,  bash reads and executes commands from ~/.bashrc,\n       if that file exists.  This may be inhibited by  using  the\n       --norc  option.   The --rcfile file option will force bash\n       to  read  and  execute  commands  from  file  instead   of\n       ~/.bashrc.\n\n       When  bash  is  started  non-interactively, to run a shell\n       script, for example, it looks for the variable BASH_ENV in\n       the  environment,  expands  its value if it appears there,\n       and uses the expanded value as the name of a file to  read\n       and  execute.   Bash  behaves  as if the following command\n       were executed:\n              if [ -n \"$BASH_ENV\" ]; then . \"$BASH_ENV\"; fi\n       but the value of the PATH variable is not used  to  search\n       for the file name.\n\n\n```"}
{"instruction": "ssh -L forward multiple ports", "output": "The -L option can be specified multiple times within the same command. Every time with different ports. I.e. ssh -L localPort0:ip:remotePort0 -L localPort1:ip:remotePort1 ..."}
{"instruction": "Is there a way to continue broken scp (secure copy) command process in Linux?", "output": "If you need to resume an scp transfer from local to remote, try with rsync:\n\n```\nrsync --partial --progress --rsh=ssh local_file user@host:remote_file\n\n```\n\nShort version, as pointed out by @aurelijus-rozenas: \n\n```\nrsync -P -e ssh local_file user@host:remote_file\n\n```\n\nIn general the order of args for rsync is\n\n```\nrsync [options] SRC DEST\n\n```"}
{"instruction": "How do I POST JSON data with cURL?", "output": "You need to set your content-type to application/json. But -d (or --data) sends the Content-Type application/x-www-form-urlencoded by default, which is not accepted on Spring's side.\nLooking at the curl man page, I think you can use -H (or --header):\n\n```\n-H \"Content-Type: application/json\"\n\n```\n\nFull example:\n\n```\ncurl --header \"Content-Type: application/json\" \\\n  --request POST \\\n  --data '{\"username\":\"xyz\",\"password\":\"xyz\"}' \\\n  http://localhost:3000/api/login\n\n```\n\n(-H is short for --header, -d for --data)\nNote that -request POST is optional if you use -d, as the -d flag implies a POST request.\n\nOn Windows, things are slightly different. See the comment thread."}
{"instruction": "How do I get a YouTube video thumbnail from the YouTube API?", "output": "Each YouTube video has four generated images. They are predictably formatted as follows:\n\n```\nhttps://img.youtube.com/vi/<insert-youtube-video-id-here>/0.jpg\nhttps://img.youtube.com/vi/<insert-youtube-video-id-here>/1.jpg\nhttps://img.youtube.com/vi/<insert-youtube-video-id-here>/2.jpg\nhttps://img.youtube.com/vi/<insert-youtube-video-id-here>/3.jpg\n\n```\n\nThe first one in the list is a full size image and others are thumbnail images. The default thumbnail image (i.e., one of 1.jpg, 2.jpg, 3.jpg) is:\n\n```\nhttps://img.youtube.com/vi/<insert-youtube-video-id-here>/default.jpg\n\n```\n\nFor the high quality version of the thumbnail use a URL similar to this:\n\n```\nhttps://img.youtube.com/vi/<insert-youtube-video-id-here>/hqdefault.jpg\n\n```\n\nThere is also a medium quality version of the thumbnail, using a URL similar to the HQ:\n\n```\nhttps://img.youtube.com/vi/<insert-youtube-video-id-here>/mqdefault.jpg\n\n```\n\nFor the standard definition version of the thumbnail, use a URL similar to this:\n\n```\nhttps://img.youtube.com/vi/<insert-youtube-video-id-here>/sddefault.jpg\n\n```\n\nFor the maximum resolution version of the thumbnail use a URL similar to this:\n\n```\nhttps://img.youtube.com/vi/<insert-youtube-video-id-here>/maxresdefault.jpg\n\n```\n\nAll of the above URLs are available over HTTP too. Additionally, the slightly shorter hostname i3.ytimg.com works in place of img.youtube.com in the example URLs above.\nAlternatively, you can use the YouTube Data API (v3) to get thumbnail images."}
{"instruction": "How to send a header using a HTTP request through a cURL call?", "output": "GET:\nwith JSON:\n\n```\ncurl -i -H \"Accept: application/json\" -H \"Content-Type: application/json\" http://hostname/resource\n\n```\n\nwith XML:\n\n```\ncurl -H \"Accept: application/xml\" -H \"Content-Type: application/xml\" -X GET http://hostname/resource\n\n```\n\nPOST:\nFor posting data:\n\n```\ncurl --data \"param1=value1&param2=value2\" http://hostname/resource\n\n```\n\nFor file upload:\n\n```\ncurl --form \"fileupload=@filename.txt\" http://hostname/resource\n\n```\n\nRESTful HTTP Post:\n\n```\ncurl -X POST -d @filename http://hostname/resource\n\n```\n\nFor logging into a site (auth):\n\n```\ncurl -d \"username=admin&password=admin&submit=Login\" --dump-header headers http://localhost/Login\ncurl -L -b headers http://localhost/\n\n```\n\nSource"}
{"instruction": "How do I measure request and response times at once using cURL?", "output": "From this brilliant blog post...  https://blog.josephscott.org/2011/10/14/timing-details-with-curl/\ncURL supports formatted output for the details of the request (see the cURL manpage for details, under -w, ‚Äìwrite-out <format>). For our purposes we‚Äôll focus just on the timing details that are provided. Times below are in seconds.\n\nCreate a new file, curl-format.txt, and paste in:\n\n```\n     time_namelookup:  %{time_namelookup}s\\n\n        time_connect:  %{time_connect}s\\n\n     time_appconnect:  %{time_appconnect}s\\n\n    time_pretransfer:  %{time_pretransfer}s\\n\n       time_redirect:  %{time_redirect}s\\n\n  time_starttransfer:  %{time_starttransfer}s\\n\n                     ----------\\n\n          time_total:  %{time_total}s\\n\n\n```\n\n\nMake a request:\n\n```\n curl -w \"@curl-format.txt\" -o /dev/null -s \"http://wordpress.com/\"\n\n```\n\nOr on Windows, it's...\n\n```\n curl -w \"@curl-format.txt\" -o NUL -s \"http://wordpress.com/\"\n\n```\n\n\n\n\nWhat this does:\n-w \"@curl-format.txt\" tells cURL to use our format file\n-o /dev/null redirects the output of the request to /dev/null\n-s\ntells cURL not to show a progress meter\n\"http://wordpress.com/\" is\nthe URL we are requesting. Use quotes particularly if your URL has \"&\" query string parameters\n\nAnd here is what you get back:\n\n```\n   time_namelookup:  0.001s\n      time_connect:  0.037s\n   time_appconnect:  0.000s\n  time_pretransfer:  0.037s\n     time_redirect:  0.000s\ntime_starttransfer:  0.092s\n                   ----------\n        time_total:  0.164s\n\n```\n\nI have not yet seen an option to output the results in microseconds, but if you're aware of one, post in the comments below.\n\nMake a Linux/Mac shortcut (alias)\n\n```\nalias curltime=\"curl -w \\\"@$HOME/.curl-format.txt\\\" -o /dev/null -s \"\n\n```\n\nThen you can simply call...\n\n```\ncurltime wordpress.org\n\n```\n\nThanks to commenter Pete Doyle!\n\nMake a Linux/Mac stand-alone script\nThis script does not require a separate .txt file to contain the formatting.\nCreate a new file, curltime, somewhere in your executable path, and paste in:\n\n```\n#!/bin/bash\n\ncurl -w @- -o /dev/null -s \"$@\" <<'EOF'\n    time_namelookup:  %{time_namelookup}\\n\n       time_connect:  %{time_connect}\\n\n    time_appconnect:  %{time_appconnect}\\n\n   time_pretransfer:  %{time_pretransfer}\\n\n      time_redirect:  %{time_redirect}\\n\n time_starttransfer:  %{time_starttransfer}\\n\n                    ----------\\n\n         time_total:  %{time_total}\\n\nEOF\n\n```\n\nThen call it the same way as the alias:\n\n```\ncurltime wordpress.org\n\n```\n\n\nMake a Windows shortcut (aka BAT file)\nCreate a new text file called curltime.bat in the same folder as curl.exe and curl-format.txt, and paste in the following line:\n\n```\ncurl -w \"@%~dp0curl-format.txt\" -o NUL -s %*\n\n```\n\nThen from the command line you can simply call:\n\n```\ncurltime wordpress.org\n\n```\n\n(Make sure the folder is listed in your Windows PATH variable to be able to use the command from any folder.)"}
{"instruction": "How do I get cURL to not show the progress bar?", "output": "```\ncurl -s http://google.com > temp.html\n\n```\n\nworks for curl version 7.19.5 on Ubuntu 9.10 (no progress bar). But if for some reason that does not work on your platform, you could always redirect stderr to /dev/null:\n\n```\ncurl  http://google.com 2>/dev/null > temp.html\n\n```"}
{"instruction": "Converting a Postman request to curl", "output": "You can see the button </> Code icon in right side of the Postman app (attached screenshot).\nPress it and you can get your code in many different languages including PHP cURL"}
{"instruction": "How to capture cURL output to a file?", "output": "```\ncurl -K myconfig.txt -o output.txt \n\n```\n\nWrites the first output received in the file you specify (overwrites if an old one exists).\n\n```\ncurl -K myconfig.txt >> output.txt\n\n```\n\nAppends all output you receive to the specified file.\nNote: The -K is optional.\nIf you are posting to a URL like https://example.org/?foo=1&baz=4\nthen you need to put double quotes around the URL:\n\n```\ncurl \\\n   -X POST \\\n   -H \"Content-Type: application/octet-stream\" \\\n   --data-binary \"@/home/path/file.xyz\" \\\n   \"https://xample.org:8080/v1/?filename=file.xyz&food=1&z=bee\" \\\n  >out.txt 2>err.txt\n\n```"}
{"instruction": "Unable to resolve &quot;unable to get local issuer certificate&quot; using git on Windows with self-signed certificate", "output": "An answer to  Using makecert for Development SSL fixed this for me.\nI do not know why, but the certificate created by the simple 'Create Self Signed Certificate' link in IIS Manager does not do the trick.  I followed the approach in the linked question of creating and installing a self-signed CA Root; then using that to issue a Server Authentication Certificate for my server.  I installed both of them in IIS.\nThat gets my situation the same as the blog post referenced in the original question.  Once the root certificate was copy/pasted into curl-ca-bundle.crt the git/curl combo were satisfied."}
{"instruction": "How do I install and use cURL on Windows?", "output": "Assuming you got it from https://curl.haxx.se/download.html, just unzip it wherever you want. No need to install. If you are going to use SSL, you need to download the OpenSSL DLLs, available from curl's website."}
{"instruction": "JSONDecodeError: Expecting value: line 1 column 1 (char 0)", "output": "Your code produced an empty response body; you'd want to check for that or catch the exception raised. It is possible the server responded with a 204 No Content response, or a non-200-range status code was returned (404 Not Found, etc.). Check for this.\nNote:\n\nThere is no need to decode a response from UTF8 to Unicode, the  json.loads() method can handle UTF8-encoded data natively.\n\npycurl has a very archaic API. Unless you have a specific requirement for using it, there are better choices.\n\n\nEither requests or httpx offer much friendlier APIs, including JSON support.\nIf you can, replace your call with the following httpx code:\n\n```\nimport httpx\n\nresponse = httpx.get(url)\nresponse.raise_for_status()  # raises exception when not a 2xx response\nif response.status_code != 204:\n    return response.json()\n\n```\n\nOf course, this won't protect you from a URL that doesn't comply with HTTP standards; when using arbitrary URLs where this is a possibility, check if the server intended to give you JSON by checking the Content-Type header, and for good measure catch the exception:\n\n```\nif (\n    response.status_code != 204 and\n    response.headers[\"content-type\"].strip().startswith(\"application/json\")\n):\n    try:\n        return response.json()\n    except ValueError:\n        # decide how to handle a server that's misbehaving to this extent\n\n```"}
{"instruction": "Is there a way to follow redirects with command line cURL?", "output": "Use the location header flag:\n\n```\ncurl -L <URL>\ncurl --location <URL>  # or (same thing)\n\n```"}
{"instruction": "PHP, cURL, and HTTP POST example?", "output": "A very simple PHP example that sends an HTTP POST request to a remote site\n\n```\n$url = \"http://www.example.com/tester.phtml\";\n$post_data = array('postvar1' => 'value1');\n\n\n$ch = curl_init($url);\n// return the response instead of sending it to stdout:\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER, true);\n// set the POST data, corresponding method and headers:\ncurl_setopt($ch, CURLOPT_POSTFIELDS, http_build_query($post_data));\n// send the request and get the response\n$server_output = curl_exec($ch);\n\n```"}
{"instruction": "How to set the authorization header using cURL", "output": "http://curl.se/docs/httpscripting.html\nSee part 6. HTTP Authentication\n\nHTTP Authentication\n\n\nHTTP Authentication is the ability to tell the server your username and\npassword so that it can verify that you're allowed to do the request you're\ndoing. The Basic authentication used in HTTP (which is the type curl uses by\ndefault) is plain text based, which means it sends username and password\nonly slightly obfuscated, but still fully readable by anyone that sniffs on\nthe network between you and the remote server.\n\n\nTo tell curl to use a user and password for authentication:\n\n\n\n```\ncurl --user name:password http://www.example.com\n\n```\n\n\n\nThe site might require a different authentication method (check the headers\nreturned by the server), and then --ntlm, --digest, --negotiate or even\n--anyauth might be options that suit you.\n\n\nSometimes your HTTP access is only available through the use of a HTTP\nproxy. This seems to be especially common at various companies. A HTTP proxy\nmay require its own user and password to allow the client to get through to\nthe Internet. To specify those with curl, run something like:\n\n\n\n```\ncurl --proxy-user proxyuser:proxypassword curl.haxx.se\n\n```\n\n\n\nIf your proxy requires the authentication to be done using the NTLM method,\nuse --proxy-ntlm, if it requires Digest use --proxy-digest.\n\n\nIf you use any one these user+password options but leave out the password\npart, curl will prompt for the password interactively.\n\n\nDo note that when a program is run, its parameters might be possible to see\nwhen listing the running processes of the system. Thus, other users may be\nable to watch your passwords if you pass them as plain command line\noptions. There are ways to circumvent this.\n\n\nIt is worth noting that while this is how HTTP Authentication works, very\nmany web sites will not use this concept when they provide logins etc. See\nthe Web Login chapter further below for more details on that."}
{"instruction": "wget/curl large file from google drive", "output": "WARNING: This functionality is deprecated.  See warning below in comments.\n\nHave a look at this question: Direct download from Google Drive using Google Drive API\nBasically you have to create a public directory and access your files by relative reference with something like\n\n```\nwget https://googledrive.com/host/LARGEPUBLICFOLDERID/index4phlat.tar.gz\n\n```\n\nAlternatively, you can use this script: https://github.com/circulosmeos/gdown.pl"}
{"instruction": "performing HTTP requests with cURL (using PROXY)", "output": "General way:\n\n```\nexport http_proxy=http://your.proxy.server:port/\n\n```\n\nThen you can connect through proxy from (many) application.\nAnd, as per comment below, for https:\n\n```\nexport https_proxy=https://your.proxy.server:port/\n\n```"}
{"instruction": "Using cURL to upload POST data with files", "output": "You need to use the -F option:\n-F/--form <name=content> Specify HTTP multipart POST data (H)\nTry this:\n\n```\ncurl \\\n  -F \"userid=1\" \\\n  -F \"filecomment=This is an image file\" \\\n  -F \"image=@/home/user1/Desktop/test.jpg\" \\\n  localhost/uploader.php\n\n```"}
{"instruction": "How to do a PUT request with cURL?", "output": "Using the uppercase -X flag with whatever HTTP verb you want:\n\n```\ncurl -X PUT -d argument=value -d argument2=value2 http://localhost:8080\n\n```\n\nThis example also uses the -d flag to provide arguments with your PUT request."}
{"instruction": "How to urlencode data for curl command?", "output": "Use curl --data-urlencode; from man curl:\n\nThis posts data, similar to the other --data options with the exception that this performs URL-encoding. To be CGI-compliant, the <data> part should begin with a name followed by a separator and a content specification.\n\nExample usage:\n\n```\ncurl \\\n    --data-urlencode \"paramName=value\" \\\n    --data-urlencode \"secondParam=value\" \\\n    http://example.com\n\n```\n\nSee the man page for more info.\nThis requires curl 7.18.0 or newer (released January 2008). Use  curl -V to check which version you have.\nYou can as well encode the query string:\n\n```\ncurl --get \\\n    --data-urlencode \"p1=value 1\" \\\n    --data-urlencode \"p2=value 2\" \\\n    http://example.com\n    # http://example.com?p1=value%201&p2=value%202\n\n```"}
{"instruction": "PHP cURL custom headers", "output": "```\ncurl_setopt($ch, CURLOPT_HTTPHEADER, [\n    'X-Apple-Tz: 0',\n    'X-Apple-Store-Front: 143444,12'\n]);\n\n```\n\nhttps://www.php.net/manual/en/function.curl-setopt.php"}
{"instruction": "How can you debug a CORS request with cURL?", "output": "Here's how you can debug CORS requests using curl.\nSending a regular CORS request using cUrl:\n\n```\ncurl -H \"Origin: http://example.com\" --verbose \\\n  https://www.googleapis.com/discovery/v1/apis?fields=\n\n```\n\nThe -H \"Origin: http://example.com\" flag is the third party domain making the request. Substitute in whatever your domain is.\nThe --verbose flag prints out the entire response so you can see the request and response headers.\nThe URL I'm using above is a sample request to a Google API that supports CORS, but you can substitute in whatever URL you are testing.\nThe response should include the Access-Control-Allow-Origin header.\nSending a preflight request using cUrl:\n\n```\ncurl -H \"Origin: http://example.com\" \\\n  -H \"Access-Control-Request-Method: POST\" \\\n  -H \"Access-Control-Request-Headers: X-Requested-With\" \\\n  -X OPTIONS --verbose \\\n  https://www.googleapis.com/discovery/v1/apis?fields=\n\n```\n\nThis looks similar to the regular CORS request with a few additions:\nThe -H flags send additional preflight request headers to the server\nThe -X OPTIONS flag indicates that this is an HTTP OPTIONS request.\nIf the preflight request is successful, the response should include the Access-Control-Allow-Origin, Access-Control-Allow-Methods, and  Access-Control-Allow-Headers response headers.  If the preflight request was not successful, these headers shouldn't appear, or the HTTP response won't be 200.\nYou can also specify additional headers, such as User-Agent, by using the -H flag."}
{"instruction": "Run cURL commands from Windows console", "output": "If you are not into Cygwin, you can use native Windows builds. Some are here: curl Download Wizard."}
{"instruction": "How to count items in JSON object using command line?", "output": "Just throwing another solution in the mix...\nTry jq, a lightweight and flexible command-line JSON processor:\n\n```\njq length /tmp/test.json\n\n```\n\nPrints the length of the array of objects."}
{"instruction": "Call to undefined function curl_init()?", "output": "If you're on Windows:\nGo to your php.ini file and remove the ; mark from the beginning of the following line:\n\n```\n;extension=php_curl.dll\n\n```\n\nAfter you have saved the file you must restart your HTTP server software (e.g. Apache) before this can take effect.\n\nFor Ubuntu 13.0 and above, simply use the debundled package. In a terminal type the following to install it and do not forgot to restart server.\n\n```\nsudo apt-get install php-curl\n\n```\n\nOr if you're using the old PHP5\n\n```\nsudo apt-get install php5-curl\n\n```\n\nor\n\n```\nsudo apt-get install php5.6-curl\n\n```\n\nThen restart apache to activate the package with\n\n```\nsudo service apache2 restart\n\n```"}
{"instruction": "How to use cURL to send Cookies?", "output": "This worked for me:\n\n```\ncurl -v --cookie \"USER_TOKEN=Yes\" http://127.0.0.1:5000/\n\n```\n\nI could see the value in backend using\n\n```\nprint(request.cookies)\n\n```"}
{"instruction": "How to define the basic HTTP authentication using cURL correctly?", "output": "```\ncurl -u username:password http://\ncurl -u username http://\n\n```\n\nFrom the documentation page:\n\n-u, --user <user:password>\nSpecify the user name and password to use for server authentication.\nOverrides -n, --netrc and --netrc-optional.\nIf you simply specify the user name, curl will prompt for a password.\nThe user name and passwords are split up on the first colon, which\nmakes it impossible to use a colon in the user name with this option.\nThe password can, still.\nWhen using Kerberos V5 with a Windows based server you should include\nthe Windows domain name in the user name, in order for the server to\nsuccesfully obtain a Kerberos Ticket. If you don't then the initial\nauthentication handshake may fail.\nWhen using NTLM, the user name can be specified simply as the user\nname, without the domain, if there is a single domain and forest in\nyour setup for example.\nTo specify the domain name use either Down-Level Logon Name or UPN\n(User Principal Name) formats. For example, EXAMPLE\\user and\nuser@example.com respectively.\nIf you use a Windows SSPI-enabled curl binary and perform Kerberos V5,\nNegotiate, NTLM or Digest authentication then you can tell curl to\nselect the user name and password from your environment by specifying\na single colon with this option: \"-u :\".\nIf this option is used several times, the last one will be used.\n\nhttp://curl.haxx.se/docs/manpage.html#-u\nNote that you do not need --basic flag as it is the default."}
{"instruction": "Curl to return http status code along with the response", "output": "I was able to get a solution by looking at the curl doc which specifies to use - for the output to get the output to stdout.\n\n```\ncurl -o - -I http://localhost\n\n```\n\nTo get the response with just the http return code, I could just do\n\n```\ncurl -o /dev/null -s -w \"%{http_code}\\n\" http://localhost\n\n```"}
{"instruction": "Passing a URL with brackets to curl", "output": "Add -g to your command:\n\n```\n-g, --globoff\n      This option switches off the \"URL globbing parser\". When you set this option, you can\n      specify URLs that contain the letters {}[] without having curl itself interpret them.\n      Note that these letters are not normal legal URL contents but they should be encoded\n      according to the URI standard.\n\n      Example:\n       curl -g \"https://example.com/{[]}}}}\"\n\n```\n\ncurl.se/docs/manpage.html#-g"}
{"instruction": "Can PHP cURL retrieve response headers AND body in a single request?", "output": "One solution to this was posted in the PHP documentation comments: http://www.php.net/manual/en/function.curl-exec.php#80442\nCode example:\n\n```\n$ch = curl_init();\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);\ncurl_setopt($ch, CURLOPT_HEADER, 1);\n// ...\n\n$response = curl_exec($ch);\n\n// Then, after your curl_exec call:\n$header_size = curl_getinfo($ch, CURLINFO_HEADER_SIZE);\n$header = substr($response, 0, $header_size);\n$body = substr($response, $header_size);\n\n```\n\nWarning: As noted in the comments below, this may not be reliable when used with proxy servers or when handling certain types of redirects. @Geoffrey's answer may handle these more reliably."}
{"instruction": "List all indexes on ElasticSearch server?", "output": "For a concise list of all indices in your cluster, call\n\n```\ncurl http://localhost:9200/_aliases\n\n```\n\nthis will give you a list of indices and their aliases.\nIf you want it pretty-printed, add pretty=true:\n\n```\ncurl http://localhost:9200/_aliases?pretty=true\n\n```\n\nThe result will look something like this, if your indices are called old_deuteronomy and mungojerrie:\n\n```\n{\n  \"old_deuteronomy\" : {\n    \"aliases\" : { }\n  },\n  \"mungojerrie\" : {\n    \"aliases\" : {\n      \"rumpleteazer\" : { },\n      \"that_horrible_cat\" : { }\n    }\n  }\n}\n\n```"}
{"instruction": "Send request to cURL with post data sourced from a file", "output": "You're looking for the --data-binary argument:\n\n```\ncurl -i -X POST host:port/post-file \\\n  -H \"Content-Type: text/xml\" \\\n  --data-binary \"@path/to/file\"\n\n```\n\nIn the example above, -i prints out all the headers so that you can see what's going on, and -X POST makes it explicit that this is a post.  Both of these can be safely omitted without changing the behaviour on the wire.  The path to the file needs to be preceded by an @ symbol, so curl knows to read from a file."}
{"instruction": "POST XML file using cURL command line", "output": "If that question is connected to your other Hudson questions use the command they provide.  This way with XML from the command line:\n\n```\n$ curl -X POST -d '<run>...</run>' \\\nhttp://user:pass@myhost:myport/path/of/url\n\n```\n\nYou need to change it a little bit to read from a file:\n\n```\n $ curl -X POST -d @myfilename http://user:pass@myhost:myport/path/of/url\n\n```\n\nRead the manpage. following an abstract for -d Parameter.\n\n-d/--data \n(HTTP) Sends the specified data in a\n  POST request to the HTTP server, in\n  the same way that a browser does when\n  a user has filled in an HTML form and\n  presses the submit button. This will\n  cause curl to pass the data to the\n  server using the content-type\n  application/x-www-form-urlencoded.\n  Compare to -F/--form.\n-d/--data is the same as --data-ascii. To post data purely binary, you should\n  instead use the --data-binary option.\n  To URL-encode the value of a form\n  field you may use --data-urlencode.\nIf any of these options is used more\n  than once on the same command line,\n  the data pieces specified will be\n  merged together with a separating\n  &-symbol. Thus, using '-d name=daniel\n  -d skill=lousy' would generate a post chunk that looks like\n  'name=daniel&skill=lousy'.\nIf you start the data with the letter\n  @, the rest should be a file name to\n  read the data from, or - if you want\n  curl to read the data from stdin. The\n  contents of the file must already be\n  URL-encoded. Multiple files can also\n  be specified. Posting data from a file\n  named 'foobar' would thus be done with\n  --data @foobar."}
{"instruction": "Simulate a specific CURL in PostMan", "output": "A simpler approach would be:\n\nOpen POSTMAN\nClick on \"import\" tab on the upper left side.\nSelect the Raw Text option and paste your cURL command.\nHit import and you will have the command in your Postman builder!\nClick Send to post the command"}
{"instruction": "Automatically add newline at end of curl response body", "output": "From the man file:\n\nTo better allow script programmers to get to know about the progress of\n  curl, the -w/--write-out option was introduced. Using this, you can specify\n  what information from the previous transfer you want to extract.\nTo display the amount of bytes downloaded together with some text and an\n  ending newline:\n\n```\ncurl -w 'We downloaded %{size_download} bytes\\n' www.download.com\n\n```\n\n\nSo try adding the following to your ~/.curlrc file:\n\n```\n-w \"\\n\"\n\n```"}
{"instruction": "cURL error 60: SSL certificate: unable to get local issuer certificate", "output": "How to solve this problem:\n\ndownload and extract cacert.pem following the instructions at https://curl.se/docs/caextract.html\n\nsave it on your filesystem somewhere (for example, XAMPP users might use C:\\xampp\\php\\extras\\ssl\\cacert.pem)\n\nin your php.ini, put this file location in the [curl] section (putting it in the [openssl] section is also a good idea):\n\n\n\n```\n[curl]\ncurl.cainfo = \"C:\\xampp\\php\\extras\\ssl\\cacert.pem\"\n\n[openssl]\nopenssl.cafile = \"C:\\xampp\\php\\extras\\ssl\\cacert.pem\"\n\n```\n\n\nrestart your webserver (e.g. Apache) and PHP FPM server if applicable\n\n(Reference: https://laracasts.com/discuss/channels/general-discussion/curl-error-60-ssl-certificate-problem-unable-to-get-local-issuer-certificate)"}
{"instruction": "curl: (35) error:1408F10B:SSL routines:ssl3_get_record:wrong version number", "output": "```\n* Uses proxy env variable http_proxy == 'https://proxy.in.tum.de:8080'   \n                                         ^^^^^\n\n```\n\n\nThe https:// is wrong, it should be http://. The proxy itself should be accessed by HTTP and not HTTPS even though the target URL is HTTPS. The proxy will nevertheless properly handle HTTPS connection and keep the end-to-end encryption. See HTTP CONNECT method for details how this is done."}
{"instruction": "How to send file contents as body entity using cURL", "output": "I believe you're looking for the @filename syntax, e.g.:\nstrip new lines\n\n```\ncurl --data \"@/path/to/filename\" http://...\n\n```\n\nkeep new lines\n\n```\ncurl --data-binary \"@/path/to/filename\" http://...\n\n```\n\n\ncurl will strip all newlines from the file. If you want to send the file with newlines intact, use --data-binary in place of --data"}
{"instruction": "Save file to specific folder with curl command", "output": "I don't think you can give a path to curl, but you can CD to the location, download and CD back.\n\n```\ncd target/path && { curl -O URL ; cd -; }\n\n```\n\nOr using subshell.\n\n```\n(cd target/path && curl -O URL)\n\n```\n\nBoth ways will only download if path exists. -O keeps remote file name. After download it will return to original location.\nIf you need to set filename explicitly, you can use small -o option:\n\n```\ncurl -o target/path/filename URL\n\n```"}
{"instruction": "How do I make a request using HTTP basic authentication with PHP curl?", "output": "You want this:\n\n```\ncurl_setopt($ch, CURLOPT_USERPWD, $username . \":\" . $password);  \n\n```\n\nZend has a REST client and zend_http_client and I'm sure PEAR has some sort of wrapper.\nBut its easy enough to do on your own.\nSo the entire request might look something like this:\n\n```\n$ch = curl_init($host);\ncurl_setopt($ch, CURLOPT_HTTPHEADER, array('Content-Type: application/xml', $additionalHeaders));\ncurl_setopt($ch, CURLOPT_HEADER, 1);\ncurl_setopt($ch, CURLOPT_USERPWD, $username . \":\" . $password);\ncurl_setopt($ch, CURLOPT_TIMEOUT, 30);\ncurl_setopt($ch, CURLOPT_POST, 1);\ncurl_setopt($ch, CURLOPT_POSTFIELDS, $payloadName);\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER, TRUE);\n$return = curl_exec($ch);\ncurl_close($ch);\n\n```"}
{"instruction": "CURL Command Line URL Parameters", "output": "The application/x-www-form-urlencoded Content-type header is not required (well, kinda depends).   Unless the request handler expects parameters coming from the form body.   Try it out:\n\n```\ncurl -X DELETE \"http://localhost:5000/locations?id=3\"\n\n```\n\nor\n\n```\ncurl -X GET \"http://localhost:5000/locations?id=3\"\n\n```"}
{"instruction": "What is the right way to POST multipart/form-data using curl?", "output": "The following syntax fixes it for you:\n\n```\ncurl -v -F key1=value1 -F upload=@localfilename URL\n\n```"}
{"instruction": "Composer install error - requires ext_curl when it&#39;s actually enabled", "output": "This is caused because you don't have a library php5-curl installed in your system,\nOn Ubuntu its just simple run the line code below, in your case on Xamp take a look in Xamp documentation\n\n```\nsudo apt-get install php5-curl\n\n```\n\nFor anyone who uses php7.0\n\n```\nsudo apt-get install php7.0-curl\n\n```\n\nFor those who uses php7.1\n\n```\nsudo apt-get install php7.1-curl\n\n```\n\nFor those who use php7.2\n\n```\nsudo apt-get install php7.2-curl\n\n```\n\nFor those who use php7.3\n\n```\nsudo apt-get install php7.3-curl\n\n```\n\nFor those who use php7.4\n\n```\nsudo apt-get install php7.4-curl\n\n```\n\nFor those who use php8.0\n\n```\nsudo apt-get install php8.0-curl\n\n```\n\nFor those who use php8.1\n\n```\nsudo apt-get install php8.1-curl\n\n```\n\nOr simply run below command to install by your version:\n\n```\nsudo apt-get install php-curl\n\n```"}
{"instruction": "How do I deal with certificates using cURL while trying to access an HTTPS url?", "output": "This error is related to a missing package: ca-certificates. Install it.\nIn Ubuntu Linux (and similar distro):\n\n```\n# apt-get install ca-certificates\n\n```\n\nIn CygWin via Apt-Cyg\n\n```\n# apt-cyg install ca-certificates\n\n```\n\nIn Arch Linux (Raspberry Pi)\n\n```\n# pacman -S ca-certificates\n\n```\n\n\nThe documentation tells:\n\nThis package includes PEM files of CA certificates to allow SSL-based applications to check for the authenticity of SSL connections.\n\nAs seen at: Debian -- Details of package ca-certificates in squeeze"}
{"instruction": "How to pass payload via JSON file for curl?", "output": "curl sends POST requests with the default content type of application/x-www-form-urlencoded. If you want to send a JSON request, you will have to specify the correct content type header:\n\n```\n$ curl -vX POST http://server/api/v1/places.json -d @testplace.json \\\n--header \"Content-Type: application/json\"\n\n```\n\nBut that will only work if the server accepts json input. The .json at the end of the url may only indicate that the output is json, it doesn't necessarily mean that it also will handle json input. The API documentation should give you a hint on whether it does or not.     \nThe reason you get a 401 and not some other error is probably because the server can't extract the auth_token from your request."}
{"instruction": "Using curl POST with variables defined in bash script functions", "output": "You don't need to pass the quotes enclosing the custom headers to curl. Also, your variables in the middle of the data argument should be quoted.\nFirst, write a function that generates the post data of your script. This saves you from all sort of headaches concerning shell quoting and makes it easier to read and maintain the script than feeding the post data on curl's invocation line as in your attempt:\n\n```\ngenerate_post_data()\n{\n  cat <<EOF\n{\n  \"account\": {\n    \"email\": \"$email\",\n    \"screenName\": \"$screenName\",\n    \"type\": \"$theType\",\n    \"passwordSettings\": {\n      \"password\": \"$password\",\n      \"passwordConfirm\": \"$password\"\n    }\n  },\n  \"firstName\": \"$firstName\",\n  \"lastName\": \"$lastName\",\n  \"middleName\": \"$middleName\",\n  \"locale\": \"$locale\",\n  \"registrationSiteId\": \"$registrationSiteId\",\n  \"receiveEmail\": \"$receiveEmail\",\n  \"dateOfBirth\": \"$dob\",\n  \"mobileNumber\": \"$mobileNumber\",\n  \"gender\": \"$gender\",\n  \"fuelActivationDate\": \"$fuelActivationDate\",\n  \"postalCode\": \"$postalCode\",\n  \"country\": \"$country\",\n  \"city\": \"$city\",\n  \"state\": \"$state\",\n  \"bio\": \"$bio\",\n  \"jpFirstNameKana\": \"$jpFirstNameKana\",\n  \"jpLastNameKana\": \"$jpLastNameKana\",\n  \"height\": \"$height\",\n  \"weight\": \"$weight\",\n  \"distanceUnit\": \"MILES\",\n  \"weightUnit\": \"POUNDS\",\n  \"heightUnit\": \"FT/INCHES\"\n}\nEOF\n}\n\n```\n\nIt is then easy to use that function in the invocation of curl:\n\n```\ncurl -i \\\n-H \"Accept: application/json\" \\\n-H \"Content-Type:application/json\" \\\n-X POST --data \"$(generate_post_data)\" \"https://xxx:xxxxx@xxxx-www.xxxxx.com/xxxxx/xxxx/xxxx\"\n\n```\n\nThis said, here are a few clarifications about shell quoting rules:\nThe double quotes in the -H arguments (as in -H \"foo bar\") tell bash to keep what's inside as a single argument (even if it contains spaces).\nThe single quotes in the --data argument (as in --data 'foo bar') do the same, except they pass all text verbatim (including double quote characters and the dollar sign).\nTo insert a variable in the middle of a single quoted text, you have to end the single quote, then concatenate with the double quoted variable, and re-open the single quote to continue the text: 'foo bar'\"$variable\"'more foo'."}
{"instruction": "How do I install the ext-curl extension with PHP 7?", "output": "Well I was able to install it by :\n\n```\nsudo apt-get install php-curl\n\n```\n\non my system. This will install a dependency package, which depends on the default php version.\nAfter that restart apache\n\n```\nsudo service apache2 restart\n\n```"}
{"instruction": "PHP - Debugging Curl", "output": "You can enable the CURLOPT_VERBOSE option:\n\n```\ncurl_setopt($curlhandle, CURLOPT_VERBOSE, true);\n\n```\n\nWhen CURLOPT_VERBOSE is set, output is written to STDERR or the file specified using CURLOPT_STDERR. The output is very informative.\nYou can also use tcpdump or wireshark to watch the network traffic."}
{"instruction": "Getting HTTP code in PHP using curl", "output": "First make sure if the URL is actually valid (a string, not empty, good syntax), this is quick to check server side. For example, doing this first could save a lot of time:\n\n```\nif(!$url || !is_string($url) || ! preg_match('/^http(s)?:\\/\\/[a-z0-9-]+(.[a-z0-9-]+)*(:[0-9]+)?(\\/.*)?$/i', $url)){\n    return false;\n}\n\n```\n\nMake sure you only fetch the headers, not the body content:\n\n```\n@curl_setopt($ch, CURLOPT_HEADER  , true);  // we want headers\n@curl_setopt($ch, CURLOPT_NOBODY  , true);  // we don't need body\n\n```\n\nFor more details on getting the URL status http code I refer to another post I made (it also helps with following redirects):\n\nHow can I check if a URL exists via PHP?\n\n\nAs a whole:\n\n```\n$url = 'http://www.example.com';\n$ch = curl_init($url);\ncurl_setopt($ch, CURLOPT_HEADER, true);    // we want headers\ncurl_setopt($ch, CURLOPT_NOBODY, true);    // we don't need body\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER,1);\ncurl_setopt($ch, CURLOPT_TIMEOUT,10);\n$output = curl_exec($ch);\n$httpcode = curl_getinfo($ch, CURLINFO_HTTP_CODE);\ncurl_close($ch);\n\necho 'HTTP code: ' . $httpcode;\n\n```"}
{"instruction": "PHP - SSL certificate error: unable to get local issuer certificate", "output": "Finally got this to work!\n\nDownload the certificate bundle.\nPut it somewhere. In my case, that was c:\\wamp\\ directory (if you are using Wamp 64 bit then it's c:\\wamp64\\).\nEnable mod_ssl in Apache and php_openssl.dll in php.ini (uncomment them by removing ; at the beginning). But be careful, my problem was that I had two php.ini files and I need to do this in both of them. One is the one you get from your WAMP taskbar icon, and another one is, in my case, in C:\\wamp\\bin\\php\\php5.5.12\\\nAdd these lines to your cert in both php.ini files:\n\n```\ncurl.cainfo=\"C:/wamp/cacert.pem\"\nopenssl.cafile=\"C:/wamp/cacert.pem\"\n\n```\n\nRestart Wamp services."}
{"instruction": "Execute bash script from URL", "output": "```\nsource <(curl -s http://mywebsite.example/myscript.txt)\n\n```\n\nought to do it. Alternately, leave off the initial redirection on yours, which is redirecting standard input; bash takes a filename to execute just fine without redirection, and <(command) syntax provides a path.\n\n```\nbash <(curl -s http://mywebsite.example/myscript.txt)\n\n```\n\nIt may be clearer if you look at the output of echo <(cat /dev/null)"}
{"instruction": "What is cURL in PHP?", "output": "cURL is a library that lets you make HTTP requests in PHP. Everything you need to know about it (and most other extensions) can be found in the PHP manual.\n\nIn order to use PHP's cURL functions\n  you need to install the ¬ª libcurl\n  package. PHP requires that you use\n  libcurl 7.0.2-beta or higher. In PHP\n  4.2.3, you will need libcurl version 7.9.0 or higher. From PHP 4.3.0, you will need a libcurl version that's\n  7.9.8 or higher. PHP 5.0.0 requires a libcurl version 7.10.5 or greater.\n\nYou can make HTTP requests  without cURL, too, though it requires allow_url_fopen to be enabled in your php.ini file.\n\n```\n// Make a HTTP GET request and print it (requires allow_url_fopen to be enabled)\nprint file_get_contents('http://www.example.com/');\n\n```"}
{"instruction": "How to specify the download location with wget?", "output": "From the manual page:\n\n```\n-P prefix\n--directory-prefix=prefix\n           Set directory prefix to prefix.  The directory prefix is the\n           directory where all other files and sub-directories will be\n           saved to, i.e. the top of the retrieval tree.  The default\n           is . (the current directory).\n\n```\n\nSo you need to add -P /tmp/cron_test/ (short form) or --directory-prefix=/tmp/cron_test/ (long form) to your command. Also note that if the directory does not exist it will get created."}
{"instruction": "How to change filename of a file downloaded with wget?", "output": "Use the -O file option. \nE.g.\n\n```\nwget google.com\n...\n16:07:52 (538.47 MB/s) - `index.html' saved [10728]\n\n```\n\nvs.\n\n```\nwget -O foo.html google.com\n...\n16:08:00 (1.57 MB/s) - `foo.html' saved [10728]\n\n```"}
{"instruction": "Using wget to recursively fetch a directory with arbitrary files in it", "output": "You have to pass the -np/--no-parent option to wget (in addition to -r/--recursive, of course), otherwise it will follow the link in the directory index on my site to the parent directory. So the command would look like this:\n\n```\nwget --recursive --no-parent http://example.com/configs/.vim/\n\n```\n\nTo avoid downloading the auto-generated index.html files, use the -R/--reject option:\n\n```\nwget -r -np -R \"index.html*\" http://example.com/configs/.vim/\n\n```"}
{"instruction": "Downloading Java JDK on Linux via wget is shown license page instead", "output": "Works as of December 23rd, 2021 for JDK 17\n\n```\nwget --no-check-certificate -c --header \"Cookie: oraclelicense=accept-securebackup-cookie\" https://download.oracle.com/java/17/archive/jdk-17.0.1_linux-x64_bin.rpm\n\n```\n\nWorks as of July 27th, 2021 for JDK 16\n\n```\nwget --no-check-certificate -c --header \"Cookie: oraclelicense=accept-securebackup-cookie\" https://download.oracle.com/otn-pub/java/jdk/16.0.2%2B7/d4a915d82b4c4fbb9bde534da945d746/jdk-16.0.2_linux-x64_bin.rpm\n\n```\n\nWorks as of November 5th, 2020 for JDK 15\n\n```\nwget --no-check-certificate -c --header \"Cookie: oraclelicense=accept-securebackup-cookie\" https://download.oracle.com/otn-pub/java/jdk/15.0.1+9/51f4f36ad4ef43e39d0dfdbaf6549e32/jdk-15.0.1_linux-x64_bin.rpm\n\n```\n\nWorks as of 07-11-2020 for JDK 14\n\n```\nwget --no-check-certificate -c --header \"Cookie: oraclelicense=accept-securebackup-cookie\" https://download.oracle.com/otn-pub/java/jdk/14.0.1+7/664493ef4a6946b186ff29eb326336a2/jdk-14.0.1_linux-x64_bin.rpm -O ~/Downloads/jdk-14.0.1_linux-x64_bin.rpm\n\n```\n\nPS: Alf added this ( me ) :-) this, I couldn't figured out how to just commented at the end... Enjoy it.\nUPDATED FOR Oracle JDK 11\n\n```\nwget --no-check-certificate -c --header \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/11+28/55eed80b163941c8885ad9298e6d786a/jdk-11_linux-x64_bin.tar.gz\n\n```\n\nUPDATED FOR JDK 10.0.2\n\n```\nwget --no-check-certificate -c --header \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/10.0.2+13/19aef61b38124481863b1413dce1855f/jdk-10.0.2_linux-x64_bin.tar.gz\n\n```\n\nUPDATED FOR JDK 10.0.1\n\n```\nwget --no-check-certificate -c --header \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/10.0.1+10/fb4372174a714e6b8c52526dc134031e/jdk-10.0.1_linux-x64_bin.tar.gz\n\n```\n\nUPDATED FOR JDK 9\nit looks like you can download it now directly from java.net without sending a header\n\n```\nwget http://download.java.net/java/GA/jdk9/9/binaries/jdk-9+181_linux-x64_bin.tar.gz\n\n```\n\nUPDATED FOR JDK 8u191\nTAR GZ:\n\n```\nwget --no-cookies --no-check-certificate --header \"Cookie: gpw_e24=http%3a%2F%2Fwww.oracle.com%2Ftechnetwork%2Fjava%2Fjavase%2Fdownloads%2Fjdk8-downloads-2133151.html; oraclelicense=accept-securebackup-cookie;\" \"https://download.oracle.com/otn-pub/java/jdk/8u191-b12/2787e4a523244c269598db4e85c51e0c/jdk-8u191-linux-x64.tar.gz\"\n\n```\n\nRPM:\n\n```\nwget --no-cookies --no-check-certificate --header \"Cookie: gpw_e24=http%3a%2F%2Fwww.oracle.com%2Ftechnetwork%2Fjava%2Fjavase%2Fdownloads%2Fjdk8-downloads-2133151.html; oraclelicense=accept-securebackup-cookie;\" \"https://download.oracle.com/otn-pub/java/jdk/8u191-b12/2787e4a523244c269598db4e85c51e0c/jdk-8u191-linux-x64.rpm\"\n\n```\n\nUPDATED FOR JDK 8u131\nRPM:\n\n```\n  wget -c --header \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/8u131-b11/d54c1d3a095b4ff2b6607d096fa80163/jdk-8u131-linux-x64.rpm\n\n```\n\nTAR GZ:\n\n```\n wget -c --header \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/8u131-b11/d54c1d3a095b4ff2b6607d096fa80163/jdk-8u131-linux-x64.tar.gz\n\n```\n\nRPM using curl:\n\n```\n curl -v -j -k -L -H \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/8u131-b11/d54c1d3a095b4ff2b6607d096fa80163/jdk-8u131-linux-x64.rpm > jdk-8u112-linux-x64.rpm\n\n```\n\nIn all cases above, subst 'i586' for 'x64' to download the 32-bit build.\n\n-j -> junk cookies\n-k -> ignore certificates\n-L -> follow redirects\n-H [arg] -> headers\n\ncurl can be used in place of wget.\nUPDATE FOR JDK 7u79\nTAR GZ:\n\n```\nwget --no-check-certificate --no-cookies --header \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/7u79-b15/jdk-7u79-linux-x64.tar.gz\n\n```\n\nRPM using curl:\n\n```\ncurl -v -j -k -L -H \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/7u79-b15/jdk-7u79-linux-x64.rpm > jdk-7u79-linux-x64.rpm\n\n```\n\nOnce again, make sure you specify the correct URL for the version you are downloading. You can find the URL here: Oracle JDK download site\nORIGINAL ANSWER FROM 9th June 2012\nIf you are looking to download the Oracle JDK from the command line using wget, there is a workaround. Run the wget command as follows:\n\n```\nwget --no-cookies --header \"Cookie: gpw_e24=http%3A%2F%2Fwww.oracle.com\" \"http://download.oracle.com/otn-pub/java/jdk/7/jdk-7-linux-x64.tar.gz\"\n\n```\n\nBe sure to replace the download link with the correct one for the version you are downloading."}
{"instruction": "wget/curl large file from google drive", "output": "WARNING: This functionality is deprecated.  See warning below in comments.\n\nHave a look at this question: Direct download from Google Drive using Google Drive API\nBasically you have to create a public directory and access your files by relative reference with something like\n\n```\nwget https://googledrive.com/host/LARGEPUBLICFOLDERID/index4phlat.tar.gz\n\n```\n\nAlternatively, you can use this script: https://github.com/circulosmeos/gdown.pl"}
{"instruction": "How to download HTTP directory with all files and sub-directories as they appear on the online files/folders list?", "output": "Solution:\n\n```\nwget -r -np -nH --cut-dirs=3 -R index.html http://hostname/aaa/bbb/ccc/ddd/\n\n```\n\nExplanation:\n\nIt will download all files and subfolders in ddd directory\n-r : recursively \n-np : not going to upper directories, like ccc/‚Ä¶\n-nH : not saving files to hostname folder \n--cut-dirs=3 : but saving it to ddd by omitting\nfirst 3 folders aaa, bbb, ccc\n-R index.html : excluding index.html\nfiles \n\nReference: http://bmwieczorek.wordpress.com/2008/10/01/wget-recursively-download-all-files-from-certain-directory-listed-by-apache/"}
{"instruction": "How to install wget in macOS?", "output": "Using brew\nFirst install brew:\n\n```\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\n```\n\nAnd then install wget with brew:\n\n```\nbrew install wget\n\n```\n\nUsing MacPorts\nFirst, download and run MacPorts installer (.pkg)\nAnd then install wget:\n\n```\nsudo port install wget\n\n```"}
{"instruction": "How to get past the login page with Wget?", "output": "Based on the manual page:\n\n```\n# Log in to the server.  This only needs to be done once.\nwget --save-cookies cookies.txt \\\n     --keep-session-cookies \\\n     --post-data 'user=foo&password=bar' \\\n     --delete-after \\\n     http://server.com/auth.php\n\n# Now grab the page or pages we care about.\nwget --load-cookies cookies.txt \\\n     http://server.com/interesting/article.php\n\n```\n\nMake sure the --post-data parameter is properly percent-encoded (especially ampersands!) or the request will probably fail. Also make sure that user and password are the correct keys; you can find out the correct keys by sleuthing the HTML of the login page (look into your browser‚Äôs ‚Äúinspect element‚Äù feature and find the name attribute on the username and password fields)."}
{"instruction": "Download a working local copy of a webpage", "output": "wget is capable of doing what you are asking. Just try the following:\n\n```\nwget -p -k http://www.example.com/\n\n```\n\nThe -p will get you all the required elements to view the site correctly (css, images, etc).\nThe -k will change all links (to include those for CSS & images) to allow you to view the page offline as it appeared online.\nFrom the Wget docs:\n\n```\n‚Äò-k‚Äô\n‚Äò--convert-links‚Äô\nAfter the download is complete, convert the links in the document to make them\nsuitable for local viewing. This affects not only the visible hyperlinks, but\nany part of the document that links to external content, such as embedded images,\nlinks to style sheets, hyperlinks to non-html content, etc.\n\nEach link will be changed in one of the two ways:\n\n    The links to files that have been downloaded by Wget will be changed to refer\n    to the file they point to as a relative link.\n\n    Example: if the downloaded file /foo/doc.html links to /bar/img.gif, also\n    downloaded, then the link in doc.html will be modified to point to\n    ‚Äò../bar/img.gif‚Äô. This kind of transformation works reliably for arbitrary\n    combinations of directories.\n\n    The links to files that have not been downloaded by Wget will be changed to\n    include host name and absolute path of the location they point to.\n\n    Example: if the downloaded file /foo/doc.html links to /bar/img.gif (or to\n    ../bar/img.gif), then the link in doc.html will be modified to point to\n    http://hostname/bar/img.gif. \n\nBecause of this, local browsing works reliably: if a linked file was downloaded,\nthe link will refer to its local name; if it was not downloaded, the link will\nrefer to its full Internet address rather than presenting a broken link. The fact\nthat the former links are converted to relative links ensures that you can move\nthe downloaded hierarchy to another directory.\n\nNote that only at the end of the download can Wget know which links have been\ndownloaded. Because of that, the work done by ‚Äò-k‚Äô will be performed at the end\nof all the downloads. \n\n```"}
{"instruction": "How do I fix certificate errors when running wget on an HTTPS URL in Cygwin?", "output": "Looking at current hacky solutions in here, I feel I have to describe a proper solution after all.\nFirst, you need to install the cygwin package ca-certificates via Cygwin's setup.exe to get the certificates. \nDo NOT use curl or similar hacks to download certificates (as a neighboring answer advices) because that's fundamentally insecure and may compromise the system.\nSecond, you need to tell wget where your certificates are, since it doesn't pick them up by default in Cygwin environment. If you can do that either with the command-line parameter --ca-directory=/usr/ssl/certs (best for shell scripts) or by adding ca_directory = /usr/ssl/certs to ~/.wgetrc file.\nYou can also fix that by running ln -sT /usr/ssl /etc/ssl as pointed out in another answer, but that will work only if you have administrative access to the system. Other solutions I described do not require that."}
{"instruction": "Get final URL after curl is redirected", "output": "curl's -w option and the sub variable url_effective is what you are\nlooking for.\nSomething like\n\n```\ncurl -Ls -o /dev/null -w %{url_effective} https://example.com\n\n```\n\nMore info\n\n```\n\n-L         Follow redirects\n-s         Silent mode. Don't output anything\n-o FILE    Write output to <file> instead of stdout\n-w FORMAT  What to output after completion\n\n```\n\nMore\nYou might want to add -I (that is an uppercase i) as well, which will make the command not download any \"body\", but it then also uses the HEAD method, which is not what the question included and risk changing what the server does. Sometimes servers don't respond well to HEAD even when they respond fine to GET."}
{"instruction": "How can I use Python&#39;s Requests to fake a browser visit a.k.a and generate User Agent?", "output": "Provide a User-Agent header:\n\n```\nimport requests\n\nurl = 'http://www.ichangtou.com/#company:data_000008.html'\nheaders = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n\nresponse = requests.get(url, headers=headers)\nprint(response.content)\n\n```\n\nFYI, here is a list of User-Agent strings for different browsers:\n\nList of all Browsers\n\n\nAs a side note, there is a pretty useful third-party package called fake-useragent that provides a nice abstraction layer over user agents:\n\nfake-useragent\nUp to date simple useragent faker with real world database\n\nDemo:\n\n```\n>>> from fake_useragent import UserAgent\n>>> ua = UserAgent()\n>>> ua.chrome\nu'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1667.0 Safari/537.36'\n>>> ua.random\nu'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.67 Safari/537.36'\n\n```"}
{"instruction": "How to download an entire directory and subdirectories using wget?", "output": "You may use this in shell:\n\n```\nwget -r --no-parent http://abc.tamu.edu/projects/tzivi/repository/revisions/2/raw/tzivi/\n\n```\n\nThe Parameters are:\n\n```\n-r     //recursive Download\n\n```\n\nand  \n\n```\n--no-parent // Don¬¥t download something from the parent directory\n\n```\n\nIf you don't want to download the entire content, you may use:  \n\n```\n-l1 just download the directory (tzivi in your case)\n\n-l2 download the directory and all level 1 subfolders ('tzivi/something' but not 'tivizi/somthing/foo')  \n\n```\n\nAnd so on. If you insert no -l option, wget will use -l 5 automatically.\nIf you insert a -l 0 you¬¥ll download the whole Internet, because wget will follow every link it finds."}
{"instruction": "How to download all files (but not HTML) from a website using wget?", "output": "To filter for specific file extensions:\n\n```\nwget -A pdf,jpg -m -p -E -k -K -np http://site/path/\n\n```\n\nOr, if you prefer long option names:\n\n```\nwget --accept pdf,jpg --mirror --page-requisites --adjust-extension --convert-links --backup-converted --no-parent http://site/path/\n\n```\n\nThis will mirror the site, but the files without jpg or pdf extension will be automatically removed."}
{"instruction": "How to `wget` a list of URLs in a text file?", "output": "Quick man wget gives me the following:\n\n[..]\n-i file\n--input-file=file\nRead URLs from a local or external file. If - is specified as file, URLs are read from the standard input. (Use ./- to read from a file literally named -.)\nIf this function is used, no URLs need be present on the command line. If there are URLs both on the command line and in an input file, those on the command lines will be the first ones to be retrieved. If --force-html is not specified, then file should consist of a series of URLs, one per line.\n[..]\n\nSo: wget -i text_file.txt"}
{"instruction": "How do I request a file but not save it with Wget?", "output": "Use q flag for quiet mode, and tell wget to output to stdout with O- (uppercase o) and redirect to /dev/null to discard the output:\nwget -qO- $url &> /dev/null\n> redirects application output (to a file). if > is preceded by ampersand, shell redirects all outputs (error and normal) to the file right of >. If you don't specify ampersand, then only normal output is redirected.\n\n```\n./app &>  file # redirect error and standard output to file\n./app >   file # redirect standard output to file\n./app 2>  file # redirect error output to file\n\n```\n\nif file is /dev/null then all is discarded.\nThis works as well, and simpler:\n\n```\nwget -O/dev/null -q $url\n\n```"}
{"instruction": "How do I use Wget to download all images into a single folder, from a URL?", "output": "Try this:\n\n```\nwget -nd -r -P /save/location -A jpeg,jpg,bmp,gif,png http://www.somedomain.com\n\n```\n\nHere is some more information:\n-nd prevents the creation of a directory hierarchy (i.e. no directories).\n-r enables recursive retrieval. See Recursive Download for more information.\n-P sets the directory prefix where all files and directories are saved to.\n-A sets a whitelist for retrieving only certain file types. Strings and patterns are accepted, and both can be used in a comma separated list (as seen above). See Types of Files for more information."}
{"instruction": "How to hide wget output in Linux?", "output": "Why don't you use -q?\nFrom man wget:\n\n```\n-q\n--quiet\n   Turn off Wget's output.\n\n```\n\nTest\n\n```\n$ wget www.google.com\n--2015-05-08 14:07:42--  http://www.google.com/\nResolving www.google.com (www.google.com)... \n  (...)\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/html]\nSaving to: ‚Äòindex.html‚Äô\n\n    [ <=>                                                                                       ] 17,913      --.-K/s   in 0.01s   \n\n2015-05-08 14:07:42 (1.37 MB/s) - ‚Äòindex.html‚Äô saved [17913]\n\n```\n\nAnd:\n\n```\n$ wget -q www.google.com\n$\n\n```"}
{"instruction": "How to get the contents of a webpage in a shell variable?", "output": "You can use wget command to download the page and read it into a variable as:\n\n```\ncontent=$(wget google.com -q -O -)\necho $content\n\n```\n\nWe use the -O option of wget which allows us to specify the name of the file into which wget dumps the page contents. We specify - to get the dump onto standard output and collect that into the variable content. You can add the -q quiet option to turn off's wget output. \nYou can use the curl command for this aswell as:\n\n```\ncontent=$(curl -L google.com)\necho $content\n\n```\n\nWe need to use the -L option as the page we are requesting might have moved. In which case we need to get the page from the new location. The -L or --location option helps us with this."}
{"instruction": "How to run wget inside Ubuntu Docker image?", "output": "You need to install it first. Create a new Dockerfile, and install wget in it:\n\n```\nFROM ubuntu:14.04\nRUN  apt-get update \\\n  && apt-get install -y wget \\\n  && rm -rf /var/lib/apt/lists/*\n\n```\n\nThen, build that image:\n\n```\ndocker build -t my-ubuntu .\n\n```\n\nFinally, run it:\n\n```\ndocker run my-ubuntu wget https://downloads-packages.s3.amazonaws.com/ubuntu-14.04/gitlab_7.8.2-omnibus.1-1_amd64.deb\n\n```"}
{"instruction": "Python equivalent of a given wget command", "output": "urllib.request should work. \nJust set it up in a while(not done) loop, check if a localfile already exists, if it does send a GET with a RANGE header, specifying how far you got in downloading the localfile. \nBe sure to use read() to append to the localfile until an error occurs.\nThis is also potentially a duplicate of Python urllib2 resume download doesn't work when network reconnects"}
{"instruction": "How can I show the wget progress bar only?", "output": "You can use the following filter:\n\n```\nprogressfilt ()\n{\n    local flag=false c count cr=$'\\r' nl=$'\\n'\n    while IFS='' read -d '' -rn 1 c\n    do\n        if $flag\n        then\n            printf '%s' \"$c\"\n        else\n            if [[ $c != $cr && $c != $nl ]]\n            then\n                count=0\n            else\n                ((count++))\n                if ((count > 1))\n                then\n                    flag=true\n                fi\n            fi\n        fi\n    done\n}\n\n```\n\nUsage:\n\n```\n$ wget --progress=bar:force http://somesite.com/TheFile.jpeg 2>&1 | progressfilt\n100%[======================================>] 15,790      48.8K/s   in 0.3s\n\n2011-01-13 22:09:59 (48.8 KB/s) - 'TheFile.jpeg' saved [15790/15790]\n\n```\n\nThis function depends on a sequence of 0x0d0x0a0x0d0x0a0x0d being sent right before the progress bar is started. This behavior may be implementation dependent."}
{"instruction": "How do I download a tarball from GitHub using cURL?", "output": "Use the -L option to follow redirects:\n\n```\ncurl -L https://github.com/pinard/Pymacs/tarball/v0.24-beta2 | tar zx\n\n```"}
{"instruction": "Does WGET timeout?", "output": "According to the man page of wget, there are a couple of options related to timeouts -- and there is a default read timeout of 900s -- so I say that, yes, it could timeout.\n\nHere are the options in question :\n\n```\n-T seconds\n--timeout=seconds\n\n```\n\n\nSet the network timeout to seconds\n  seconds.  This is equivalent to\n  specifying --dns-timeout,\n  --connect-timeout, and\n  --read-timeout, all at the same\n  time.\n\n\nAnd for those three options :\n\n```\n--dns-timeout=seconds\n\n```\n\n\nSet the DNS lookup timeout to seconds\n  seconds.  DNS lookups that don't\n  complete within the specified time\n  will fail. By default, there is no\n  timeout on DNS lookups, other than\n  that implemented by system libraries.\n\n\n```\n--connect-timeout=seconds\n\n```\n\n\nSet the connect timeout to seconds\n  seconds. TCP connections that take\n  longer to establish will be aborted.\n  By default, there is no connect\n  timeout, other than that implemented\n  by system libraries.\n\n\n```\n--read-timeout=seconds\n\n```\n\n\nSet the read (and write) timeout to\n  seconds seconds. The \"time\" of\n  this timeout refers to idle time: if,\n  at any point in the download, no data\n  is received for more than the\n  specified number of seconds, reading\n  fails and the download is restarted.\n  This option does not directly\n  affect the duration of the entire\n  download.\n\n\nI suppose using something like \n\n```\nwget -O - -q -t 1 --timeout=600 http://www.example.com/cron/run\n\n```\n\nshould make sure there is no timeout before longer than the duration of your script.\n(Yeah, that's probably the most brutal solution possible ^^ )"}
{"instruction": "POST request with Wget?", "output": "Wget does not support sending \"multipart/form-data\" data. --post-file is not for transmitting files as form attachments, it expects data in this form: key=value&otherkey=example. It is actually possible to POST other formats (like JSON) if you send the corresponding header.\n--post-data and --post-file work the same way: the only difference is that --post-data allows you to specify the data in the command line, while --post-file allows you to specify the path of the file that contains the data to send.\nHere's the documentation:\n\n\n```\n--post-data=string\n--post-file=file\n\n```\n\nUse POST as the method for all HTTP requests and send the specified data in the request body. --post-data sends string as data, whereas --post-file sends the contents of file. Other than that, they work in exactly the same way. In particular, they both expect content of the form key1=value1&key2=value2, with percent-encoding for special characters; the only difference is that one expects its content as a command-line parameter and the other accepts its content from a file. In particular, --post-file is not for transmitting files as form attachments: those must appear as key=value data (with appropriate percent-coding) just like everything else. Wget does not currently support multipart/form-data for transmitting POST data; only application/x-www-form-urlencoded. Only one of --post-data and --post-file should be specified.\n\nRegarding your authentication token, it should either be provided in the header, in the path of the URL, or in the data itself. This must be indicated somewhere in the documentation of the service you use. In a POST request, as in a GET request, you must specify the data using keys and values. This way the server will be able to receive multiple pieces of information with specific names. It's similar with variables.\nHence, you can't just send a magic token to the server, you also need to specify the name of the key. If the key is \"token\", then it should be token=YOUR_TOKEN.\n\n```\nwget --post-data 'user=foo&password=bar' http://example.com/auth.php\n\n```\n\nAlso, you should consider using curl if you can because it is easier to send files using it."}
{"instruction": "How to download all links to .zip files on a given web page using wget/curl?", "output": "The command is:\n\n```\nwget -r -np -l 1 -A zip http://example.com/download/\n\n```\n\nOptions meaning:\n\n```\n-r,  --recursive          specify recursive download.\n-np, --no-parent          don't ascend to the parent directory.\n-l,  --level=NUMBER       maximum recursion depth (inf or 0 for infinite).\n-A,  --accept=LIST        comma-separated list of accepted extensions.\n\n```"}
{"instruction": "Parallel wget in Bash", "output": "Much preferrable to pushing wget into the background using & or -b, you can use xargs to the same effect, and better.\nThe advantage is that xargs will synchronize properly with no extra work. Which means that you are safe to access the downloaded files (assuming no error occurs). All downloads will have completed (or failed) once xargs exits, and you know by the exit code whether all went well. This is much preferrable to busy waiting with sleep and testing for completion manually.\nAssuming that URL_LIST is a variable containing all the URLs (can be constructed with a loop in the OP's example, but could also be a manually generated list), running this:\n\n```\necho $URL_LIST | xargs -n 1 -P 8 wget -q\n\n```\n\nwill pass one argument at a time (-n 1) to wget, and execute at most 8 parallel wget processes at a time (-P 8). xarg returns after the last spawned process has finished, which is just what we wanted to know. No extra trickery needed.\nThe \"magic number\" of 8 parallel downloads that I've chosen is not set in stone, but it is probably a good compromise. There are two factors in \"maximising\" a series of downloads:\nOne is filling \"the cable\", i.e. utilizing the available bandwidth. Assuming \"normal\" conditions (server has more bandwidth than client), this is already the case with one or at most two downloads. Throwing more connections at the problem will only result in packets being dropped and TCP congestion control kicking in, and N downloads with asymptotically 1/N bandwidth each, to the same net effect (minus the dropped packets, minus window size recovery). Packets being dropped is a normal thing to happen in an IP network, this is how congestion control is supposed to work (even with a single connection), and normally the impact is practically zero. However, having an unreasonably large number of connections amplifies this effect, so it can be come noticeable. In any case, it doesn't make anything faster.  \nThe second factor is connection establishment and request processing. Here, having a few extra connections in flight really helps. The problem one faces is the latency of two round-trips (typically 20-40ms within the same geographic area, 200-300ms inter-continental) plus the odd 1-2 milliseconds that the server actually needs to process the request and push a reply to the socket. This is not a lot of time per se, but multiplied by a few hundred/thousand requests, it quickly adds up.\nHaving anything from half a dozen to a dozen requests in-flight hides most or all of this latency (it is still there, but since it overlaps, it does not sum up!). At the same time, having only a few concurrent connections does not have adverse effects, such as causing excessive congestion, or forcing a server into forking new processes."}
{"instruction": "Unable to establish SSL connection, how do I fix my SSL cert?", "output": "SSL23_GET_SERVER_HELLO:unknown protocol\n\nThis error happens when OpenSSL receives something other than a ServerHello in a protocol version it understands from the server.  It can happen if the server answers with a plain (unencrypted) HTTP.  It can also happen if the server only supports e.g. TLS 1.2 and the client does not understand that protocol version.  Normally, servers are backwards compatible to at least SSL 3.0 / TLS 1.0, but maybe this specific server isn't (by implementation or configuration).\nIt is unclear whether you attempted to pass --no-check-certificate or not.  I would be rather surprised if that would work.\nA simple test is to use wget (or a browser) to request http://example.com:443 (note the http://, not https://); if it works, SSL is not enabled on port 443.  To further debug this, use openssl s_client with the -debug option, which right before the error message dumps the first few bytes of the server response which OpenSSL was unable to parse.  This may help to identify the problem, especially if the server does not answer with a ServerHello message.  To see what exactly OpenSSL is expecting, check the source: look for SSL_R_UNKNOWN_PROTOCOL in ssl/s23_clnt.c.\nIn any case, looking at the apache error log may provide some insight too."}
{"instruction": "Spider a Website and Return URLs Only", "output": "The absolute last thing I want to do is download and parse all of the content myself (i.e. create my own spider). Once I learned that Wget writes to stderr by default, I was able to redirect it to stdout and filter the output appropriately.\n\n```\nwget --spider --force-html -r -l2 $url 2>&1 \\\n  | grep '^--' | awk '{ print $3 }' \\\n  | grep -v '\\.\\(css\\|js\\|png\\|gif\\|jpg\\)$' \\\n  > urls.m3u\n\n```\n\nThis gives me a list of the content resource (resources that aren't images, CSS or JS source files) URIs that are spidered. From there, I can send the URIs off to a third party tool for processing to meet my needs.\nThe output still needs to be streamlined slightly (it produces duplicates as it's shown above), but it's almost there and I haven't had to do any parsing myself."}
{"instruction": "get file size of a file to wget before wget-ing it?", "output": "Hmm.. for me --spider does display the size:\n\n```\n$ wget --spider http://henning.makholm.net/\nSpider mode enabled. Check if remote file exists.\n--2011-08-08 19:39:48--  http://henning.makholm.net/\nResolving henning.makholm.net (henning.makholm.net)... 85.81.19.235\nConnecting to henning.makholm.net (henning.makholm.net)|85.81.19.235|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 9535 (9.3K) [text/html]     <-------------------------\nRemote file exists and could contain further links,\nbut recursion is disabled -- not retrieving.\n\n$ \n\n```\n\n(But beware that not all web servers will inform clients of the length of the data except by closing the connection when it's all been sent.)\nIf you're concerned about wget changing the format it reports the length in, you might use wget --spider --server-response and look for a Content-Length header in the output."}
{"instruction": "How do I mirror a directory with wget without creating parent directories?", "output": "For a path like: ftp.site.com/a/b/c/d\n-nH would download all files to the directory a/b/c/d in the current directory, and -nH --cut-dirs=3 would download all files to the directory d in the current directory."}
{"instruction": "Download a file from google drive using wget", "output": "Insert your file ID into this URL (https://drive.google.com/uc?export=download&id=), then surround the URL with quotes so that Bash doesn't misinterpret the &, like so:\nwget \"https://drive.google.com/uc?export=download&id=0Bz7KyqmuGsilT0J5dmRCM0ROVHc\"\nReference here.\n\nWhen downloading big files, Google Drive adds a security warning that breaks the script above. In that case, you can download the file using:\n\n```\nwget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=FILEID' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=FILEID\" -O FILENAME && rm -rf /tmp/cookies.txt\n\n```\n\n(Script taken from here)"}
{"instruction": "How to download a file into a directory using curl or wget?", "output": "The following line will download all the files to a directory mentioned by you.\n\n```\nwget -P /home/test www.xyz.com\n\n```\n\nHere the files will be downloaded to /home/test directory"}
{"instruction": "Sites not accepting wget user agent header", "output": "It seems Yahoo server does some heuristic based on User-Agent in a case Accept header is set to */*. \n\nAccept: text/html\n\ndid the trick for me. \ne.g.\n\n```\nwget  --header=\"Accept: text/html\" --user-agent=\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.8; rv:21.0) Gecko/20100101 Firefox/21.0\"  http://yahoo.com\n\n```\n\nNote: if you don't declare Accept header then wget automatically adds Accept:*/* which means give me anything you have."}
{"instruction": "wget ssl alert handshake failure", "output": "It works from here with same OpenSSL version, but a newer version of wget (1.15). Looking at the Changelog there is the following significant change regarding your problem:\n\n1.14: Add support for TLS Server Name Indication.\n\nNote that this site does not require SNI. But www.coursera.org requires it.\nAnd if you would call wget with -v --debug (as I've explicitly recommended in my comment!) you will see:\n\n```\n$ wget https://class.coursera.org\n...\nHTTP request sent, awaiting response...\n  HTTP/1.1 302 Found\n...\nLocation: https://www.coursera.org/ [following]\n...\nConnecting to www.coursera.org (www.coursera.org)|54.230.46.78|:443... connected.\nOpenSSL: error:14077410:SSL routines:SSL23_GET_SERVER_HELLO:sslv3 alert handshake failure\nUnable to establish SSL connection.\n\n```\n\nSo the error actually happens with www.coursera.org and the reason is missing support for SNI. You need to upgrade your version of wget."}
{"instruction": "How to download a Google Drive url via curl or wget", "output": "How about this method? When the file is such large size, Google returns a code for downloading the file. You can download the file using the code. When such large file is downloaded using curl, you can see the code as follows.\n\n```\n<a id=\"uc-download-link\" class=\"goog-inline-block jfk-button jfk-button-action\" href=\"/uc?export=download&amp;confirm=ABCD&amp;id=### file ID ###\">download</a>\n\n```\n\nThe query with confirm=ABCD is important for downloading the file. This code is also included in the cookie. At the cookie, you can see it as follows.\n\n```\n#HttpOnly_.drive.google.com TRUE    /uc TRUE    #####   download_warning_#####  ABCD\n\n```\n\nIn this case, \"ABCD\" is the code. In order to retrieve the code from the cookie and download the file, you can use the following script.\nSample script :\n\n```\n#!/bin/bash\nfileid=\"### file id ###\"\nfilename=\"MyFile.csv\"\ncurl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=${fileid}\" > /dev/null\ncurl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=${fileid}\" -o ${filename}\n\n```\n\nIf this was not useful for you, I'm sorry.\n\nUpdated at February 17, 2022\nRecently, it seems that the specification of this flow has been changed. So I updated this answer. In order to download a publicly shared file of large size from Google Drive, you can use the following script.\n\n```\n#!/bin/bash\nfileid=\"### file id ###\"\nfilename=\"MyFile.csv\"\nhtml=`curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=${fileid}\"`\ncurl -Lb ./cookie \"https://drive.google.com/uc?export=download&`echo ${html}|grep -Po '(confirm=[a-zA-Z0-9\\-_]+)'`&id=${fileid}\" -o ${filename}\n\n```\n\n\nIn this case, the ID for downloading is retrieved from the HTML data as follows.\n\n```\n<form id=\"downloadForm\" action=\"https://drive.google.com/uc?export=download&amp;id={fileId}&amp;confirm={value for downloading}\" method=\"post\">\n\n```\n\n\nWhen you want to download a publicly shared file of small size from Google Drive, you can use the following command.\n\n```\ncurl -L \"https://drive.google.com/uc?export=download&id=### fileId ###\" -o sampleoutput.csv\n\n```\n\n\n\nUpdated at January 21, 2024\nFrom the following @excellproj 's comment,\n\nJanuary 2024\ncurl -L \"https://drive.usercontent.google.com/download?id=${fileId}&export=download&confirm=t\" -o \"file.zip\"\nworking great for me. Small and large files\n\nI checked this endpoint. By this, the following result is obtained. In the current stage, the following endpoint can be used.\n\n```\nhttps://drive.usercontent.google.com/download?id={fileId}&confirm=xxx\n\n```\n\nIt seems that in the current stage, various values can be used to xxx of confirm=xxx. So, even confirm=xxx and confirm=yy and confirm=z can be used.\nThe sample curl command is as follows.\n\n```\ncurl \"https://drive.usercontent.google.com/download?id={fileId}&confirm=xxx\" -o filename\n\n```\n\nAnd/or,\n\n```\ncurl -L \"https://drive.usercontent.google.com/download?id={fileId}&confirm=xxx\" -o filename\n\n```"}
{"instruction": "How to unzip a piped zip file (from &quot;wget -qO-&quot;)?", "output": "```\nwget -q -O tmp.zip http://downloads.wordpress.org/plugin/akismet.2.5.3.zip && unzip tmp.zip && rm tmp.zip\n\n```"}
{"instruction": "What headers are automatically sent by wget?", "output": "Using the -d (--debug) option I see it set:\n\n```\n---request begin---\nGET / HTTP/1.0\nUser-Agent: Wget/1.12 (cygwin)\nAccept: */*\nHost: www.uml.edu\nConnection: Keep-Alive\n\n---request end---\n\n```"}
{"instruction": "What is the correct wget command syntax for HTTPS with username and password?", "output": "By specifying the option --user and --ask-password wget will ask for the credentials. Below is an example. Change the username and download link to your needs.\n\n```\nwget --user=username --ask-password https://xyz.com/changelog-6.40.txt\n\n```"}
{"instruction": "Using WGET to run a cronjob PHP", "output": "You could tell wget to not download the contents in a couple of different ways:\n\n```\nwget --spider http://www.example.com/cronit.php\n\n```\n\nwhich will just perform a HEAD request but probably do what you want\n\n```\nwget -O /dev/null http://www.example.com/cronit.php\n\n```\n\nwhich will save the output to /dev/null (a black hole)\nYou might want to look at wget's -q switch too which prevents it from creating output\nI think that the best option would probably be:\n\n```\nwget -q --spider http://www.example.com/cronit.php\n\n```\n\nthat's unless you have some special logic checking the HTTP method used to request the page"}
{"instruction": "wget - Download a sub directory", "output": "You can do:\nwget -r -l1 --no-parent http://www.domain.com/subdirectory/\nwhere:\n\n```\n-r: recursive retrieving\n-l1: sets the maximum recursion depth to be 1\n--no-parent: does not ascend to the parent; only downloads from the specified subdirectory and downwards hierarchy\n\n```"}
{"instruction": "How can I make a Docker healthcheck with wget instead of curl?", "output": "The following seems to be the equivalent:\n\n```\nHEALTHCHECK  --interval=5m --timeout=3s \\\n  CMD wget --no-verbose --tries=1 --spider http://localhost/ || exit 1\n\n```\n\nWhere:\n\n--no-verbose - Turn off verbose without being completely quiet (use -q for that), which means that error messages and basic information still get printed.\n--tries=1 - If not set, some wget implementations will retry indefinitely when HTTP 200 response is not returned.\n--spider - Behave as a Web spider, which means that it will not download the pages, just check that they are there.\nexit 1 - Ensures exit code 1 on failure. Heathcheck only expects the following:\n\n0: success - the container is healthy and ready for use\n1: unhealthy - the container is not working correctly\n2: reserved - do not use this exit code\n\n\n\nDocker compose example:\n\n```\nhealthcheck:\n   test: wget --no-verbose --tries=1 --spider http://localhost || exit 1\n   interval: 5m\n   timeout: 3s\n   retries: 3\n   start_period: 2m\n\n```\n\nhttps://docs.docker.com/compose/compose-file/compose-file-v3/#healthcheck"}
{"instruction": "How can I download a file from an S3 bucket with wget?", "output": "Got it ... If you upload a file in an S3 bucket with S3CMD with the --acl public flag then one shall be able to download the file from S3 with wget easily ... \nConclusion: In order to download with wget, first of one needs to upload the content in S3 with s3cmd put --acl public --guess-mime-type <test_file> s3://test_bucket/test_file\nalternatively you can try:\n\n```\ns3cmd setacl --acl-public --guess-mime-type s3://test_bucket/test_file\n\n```\n\nnotice the setacl flag above. THAT WILL set the file in s3 accessible publicly\nthen you can execute the wget http://s3.amazonaws.com/test_bucket/test_file"}
{"instruction": "Check wget&#39;s return value", "output": "$r is the text output of wget (which you've captured with backticks). To access the return code, use the $? variable."}
{"instruction": "BASH script: Downloading consecutive numbered files with wget", "output": "```\n#!/bin/sh\n\nif [ $# -lt 3 ]; then\n        echo \"Usage: $0 url_format seq_start seq_end [wget_args]\"\n        exit\nfi\n\nurl_format=$1\nseq_start=$2\nseq_end=$3\nshift 3\n\nprintf \"$url_format\\\\n\" `seq $seq_start $seq_end` | wget -i- \"$@\"\n\n```\n\nSave the above as seq_wget, give it execution permission (chmod +x seq_wget), and then run, for example:\n\n```\n\n$ ./seq_wget http://someaddress.com/logs/dbsclog01s%03d.log 1 50\n\n```\n\nOr, if you have Bash 4.0, you could just type\n\n```\n\n$ wget http://someaddress.com/logs/dbsclog01s{001..050}.log\n\n```\n\nOr, if you have curl instead of wget, you could follow Dennis Williamson's answer."}
{"instruction": "Unable to establish SSL connection upon wget on Ubuntu 14.04 LTS", "output": "... right now it happens only to the website I'm testing. I can't post it here because it's confidential. \n\nThen I guess it is one of the sites which is incompatible with TLS1.2. The openssl as used in 12.04 does not use TLS1.2 on the client side while with 14.04 it uses TLS1.2 which might explain the difference. To work around try to explicitly use \n--secure-protocol=TLSv1. If this does not help check if you can access the site with openssl s_client -connect ... (probably not) and with openssl s_client -tls1 -no_tls1_1, -no_tls1_2 ....\nPlease note that it might be other causes, but this one is the most probable and without getting access to the site everything is just speculation anyway.\nThe assumed problem in detail: Usually clients use the most compatible handshake to access a server. This is the SSLv23 handshake which is compatible to older SSL versions but announces the best TLS version the client supports, so that the server can pick the best version. In this case wget would announce TLS1.2. But there are some broken servers which never assumed that one day there would be something like TLS1.2 and which refuse the handshake if the client announces support for this hot new version (from 2008!) instead of just responding with the best version the server supports. To access these broken servers the client has to lie and claim that it only supports TLS1.0 as the best version. \n\nIs Ubuntu 14.04 or wget 1.15 not compatible with TLS 1.0 websites? Do I need to install/download any library/software to enable this connection?\n\nThe problem is the server, not the client.\nMost browsers work around these broken servers by retrying with a lower version. Most other applications fail permanently if the first connection attempt fails, i.e. they don't downgrade by itself and one has to enforce another version by some application specific settings."}
{"instruction": "Why does wget ignore the query string in the URL?", "output": "& is a special character in most shell environments. You can use double quotes to quote the URL to pass the whole thing in as the parameter to wget:\n\n```\nwget \"http://www.ted.com/talks/quick-list?sort=date&order=desc&page=18\"\n\n```"}
{"instruction": "Downloading a file with wget using multiple connections", "output": "use aria2\n\n```\n aria2c -x 16 [url] #where 16 is the number of connections\n\n```\n\nOR\nJust repeat the wget -r -np -N [url] for as many threads as you need. This isn‚Äôt pretty and there are surely better ways to do this, but if you want something quick and dirty it should do the trick."}
{"instruction": "wget -O for non-existing save path?", "output": "Try curl\n\n```\ncurl http://www.site.org/image.jpg --create-dirs -o /path/to/save/images.jpg\n\n```"}
{"instruction": "How to ignore specific type of files to download in wget?", "output": "Use the\n\n```\n --reject jpg,png  --accept html\n\n```\n\noptions to exclude/include files with certain extensions, see http://www.gnu.org/software/wget/manual/wget.html#Recursive-Accept_002fReject-Options. \nPut patterns with wildcard characters in quotes, otherwise your shell will expand them, see http://www.gnu.org/software/wget/manual/wget.html#Types-of-Files"}
{"instruction": "Is it possible to specify a different ssh port when using rsync?", "output": "Another option, in the host you run rsync from, set the port in the ssh config file, ie:\n\n```\ncat ~/.ssh/config\nHost host\n    Port 2222\n\n```\n\nThen rsync over ssh will talk to port 2222:\n\n```\nrsync -rvz --progress --remove-sent-files ./dir user@host:/path\n\n```"}
{"instruction": "Copying files using rsync from remote server to local machine", "output": "From your local machine:\n\n```\nrsync -chavzP --stats user@remote.host:/path/to/copy /path/to/local/storage\n\n```\n\nFrom your local machine with a non standard ssh port:\n\n```\nrsync -chavzP -e \"ssh -p $portNumber\" user@remote.host:/path/to/copy /local/path\n\n```\n\nOr from the remote host, assuming you really want to work this way and your local machine is listening on SSH:\n\n```\nrsync -chavzP --stats /path/to/copy user@host.remoted.from:/path/to/local/storage\n\n```\n\nSee man rsync for an explanation of my usual switches."}
{"instruction": "How does `scp` differ from `rsync`?", "output": "The major difference between these tools is how they copy files.\nscp basically reads the source file and writes it to the destination. It performs a plain linear copy, locally, or over a network.\nrsync also copies files locally or over a network. But it employs a special delta transfer algorithm and a few optimizations to make the operation a lot faster. Consider the call.\n\n```\nrsync A host:B\n\n```\n\n\nrsync will check files sizes and modification timestamps of both A and B, and skip any further processing if they match.\n\nIf the destination file B already exists, the delta transfer algorithm will make sure only differences between A and B are sent over the wire.\n\nrsync will write data to a temporary file T, and then replace the destination file B with T to make the update look \"atomic\" to processes that might be using B.\n\n\nAnother difference between them concerns invocation. rsync has a plethora of command line options, allowing the user to fine tune its behavior. It supports complex filter rules, runs in batch mode, daemon mode, etc. scp has only a few switches.\nIn summary, use scp for your day to day tasks. Commands that you type once in a while on your interactive shell. It's simpler to use, and in those cases rsync optimizations won't help much.\nFor recurring tasks, like cron jobs, use rsync. As mentioned, on multiple invocations it will take advantage of data already transferred, performing very quickly and saving on resources. It is an excellent tool to keep two directories synchronized over a network.\nAlso, when dealing with large files, use rsync with the -P option. If the transfer is interrupted, you can resume it where it stopped by reissuing the command. See Sid Kshatriya's answer.\nFinally, note that rsync:// the protocol is similar to plain HTTP: unencrypted and no integrity checks. Be sure to always use rsync via SSH (as in the examples from the question above), not via the rsync protocol, unless you really know what you're doing. scp will always use SSH as underlying transport mechanism which has both integrity and confidentiality guarantees, so that is another difference between the two utilities."}
{"instruction": "rsync error: failed to set times on &quot;/foo/bar&quot;: Operation not permitted", "output": "If /foo/bar is on NFS (or possibly some FUSE filesystem), that might be the problem.\nEither way, adding -O / --omit-dir-times to your command line will avoid it trying to set modification times on directories."}
{"instruction": "How can I configure rsync to create target directory on remote server?", "output": "If you have more than the last leaf directory to be created, you can either run a separate ssh ... mkdir -p first, or use the --rsync-path trick as explained here :\n\n```\nrsync -a --rsync-path=\"mkdir -p /tmp/x/y/z/ && rsync\" $source user@remote:/tmp/x/y/z/\n\n```\n\nOr use the --relative option as suggested by Tony. In that case, you only specify the root of the destination, which must exist, and not the directory structure of the source, which will be created:\n\n```\nrsync -a --relative /new/x/y/z/ user@remote:/pre_existing/dir/\n\n```\n\nThis way, you will end up with /pre_existing/dir/new/x/y/z/\nAnd if you want to have \"y/z/\" created, but not inside \"new/x/\", you can add ./ where you want --relativeto begin:\n\n```\nrsync -a --relative /new/x/./y/z/ user@remote:/pre_existing/dir/\n\n```\n\nwould create /pre_existing/dir/y/z/."}
{"instruction": "rsync copy over only certain types of files using include option", "output": "I think --include is used to include a subset of files that are otherwise excluded by --exclude, rather than including only those files.\nIn other words: you have to think about include meaning don't exclude.\nTry instead:\n\n```\nrsync -zarv --include=\"*/\" --include=\"*.sh\" --exclude=\"*\" \"$from\" \"$to\" \n\n```\n\nFor rsync versions below 3.0.6, the order needs to be modified as follows (see comments):\n\n```\nrsync -zarv  --include \"*/\" --exclude=\"*\" --include=\"*.sh\" \"$from\" \"$to\"\n\n```\n\nAdding the -m flag will avoid creating empty directory structures in the destination. Tested in  version 3.1.2.\nSo if we only want *.sh files we have to exclude all files --exclude=\"*\", include all directories --include=\"*/\" and include all *.sh files --include=\"*.sh\".\nYou can find some good examples in the section Include/Exclude Pattern Rules of the man page"}
{"instruction": "Rsync copy directory contents but not directory itself", "output": "rsync interprets a directory with no trailing slash as copy this directory, and a directory with a trailing slash as copy the contents of this directory.\nTry rsync -av ~/foo/ user@remote.com:/var/www/bar/"}
{"instruction": "How to resume scp with partially copied files?", "output": "You should use rsync over ssh\n\n```\nrsync -P -e ssh remoteuser@remotehost:/remote/path /local/path\n\n```\n\nThe key option is -P, which is the same as --partial --progress\n\nBy default, rsync will delete any partially transferred file if the transfer is interrupted. In some circumstances it is more desirable to keep partially transferred files. Using the --partial option tells rsync to keep the partial file which should make a subsequent transfer of the rest of the file much faster.\n\nOther options, such -a (for archive mode), and -z (to enable compression) can also be used.\nThe manual: https://download.samba.org/pub/rsync/rsync.html"}
{"instruction": "rsync: difference between --size-only and --ignore-times", "output": "There are several ways rsync compares files -- the authoritative source is the rsync algorithm description: https://www.andrew.cmu.edu/course/15-749/READINGS/required/cas/tridgell96.pdf. The wikipedia article on rsync is also very good.\nFor local files, rsync compares metadata and if it looks like it doesn't need to copy the file because size and timestamp match between source and destination it doesn't look further. If they don't match, it cp's the file. However, what if the metadata do match but files aren't actually the same? Then rsync probably didn't do what you intended.\nFiles that are the same size may still have changed. One simple example is a text file where you correct a typo -- like changing \"teh\" to \"the\". The file size is the same, but the corrected file will have a newer timestamp. --size-only says \"don't look at the time; if size matches assume files match\", which would be the wrong choice in this case.\nOn the other hand, suppose you accidentally did a big cp -r A B yesterday, but you forgot to preserve the time stamps, and now you want to do the operation in reverse rsync B A. All those files you cp'ed have yesterday's time stamp, even though they weren't really modified yesterday, and rsync will by default end up copying all those files, and updating the timestamp to yesterday too. --size-only may be your friend in this case (modulo the example above).\n--ignore-times says to compare the files regardless of whether the files have the same modify time. Consider the typo example above, but then not only did you correct the typo but you used touch to make the corrected file have the same modify time as the original file -- let's just say you're sneaky that way. Well --ignore-times will do a diff of the files even though the size and time match."}
{"instruction": "What is the simplest way to remove a trailing slash from each parameter?", "output": "You can use the ${parameter%word} expansion that is detailed here.  Here is a simple test script that demonstrates the behavior:\n\n```\n#!/bin/bash\n\n# Call this as:\n#   ./test.sh one/ two/ three/ \n#\n# Output:\n#  one two three\n\necho ${@%/}\n\n```"}
{"instruction": "How to pass password automatically for rsync SSH command?", "output": "You should use a keyfile without passphrase for scripted ssh logins. This is obviously a security risk, take care that the keyfile itself is adequately secured.\nFrom Instructions for setting up passwordless ssh access:\n\nRun ssh-keygen\nIt will ask for a path to the key file, a passphrase, and a repeat of the same passphrase. Answer all three by just pressing Enter (accepting the defaults).\n\nRun ssh-copy-id -i ~/.ssh/id_rsa.pub <remote-host>\nThe path ~/.ssh/id_rsa.pub to the public key file may be different if you chose another path in step 1. Replace <remote-host> with the IP or hostname of the remote host you want to log in to.\n\nRun ssh <remote-host>\nThe remote host should not ask for a password, and you should be logged in to the remote host."}
{"instruction": "What does f+++++++++ mean in rsync logs?", "output": "Let's take a look at how rsync works and better understand the cryptic result lines:\n1 - A huge advantage of rsync is that after an interruption the next time it continues smoothly.\nThe next rsync invocation will not transfer the files again, that it had already transferred, if they were not changed in the meantime. But it will start checking all the files again from the beginning to find out, as it is not aware that it had been interrupted.\n2 - Each character is a code that can be translated if you read the section for -i, --itemize-changes in man rsync\nDecoding your example log file from the question:\n>f.st......\n\n```\n> - the item is received\nf - it is a regular file\ns - the file size is different\nt - the time stamp is different\n\n```\n\n.d..t......\n\n```\n. - the item is not being updated (though it might have attributes \n    that are being modified)\nd - it is a directory\nt - the time stamp is different\n\n```\n\n>f+++++++++\n\n```\n> - the item is received\nf - a regular file\n+++++++++ - this is a newly created item\n\n```\n\n\nThe relevant part of the rsync man page:\n\n-i, --itemize-changes\nRequests a simple itemized list of the changes that are being made to\neach file, including attribute changes. This is exactly the same as\nspecifying --out-format='%i %n%L'. If you repeat the option, unchanged\nfiles will also be output, but only if the receiving rsync is at least\nversion 2.6.7 (you can use -vv with older versions of rsync, but that\nalso turns on the output of other verbose messages).\nThe \"%i\" escape has a cryptic output that is 11 letters long. The\ngeneral format is like the string YXcstpoguax, where Y is replaced by\nthe type of update being done, X is replaced by the file-type, and the\nother letters represent attributes that may be output if they are\nbeing modified.\nThe update types that replace the Y are as follows:\n\nA < means that a file is being transferred to the remote host (sent).\nA > means that a file is being transferred to the local host (received).\nA c means that a local change/creation is occurring for the item (such as the creation of a directory or the changing of a symlink,\netc.).\nA h means that the item is a hard link to another item (requires --hard-links).\nA . means that the item is not being updated (though it might have attributes that are being modified).\nA * means that the rest of the itemized-output area contains a message (e.g. \"deleting\").\n\nThe file-types that replace the X are: f for a file, a d for a\ndirectory, an L for a symlink, a D for a device, and a S for a\nspecial file (e.g. named sockets and fifos).\nThe other letters in the string above are the actual letters that will\nbe output if the associated attribute for the item is being updated or\na \".\" for no change. Three exceptions to this are: (1) a newly created\nitem replaces each letter with a \"+\", (2) an identical item replaces\nthe dots with spaces, and (3) an unknown attribute replaces each\nletter with a \"?\" (this can happen when talking to an older rsync).\nThe attribute that is associated with each letter is as follows:\n\nA c means either that a regular file has a different checksum (requires --checksum) or that a symlink, device, or special file has a\nchanged value. Note that if you are sending files to an rsync prior to\n3.0.1, this change flag will be present only for checksum-differing regular files.\nA s means the size of a regular file is different and will be updated by the file transfer.\nA t means the modification time is different and is being updated to the sender‚Äôs value (requires --times). An alternate value of T\nmeans that the modification time will be set to the transfer time,\nwhich happens when a file/symlink/device is updated without --times\nand when a symlink is changed and the receiver can‚Äôt set its time.\n(Note: when using an rsync 3.0.0 client, you might see the s flag\ncombined with t instead of the proper T flag for this time-setting\nfailure.)\nA p means the permissions are different and are being updated to the sender‚Äôs value (requires --perms).\nAn o means the owner is different and is being updated to the sender‚Äôs value (requires --owner and super-user privileges).\nA g means the group is different and is being updated to the sender‚Äôs value (requires --group and the authority to set the group).\nThe u slot is reserved for future use.\nThe a means that the ACL information changed.\nThe x means that the extended attribute information changed.\n\nOne other output is possible: when deleting files, the \"%i\" will\noutput the string \"*deleting\" for each item that is being removed\n(assuming that you are talking to a recent enough rsync that it logs\ndeletions instead of outputting them as a verbose message)."}
{"instruction": "How to rsync only a specific list of files?", "output": "Edit: atp's answer below is better.  Please use that one!\nYou might have an easier time, if you're looking for a specific list of files, putting them directly on the command line instead:\n\n```\n# rsync -avP -e ssh `cat deploy/rsync_include.txt` root@0.0.0.0:/var/www/\n\n```\n\nThis is assuming, however, that your list isn't so long that the command line length will be a problem and that the rsync_include.txt file contains just real paths (i.e. no comments, and no regexps)."}
{"instruction": "How do you use an identity file with rsync?", "output": "You may want to use ssh-agent and ssh-add to load the key into memory.  ssh will try identities from ssh-agent automatically if it can find them. Commands would be\n\n```\neval $(ssh-agent) # Create agent and environment variables\nssh-add ~/.ssh/1234-identity\n\n```\n\nssh-agent is a user daemon which holds unencrypted ssh keys in memory.  ssh finds it based on environment variables which ssh-agent outputs when run.  Using eval to evaluate this output creates the environment variables.  ssh-add is the command which manages the keys memory.  The agent can be locked using ssh-add.  A default lifetime for a key can be specified when ssh-agent is started, and or specified for a key when it is added.\nYou might also want to setup a ~/.ssh/config file to supply the port and key definition. (See `man ssh_config for more options.)\n\n```\nhost 22.33.44.55\n    IdentityFile ~/.ssh/1234-identity\n    Port 1234\n\n```\n\nSingle quoting the ssh command will prevent shell expansion which is needed for ~ or $HOME.  You could use the full or relative path to the key in single quotes."}
{"instruction": "How to RSYNC a single file?", "output": "You do it the same way as you would a directory, but you specify the full path to the filename as the source. In your example:\n\n```\nrsync -avz --progress /var/www/public_html/.htaccess root@<remote-ip>:/var/www/public_html/\n\n```\n\nAs mentioned in the comments: since -a includes recurse, one little typo can make it kick off a full directory tree transfer, so a more fool-proof approach might to just use -vz, or replace it with -lptgoD."}
{"instruction": "How do I synchronize in both directions?", "output": "Just run it twice, with \"newer\" mode (-u or --update flag) plus -t (to copy file modified time), -r (for recursive folders), and -v (for verbose output to see what it is doing):\n\n```\nrsync -rtuv /path/to/dir_a/* /path/to/dir_b\nrsync -rtuv /path/to/dir_b/* /path/to/dir_a\n\n```\n\nThis won't handle deletes, but I'm not sure there is a good solution to that problem with only periodic sync'ing."}
{"instruction": "Copy or rsync command", "output": "Rsync is better since it will only copy only the updated parts of the updated file, instead of the whole file. It also uses compression and encryption if you want. Check out this tutorial."}
{"instruction": "Using Rsync include and exclude options to include directory and file by pattern", "output": "The problem is that --exclude=\"*\" says to exclude (for example) the 1260000000/ directory, so rsync never examines the contents of that directory, so never notices that the directory contains files that would have been matched by your --include.\nI think the closest thing to what you want is this:\n\n```\nrsync -nrv --include=\"*/\" --include=\"file_11*.jpg\" --exclude=\"*\" /Storage/uploads/ /website/uploads/\n\n```\n\n(which will include all directories, and all files matching file_11*.jpg, but no other files), or maybe this:\n\n```\nrsync -nrv --include=\"/[0-9][0-9][0-9]0000000/\" --include=\"file_11*.jpg\" --exclude=\"*\" /Storage/uploads/ /website/uploads/\n\n```\n\n(same concept, but much pickier about the directories it will include).\nIn either case, note that the --include=... option needs to come before the --exclude=... option, because we need the former to take precedence over the latter when a file matches both patterns."}
{"instruction": "Symbolic links and synced folders in Vagrant", "output": "Virtualbox does not allow symlinks on shared folders for security reasons. To enable symlinks the following line needs to be added to the vm provider config block in the Vagrantfile:\n\n```\nconfig.vm.provider \"virtualbox\" do |v|\n    v.customize [\"setextradata\", :id, \"VBoxInternal2/SharedFoldersEnableSymlinksCreate/v-root\", \"1\"]\nend\n\n```\n\nAdditionally, on windows vagrant up needs to be executed in a shell with admin rights. \nNo workarounds necessary."}
{"instruction": "Two way sync with rsync", "output": "Try Unison: http://www.cis.upenn.edu/~bcpierce/unison/\nSyntax:\n\n```\nunison dirA/ dirB/\n\n```\n\nUnison asks what to do when files are different, but you can automate the process by using the following which accepts default (nonconflicting) options:\nunison -auto dirA/ dirB/\nunison -batch dirA/ dirB/ asks no questions at all, and writes to output how many files were ignored (because they conflicted).\nNote: I am no longer using Unison (I use NextCloud, which doesn't address the original use case). However, note that rsync is not designed for bidirectional sync, while unison is. unison may have its bugs (as any other piece of software) and its wrinkles. I am surprised it seems to be actively maintained now (last time I looked I think I thought it looked dead), but I'm not sure what's the state nowadays. I haven't had the need to have a two-way file synchronizer, so there may be better options, though.\nNote 2: I am now currently using Mutagen for some purposes. It still doesn't cover the question's use case completely, but it does implement two-way sync."}
{"instruction": "Slashes and the rsync command", "output": "It's described in the rsync(1) manpage:\n\nA trailing slash on the source changes this behavior to avoid creating an additional directory level at the destination.  You can think of a\ntrailing  /  on  a source as meaning \"copy the contents of this directory\" as opposed to \"copy the directory by name\", but in both cases the\nattributes of the containing directory are transferred to the containing directory on the destination.  In other words, each of the  following commands copies the files in the same way, including their setting of the attributes of /dest/foo:\n\n```\nrsync -av /src/foo /dest\nrsync -av /src/foo/ /dest/foo\n\n```\n\n\nAs to the destination, I don't think it has any major consequences. There is a difference if the source is a file and destination doesn't exist ‚Äî this will make a copy of SRC called DEST:\n\n```\nrsync SRC DEST\n\n```\n\n, whereas, this will create directory DEST and copy the SRC file into it:\n\n```\nrsync SRC DEST/\n\n```"}
{"instruction": "Rsync to Amazon Ec2 Instance", "output": "I just received that same error. I had been consistently able to ssh with:\n\n```\nssh -i ~/path/mykeypair.pem \\\nubuntu@ec2-XX-XXX-XXX-XXX.us-west-2.compute.amazonaws.com\n\n```\n\nBut when using the longer rsync construction, it seemed to cause errors. I ended up encasing the ssh statement in quotations and using the full path to the key. In your example:\n\n```\nrsync -avL --progress -e \"ssh -i /path/to/mykeypair.pem\" \\\n       ~/Sites/my_site/* \\ \n       root@ec2-XX-XXX-XXX-XXX.us-west-2.compute.amazonaws.com:/var/www/html/\n\n```\n\nThat seemed to do the trick."}
{"instruction": "rsync delete files on sending side after transfer", "output": "You need to pass the --remove-source-files option to the rsync command. It tells rsync to remove from the sending side the files (meaning non-directories) that are a part of the transfer and have been successfully duplicated on the receiving side. Do not pass the --delete option to rsync command as it delete extraneous files from destination directory.\nDelete source after successful transfer using rsync.\nThe syntax is:\n\n```\nrsync --remove-source-files -options /path/to/src/ /path/to/dest\nrsync --remove-source-files -options /path/to/src/ computerB:/path/to/dest\nrsync --remove-source-files -av /path/to/src/*.avi computerB:/path/to/dest\n\n```\n\nReference : http://www.cyberciti.biz/faq/linux-unix-bsd-appleosx-rsync-delete-file-after-transfer/"}
{"instruction": "Recursive copy of a specific file type maintaining the file structure in Unix/Linux?", "output": "rsync is useful for local file copying as well as between machines. This will do what you want:\nrsync -avm --include='*.jar' -f 'hide,! */' . /destination_dir\nThe entire directory structure from . is copied to /destination_dir, but only the .jar files are copied. The -a ensures all permissions and times on files are unchanged. The -m will omit empty directories. -v is for verbose output. \nFor a dry run add a -n, it will tell you what it would do but not actually copy anything."}
{"instruction": "Why is this rsync connection unexpectedly closed on Windows?", "output": "i get the solution. i've using cygwin and this is the problem the rsync command for Windows work only in windows shell and works in the windows powershell.\nA few times it has happened the same error between two linux boxes. and appears to be by incompatible versions of rsync"}
{"instruction": "How to change the owner for a rsync", "output": "There are hacks you could put together on the receiving machine to get the ownership right -- run 'chmod -R apache /website' out of cron would be an effective but pretty kludgey option -- but instead, I'd recommend securely allowing rsync-over-ssh-as-apache.\nYou'd create a dedicated ssh keypair for this: \n\n```\nssh-keygen -f ~/.ssh/apache-rsync\n\n```\n\nand then take ~/.ssh/apache-rsync.pub over to the webserver, where you'd put it into ~apache/.ssh/authorized_keys and carefully specify the allowed command, something like so, all on one line:\n\n```\ncommand=\"rsync --server -vlogDtprCz --delete . /website\",from=\"IP.ADDR.OF.SENDER\",no-port-forwarding,no-X11-forwarding,no-pty ssh-rsa AAABKEYPUBTEXTsVX9NjIK59wJ+fjDgTQtGwhATsfidQbO6u77dbAjTUmWCZjKAQ/fEFWZGSlqcO2yXXXXXXXXXXVd9DSS1tjE6vAQaRdnMXBggtn4M9rnePD2qlR5QOAUUwhyFPhm6U4VFhRoa3wLvoqCVtCV0cuirB6I45On96OPijOwvAuz3KIE3+W9offomzHsljUMXXXXXXXXXXMoYLywMG/GPrZ8supIDYk57waTQWymUyRohoQqFGMzuDNbq+U0JSRlvLFoVUZ5Piz+gKJwwiFwwAW2iNag/c4Mrb/BVDQAyEQ== comment@email.address\n\n```\n\nand then your rsync command on your \"home\" machine would be something like\n\n```\nrsync -av --delete -e 'ssh -i ~/.ssh/apache-rsync apache@server' ./ /website\n\n```\n\nThere are other ways to skin this cat, but this is the clearest and involves the fewest workarounds, to my mind.  It prevents getting a shell as apache, which is the biggest security concern, natch.  If you're really deadset against allowing ssh as apache, there are  other ways ... but this is how I've done it.\nReferences here: http://ramblings.narrabilis.com/using-rsync-with-ssh, http://www.sakana.fr/blog/2008/05/07/securing-automated-rsync-over-ssh/"}
{"instruction": "How to copy a directory from local machine to remote machine", "output": "Easiest way is scp\n\n```\nscp -r /path/to/local/storage user@remote.host:/path/to/copy\n\n```\n\nrsync is best for when you want to update versions where it has been previously copied.\nIf that doesn't work, rerun with -v and see what the error is."}
{"instruction": "How to use Rsync to copy only specific subdirectories (same names in several directories)", "output": "I've found the reason. As for me - it wasn't clear that Rsync works in this way.\nSo correct command (for company1 directory only) must be:\n\n```\nrsync -avzn --list-only --include 'company1/' --include 'company1/unique_folder1/***' --exclude '*' -e ssh user@server.com:/path/to/old/data/ /path/to/new/data\n\n```\n\nI.e. we need include each parent company directory. And of course we cannot write manually all these company directories in the command line, so we save the list into the file and use it.\n\nFinal things we need to do:\n1.Generate include file on server 1, so its content will be (I've used ls and awk):\n\n```\n+ company1/  \n+ company1/unique_folder1/***  \n...  \n+ companyN/  \n+ companyN/unique_folder1/***  \n\n```\n\n2.Copy include.txt to server 2 and use such command:\n\n```\nrsync -avzn                                        \\\n      --list-only                                  \\\n      --include-from '/path/to/new/include.txt'    \\\n      --exclude '*'                                \\\n      -e ssh user@server.com:/path/to/old/data/    \\\n      /path/to/new/data\n\n```"}
{"instruction": "rsync through ssh tunnel", "output": "Here's what worked for me.\nI run a command in the background to tunnel to the remote host:\n\n```\n ssh -N -L 2222:remote.example.com:22 bridge.example.com&\n\n```\n\nthen I rsync to localhost like this:\n\n```\nrsync -auve \"ssh -p 2222\" . me@localhost:/some/path\n\n```"}
{"instruction": "Rsync on Windows: wrong permissions for created directories", "output": "The option to ignore NTFS permissions has changed in Cygwin version 1.7. This might be what's causing the problem.\nTry adding the 'noacl' flag to your Cygwin mounts in C:\\cygwin\\etc\\fstab, for example:\n\n```\nnone /cygdrive cygdrive user,noacl,posix=0 0 0\n\n```\n\nYou can pass custom permissions via rsync using the 'chmod' option:\n\n```\nrsync -rvz --chmod=ugo=rwX -e ssh source destination\n\n```"}
{"instruction": "Upgrading rsync on OS X using Homebrew", "output": "Follow the instructions here:\n\n```\nbrew tap homebrew/dupes\nbrew install rsync\n\n```\n\nAnd then edit /private/etc/paths to put /usr/local/bin before /usr/bin.\nEdit:\n\nWarning: homebrew/dupes was deprecated. This tap is now empty as all its formulae were migrated.\n\nSo, only \n\n```\nbrew install rsync\n\n```\n\nis enough."}
{"instruction": "Prevent rsync from deleting destination files that match a given pattern", "output": "You can exclude files/directories with --exclude.  This will prevent the somedir directory from being synced/deleted:\n\n```\nrsync -avrc --delete --exclude somedir source destination\n\n```"}
{"instruction": "How to automatically accept the remote key when rsyncing?", "output": "If they genuinely are new hosts, and you can't add the keys to known_hosts beforehand (see York.Sar's answer), then you can use this option:\n\n```\n-e \"ssh -o StrictHostKeyChecking=no\"\n\n```"}
{"instruction": "What do the numbers in rsync&#39;s output mean?", "output": "When the file transfer finishes, rsync\n  replaces the progress line with a\n  summary line that looks like this:\n\n```\n 1238099 100%  146.38kB/s    0:00:08  (xfer#5, to-check=169/396)\n\n```\n\nIn this example, the file was 1238099\n  bytes long in total, the average rate\n  of transfer for the whole file was\n  146.38 kilobytes per second over the 8 seconds that it took to complete, it\n  was the 5th transfer of a regular file\n  during the current rsync session, and\n  there are 169 more files for the\n  receiver to check (to see if they are\n  up-to-date or not) remaining out of\n  the 396 total files in the file-list.\n\nfrom http://samba.anu.edu.au/ftp/rsync/rsync.html under --progress switch"}
{"instruction": "rsync multiple remote directories to local machine preserving directory paths", "output": "Directly from the rsync man page:\n\n```\nThe syntax for requesting multiple files from a remote host is done\nby specifying additional remote-host args in the same style as the \nfirst, or with the hostname omitted.  For instance, all these work:\n\n    rsync -av host:file1 :file2 host:file{3,4} /dest/\n    rsync -av host::modname/file{1,2} host::modname/file3 /dest/\n    rsync -av host::modname/file1 ::modname/file{3,4}\n\n```\n\nThis means your example should have a space added before the second path:\n\n```\nrsync -e ssh root@remote.com:/path/to/file :/path/to/second/file/ /local/directory/\n\n```\n\nI'd suggest you first try it with the -n or --dry-run option, so you see what will be done, before the copy (and possible deletions) are actually performed."}
{"instruction": "compare contents of two directories on remote server using unix", "output": "You can use rsync with the -n flag to find out if the files are in sync, without actually doing a sync.\nFor example, from server1:\n\n```\nrsync -n -avrc /abc/home/sample1/* server2:/abc/home/sample2/\n\n```\n\nThis will print the names of all files (recursive, with the -r flag) that differ between server1:/abc/home/sample1/ and server2:/abc/home/sample2/\n\nrsync used parameters explanation\n-n, --dry-run - perform a trial run with no changes made\n-a, --archive - archive mode; equals -rlptgoD (no -H,-A,-X)\n-v, --verbose - increase verbosity\n-r, --recursive - recurse into directories\n-c, --checksum - skip based on checksum, not mod-time & size"}
{"instruction": "How to recursively copy directories starting with &quot;abc&quot; on Linux/Unix?", "output": "Not sure why what you're trying didn't work (but what is the \"copy\" command?), but this works on Linux at least:\n\n```\ncp -r ~/plugins/abc* ~/destination\n\n```"}
{"instruction": "Rsync Encryption", "output": "rsync performs no encryption on its own. If you don't use ssh, nor do you tunnel the rsync traffic through stunnel or some kind of VPN, then no encryption is performed. Yes, you can save some CPU cycles this way."}
{"instruction": "Bash scripting rsync: rsync: link_stat (blah) failed: No such file or directory (2)", "output": "Check that your \\ characters have no whitespace after them at the end of the line. This will cause BASH to not interpret the line wrap correctly, giving the rsync error above."}
{"instruction": "Install rsync with cygwin", "output": "Run your cygwin setup.exe again, and expand the categories to find 'rsync' and 'ssh'."}
{"instruction": "read input from a file and sync accordingly", "output": "There is more than one way to answer this question depending on how you want to copy these files. If your intent is to copy the file list with absolute paths, then it might look something like:\nrsync -av --files-from=/path/to/files.txt / /destination/path/\n...This would expect the paths to be relative to the source location of / and would retain the entire absolute structure under that destination.\nIf your goal is to copy all of those files in the list to the destination, without preserving any kind of path hierarchy (just a collection of files), then you could try one of the following:\n\n```\n# note this method might break if your file it too long and\n# exceed the maximum arg limit\nrsync -av `cat /path/to/file` /destination/\n\n# or get fancy with xargs to batch 200 of your items at a time\n# with multiple calls to rsync\ncat /path/to/file | xargs -n 200 -J % rsync -av % /destination/\n\n```\n\nOr a for-loop and copy:\n\n```\n# bash shell\nfor f in `cat /path/to/files.txt`; do cp $f /dest/; done\n\n```"}
{"instruction": "Comprehensive list of rsync error codes", "output": "Per the rsync \"man\" page, here are the error codes it could return and what they mean. If you're scripting it in bash, you could look at $?\n\n```\n 0     Success\n 1     Syntax or usage error\n 2     Protocol incompatibility\n 3     Errors selecting input/output files, dirs\n 4     Requested action not supported: an attempt was made to manipulate 64-bit\n       files on a platform that cannot support them; or an option was specified\n       that is supported by the client and not by the server.\n 5     Error starting client-server protocol\n 6     Daemon unable to append to log-file\n10     Error in socket I/O\n11     Error in file I/O\n12     Error in rsync protocol data stream\n13     Errors with program diagnostics\n14     Error in IPC code\n20     Received SIGUSR1 or SIGINT\n21     Some error returned by waitpid()\n22     Error allocating core memory buffers\n23     Partial transfer due to error\n24     Partial transfer due to vanished source files\n25     The --max-delete limit stopped deletions\n30     Timeout in data send/receive\n35     Timeout waiting for daemon connection\n\n```\n\nI've never seen a comprehensive \"most common errors\" list but I'm betting error code 1 would be at the top."}
{"instruction": "Python Subprocess.Popen from a thread", "output": "You didn't supply any code for us to look at, but here's a sample that does something similar to what you describe:\n\n```\nimport threading\nimport subprocess\n\nclass MyClass(threading.Thread):\n    def __init__(self):\n        self.stdout = None\n        self.stderr = None\n        threading.Thread.__init__(self)\n\n    def run(self):\n        p = subprocess.Popen('rsync -av /etc/passwd /tmp'.split(),\n                             shell=False,\n                             stdout=subprocess.PIPE,\n                             stderr=subprocess.PIPE)\n\n        self.stdout, self.stderr = p.communicate()\n\nmyclass = MyClass()\nmyclass.start()\nmyclass.join()\nprint myclass.stdout\n\n```"}
{"instruction": "rsync: --include-from vs. --exclude-from what is the actual difference?", "output": "rsync doesn't work like that. Any file with a filename pattern that does not match any of the include or exclude patterns are considered to be included. In other words, think of the include pattern as a way of overriding exclude pattern.\nFrom the docs (emphasis mine):\n\nRsync builds an ordered list of include/exclude options as specified on the command line. Rsync checks each file and directory name against each exclude/include pattern in turn. The first matching pattern is acted on. If it is an exclude pattern, then that file is skipped. If it is an include pattern then that filename is not skipped. If no matching include/exclude pattern is found then the filename is not skipped.\n\nSo, if you want to include only specific files, you first need to include those specific files, then exclude all other files:\n\n```\n--include=\"*/\" --include=\"*.cfg\" --exclude=\"*\"\n\n```\n\nCouple of things to note here:\n\nThe include patterns have to come before the excludes, because the first pattern that matches is the one that gets considered. If the file name matches the exclude pattern first, it gets excluded.\n\nYou need to either include all subdirectories individually, like --include=\"/opt\" --include=\"/opt/dir1\" etc. for all subdirectories, or use --include=\"*/\" to include all directories (not files). I went with the second option for brevity.\n\n\nIt is quirky and not very intuitive. So read the docs carefully (the \"EXCLUDE PATTERNS\" section in the link) and use the --dry-run or -n option to make sure it is going to do what you think it should do."}
{"instruction": "Errors using rspec, missing libraries after installing Homebrew and uninstalling MacPorts", "output": "I got things working again for anyone interested. I removed and re-installed nokogiri gem and everything seems to be working again."}
{"instruction": "rsync --delete --files-from=list / dest/ does not delete unwanted files", "output": "Perhaps you could do this using a list of include patterns instead, and use --delete-excluded (which does as the name suggests)? Something like:\n\n```\nrsync -r --include-from=<patternlistfile> --exclude=* --delete-excluded / dest/\n\n```\n\nIf filenames are likely to contain wildcard characters (*, ? and [) then you may need to modify the Python to escape them:\n\n```\nre.sub(\"([[*?])\", r\"\\\\\\1\", \"abc[def*ghi?klm\")\n\n```\n\nEdit: Pattern-based matching works slightly differently to --files-from in that rsync won't recurse into directories that match the exclude pattern, for reasons of efficiency. So if your files are in /some/dir and /some/other/dir then your pattern file needs to look like:\n\n```\n/some/\n/some/dir/\n/some/dir/file1\n/some/dir/file2\n/some/other/\n/some/other/dir/\n/some/other/dir/file3\n...\n\n```\n\nAlternatively, if all files are in the same directory then you could rewrite the command slightly:\n\n```\nrsync -r --include-from=<patternlistfile> --exclude=* --delete-excluded /some/dir/ dest/\n\n```\n\nand then your patterns become:\n\n```\n/file1\n/file2\n\n```\n\nEdit: Thinking about it, you could include all directories with one pattern:\n\n```\n/**/\n\n```\n\nbut then you'd end up with the entire directory tree in dest/ which probably isn't what you want. But combining it with -m (which prunes empty directories) should solve that - so the command ends up something like:\n\n```\nrsync -m -r --delete-excluded --include-from=<patternfile> --exclude=* / dest/\n\n```\n\nand the pattern file:\n\n```\n/**/\n/some/dir/file1\n/some/other/dir/file3\n\n```"}
{"instruction": "rsync exclude a directory but include a subdirectory", "output": "Sometime it's just a detail.\nJust change your include pattern adding a trailing / at the end of include pattern and it'll work:\n\n```\nrsync -avz --delete --include=specs/install/project1/ \\\n    --exclude=specs/* /srv/http/projects/project/ \\\n    user@server.com:~/projects/project\n\n```\n\nOr, in alternative, prepare a filter file like this:\n\n```\n$ cat << EOF >pattern.txt\n> + specs/install/project1/\n> - specs/*\n> EOF\n\n```\n\nThen use the --filter option:\n\n```\nrsync -avz --delete --filter=\". pattern.txt\" \\\n    /srv/http/projects/project/ \\\n    user@server.com:~/projects/project\n\n```\n\nFor further info go to the FILTER RULES section in the rsync(1) manual page."}
{"instruction": "StrictHostKeyChecking not Ignoring Fingerprint Validation", "output": "I've resolved it.\nHere's the correct (or mostly correct) way to add that option:\n\n```\n# rsync -e \"ssh -o StrictHostKeyChecking=no\" -arvc /tmp/rsync/file.txt user@domain:/tmp/rsync/\n\n```\n\nAppears there's a finicky way to apply the option, by adding double quotes and placing the ssh inside."}
{"instruction": "Do not show directories in rsync output", "output": "If you're using --delete in your rsync command, the problem with calling grep -E -v '/$' is that it will omit the information lines like:\n\n```\ndeleting folder1/\ndeleting folder2/\ndeleting folder3/folder4/\n\n```\n\nIf you're making a backup and the remote folder has been completely wiped out for X reason, it will also wipe out your local folder because you don't see the deleting lines.\nTo omit the already existing folder but keep the deleting lines at the same time, you can use this expression : \n\n```\nrsync -av --delete remote_folder local_folder | grep -E '^deleting|[^/]$'\n\n```"}
{"instruction": "rsync --include option does not exclude other files", "output": "You need to add a --exclude all and it has to come after the --include\n\n```\nrsync -zvr --include=\"*.sh\" --exclude=\"*\" $from/*  root@$host:/home/tmp/\n\n```"}
{"instruction": "Shell command to tar directory excluding certain files/folders", "output": "You can have multiple exclude options for tar so\n\n```\n$ tar --exclude='./folder' --exclude='./upload/folder2' -zcvf /backup/filename.tgz .\n\n```\n\netc will work. Make sure to put --exclude before the source and destination items."}
{"instruction": "How do I tar a directory of files and folders without including the directory itself?", "output": "```\ncd my_directory/ && tar -zcvf ../my_dir.tgz . && cd - \n\n```\n\nshould do the job in one line. It works well for hidden files as well. \"*\" doesn't expand hidden files by path name expansion at least in bash. Below is my experiment:\n\n```\n$ mkdir my_directory\n$ touch my_directory/file1\n$ touch my_directory/file2\n$ touch my_directory/.hiddenfile1\n$ touch my_directory/.hiddenfile2\n$ cd my_directory/ && tar -zcvf ../my_dir.tgz . && cd ..\n./\n./file1\n./file2\n./.hiddenfile1\n./.hiddenfile2\n$ tar ztf my_dir.tgz\n./\n./file1\n./file2\n./.hiddenfile1\n./.hiddenfile2\n\n```"}
{"instruction": "Tar a directory, but don&#39;t store full absolute paths in the archive", "output": "```\ntar -cjf site1.tar.bz2 -C /var/www/site1 .\n\n```\n\nIn the above example, tar will change to directory /var/www/site1 before doing its thing because the option -C /var/www/site1 was given.\nFrom man tar:\n\n```\nOTHER OPTIONS\n\n  -C, --directory DIR\n       change to directory DIR\n\n```"}
{"instruction": "Utilizing multi core for tar+gzip/bzip compression/decompression", "output": "You can use pigz instead of gzip, which does gzip compression on multiple cores.  Instead of using the -z option, you would pipe it through pigz:\n\n```\ntar cf - paths-to-archive | pigz > archive.tar.gz\n\n```\n\nBy default, pigz uses the number of available cores, or eight if it could not query that.  You can ask for more with -p n, e.g. -p 32.  pigz has the same options as gzip, so you can request better compression with -9.  E.g.\n\n```\ntar cf - paths-to-archive | pigz -9 -p 32 > archive.tar.gz\n\n```"}
{"instruction": "How to uncompress a tar.gz in another directory", "output": "```\ngzip -dc archive.tar.gz | tar -xf - -C /destination\n\n```\n\nor, with GNU tar\n\n```\ntar xzf archive.tar.gz -C /destination\n\n```"}
{"instruction": "What is the difference between tar and zip?", "output": "tar in itself just bundles files together (the result is called a tarball), while zip applies compression as well.\nUsually you use gzip along with tar to compress the resulting tarball, thus achieving similar results as with zip.\nFor reasonably large archives there are important differences though.  A zip archive is a collection of compressed files.  A gzipped tar is a compressed collection (of uncompressed files).  Thus a zip archive is a randomly accessible list of concatenated compressed items, and a .tar.gz is an archive that must be fully expanded before the catalog is accessible.\n\nThe caveat of a zip is that you don't get compression across files (because each file is compressed independent of the others in the archive, the compression cannot take advantage of similarities among the contents of different files); the advantage is that you can access any of the files contained within by looking at only a specific (target file dependent) section of the archive (as the \"catalog\" of the collection is separate from the collection itself).\nThe caveat of a .tar.gz is that you must decompress the whole archive to access files contained therein (as the files are within the tarball); the advantage is that the compression can take advantage of similarities among the files (as it compresses the whole tarball)."}
{"instruction": "Tar archiving that takes input from a list of files", "output": "Yes:\n\n```\ntar -cvf allfiles.tar -T mylist.txt\n\n```"}
{"instruction": "Create a tar.xz in one command", "output": "Use the -J compression option for xz. And remember to man tar :)\n\n```\ntar cfJ <archive.tar.xz> <files>\n\n```\n\nEdit 2015-08-10:\nIf you're passing the arguments to tar with dashes (ex: tar -cf as opposed to tar cf), then the -f option must come last, since it specifies the filename (thanks to @A-B-B for pointing that out!). In that case, the command looks like:\n\n```\ntar -cJf <archive.tar.xz> <files>\n\n```"}
{"instruction": "How do I turn off the output from tar commands on Unix?", "output": "Just drop the option v.\n-v is for verbose. If you don't use it then it won't display:\n\n```\ntar -zxf tmp.tar.gz -C ~/tmp1\n\n```"}
{"instruction": "Are tar.gz and tgz the same thing?", "output": "I think in the old package repo days, .tgz was used because files on DOS floppies could only have three letter extensions. When this limitation was removed, .tar.gz was used to be more verbose by showing both the archive type (tar) and zipper (gzip).\nThey are identical."}
{"instruction": "Excluding directory when creating a .tar.gz file", "output": "Try removing the last / at the end of the directory path to exclude\n\n```\ntar -pczf MyBackup.tar.gz /home/user/public_html/ --exclude \"/home/user/public_html/tmp\" \n\n```"}
{"instruction": "How to check if a Unix .tar.gz file is a valid file without uncompressing?", "output": "What about just getting a listing of the tarball and throw away the output, rather than decompressing the file?\n\n```\ntar -tzf my_tar.tar.gz >/dev/null\n\n```\n\nEdit as per comment. Thanks Frozen Flame! This test in no way implies integrity of the data. Because it was designed as a tape archival utility most implementations of tar will allow multiple copies of the same file!"}
{"instruction": "How to extract filename.tar.gz file", "output": "If file filename.tar.gz gives this message: POSIX tar archive, \nthe archive is a tar, not a GZip archive.\nUnpack a tar without the z, it is for gzipped (compressed), only:\n\n```\nmv filename.tar.gz filename.tar # optional\ntar xvf filename.tar\n\n```\n\nOr try a generic Unpacker like unp (https://packages.qa.debian.org/u/unp.html), a script for unpacking a wide variety of archive formats.\ndetermine the file type: \n\n```\n$ file ~/Downloads/filename.tbz2\n/User/Name/Downloads/filename.tbz2: bzip2 compressed data, block size = 400k\n\n```"}
{"instruction": "How do I tar a directory without retaining the directory structure?", "output": "```\ncd /home/username/drupal/sites/default/files\ntar czf ~/backup.tgz *\n\n```"}
{"instruction": "Find files and tar them (with spaces)", "output": "Use this:\n\n```\nfind . -type f -print0 | tar -czvf backup.tar.gz --null -T -\n\n```\n\nIt will:\n\ndeal with files with spaces, newlines, leading dashes, and other funniness\nhandle an unlimited number of files\nwon't repeatedly overwrite your backup.tar.gz like using tar -c with xargs will do when you have a large number of files\n\nAlso see:\n\nGNU tar manual\nHow can I build a tar from stdin?, search for null"}
{"instruction": "How to tar certain file types in all subdirectories?", "output": "find ./someDir -name \"*.php\" -o -name \"*.html\" | tar -cf my_archive -T -"}
{"instruction": "reading tar file contents without untarring it, in python script", "output": "you can use getmembers()\n\n```\n>>> import  tarfile\n>>> tar = tarfile.open(\"test.tar\")\n>>> tar.getmembers()\n\n```\n\nAfter that, you can use extractfile() to extract the members as file object. Just an example\n\n```\nimport tarfile,os\nimport sys\nos.chdir(\"/tmp/foo\")\ntar = tarfile.open(\"test.tar\")\nfor member in tar.getmembers():\n    f=tar.extractfile(member)\n    content=f.read()\n    print \"%s has %d newlines\" %(member, content.count(\"\\n\"))\n    print \"%s has %d spaces\" % (member,content.count(\" \"))\n    print \"%s has %d characters\" % (member, len(content))\n    sys.exit()\ntar.close()\n\n```\n\nWith the file object f in the above example, you can use read(), readlines() etc."}
{"instruction": "How do I extract files without folder structure using tar", "output": "You can use the --strip-components option of tar.\n\n\n```\n --strip-components count\n         (x mode only) Remove the specified number of leading path ele-\n         ments.  Pathnames with fewer elements will be silently skipped.\n         Note that the pathname is edited after checking inclusion/exclu-\n         sion patterns but before security checks.\n\n```\n\n\nI create a tar file with a similar structure to yours:\n\n```\n$tar -tf tarfolder.tar\ntarfolder/\ntarfolder/file.a\ntarfolder/file.b\n\n$ls -la file.*\nls: file.*: No such file or directory\n\n```\n\nThen extracted by doing:\n\n```\n$tar -xf tarfolder.tar --strip-components 1\n$ls -la file.*\n-rw-r--r--  1 ericgorr  wheel  0 Jan 12 12:33 file.a\n-rw-r--r--  1 ericgorr  wheel  0 Jan 12 12:33 file.b\n\n```"}
{"instruction": "gzip: stdin: not in gzip format tar: Child returned status 1 tar: Error is not recoverable: exiting now", "output": "This is probably because of your gzip version incompatibility.\nCheck these points first:\n\nwhich gzip\n\n/usr/bin/gzip   or /bin/gzip\nIt should be either /bin/gzip or /usr/bin/gzip. If your gzip points to some other gzip application, please try by removing that path from your PATH environment variable.\nNext is\n\ngzip -V\n\ngzip 1.3.5\n(2002-09-30)\nYour problem can be resolved with these check points."}
{"instruction": "Create a .tar.bz2 file Linux", "output": "You are not indicating what to include in the archive.\nGo one level outside your folder and try:\n\n```\nsudo tar -cvjSf folder.tar.bz2 folder\n\n```\n\nOr from the same folder try\n\n```\nsudo tar -cvjSf folder.tar.bz2 *\n\n```"}
{"instruction": "Listing the content of a tar file or a directory only down to some level", "output": "```\ntar tvf scripts.tar | awk -F/ '{if (NF<4) print }'\n\n\ndrwx------ glens/glens       0 2010-03-17 10:44 scripts/\n-rwxr--r-- glens/www-data 1051 2009-07-27 10:42 scripts/my2cnf.pl\n-rwxr--r-- glens/www-data  359 2009-08-14 00:01 scripts/pastebin.sh\n-rwxr--r-- glens/www-data  566 2009-07-27 10:42 scripts/critic.pl\n-rwxr-xr-x glens/glens     981 2009-12-16 09:39 scripts/wiki_sys.pl\n-rwxr-xr-x glens/glens    3072 2009-07-28 10:25 scripts/blacklist_update.pl\n-rwxr--r-- glens/www-data 18418 2009-07-27 10:42 scripts/sysinfo.pl\n\n```\n\nMake sure to note, that the number is 3+ however many levels you want, because of the / in the username/group.  If you just do\n\n```\ntar tf scripts.tar | awk -F/ '{if (NF<3) print }'\n\nscripts/\nscripts/my2cnf.pl\nscripts/pastebin.sh\nscripts/critic.pl\nscripts/wiki_sys.pl\nscripts/blacklist_update.pl\nscripts/sysinfo.pl\n\n```\n\nit's only two more.\nYou could probably pipe the output of ls -R to this awk script, and have the same effect."}
{"instruction": "Uncompress tar.gz file", "output": "Use -C option of tar:\n\n```\ntar zxvf <yourfile>.tar.gz -C /usr/src/\n\n```\n\nand then, the content of the tar should be in:\n\n```\n/usr/src/<yourfile>\n\n```"}
{"instruction": "Check the total content size of a tar gz file", "output": "This will sum the total content size of the extracted files:\n\n```\n$ tar tzvf archive.tar.gz | sed 's/ \\+/ /g' | cut -f3 -d' ' | sed '2,$s/^/+ /' | paste -sd' ' | bc\n\n```\n\nThe output is given in bytes.\nExplanation: tar tzvf lists the files in the archive in verbose format like ls -l. sed and cut isolate the file size field. The second sed puts a + in front of every size except the first and paste concatenates them, giving a sum expression that is then evaluated by bc.\nNote that this doesn't include metadata, so the disk space taken up by the files when you extract them is going to be larger - potentially many times larger if you have a lot of very small files."}
{"instruction": "How can files be added to a tarfile with Python, without adding the directory hierarchy?", "output": "You can use tarfile.addfile(), in the TarInfo object, which is the first parameter, you can specify a name that's different from the file you're adding.\nThis piece of code should add /path/to/filename to the TAR file but will extract it as myfilename:\n\n```\ntar.addfile(tarfile.TarInfo(\"myfilename.txt\"), open(\"/path/to/filename.txt\"))\n\n```"}
{"instruction": "How can I build a tar from stdin?", "output": "Something like:\n\n```\ntar cfz foo.tgz --files-from=-\n\n```\n\nBut keep in mind that this won't work for all possible filenames; you should consider the --null option and feed tar from find -print0.  (The xargs example won't quite work for large file lists because it will spawn multiple tar commands.)"}
{"instruction": "How to extract a single file from tar to a different directory?", "output": "The problem is that your arguments are in incorrect order. The single file argument must be last.\nE.g. \n\n```\n$ tar xvf test.tar -C anotherDirectory/ testfile1\n\n```\n\nshould do the trick.\nPS: You should have asked this question on superuser instead of SO"}
{"instruction": "Deleting files after adding to tar archive", "output": "With GNU tar, use the option --remove-files."}
{"instruction": "How to create tar.gz archive file in Windows?", "output": "tar.gz file is just a tar file that's been gzipped. Both tar and gzip are available for windows.\nIf you like GUIs (Graphical user interface), 7zip can pack with both tar and gzip."}
{"instruction": "Native .tar extraction in Powershell", "output": "So it's been eleven days since I asked this and the general consensus is: \"No, there are no native tools in a vanilla window install that can handle tar extraction 'for you'\". \nThis answer comes from Matthias R. Jensen and TessellatingHeckler, who both declined to answer outside of comments (I suspect due to not wanting to say \"no\" without an intimate knowledge of the entire Windows system architecture, which is fair).\nThere are certainly scripts and classes and programs you can install, but nothing \"native\"."}
{"instruction": "How to extract tar archive from stdin?", "output": "Use - as the input file:\n\n```\ncat largefile.tgz.aa largefile.tgz.ab | tar zxf -\n\n```\n\nMake sure you cat them in the same order they were split.\nIf you're using zsh you can use the multios feature and avoid invoking cat:\n\n```\n< largefile.tgz.aa < largefile.tgz.ab tar zxf -\n\n```\n\nOr if they are in alphabetical order:\n\n```\n<largefile.tgz.* | tar zxf -\n\n```"}
{"instruction": "How to extract tar file in Mac terminal", "output": "Yes, you can run:\n\n```\ntar -xvf myfile.tar\n\n```\n\nFor .tar.gz, you can run:\n\n```\ntar -xzvf myfile.tar.gz\n\n```\n\nIf you want to extract to any directory other than your cwd, use -C. e.g:\n\n```\ntar -xvf myfile.tar -C somedirectory\n\n```\n\nI suggest you read the man page for tar if you wish to do anything further:\n\n```\nman tar\n\n```"}
{"instruction": "How do I extract a tar file in Java?", "output": "Note: This functionality was later published through a separate project, Apache Commons Compress, as described in another answer. This answer is out of date.\n\nI haven't used a tar API directly, but tar and bzip2 are implemented in Ant; you could borrow their implementation, or possibly use Ant to do what you need.\nGzip is part of Java SE (and I'm guessing the Ant implementation follows the same model).\nGZIPInputStream is just an InputStream decorator. You can wrap, for example, a FileInputStream in a GZIPInputStream and use it in the same way you'd use any InputStream:\n\n```\nInputStream is = new GZIPInputStream(new FileInputStream(file));\n\n```\n\n(Note that the GZIPInputStream has its own, internal buffer, so wrapping the FileInputStream in a BufferedInputStream would probably decrease performance.)"}
{"instruction": "Rename Directory Name Before tar Happens", "output": "Which tar?\nGNU Tar accepts a --transform argument, to which you give a sed expression to manipulate filenames.\nFor example, to rename during unpacking:\n\n```\ntar -zxf my-dir.tar.gz --transform s/my-dir/your-dir/\n\n```\n\nBSD tar and S tar similarly have an -s argument, taking a simple /old/new/ (not a general sed expression)."}
{"instruction": "How do I untar a subdirectory into the current directory?", "output": "Why don't you untar normally, then just:\n\n```\nmv wordpress/.* .\nmv wordpress/* .\nrmdir wordpress\n\n```\n\nBut alas, there's:\n\n```\ntar --strip-components=1 -zxvf wordpress.tgz\n\n```"}
{"instruction": "How to send a compressed archive that contains executables so that Google&#39;s attachment filter won&#39;t reject it", "output": "```\ntar -cvzf filename.tar.gz directory_to_compress/\n\n```\n\nMost tar commands have a z option to create a gziped version. \nThough seems to me the question is how to circumvent Google. I'm not sure if renaming your output file would fool Google, but you could try. I.e.,\n\n```\ntar -cvzf filename.bla directory_to_compress/\n\n```\n\nand then send the filename.bla - contents will would be a zipped tar, so at the other end it could be retrieved as usual."}
{"instruction": "How to install Go in alpine linux", "output": "Thanks BMitch.\nI compiled the go source code and performed the below steps inside alpine image container.\n\n```\necho \"installing go version 1.10.3...\" \napk add --no-cache --virtual .build-deps bash gcc musl-dev openssl go \n\n# download go tar \nwget -O go.tgz https://dl.google.com/go/go1.10.3.src.tar.gz \ntar -C /usr/local -xzf go.tgz \ncd /usr/local/go/src/ \n\n# compile code\n./make.bash \nexport PATH=\"/usr/local/go/bin:$PATH\"\nexport GOPATH=/opt/go/ \nexport PATH=$PATH:$GOPATH/bin \napk del .build-deps \ngo version\n\n```"}
{"instruction": "How to use Pigz with Tar", "output": "Mark Adler's top voted answer on the SO link that you included in your question does provide a solution for specifying compression-level as well as number of processors to use:\n\n```\ntar cf - paths-to-archive | pigz -9 -p 32 > archive.tar.gz\n\n```\n\nSee : https://stackoverflow.com/a/12320421"}
{"instruction": "Python packaging: wheels vs tarball (tar.gz)", "output": "This answered it for me (directly from the wheel PEP):\n\nPython needs a package format that is easier to install than sdist.\n  Python's sdist packages are defined by and require the distutils and\n  setuptools build systems, running arbitrary code to build-and-install,\n  and re-compile, code just so it can be installed into a new\n  virtualenv. This system of conflating build-install is slow, hard to\n  maintain, and hinders innovation in both build systems and installers.\nWheel attempts to remedy these problems by providing a simpler\n  interface between the build system and the installer. The wheel binary\n  package format frees installers from having to know about the build\n  system, saves time by amortizing compile time over many installations,\n  and removes the need to install a build system in the target\n  environment.\n\nhttps://www.python.org/dev/peps/pep-0427/#rationale\nNote the tarballs we're speaking of are what are referred to as \"sdists\" above."}
{"instruction": "Tarballing without Git metadata", "output": "You will get a nasty surprise when the number of files increase to more than one xargs command: Then you will first make a tar file of the first files and then overwrite the same tar file with the rest of the files.\nGNU tar has the --exclude option which will solve this issue:\n\n```\ntar cvf ~/app.tar --exclude .git --exclude \"*.log\" .\n\n```"}
{"instruction": "Extract tar the tar.bz2 file error", "output": "For bz2 you need to execute like this,\n\n```\ntar -jxvf\n\n```\n\nAlternatively, you can also execute like this\n\n```\nbunzip2 myfile.tar.bz2\n\n```\n\nFor more information you should check it,\n\n```\ntar --help\n\n```\n\nIf in doubt, run file on the archive to make sure it actually is compressed in bz2 format."}
{"instruction": "Shell &#39;tar: not found in archive&#39; error when using regular expression", "output": "When you write:\n\n```\ntar -xf *.gz\n\n```\n\nthe tar command sees (for example):\n\n```\ntar -xf abc.tar.gz def.tar.gz ghi.tar.gz\n\n```\n\nThis is interpreted as a request to extract def.tar.gz and ghi.tar.gz from the archive abc.tar.gz.  Since the files aren't in there, you get the warning message.\nIn other words, tar operates on a single tar file (possibly compressed) at a time (in one invocation).  It does not operate on multiple tar files.\nNote that if abc.tar.gz contains a file pqr/xyz/important.c, you can extract just the one file by specifying:\n\n```\ntar -xf abc.tar.gz pqr/xyz/important.c\n\n```\n\nThe notation you used is only a variant on this notation.\n(And yes, there can be reasons to tar a tar file.  For example, Gmail does not allow you to ship a tar file or a gzipped tar file which contains a file that is executable.  However, if you embed a gzipped tar file inside a non-compressed tar file, it does not look inside the inner file to find the executable file.  I use this when I need to ship a tar file with an executable configure script.)"}
{"instruction": "How to Compress/Decompress tar.gz files in java", "output": "My favorite is plexus-archiver - see sources on GitHub.\nAnother option is Apache commons-compress - (see mvnrepository).\nWith plexus-utils, the code for unarchiving looks like this:\n\n```\nfinal TarGZipUnArchiver ua = new TarGZipUnArchiver();\n// Logging - as @Akom noted, logging is mandatory in newer versions, so you can use a code like this to configure it:\nConsoleLoggerManager manager = new ConsoleLoggerManager();\nmanager.initialize();\nua.enableLogging(manager.getLoggerForComponent(\"bla\"));\n// -- end of logging part\nua.setSourceFile(sourceFile);\ndestDir.mkdirs();\nua.setDestDirectory(destDir);\nua.extract();\n\n```\n\nSimilar *Archiver classes are there for archiving.\nWith Maven, you can use this dependency:\n\n```\n<dependency>\n  <groupId>org.codehaus.plexus</groupId>\n  <artifactId>plexus-archiver</artifactId>\n  <version>2.2</version>\n</dependency>\n\n```"}
{"instruction": "How can I read tar.gz file using pandas read_csv with gzip compression option?", "output": "```\ndf = pd.read_csv('sample.tar.gz', compression='gzip', header=0, sep=' ', quotechar='\"', error_bad_lines=False)\n\n```\n\nNote: error_bad_lines=False will ignore the offending rows."}
{"instruction": "How to create a tar file that omits timestamps for its contents?", "output": "To have a truly idempotent tar, mtime is a good step but not enough.\nYou also need to set the sort order, the owner and group (together with their mapping) and a proper timezone for mtime (since otherwise you're gonna have issues as well between Mac and Linux).\nI ended up with\n\n```\ntar --sort=name --owner=root:0 --group=root:0 --mtime='UTC 1980-02-01' ... | gzip -n\n\n```"}
{"instruction": "How to compare two tarball&#39;s content", "output": "tarsum is almost what you need.  Take its output, run it through sort to get the ordering identical on each, and then compare the two with diff.  That should get you a basic implementation going, and it would be easily enough to pull those steps into the main program by modifying the Python code to do the whole job."}
{"instruction": "what does -zxvf mean in tar -zxvf &lt;filename&gt;?", "output": "z means (un)zÃ≤ip.\nx means exÃ≤tract files from the archive.\nv means print the filenames vÃ≤erbosely.\nf means the following argument is a fÃ±ilename.\n\nFor more details, see tar's man page."}
{"instruction": "Opening a .tar.gz file with a single command", "output": "```\ntar xzf file.tar.gz\n\n```\n\nThe letters are:\n\nx - extract\nz - gunzip the input\nf - Read from a file, not stdin"}
{"instruction": "Extract files contained in archive.tar.gz to new directory named archive", "output": "Update since GNU tar 1.28:\nuse --one-top-level, see https://www.gnu.org/software/tar/manual/tar.html#index-one_002dtop_002dlevel_002c-summary\nOlder versions need to script this. You can specify the directory that the extract is placed in by using the tar -C option.\nThe script below assumes that the directories do not exist and must be created. If the directories do exist the script will still work - the mkdir will simply fail.\n\n```\ntar -xvzf archive.tar.gx -C archive_dir\n\n```\n\ne.g.\n\n```\nfor a in *.tar.gz\ndo\n    a_dir=${a%.tar.gz}\n    mkdir --parents $a_dir\n    tar -xvzf $a -C $a_dir\ndone\n\n```"}
{"instruction": "How to fix &#39;tar: Failed to set default locale&#39; error?", "output": "Step 1 (In R Console)\n\n```\nsystem('defaults write org.R-project.R force.LANG en_US.UTF-8')\n\n```\n\nStep 2: Restart R\nSource: http://cran.r-project.org/bin/macosx/RMacOSX-FAQ.html#Internationalization-of-the-R_002eapp"}
{"instruction": "How to construct a TarFile object in memory from byte buffer in Python 3?", "output": "BytesIO() from IO module does exactly what you need.\n\n```\nimport tarfile, io\nbyte_array = client.read_bytes()\nfile_like_object = io.BytesIO(byte_array)\ntar = tarfile.open(fileobj=file_like_object)\n# use \"tar\" as a regular TarFile object\nfor member in tar.getmembers():\n    f = tar.extractfile(member)\n    print(f)\n\n```"}
{"instruction": "How are zlib, gzip and zip related? What do they have in common and how are they different?", "output": "Short form:\n.zip is an archive format using, usually, the Deflate compression method.  The .gz gzip format is for single files, also using the Deflate compression method.  Often gzip is used in combination with tar to make a compressed archive format, .tar.gz.  The zlib library provides Deflate compression and decompression code for use by zip, gzip, png (which uses the zlib wrapper on deflate data), and many other applications.\nLong form:\nThe ZIP format was developed by Phil Katz as an open format with an open specification, where his implementation, PKZIP, was shareware.  It is an archive format that stores files and their directory structure, where each file is individually compressed.  The file type is .zip.  The files, as well as the directory structure, can optionally be encrypted.\nThe ZIP format supports several compression methods:\n\n```\n    0 - The file is stored (no compression)\n    1 - The file is Shrunk\n    2 - The file is Reduced with compression factor 1\n    3 - The file is Reduced with compression factor 2\n    4 - The file is Reduced with compression factor 3\n    5 - The file is Reduced with compression factor 4\n    6 - The file is Imploded\n    7 - Reserved for Tokenizing compression algorithm\n    8 - The file is Deflated\n    9 - Enhanced Deflating using Deflate64(tm)\n   10 - PKWARE Data Compression Library Imploding (old IBM TERSE)\n   11 - Reserved by PKWARE\n   12 - File is compressed using BZIP2 algorithm\n   13 - Reserved by PKWARE\n   14 - LZMA\n   15 - Reserved by PKWARE\n   16 - IBM z/OS CMPSC Compression\n   17 - Reserved by PKWARE\n   18 - File is compressed using IBM TERSE (new)\n   19 - IBM LZ77 z Architecture \n   20 - deprecated (use method 93 for zstd)\n   93 - Zstandard (zstd) Compression \n   94 - MP3 Compression \n   95 - XZ Compression \n   96 - JPEG variant\n   97 - WavPack compressed data\n   98 - PPMd version I, Rev 1\n   99 - AE-x encryption marker (see APPENDIX E)\n\n```\n\nMethods 1 to 7 are historical and are not in use.  Methods 9 through 98 are relatively recent additions and are in varying, small amounts of use.  The only method in truly widespread use in the ZIP format is method 8, Deflate, and to some smaller extent method 0, which is no compression at all.  Virtually every .zip file that you will come across in the wild will use exclusively methods 8 and 0, likely just method 8.  (Method 8 also has a means to effectively store the data with no compression and relatively little expansion, and Method 0 cannot be streamed whereas Method 8 can be.)\nThe ISO/IEC 21320-1:2015 standard for file containers is a restricted zip format, such as used in Java archive files (.jar), Office Open XML files (Microsoft Office .docx, .xlsx, .pptx), Office Document Format files (.odt, .ods, .odp), and EPUB files (.epub). That standard limits the compression methods to 0 and 8, as well as other constraints such as no encryption or signatures.\nAround 1990, the Info-ZIP group wrote portable, free, open-source implementations of zip and unzip utilities, supporting compression with the Deflate format, and decompression of that and the earlier formats.  This greatly expanded the use of the .zip format.\nIn the early '90s, the gzip format was developed as a replacement for the Unix compress utility, derived from the Deflate code in the Info-ZIP utilities.  Unix compress was designed to compress a single file or stream, appending a .Z to the file name.  compress uses the LZW compression algorithm, which at the time was under patent and its free use was in dispute by the patent holders.  Though some specific implementations of Deflate were patented by Phil Katz, the format was not, and so it was possible to write a Deflate implementation that did not infringe on any patents.  That implementation has not been so challenged in the last 20+ years.  The Unix gzip utility was intended as a drop-in replacement for compress, and in fact is able to decompress compress-compressed data (assuming that you were able to parse that sentence).  gzip appends a .gz to the file name.  gzip uses the Deflate compressed data format, which compresses quite a bit better than Unix compress, has very fast decompression, and adds a CRC-32 as an integrity check for the data.  The header format also permits the storage of more information than the compress format allowed, such as the original file name and the file modification time.\nThough compress only compresses a single file, it was common to use the tar utility to create an archive of files, their attributes, and their directory structure into a single .tar file, and then compress it with compress to make a .tar.Z file.  In fact, the tar utility had and still has the option to do the compression at the same time, instead of having to pipe the output of tar to compress.  This all carried forward to the gzip format, and tar has an option to compress directly to the .tar.gz format.  The tar.gz format compresses better than the .zip approach, since the compression of a .tar can take advantage of redundancy across files, especially many small files.  .tar.gz is the most common archive format in use on Unix due to its very high portability, but there are more effective compression methods in use as well, so you will often see .tar.bz2 and .tar.xz archives.\nUnlike .tar, .zip has a central directory at the end, which provides a list of the contents. That and the separate compression provides random access to the individual entries in a .zip file. A .tar file would have to be decompressed and scanned from start to end in order to build a directory, which is how a .tar file is listed.\nShortly after the introduction of gzip, around the mid-1990s, the same patent dispute called into question the free use of the .gif image format, very widely used on bulletin boards and the World Wide Web (a new thing at the time).  So a small group created the PNG losslessly compressed image format, with file type .png, to replace .gif.  That format also uses the Deflate format for compression, which is applied after filters on the image data expose more of the redundancy.  In order to promote widespread usage of the PNG format, two free code libraries were created.  libpng and zlib.  libpng handled all of the features of the PNG format, and zlib provided the compression and decompression code for use by libpng, as well as for other applications.  zlib was adapted from the gzip code.\nAll of the mentioned patents have since expired.\nThe zlib library supports Deflate compression and decompression, and three kinds of wrapping around the deflate streams.  Those are no wrapping at all (\"raw\" deflate), zlib wrapping, which is used in the PNG format data blocks, and gzip wrapping, to provide gzip routines for the programmer.  The main difference between zlib and gzip wrapping is that the zlib wrapping is more compact, six bytes vs. a minimum of 18 bytes for gzip, and the integrity check, Adler-32, runs faster than the CRC-32 that gzip uses.  Raw deflate is used by programs that read and write the .zip format, which is another format that wraps around deflate compressed data.\nzlib is now in wide use for data transmission and storage.  For example, most HTTP transactions by servers and browsers compress and decompress the data using zlib, specifically HTTP header Content-Encoding: deflate means deflate compression method wrapped inside the zlib data format.\nDifferent implementations of deflate can result in different compressed output for the same input data, as evidenced by the existence of selectable compression levels that allow trading off compression effectiveness for CPU time. zlib and PKZIP are not the only implementations of deflate compression and decompression. Both the 7-Zip archiving utility and Google's zopfli library have the ability to use much more CPU time than zlib in order to squeeze out the last few bits possible when using the deflate format, reducing compressed sizes by a few percent as compared to zlib's highest compression level. The pigz utility, a parallel implementation of gzip, includes the option to use zlib (compression levels 1-9) or zopfli (compression level 11), and somewhat mitigates the time impact of using zopfli by splitting the compression of large files over multiple processors and cores."}
{"instruction": "How do I tar a directory of files and folders without including the directory itself?", "output": "```\ncd my_directory/ && tar -zcvf ../my_dir.tgz . && cd - \n\n```\n\nshould do the job in one line. It works well for hidden files as well. \"*\" doesn't expand hidden files by path name expansion at least in bash. Below is my experiment:\n\n```\n$ mkdir my_directory\n$ touch my_directory/file1\n$ touch my_directory/file2\n$ touch my_directory/.hiddenfile1\n$ touch my_directory/.hiddenfile2\n$ cd my_directory/ && tar -zcvf ../my_dir.tgz . && cd ..\n./\n./file1\n./file2\n./.hiddenfile1\n./.hiddenfile2\n$ tar ztf my_dir.tgz\n./\n./file1\n./file2\n./.hiddenfile1\n./.hiddenfile2\n\n```"}
{"instruction": "Utilizing multi core for tar+gzip/bzip compression/decompression", "output": "You can use pigz instead of gzip, which does gzip compression on multiple cores.  Instead of using the -z option, you would pipe it through pigz:\n\n```\ntar cf - paths-to-archive | pigz > archive.tar.gz\n\n```\n\nBy default, pigz uses the number of available cores, or eight if it could not query that.  You can ask for more with -p n, e.g. -p 32.  pigz has the same options as gzip, so you can request better compression with -9.  E.g.\n\n```\ntar cf - paths-to-archive | pigz -9 -p 32 > archive.tar.gz\n\n```"}
{"instruction": "How to uncompress a tar.gz in another directory", "output": "```\ngzip -dc archive.tar.gz | tar -xf - -C /destination\n\n```\n\nor, with GNU tar\n\n```\ntar xzf archive.tar.gz -C /destination\n\n```"}
{"instruction": "Command Line Tool - Error - xcrun: error: unable to find utility &quot;xcodebuild&quot;, not a developer tool or in PATH", "output": "I solved that problem by setting the Command Line Tools in Xcode. Go to:\n\nXcode > Preferences > Locations\n\nAnd select the command line tool from the dropdown. If you have only one version of Xcode installed, there should be only one option. If you have several versions of Xcode, then you must choose the one you need.\nUpdate (added image for reference)"}
{"instruction": "TypeError: &#39;str&#39; does not support the buffer interface", "output": "If you use Python3x then string is not the same type as for Python 2.x, you must cast it to bytes (encode it).\n\n```\nplaintext = input(\"Please enter the text you want to compress\")\nfilename = input(\"Please enter the desired filename\")\nwith gzip.open(filename + \".gz\", \"wb\") as outfile:\n    outfile.write(bytes(plaintext, 'UTF-8'))\n\n```\n\nAlso do not use variable names like string or file while those are names of module or function.\nEDIT @Tom\nYes, non-ASCII text is also compressed/decompressed. I use Polish letters with UTF-8 encoding:\n\n```\nplaintext = 'Polish text: ƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈ºƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ª'\nfilename = 'foo.gz'\nwith gzip.open(filename, 'wb') as outfile:\n    outfile.write(bytes(plaintext, 'UTF-8'))\nwith gzip.open(filename, 'r') as infile:\n    outfile_content = infile.read().decode('UTF-8')\nprint(outfile_content)\n\n```"}
{"instruction": "How to gzip all files in all sub-directories into one compressed file in bash", "output": "```\ntar -zcvf compressFileName.tar.gz folderToCompress\n\n```\n\neverything in folderToCompress will go to compressFileName\nEdit: After review and comments I realized that people may get confused with compressFileName without an extension. If you want you can use .tar.gz extension(as suggested) with the compressFileName"}
{"instruction": "Why use deflate instead of gzip for text files served by Apache?", "output": "Why use deflate instead of gzip for text files served by Apache?\n\nThe simple answer is don't. \n\nRFC 2616 defines deflate as: \n\ndeflate The \"zlib\" format defined in RFC 1950 in combination with the \"deflate\" compression mechanism described in RFC 1951\n\nThe zlib format is defined in RFC 1950 as :\n\n```\n     0   1\n     +---+---+\n     |CMF|FLG|   (more-->)\n     +---+---+\n\n       0   1   2   3\n     +---+---+---+---+\n     |     DICTID    |   (more-->)\n     +---+---+---+---+\n\n     +=====================+---+---+---+---+\n     |...compressed data...|    ADLER32    |\n     +=====================+---+---+---+---+\n\n```\n\nSo, a few headers and an ADLER32 checksum\nRFC 2616 defines gzip as: \n\ngzip An encoding format produced by the file compression program\n         \"gzip\" (GNU zip) as described in RFC 1952 [25]. This format is a\n         Lempel-Ziv coding (LZ77) with a 32 bit CRC.\n\nRFC 1952 defines the compressed data as: \n\nThe format presently uses the DEFLATE method of compression but can be easily extended to use other compression methods.\n\nCRC-32 is slower than ADLER32\n\nCompared to a cyclic redundancy check of the same length, it trades reliability for speed (preferring the latter). \n\nSo ... we have 2 compression mechanisms that use the same algorithm for compression, but a different algorithm for headers and checksum.\nNow, the underlying TCP packets are already pretty reliable, so the issue here is not Adler 32 vs CRC-32 that GZIP uses.\n\nTurns out many browsers over the years implemented an incorrect deflate algorithm. Instead of expecting the zlib header in RFC 1950 they simply expected the compressed payload. Similarly various web servers made the same mistake. \nSo, over the years browsers started implementing a fuzzy logic deflate implementation, they try for zlib header and adler checksum, if that fails they try for payload.\nThe result of having complex logic like that is that it is often broken. Verve Studio have a user contributed test section that show how bad the situation is. \nFor example: deflate works in Safari 4.0 but is broken in Safari 5.1, it also always has issues on IE. \n\nSo, best thing to do is avoid deflate altogether, the minor speed boost (due to adler 32) is not worth the risk of broken payloads."}
{"instruction": "How can I tell if my server is serving GZipped content?", "output": "It looks like one possible answer is, unsurprisingly, curl:\n\n```\n$ curl http://example.com/ --silent --write-out \"%{size_download}\\n\" --output /dev/null\n31032\n$ curl http://example.com/ --silent -H \"Accept-Encoding: gzip,deflate\" --write-out \"%{size_download}\\n\" --output /dev/null\n2553\n\n```\n\nIn the second case the client tells the server that it supports content encoding and you can see that the response was indeed shorter, compressed."}
{"instruction": "How to properly handle a gzipped page when using curl?", "output": "curl will automatically decompress the response if you set the --compressed flag:\n\n```\ncurl --compressed \"http://example.com\"\n\n```\n\n\n--compressed\n   (HTTP)  Request  a compressed response using one of the algorithms libcurl supports, and save the uncompressed document.  If this option is used and the server sends an unsupported encoding, curl will report an error.\n\ngzip is most likely supported, but you can check this by running curl -V and looking for libz somewhere in the \"Features\" line:\n\n```\n$ curl -V\n...\nProtocols: ...\nFeatures: GSS-Negotiate IDN IPv6 Largefile NTLM SSL libz \n\n```\n\n\nNote that it's really the website in question that is at fault here. If curl did not pass an Accept-Encoding: gzip request header, the server should not have sent a compressed response."}
{"instruction": "Serving gzipped CSS and JavaScript from Amazon CloudFront via S3", "output": "UPDATE:  Amazon now supports gzip compression, so this is no longer needed.  Amazon Announcement\nOriginal answer:\nThe answer is to gzip the CSS and JavaScript files. Yes, you read that right.\n\n```\ngzip -9 production.min.css\n\n```\n\nThis will produce production.min.css.gz. Remove the .gz, upload to S3 (or whatever origin server you're using) and explicitly set the Content-Encoding header for the file to gzip.\nIt's not on-the-fly gzipping, but you could very easily wrap it up into your build/deployment scripts. The advantages are:\n\nIt requires no CPU for Apache to gzip the content when the file is requested.\nThe files are gzipped at the highest compression level (assuming gzip -9).\nYou're serving the file from a CDN.\n\nAssuming that your CSS/JavaScript files are (a) minified and (b) large enough to justify the CPU required to decompress on the user's machine, you can get significant performance gains here.\nJust remember: If you make a change to a file that is cached in CloudFront, make sure you invalidate the cache after making this type of change."}
{"instruction": "Excluding directory when creating a .tar.gz file", "output": "Try removing the last / at the end of the directory path to exclude\n\n```\ntar -pczf MyBackup.tar.gz /home/user/public_html/ --exclude \"/home/user/public_html/tmp\" \n\n```"}
{"instruction": "How to check if a Unix .tar.gz file is a valid file without uncompressing?", "output": "What about just getting a listing of the tarball and throw away the output, rather than decompressing the file?\n\n```\ntar -tzf my_tar.tar.gz >/dev/null\n\n```\n\nEdit as per comment. Thanks Frozen Flame! This test in no way implies integrity of the data. Because it was designed as a tape archival utility most implementations of tar will allow multiple copies of the same file!"}
{"instruction": "Gzip versus minify", "output": "Very simple to test.  I took your js, put them in different files and ran gzip -9 on them.  Here's the result.  This was done on a WinXP machine running Cygwin and gzip 1.3.12.\n\n```\n-rwx------  1 xxxxxxxx mkgroup-l-d     88 Apr 30 09:17 expanded.js.gz\n\n-rwx------  1 xxxxxxxx mkgroup-l-d     81 Apr 30 09:18 minified.js.gz\n\n```\n\nHere's a further test using a real-world JS example.  The source file is \"common.js\"  The original file size is 73134 bytes.  Minified, it came to 26232 bytes.\nOriginal file:\n\n```\n-rwxrwxrwx 1 xxxxxxxx mkgroup-l-d 73134 Apr 13 11:41 common.js\n\n```\n\nMinified file:\n\n```\n-rwxr-xr-x 1 xxxxxxxx mkgroup-l-d 26232 Apr 30 10:39 common-min.js\n\n```\n\nOriginal file gzipped with -9 option (same version as above):\n\n```\n-rwxrwxrwx 1 xxxxxxxx mkgroup-l-d 12402 Apr 13 11:41 common.js.gz\n\n```\n\nMinified file gzipped with -9 option (same version as above):\n\n```\n-rwxr-xr-x 1 xxxxxxxx mkgroup-l-d  5608 Apr 30 10:39 common-min.js.gz\n\n```\n\nAs you can see, there is a definite difference between the various methods.  The best bet is to both minify as well as gzip them."}
{"instruction": "How to extract filename.tar.gz file", "output": "If file filename.tar.gz gives this message: POSIX tar archive, \nthe archive is a tar, not a GZip archive.\nUnpack a tar without the z, it is for gzipped (compressed), only:\n\n```\nmv filename.tar.gz filename.tar # optional\ntar xvf filename.tar\n\n```\n\nOr try a generic Unpacker like unp (https://packages.qa.debian.org/u/unp.html), a script for unpacking a wide variety of archive formats.\ndetermine the file type: \n\n```\n$ file ~/Downloads/filename.tbz2\n/User/Name/Downloads/filename.tbz2: bzip2 compressed data, block size = 400k\n\n```"}
{"instruction": "Decompressing GZip Stream from HTTPClient Response", "output": "Just instantiate HttpClient like this:\n\n```\nHttpClientHandler handler = new HttpClientHandler()\n{\n    AutomaticDecompression = DecompressionMethods.GZip | DecompressionMethods.Deflate\n};\n\nusing (var client = new HttpClient(handler)) //see update below\n{\n    // your code\n}\n\n```\n\nUpdate June 19, 2020:\nIt's not recommended to use  httpclient in a 'using' block as it might cause port exhaustion.\n\n```\nprivate static HttpClient client = null;\n    \nContructorMethod()\n{\n   if(client == null)\n   {\n        HttpClientHandler handler = new HttpClientHandler()\n        {\n            AutomaticDecompression = DecompressionMethods.GZip | DecompressionMethods.Deflate\n        };        \n        client = new HttpClient(handler);\n   }\n// your code            \n }\n\n```\n\nIf using .Net Core 2.1+, consider using IHttpClientFactory and injecting like this in your startup code.\n\n```\n var timeout = Policy.TimeoutAsync<HttpResponseMessage>(\n            TimeSpan.FromSeconds(60));\n\n services.AddHttpClient<XApiClient>().ConfigurePrimaryHttpMessageHandler(() => new HttpClientHandler\n        {\n            AutomaticDecompression = DecompressionMethods.GZip | DecompressionMethods.Deflate\n        }).AddPolicyHandler(request => timeout);\n\n```"}
{"instruction": "Writing then reading in-memory bytes (BytesIO) gives a blank result", "output": "You need to seek back to the beginning of the file after writing the initial in memory file...\n\n```\nmyio.seek(0)\n\n```"}
{"instruction": "How can I decompress a gzip stream with zlib?", "output": "To decompress a gzip format file with zlib, call inflateInit2 with the windowBits parameter as 16+MAX_WBITS, like this:\n\n```\ninflateInit2(&stream, 16+MAX_WBITS);\n\n```\n\nIf you don't do this, zlib will complain about a bad stream format. By default, zlib creates streams with a zlib header, and on inflate does not recognise the different gzip header unless you tell it so. Although this is documented starting in version 1.2.1 of the zlib.h header file, it is not in the zlib manual. From the header file:\n\nwindowBits can also be greater than 15 for optional gzip decoding. Add\n     32 to windowBits to enable zlib and gzip decoding with automatic header\n     detection, or add 16 to decode only the gzip format (the zlib format will\n     return a Z_DATA_ERROR).  If a gzip stream is being decoded, strm->adler is\n     a crc32 instead of an adler32."}
{"instruction": "uncompress a .txt.gz file in mac?", "output": "gunzip file.txt.gz in Terminal."}
{"instruction": "gzip: stdin: not in gzip format tar: Child returned status 1 tar: Error is not recoverable: exiting now", "output": "This is probably because of your gzip version incompatibility.\nCheck these points first:\n\nwhich gzip\n\n/usr/bin/gzip   or /bin/gzip\nIt should be either /bin/gzip or /usr/bin/gzip. If your gzip points to some other gzip application, please try by removing that path from your PATH environment variable.\nNext is\n\ngzip -V\n\ngzip 1.3.5\n(2002-09-30)\nYour problem can be resolved with these check points."}
{"instruction": "Transfer-Encoding: gzip vs. Content-Encoding: gzip", "output": "Quoting Roy T. Fielding, one of the authors of RFC 2616:\n\nchanging content-encoding on the fly in an inconsistent manner\n  (neither \"never\" nor \"always) makes it impossible for later requests\n  regarding that content (e.g., PUT or conditional GET) to be handled\n  correctly.  This is, of course, why performing on-the-fly\n  content-encoding is a stupid idea, and why I added Transfer-Encoding\n  to HTTP as the proper way to do on-the-fly encoding without changing\n  the resource.\n\nSource: https://issues.apache.org/bugzilla/show_bug.cgi?id=39727#c31\nIn other words: Don't do on-the-fly Content-Encoding, use Transfer-Encoding instead!\nEdit: That is, unless you want to serve gzipped content to clients that only understand Content-Encoding. Which, unfortunately, seems to be most of them. But be aware that you leave the realms of the spec and might run into issues such as the one mentioned by Fielding as well as others, e.g. when caching proxies are involved."}
{"instruction": "Read a gzip file in Python", "output": "Try gzipping some data through the gzip libary like this...\n\n```\nimport gzip\ncontent = \"Lots of content here\"\nf = gzip.open('Onlyfinnaly.log.gz', 'wb')\nf.write(content)\nf.close()\n\n```\n\n... then run your code as posted ...\n\n```\nimport gzip\nf=gzip.open('Onlyfinnaly.log.gz','rb')\nfile_content=f.read()\nprint file_content\n\n```\n\nThis method worked for me as for some reason the gzip library fails to read some files."}
{"instruction": "How to get few lines from a .gz compressed file without uncompressing", "output": "zcat(1) can be supplied by either compress(1) or by gzip(1). On your system, it appears to be compress(1) -- it is looking for a file with a .Z extension.\nSwitch to gzip -cd in place of zcat and your command should work fine:\n\n```\n gzip -cd CONN.20111109.0057.gz | head\n\n```\n\nExplanation\n\n```\n   -c --stdout --to-stdout\n          Write output on standard output; keep original files unchanged.  If there are several input files, the output consists of a sequence of independently compressed members. To obtain better compression, concatenate all input files before compressing\n          them.\n\n   -d --decompress --uncompress\n          Decompress.\n\n```"}
{"instruction": "Android: HTTP communication should use &quot;Accept-Encoding: gzip&quot;", "output": "You should use http headers to indicate a connection can accept gzip encoded data, e.g:\n\n```\nHttpUriRequest request = new HttpGet(url);\nrequest.addHeader(\"Accept-Encoding\", \"gzip\");\n// ...\nhttpClient.execute(request);\n\n```\n\nCheck response for content encoding:\n\n```\nInputStream instream = response.getEntity().getContent();\nHeader contentEncoding = response.getFirstHeader(\"Content-Encoding\");\nif (contentEncoding != null && contentEncoding.getValue().equalsIgnoreCase(\"gzip\")) {\n    instream = new GZIPInputStream(instream);\n}\n\n```"}
{"instruction": "Read lines from compressed text files", "output": "Have you tried using gzip.GzipFile?  Arguments are similar to open."}
{"instruction": "Is there any way to get curl to decompress a response without sending the Accept headers in the request?", "output": "Probably the easiest thing to do is just use gunzip to do it:\n\n```\ncurl -sH 'Accept-encoding: gzip' http://example.com/ | gunzip -\n\n```\n\nOr there's also --compressed, which curl will decompress (I believe) since it knows the response is compressed. But, not sure if that meets your needs."}
{"instruction": "How can I estimate the size of my gzipped script?", "output": "http://closure-compiler.appspot.com/home lets you paste in code, and it will give you compression ratios for a particular file before and after GZIP.\n\n\n```\nOriginal Size:    90 bytes (100 bytes gzipped)\nCompiled Size:    55 bytes (68 bytes gzipped)\nSaved 38.89% off the original size (32.00% off the gzipped size)\n\n```\n\n\nYou can use the pretty-print and white-space only options to estimate the compression of non-minified content.\nIf you need an estimate:\n\nStart with 100 JS files that have gone through the same minification pipeline.\nFor each file, compute the ratio in sizes between gzip -c \"$f\" | wc -c and wc -c \"$f\"\nThe average of those ratios is an approximation of the compression you should expect for a similar JS file.\n\nCygwin contains command line implementations of gzip and wc for Windows."}
{"instruction": "Deflate compression browser compatibility and advantages over GZIP", "output": "UPDATE: Browsers have been dropping support for raw deflate. zOompf has completed some very thorough research on this very topic here. Unfortunately, it appears that raw deflate is NOT safe to use.\n\nCheck http://www.vervestudios.co/projects/compression-tests/results for more results.\n\nHere are the browsers that have been tested:\n\n```\n/*  Browser                       DEFLATE      ZLIB     */\n    XP Internet Explorer 6        PASS         FAIL\n    XP Internet Explorer 7        PASS         FAIL\n    XP Internet Explorer 8        PASS         FAIL\n    Vista Internet Explorer 8     PASS         FAIL\n    XP Firefox 3.6.*              PASS         PASS\n    XP Firefox 3.5.3              PASS         PASS\n    XP Firefox 3.0.14             PASS         PASS\n    Win 7 Firefox 3.6.*           PASS         PASS\n    Vista Firefox 3.6.*           PASS         PASS\n    Vista Firefox 3.5.3           PASS         PASS\n    XP Safari 3                   PASS         PASS\n    XP Safari 4                   PASS         PASS     \n    XP Chrome 3.0.195.27          PASS         PASS\n    XP Opera 9                    PASS         PASS\n    XP Opera 10                   PASS         PASS\n    XP Sea Monkey 1.1.8           PASS         PASS\n    Android 1.6 Browser (v4)*     N/A          N/A\n    OS-X Safari 4                 PASS         PASS\n    OS X Chrome 7.0.517.44        PASS         PASS\n    OS X Opera 10.63              PASS         PASS\n    iPhone 3.1 Safari             PASS         PASS\n\n```\n\n* Android Sends HTTP request header \"Accept-Encoding: gzip\". Deflate is not permitted.\n\nI conclude that we can always send raw DEFLATE (when the HTTP request header \"Accept-Encoding\" contains \"deflate\") and the browser will be able to correctly interpret the encoded data. Can someone prove this wrong?\nnote: .NET's native implementation of DEFLATE (System.IO.Compression.DeflateStream) is raw DEFLATE. It also sucks. Please use zlib.net for all of your .NET deflating needs."}
{"instruction": "Uncompress tar.gz file", "output": "Use -C option of tar:\n\n```\ntar zxvf <yourfile>.tar.gz -C /usr/src/\n\n```\n\nand then, the content of the tar should be in:\n\n```\n/usr/src/<yourfile>\n\n```"}
{"instruction": "Check the total content size of a tar gz file", "output": "This will sum the total content size of the extracted files:\n\n```\n$ tar tzvf archive.tar.gz | sed 's/ \\+/ /g' | cut -f3 -d' ' | sed '2,$s/^/+ /' | paste -sd' ' | bc\n\n```\n\nThe output is given in bytes.\nExplanation: tar tzvf lists the files in the archive in verbose format like ls -l. sed and cut isolate the file size field. The second sed puts a + in front of every size except the first and paste concatenates them, giving a sum expression that is then evaluated by bc.\nNote that this doesn't include metadata, so the disk space taken up by the files when you extract them is going to be larger - potentially many times larger if you have a lot of very small files."}
{"instruction": "htaccess - How to force the client&#39;s browser to clear the cache?", "output": "You can force browsers to cache something, but\nYou can't force browsers to clear their cache.\nThus the only (AMAIK) way is to use a new URL for your resources. Something like versioning."}
{"instruction": "How to implement GZip compression in ASP.NET?", "output": "For compressing JS & CSS files you actually have to handle that at the IIS level, since these files are rendered directly without the ASP.NET runtime.\nYou could make a JSX & CSSX extension mapping in IIS to the aspnet_isapi.dll and then take advantage of your zip code, but IIS is likely to do a better job of this for you.\nThe content-encoding header tells the browser that it needs to unzip the content before rendering.  Some browsers are smart enough to figure this out anyway, based on the shape of the content, but it's better to just tell it.\nThe Accept-encoding cache setting is there so that a cached version of the gzipped content won't be sent to a browser that requested only text/html."}
{"instruction": "Decompress gz file using R", "output": "If you really want to uncompress the file, just use the untar function which does support gzip.\nE.g.:\n\n```\nuntar('chadwick-0.5.3.tar.gz')\n\n```"}
{"instruction": "Which compression method to use in PHP?", "output": "All of these can be used.  There are subtle differences between the three:\n\ngzencode() uses the GZIP file format, the same as the gzip command line tool.  This file format has a header containing optional metadata, DEFLATE compressed data, and footer containing a CRC32 checksum and length check.\ngzcompress() uses the ZLIB format.  It has a shorter header serving only to identify the compression format, DEFLATE compressed data, and a footer containing an ADLER32 checksum.\ngzdeflate() uses the raw DEFLATE algorithm on its own, which is the basis for both of the other formats.\n\nAll three use the same algorithm under the hood, so they won't differ in speed or efficiency.  gzencode() and gzcompress() both add a checksum, so the integrity of the archive can be verified, which can be useful over unreliable transmission and storage methods.  If everything is stored locally and you don't need any additional metadata then gzdeflate() would suffice.  For portability I'd recommend gzencode() (GZIP format) which is probably better supported than gzcompress() (ZLIB format) among other tools.\nWhen compressing very short strings the overhead of each method becomes relevant since for very short input the overhead can comprise a significant part of the output. The overhead for each method, measured by compressing an empty string, is:\n\ngzencode('') - 20 bytes\ngzcompress('') - 8 bytes\ngzdeflate('') - 2 bytes"}
{"instruction": "Python 3, read/write compressed json objects from/to gzip file", "output": "You have four steps of transformation here.\n\na Python data structure (nested dicts, lists, strings, numbers, booleans)\na Python string containing a serialized representation of that data structure (\"JSON\")\na list of bytes containing a representation of that string (\"UTF-8\")\na list of bytes containing a - shorter - representation of that previous byte list (\"gzip\")\n\nSo let's take these steps one by one.\n\n```\nimport gzip\nimport json\n\ndata = []\nfor i in range(N):\n    uid = \"whatever%i\" % i\n    dv = [1, 2, 3]\n    data.append({\n        'what': uid,\n        'where': dv\n    })                                           # 1. data\n\njson_str = json.dumps(data) + \"\\n\"               # 2. string (i.e. JSON)\njson_bytes = json_str.encode('utf-8')            # 3. bytes (i.e. UTF-8)\n\nwith gzip.open(jsonfilename, 'w') as fout:       # 4. fewer bytes (i.e. gzip)\n    fout.write(json_bytes)                       \n\n```\n\nNote that adding \"\\n\" is completely superfluous here. It does not break anything, but beyond that it has no use. I've added that only because you have it in your code sample.\nReading works exactly the other way around:\n\n```\nwith gzip.open(jsonfilename, 'r') as fin:        # 4. gzip\n    json_bytes = fin.read()                      # 3. bytes (i.e. UTF-8)\n\njson_str = json_bytes.decode('utf-8')            # 2. string (i.e. JSON)\ndata = json.loads(json_str)                      # 1. data\n\nprint(data)\n\n```\n\nOf course the steps can be combined:\n\n```\nwith gzip.open(jsonfilename, 'w') as fout:\n    fout.write(json.dumps(data).encode('utf-8'))                       \n\n```\n\nand\n\n```\nwith gzip.open(jsonfilename, 'r') as fin:\n    data = json.loads(fin.read().decode('utf-8'))\n\n```"}
{"instruction": "zlib.error: Error -3 while decompressing: incorrect header check", "output": "Update: dnozay's answer explains the problem and should be the accepted answer. \n\nTry the gzip module, code below is straight from the python docs. \n\n```\nimport gzip\nf = gzip.open('/home/joe/file.txt.gz', 'rb')\nfile_content = f.read()\nf.close()\n\n```"}
{"instruction": "How to create tar.gz archive file in Windows?", "output": "tar.gz file is just a tar file that's been gzipped. Both tar and gzip are available for windows.\nIf you like GUIs (Graphical user interface), 7zip can pack with both tar and gzip."}
{"instruction": "Does python urllib2 automatically uncompress gzip data fetched from webpage?", "output": "How can I tell if the data at a URL is gzipped?\n\n\nThis checks if the content is gzipped and decompresses it:\n\n```\nfrom StringIO import StringIO\nimport gzip\n\nrequest = urllib2.Request('http://example.com/')\nrequest.add_header('Accept-encoding', 'gzip')\nresponse = urllib2.urlopen(request)\nif response.info().get('Content-Encoding') == 'gzip':\n    buf = StringIO(response.read())\n    f = gzip.GzipFile(fileobj=buf)\n    data = f.read()\n\n```\n\n\n\nDoes urllib2 automatically uncompress the data if it is gzipped? Will the data always be a string?\n\n\nNo. The urllib2 doesn't automatically uncompress the data because the 'Accept-Encoding' header is not set by the urllib2 but by you using: request.add_header('Accept-Encoding','gzip, deflate')"}
{"instruction": "Compression formats with good support for random access within archives?", "output": "I don't know of any compressed file format which would support random access to a specific location in the uncompressed data (well, except for multimedia formats), but you can brew your own. \nFor example, bzip2 compressed files are composed of independent compressed blocks of size <1MB uncompressed, which are delimited by sequences of magic bytes, so you could parse the bzip2 file, get the block boundaries and then just uncompress the right block. This would need some indexing to remember where do the blocks start.\nStill, I think the best solution would be to split your file into chunks of your choice, and then compressing it with some archiver, like zip or rar, which support random access to individual files in the archive."}
{"instruction": "What &#39;Content-Type&#39; header to use when serving gzipped files?", "output": "Compressed content in the response is indicated in the Content-Encoding.  The Content-Type should remain the same, that is, it should reflect the underlying media type that is compressed.\n\n```\nContent-Type: application/javascript\nContent-Encoding: gzip\n\n```\n\nSee sections 14.11 Content-Encoding and 3.5 Content Codings of RFC 2616 for more information."}
{"instruction": "Does GZIP Compression Level Have Any Impact On Decompression", "output": "Great question, and an underexposed issue. Your intuition is solid ‚Äì for some compression algorithms, choosing the max level of compression can require more work from the decompressor when it's unpacked.\nLuckily, that's not true for gzip ‚Äì there's no extra overhead for the client/browser to decompress more heavily compressed gzip files (e.g. choosing 9 for compression instead of 6, assuming the standard zlib codebase that most servers use). The best measure for this is decompression rate, which for present purposes is in units of MB/sec, while also monitoring overhead like memory and CPU. Simply going by decompression time is no good because the file is smaller at higher compression settings, and we're not controlling for that factor if we're only using a stopwatch.\n\ngzip decompression quickly gets asymptotic in terms of both time-to-decompress and memory usage once you get past level 6 compressed content. The time-to-decompress flatlines for levels 7, 8, and 9 in the test results linked by Marcus M√ºller, though that's coarse-grained data given in whole seconds.\nYou'll also notice in those results that the memory requirements for decompression are flat for all levels of compression at 0.1 MiB. That's almost unbelievable, just a degree of excellence in software that we rarely see. Mark Adler and colleagues deserve massive props for what they achieved. gzip is a very nice format.\n\nThe memory use gets at your question about overhead. There really is none. You don't gain much with level 9 in terms of browser decompression speed, but you don't lose anything.\nNow, check out these test results for a bit more texture. You'll see how the gzip decompression rate is slightly faster with level 9 compressed content than with lower levels (at level 9, decomp rate is about 0.9% faster than at level 6, for example). That is interesting and surprising. I wouldn't expect the rate to increase. That was just one set of test results ‚Äì it may not hold for other scenarios (and the difference is quite small in any case).\nParting note: Precompressing static files is a good idea, but I don't recommend gzip at level 9. You'll get smaller files than gzip-9 by instead using zopfli or libdeflate. Zopfli is a well-established gzip compressor from Google. libdeflate is new but quite excellent. In my testing it consistently beats gzip-9, but still trails zopfli. You can also use 7-Zip to create gzip files, and it will consistently beat gzip-9. (In the foregoing, gzip-9 refers to using the canonical gzip or zlib application that Apache and nginx use)."}
{"instruction": "Apply GZIP compression to a CSV in Python Pandas", "output": "Using df.to_csv() with the keyword argument compression='gzip' should produce a gzip archive. I tested it using same keyword arguments as you, and it worked.\nYou may need to upgrade pandas, as gzip was not implemented until version 0.17.1, but trying to use it on prior versions will not raise an error, and just produce a regular csv.  You can determine your current version of pandas by looking at the output of pd.__version__."}
{"instruction": "Why can&#39;t browser send gzip request?", "output": "The client and server have to agree on how to communicate; part of this is whether the communication can be compressed. HTTP was designed as a request/response model, and the original creation was almost certainly envisioned to always have small requests and potentially large responses. Compression is not required to implement HTTP, there are both servers and clients that don't support it.\nHTTP compression is implemented by the client saying it can support compression, and if the server sees this in the request and it supports compression it can compress the response. To compress the request the client would have to have a \"pre-request\" that actually negotiated that the request would be made compressed OR it would have to require compression as a supported encoding for ALL requests.\n* UPDATE Feb '17 *\nIt's been 8 years, but as @Phil_1984_ notes, a 3rd possible solution would be for the client and server to negotiate compression support and then use that for subsequent requests. In fact, things like HSTS work just this way with the client caching that the server expects to only speak TLS and ignore any unencrypted links. HTTP was explicitly designed to be stateless but we've moved beyond that at this point."}
{"instruction": "How can I get gzip compression in IIS7 working?", "output": "There was a thread on forums.iis.net about this during the iis 7 beta.  Turned out the guy didn't have the modules installed, but it sounds like you've ruled that out from your opening sentence.\nMicrosofts key advice for him was to enable failed request tracing to find out what was going wrong.  This is possibly one of the most under-appreciated features of IIS7, but certainly one of the most powerful. \n\nOpen IIS Manager.\nGo to your site, and on the actions pane (the very far right), click 'Failed Request Tracing...' under the 'Configure' section.\nClick 'enable'.  \nThen, in the features view, click 'Failed request tracing rules'. Click add, next, enter 200 for the status code, next, click finish.\n\nIf you don't see \"Failed Request Tracing\" in the actions pane, you'll need to add the feature to the server - either using the \"Add Role Services\" wizard (Health and Diagnostics\\Tracing) or through the Web Platform Installer (Products\\Server\\IIS: Tracing), and then close and re-open IIS Manager.\nNext, rerun your test. This will generate some log info for us to examine.\nLook in c:\\inetpub\\logs\\FailedReqLogFiles\\w3svcx. You will see a bunch of files named fr000xx.xml.  Open up any one of them in your browser.  (By the way, if you copy these files anywhere, make sure freb.xsl is there. Also, don't delete freb.xsl - if you do, just delete the whole directory or copy it from another location, as IIS only creates it once per folder.)\nClick the 'request details' tab and select 'complete request trace'.  Search the page for 'compress' - you should find it in several areas; once for static content, and once for dynamic content.  \nIf you don't find either of them, IIS isn't configured correctly. If you do find them, you should see them followed by a compression_success and  a compression_do.  Success is self explanatory; the 'do' indicates what it did - in my case, it showed \"OriginalSize 1462784 CompressedSize 179482\"\nSince yours isn't working, hopefully you will see something different that helps you solve the problem.\nMake sure you turn this off when you're done by disabling failed request tracing in the actions pane for your website."}
{"instruction": "Compressing HTTP Post Data sent from browser", "output": "Will the browser automatically gzip-encode your data for you? The short answer is no.\nThe long answer is that some user-agents can do things like this, but you definitely can't rely on it. The apache mod_deflate docs state:\n\nsome special applications actually do support request compression, for instance some WebDAV clients.\n\nSo, no, that's not going to work. You'll need to generate the appropriate HTTP request message yourself. The appropriate header in this case is Content-Encoding: gzip and NOT Content-Type: because the content itself is application/json, you're just looking to encode the entity body of your HTTP request message for transport.\nNote that you need to also add the appropriate Content-Length: header specifying the size in bytes of the message entity body after compression -OR- send your HTTP message using Transfer-Encoding: chunked and forego the content-length specification.\nOn the receiving end, you can instruct mod_deflate to use an input filter to decompress the information:\n\n```\n<Location /dav-area>\nSetInputFilter DEFLATE\n</Location>\n\n```\n\nThis is a bit heavy handed if you're only receiving compressed message bodies for a couple of resources. Instead, you should probably just use the client-side script to check for the Content-Encoding: gzip header and decompress the request body manually. How to do this in say, PHP, is another question entirely. If you need details for that you should post another question."}
{"instruction": "Why do real-world servers prefer gzip over deflate encoding?", "output": "There is some confusion about the naming between the specifications and the HTTP:\n\nDEFLATE as defined by RFC 1951 is a compressed data format.\nZLIB as defined by RFC 1950 is a compressed data format that uses the DEFLATE data format.\nGZIP as defined by RFC 1952 is a file format that uses the DEFLATE compressed data format.\n\nBut the HTTP uses a different naming:\n\n\ngzip An encoding format produced by the file compression program \"gzip\" (GNU zip) as described in RFC 1952 [25]. This format is a Lempel-Ziv coding (LZ77) with a 32 bit CRC.\n\ndeflate The \"zlib\" format defined in RFC 1950 [31] in combination with the \"deflate\" compression mechanism described in RFC 1951 [29].\n\n\n\nSo to sum up:\n\ngzip is the GZIP file format.\ndeflate is actually the ZLIB data format. (But some clients do also accept the actual DEFLATE data format for deflate.)\n\nSee also this answer on the question What's the difference between the \"gzip\" and \"deflate\" HTTP 1.1 encodings?:\n\nWhat's the difference between the \"gzip\" and \"deflate\" HTTP 1.1 encodings?\n\"gzip\" is the gzip format, and \"deflate\" is the zlib format. They should probably have called the second one \"zlib\" instead to avoid confusion with the raw deflate compressed data format. While the HTTP 1.1 RFC 2616 correctly points to the zlib specification in RFC 1950 for the \"deflate\" transfer encoding, there have been reports of servers and browsers that incorrectly produce or expect raw deflate data per the deflate specification in RFC 1951, most notably Microsoft. So even though the \"deflate\" transfer encoding using the zlib format would be the more efficient approach (and in fact exactly what the zlib format was designed for), using the \"gzip\" transfer encoding is probably more reliable due to an unfortunate choice of name on the part of the HTTP 1.1 authors."}
{"instruction": "Import and insert sql.gz file into database with putty", "output": "The file is a gzipped (compressed) SQL file, almost certainly a plain text file with .sql as its extension. The first thing you need to do is copy the file to your database server via scp.. I think PuTTY's is pscp.exe\n\n```\n# Copy it to the server via pscp\nC:\\> pscp.exe numbers.sql.gz user@serverhostname:/home/user\n\n```\n\nThen SSH into your server and uncompress the file with gunzip\n\n```\nuser@serverhostname$  gunzip numbers.sql.gz\nuser@serverhostname$  ls \n\nnumbers.sql\n\n```\n\nFinally, import it into your MySQL database using the < input redirection operator:\n\n```\nuser@serverhostname$  mysql -u mysqluser -p < numbers.sql\n\n```\n\nIf the numbers.sql file doesn't create a database but expects one to be present already, you will need to include the database in the command as well:\n\n```\nuser@serverhostname$  mysql -u mysqluser -p databasename < numbers.sql\n\n```\n\nIf you have the ability to connect directly to your MySQL server from outside, then you could use a local MySQL client instead of having to copy and SSH.  In that case, you would just need a utility that can decompress .gz files on Windows.  I believe 7zip does so, or you can obtain the gzip/gunzip binaries for Windows."}
{"instruction": "Decode gzipped web page retrieved via cURL in PHP", "output": "The following command enables cURL's \"auto encoding\" mode, where it will announce to the server which encoding methods it supports (via the Accept-Encoding header), and then automatically decompress the response for you:\n\n```\n// Allow cURL to use gzip compression, or any other supported encoding\n// A blank string activates 'auto' mode\ncurl_setopt($ch, CURLOPT_ENCODING , '');\n\n```\n\nIf you specifically want to force the header to be Accept-Encoding: gzip you can use this command instead:\n\n```\n// Allow cURL to use gzip compression, or any other supported encoding\ncurl_setopt($ch, CURLOPT_ENCODING , 'gzip');\n\n```\n\nRead more in the PHP documentation: curl_setopt.\nThanks to commenters for helping improve this answer."}
{"instruction": "Gzip with all cores", "output": "If you are on Linux, you can use GNU's xargs to launch as many processes as you have cores.\n\n```\nCORES=$(grep -c '^processor' /proc/cpuinfo)\nfind /source -type f -print0 | xargs -0 -n 1 -P $CORES gzip -9\n\n```\n\n\nfind -print0 / xargs -0 protects you from whitespace in filenames\nxargs -n 1 means one gzip process per file\nxargs -P specifies the number of jobs\ngzip -9 means maximum compression"}
{"instruction": "How do you create a .gz file using PHP?", "output": "The other answers here load the entire file into memory during compression, which will cause 'out of memory' errors on large files.  The function below should be more reliable on large files as it reads and writes files in 512kb chunks.\n\n```\n/**\n * GZIPs a file on disk (appending .gz to the name)\n *\n * From http://stackoverflow.com/questions/6073397/how-do-you-create-a-gz-file-using-php\n * Based on function by Kioob at:\n * http://www.php.net/manual/en/function.gzwrite.php#34955\n * \n * @param string $source Path to file that should be compressed\n * @param integer $level GZIP compression level (default: 9)\n * @return string New filename (with .gz appended) if success, or false if operation fails\n */\nfunction gzCompressFile($source, $level = 9){ \n    $dest = $source . '.gz'; \n    $mode = 'wb' . $level; \n    $error = false; \n    if ($fp_out = gzopen($dest, $mode)) { \n        if ($fp_in = fopen($source,'rb')) { \n            while (!feof($fp_in)) \n                gzwrite($fp_out, fread($fp_in, 1024 * 512)); \n            fclose($fp_in); \n        } else {\n            $error = true; \n        }\n        gzclose($fp_out); \n    } else {\n        $error = true; \n    }\n    if ($error)\n        return false; \n    else\n        return $dest; \n} \n\n```\n\nUPDATE: Gerben has posted an improved version of this function that is cleaner and uses exceptions instead of returning false on an error. See https://stackoverflow.com/a/56140427/195835"}
{"instruction": "How are zlib, gzip and zip related? What do they have in common and how are they different?", "output": "Short form:\n.zip is an archive format using, usually, the Deflate compression method.  The .gz gzip format is for single files, also using the Deflate compression method.  Often gzip is used in combination with tar to make a compressed archive format, .tar.gz.  The zlib library provides Deflate compression and decompression code for use by zip, gzip, png (which uses the zlib wrapper on deflate data), and many other applications.\nLong form:\nThe ZIP format was developed by Phil Katz as an open format with an open specification, where his implementation, PKZIP, was shareware.  It is an archive format that stores files and their directory structure, where each file is individually compressed.  The file type is .zip.  The files, as well as the directory structure, can optionally be encrypted.\nThe ZIP format supports several compression methods:\n\n```\n    0 - The file is stored (no compression)\n    1 - The file is Shrunk\n    2 - The file is Reduced with compression factor 1\n    3 - The file is Reduced with compression factor 2\n    4 - The file is Reduced with compression factor 3\n    5 - The file is Reduced with compression factor 4\n    6 - The file is Imploded\n    7 - Reserved for Tokenizing compression algorithm\n    8 - The file is Deflated\n    9 - Enhanced Deflating using Deflate64(tm)\n   10 - PKWARE Data Compression Library Imploding (old IBM TERSE)\n   11 - Reserved by PKWARE\n   12 - File is compressed using BZIP2 algorithm\n   13 - Reserved by PKWARE\n   14 - LZMA\n   15 - Reserved by PKWARE\n   16 - IBM z/OS CMPSC Compression\n   17 - Reserved by PKWARE\n   18 - File is compressed using IBM TERSE (new)\n   19 - IBM LZ77 z Architecture \n   20 - deprecated (use method 93 for zstd)\n   93 - Zstandard (zstd) Compression \n   94 - MP3 Compression \n   95 - XZ Compression \n   96 - JPEG variant\n   97 - WavPack compressed data\n   98 - PPMd version I, Rev 1\n   99 - AE-x encryption marker (see APPENDIX E)\n\n```\n\nMethods 1 to 7 are historical and are not in use.  Methods 9 through 98 are relatively recent additions and are in varying, small amounts of use.  The only method in truly widespread use in the ZIP format is method 8, Deflate, and to some smaller extent method 0, which is no compression at all.  Virtually every .zip file that you will come across in the wild will use exclusively methods 8 and 0, likely just method 8.  (Method 8 also has a means to effectively store the data with no compression and relatively little expansion, and Method 0 cannot be streamed whereas Method 8 can be.)\nThe ISO/IEC 21320-1:2015 standard for file containers is a restricted zip format, such as used in Java archive files (.jar), Office Open XML files (Microsoft Office .docx, .xlsx, .pptx), Office Document Format files (.odt, .ods, .odp), and EPUB files (.epub). That standard limits the compression methods to 0 and 8, as well as other constraints such as no encryption or signatures.\nAround 1990, the Info-ZIP group wrote portable, free, open-source implementations of zip and unzip utilities, supporting compression with the Deflate format, and decompression of that and the earlier formats.  This greatly expanded the use of the .zip format.\nIn the early '90s, the gzip format was developed as a replacement for the Unix compress utility, derived from the Deflate code in the Info-ZIP utilities.  Unix compress was designed to compress a single file or stream, appending a .Z to the file name.  compress uses the LZW compression algorithm, which at the time was under patent and its free use was in dispute by the patent holders.  Though some specific implementations of Deflate were patented by Phil Katz, the format was not, and so it was possible to write a Deflate implementation that did not infringe on any patents.  That implementation has not been so challenged in the last 20+ years.  The Unix gzip utility was intended as a drop-in replacement for compress, and in fact is able to decompress compress-compressed data (assuming that you were able to parse that sentence).  gzip appends a .gz to the file name.  gzip uses the Deflate compressed data format, which compresses quite a bit better than Unix compress, has very fast decompression, and adds a CRC-32 as an integrity check for the data.  The header format also permits the storage of more information than the compress format allowed, such as the original file name and the file modification time.\nThough compress only compresses a single file, it was common to use the tar utility to create an archive of files, their attributes, and their directory structure into a single .tar file, and then compress it with compress to make a .tar.Z file.  In fact, the tar utility had and still has the option to do the compression at the same time, instead of having to pipe the output of tar to compress.  This all carried forward to the gzip format, and tar has an option to compress directly to the .tar.gz format.  The tar.gz format compresses better than the .zip approach, since the compression of a .tar can take advantage of redundancy across files, especially many small files.  .tar.gz is the most common archive format in use on Unix due to its very high portability, but there are more effective compression methods in use as well, so you will often see .tar.bz2 and .tar.xz archives.\nUnlike .tar, .zip has a central directory at the end, which provides a list of the contents. That and the separate compression provides random access to the individual entries in a .zip file. A .tar file would have to be decompressed and scanned from start to end in order to build a directory, which is how a .tar file is listed.\nShortly after the introduction of gzip, around the mid-1990s, the same patent dispute called into question the free use of the .gif image format, very widely used on bulletin boards and the World Wide Web (a new thing at the time).  So a small group created the PNG losslessly compressed image format, with file type .png, to replace .gif.  That format also uses the Deflate format for compression, which is applied after filters on the image data expose more of the redundancy.  In order to promote widespread usage of the PNG format, two free code libraries were created.  libpng and zlib.  libpng handled all of the features of the PNG format, and zlib provided the compression and decompression code for use by libpng, as well as for other applications.  zlib was adapted from the gzip code.\nAll of the mentioned patents have since expired.\nThe zlib library supports Deflate compression and decompression, and three kinds of wrapping around the deflate streams.  Those are no wrapping at all (\"raw\" deflate), zlib wrapping, which is used in the PNG format data blocks, and gzip wrapping, to provide gzip routines for the programmer.  The main difference between zlib and gzip wrapping is that the zlib wrapping is more compact, six bytes vs. a minimum of 18 bytes for gzip, and the integrity check, Adler-32, runs faster than the CRC-32 that gzip uses.  Raw deflate is used by programs that read and write the .zip format, which is another format that wraps around deflate compressed data.\nzlib is now in wide use for data transmission and storage.  For example, most HTTP transactions by servers and browsers compress and decompress the data using zlib, specifically HTTP header Content-Encoding: deflate means deflate compression method wrapped inside the zlib data format.\nDifferent implementations of deflate can result in different compressed output for the same input data, as evidenced by the existence of selectable compression levels that allow trading off compression effectiveness for CPU time. zlib and PKZIP are not the only implementations of deflate compression and decompression. Both the 7-Zip archiving utility and Google's zopfli library have the ability to use much more CPU time than zlib in order to squeeze out the last few bits possible when using the deflate format, reducing compressed sizes by a few percent as compared to zlib's highest compression level. The pigz utility, a parallel implementation of gzip, includes the option to use zlib (compression levels 1-9) or zopfli (compression level 11), and somewhat mitigates the time impact of using zopfli by splitting the compression of large files over multiple processors and cores."}
{"instruction": "How to create a zip archive of a directory?", "output": "As others have pointed out, you should use zipfile. The documentation tells you what functions are available, but doesn't really explain how you can use them to zip an entire directory. I think it's easiest to explain with some example code:\n\n```\nimport os\nimport zipfile\n    \ndef zipdir(path, ziph):\n    # ziph is zipfile handle\n    for root, dirs, files in os.walk(path):\n        for file in files:\n            ziph.write(os.path.join(root, file), \n                       os.path.relpath(os.path.join(root, file), \n                                       os.path.join(path, '..')))\n\nwith zipfile.ZipFile('Python.zip', 'w', zipfile.ZIP_DEFLATED) as zipf:\n    zipdir('tmp/', zipf)\n\n```"}
{"instruction": "Unzipping files in Python", "output": "```\nimport zipfile\nwith zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n    zip_ref.extractall(directory_to_extract_to)\n\n```\n\nThat's pretty much it!"}
{"instruction": "Create zip file and ignore directory structure", "output": "You can use -j.\n\n```\n-j\n--junk-paths\n          Store just the name of a saved file (junk the path), and do  not\n          store  directory names. By default, zip will store the full path\n          (relative to the current directory).\n\n```"}
{"instruction": "How can you zip or unzip from the script using ONLY Windows&#39; built-in capabilities?", "output": "Back in 2013, that was not possible. Microsoft didn't provide any executable for this.\nSee this link for some VBS way to do this.\nhttps://superuser.com/questions/201371/create-zip-folder-from-the-command-line-windows\nFrom Windows 8 on, .NET Framework 4.5 is installed by default, with System.IO.Compression.ZipArchive and PowerShell available, one can write scripts to achieve this, see\nhttps://stackoverflow.com/a/26843122/71312"}
{"instruction": "How to create a zip archive with PowerShell?", "output": "If you head on over to CodePlex and grab the PowerShell Community Extensions, you can use their write-zip cmdlet.\nSince \n\nCodePlex is in read-only mode in preparation for shutdown\n\nyou can go to PowerShell Gallery."}
{"instruction": "Is there a zip-like function that pads to longest length?", "output": "In Python 3 you can use itertools.zip_longest\n\n```\n>>> list(itertools.zip_longest(a, b, c))\n[('a1', 'b1', 'c1'), (None, 'b2', 'c2'), (None, 'b3', None)]\n\n```\n\nYou can pad with a different value than None by using the fillvalue parameter:\n\n```\n>>> list(itertools.zip_longest(a, b, c, fillvalue='foo'))\n[('a1', 'b1', 'c1'), ('foo', 'b2', 'c2'), ('foo', 'b3', 'foo')]\n\n```\n\nWith Python 2 you can either use itertools.izip_longest (Python 2.6+), or you can use map with None. It is a little known feature of map (but map changed in Python 3.x, so this only works in Python 2.x).\n\n```\n>>> map(None, a, b, c)\n[('a1', 'b1', 'c1'), (None, 'b2', 'c2'), (None, 'b3', None)]\n\n```"}
{"instruction": "What is the difference between tar and zip?", "output": "tar in itself just bundles files together (the result is called a tarball), while zip applies compression as well.\nUsually you use gzip along with tar to compress the resulting tarball, thus achieving similar results as with zip.\nFor reasonably large archives there are important differences though.  A zip archive is a collection of compressed files.  A gzipped tar is a compressed collection (of uncompressed files).  Thus a zip archive is a randomly accessible list of concatenated compressed items, and a .tar.gz is an archive that must be fully expanded before the catalog is accessible.\n\nThe caveat of a zip is that you don't get compression across files (because each file is compressed independent of the others in the archive, the compression cannot take advantage of similarities among the contents of different files); the advantage is that you can access any of the files contained within by looking at only a specific (target file dependent) section of the archive (as the \"catalog\" of the collection is separate from the collection itself).\nThe caveat of a .tar.gz is that you must decompress the whole archive to access files contained therein (as the files are within the tarball); the advantage is that the compression can take advantage of similarities among the files (as it compresses the whole tarball)."}
{"instruction": "BitBucket - download source as ZIP", "output": "For the latest version of Bitbucket (2016+), the download link can be found in the Download menu item.\n\n\nPre-2016\nFirst method\nIn the Overview page of the repo, there is a link to download the project. \n\n\nSecond method\nGo to Downloads -> Branches -> Download the branch that you want (as .zip, .gz or .bz2).  There you'll find download links for all tags.  The links will be in the format:\n\n```\nhttps://bitbucket.org/owner/repository/get/v0.1.2.tar.gz\n\n```\n\nBy tweaking it a little bit, you can also have access to any revision by changing the tag to the commit hash:\n\n```\nhttps://bitbucket.org/owner/repository/get/A0B1C2D.tar.gz\n\n```"}
{"instruction": "What is a good Java library to zip/unzip files?", "output": "I know its late and there are lots of answers but this zip4j is one of the best libraries for zipping I have used. Its simple (no boiler code) and can easily handle password protected files.\n\n```\nimport net.lingala.zip4j.exception.ZipException;\nimport net.lingala.zip4j.core.ZipFile;\n\n\npublic static void unzip(){\n    String source = \"some/compressed/file.zip\";\n    String destination = \"some/destination/folder\";\n    String password = \"password\";\n\n    try {\n         ZipFile zipFile = new ZipFile(source);\n         if (zipFile.isEncrypted()) {\n            zipFile.setPassword(password);\n         }\n         zipFile.extractAll(destination);\n    } catch (ZipException e) {\n        e.printStackTrace();\n    }\n}\n\n```\n\nThe Maven dependency is:\n\n```\n<dependency>\n    <groupId>net.lingala.zip4j</groupId>\n    <artifactId>zip4j</artifactId>\n    <version>1.3.2</version>\n</dependency>\n\n```"}
{"instruction": "Mac zip compress without __MACOSX folder?", "output": "When I had this problem I've done it from command line:\n\n```\nzip file.zip uncompressed\n\n```\n\nEDIT, after many downvotes: I was using this option for some time ago and I don't know where I learnt it, so I can't give you a better explanation. Chris Johnson's answer is correct, but I won't delete mine. As one comment says, it's more accurate to what OP is asking, as it compress without those files, instead of removing them from a compressed file. I find it easier to remember, too."}
{"instruction": "Creating a ZIP archive in memory using System.IO.Compression", "output": "Thanks to ZipArchive creates invalid ZIP file, I got:\n\n```\nusing (var memoryStream = new MemoryStream())\n{\n   using (var archive = new ZipArchive(memoryStream, ZipArchiveMode.Create, true))\n   {\n      var demoFile = archive.CreateEntry(\"foo.txt\");\n\n      using (var entryStream = demoFile.Open())\n      using (var streamWriter = new StreamWriter(entryStream))\n      {\n         streamWriter.Write(\"Bar!\");\n      }\n   }\n\n   using (var fileStream = new FileStream(@\"C:\\Temp\\test.zip\", FileMode.Create))\n   {\n      memoryStream.Seek(0, SeekOrigin.Begin);\n      memoryStream.CopyTo(fileStream);\n   }\n}\n\n```\n\nThat indicated we need to call Dispose on ZipArchive before we can use it, which as Amir suggests is likely because it writes final bytes like checksum to the archive that makes it complete. But in order not close the stream so we can re-use it after you need to pass true as the third parameter to ZipArchive."}
{"instruction": "Fatal error: Class &#39;ZipArchive&#39; not found in", "output": "For the ZipArchive class to be present, PHP needs to have the zip extension installed.\nSee this page for installation instructions (both Linux and Windows).\nOn Debian and Ubuntu, you can usually install it with:\n\n```\nsudo apt update\nsudo apt install php-zip\n\n```\n\nThen restart your webserver. Example:\n\n```\nsudo systemctl restart apache2\n\n```"}
{"instruction": "How do I ZIP a file in C#, using no 3rd-party APIs?", "output": "Are you using .NET 3.5?  You could use the ZipPackage class and related classes.  Its more than just zipping up a file list because it wants a MIME type for each file you add.  It might do what you want.\nI'm currently using these classes for a similar problem to archive several related files into a single file for download.  We use a file extension to associate the download file with our desktop app.  One small problem we ran into was that its not possible to just use a third-party tool like 7-zip to create the zip files because the client side code can't open it -- ZipPackage adds a hidden file describing the content type of each component file and cannot open a zip file if that content type file is missing."}
{"instruction": ".rar, .zip files MIME Type", "output": "The answers from freedompeace, Kiyarash and Sam Vloeberghs:\n\n```\n.rar    application/vnd.rar, application/x-rar-compressed, application/octet-stream\n.zip    application/zip, application/octet-stream, application/x-zip-compressed, multipart/x-zip\n\n```\n\nI would do a check on the file name too. Here is how you could check if the file is a RAR or ZIP file. I tested it by creating a quick command line application.\n\n```\n<?php\n\nif (isRarOrZip($argv[1])) {\n    echo 'It is probably a RAR or ZIP file.';\n} else {\n    echo 'It is probably not a RAR or ZIP file.';\n}\n\nfunction isRarOrZip($file) {\n    // get the first 7 bytes\n    $bytes = file_get_contents($file, FALSE, NULL, 0, 7);\n    $ext = strtolower(substr($file, - 4));\n\n    // RAR magic number: Rar!\\x1A\\x07\\x00\n    // http://en.wikipedia.org/wiki/RAR\n    if ($ext == '.rar' and bin2hex($bytes) == '526172211a0700') {\n        return TRUE;\n    }\n\n    // ZIP magic number: none, though PK\\003\\004, PK\\005\\006 (empty archive), \n    // or PK\\007\\008 (spanned archive) are common.\n    // http://en.wikipedia.org/wiki/ZIP_(file_format)\n    if ($ext == '.zip' and substr($bytes, 0, 2) == 'PK') {\n        return TRUE;\n    }\n\n    return FALSE;\n}\n\n```\n\nNotice that it still won't be 100% certain, but it is probably good enough.\n\n```\n$ rar.exe l somefile.zip\nsomefile.zip is not RAR archive\n\n```\n\nBut even WinRAR detects non RAR files as SFX archives:\n\n```\n$ rar.exe l somefile.srr\nSFX Volume somefile.srr\n\n```"}
{"instruction": "How to create full compressed tar file using Python?", "output": "To build a .tar.gz (aka .tgz) for an entire directory tree:\n\n```\nimport tarfile\nimport os.path\n\ndef make_tarfile(output_filename, source_dir):\n    with tarfile.open(output_filename, \"w:gz\") as tar:\n        tar.add(source_dir, arcname=os.path.basename(source_dir))\n\n```\n\nThis will create a gzipped tar archive containing a single top-level folder with the same name and contents as source_dir."}
{"instruction": "How to unzip a file using the command line?", "output": "You could use :\nhttp://membrane.com/synapse/library/pkunzip.html\nor\n7zip: http://www.7-zip.org/download.html\nFree byte zip: http://www.freebyte.com/fbzip/\nor infozip: http://infozip.sourceforge.net/"}
{"instruction": "How to zip a whole folder using PHP", "output": "Zip a whole folder:\n\n```\n// Remove any trailing slashes from the path\n$rootPath = rtrim($rootPath, '\\\\/');\n\n// Get real path for our folder\n$rootPath = realpath('folder-to-zip');\n\n// Initialize archive object\n$zip = new ZipArchive();\n$zip->open('file.zip', ZipArchive::CREATE | ZipArchive::OVERWRITE);\n\n// Create recursive directory iterator\n/** @var SplFileInfo[] $files */\n$files = new RecursiveIteratorIterator(\n    new RecursiveDirectoryIterator($rootPath),\n    RecursiveIteratorIterator::LEAVES_ONLY\n);\n\nforeach ($files as $file)\n{\n    // Skip directories (they would be added automatically)\n    if (!$file->isDir())\n    {\n        // Get real and relative path for current file\n        $filePath = $file->getRealPath();\n        $relativePath = substr($filePath, strlen($rootPath) + 1);\n\n        // Add current file to archive\n        $zip->addFile($filePath, $relativePath);\n    }\n}\n\n// Zip archive will be created only after closing object\n$zip->close();\n\n```\n\nZip a whole folder + delete all files except \"important.txt\":\n\n```\n// Remove any trailing slashes from the path\n$rootPath = rtrim($rootPath, '\\\\/');\n\n// Get real path for our folder\n$rootPath = realpath('folder-to-zip');\n\n// Initialize archive object\n$zip = new ZipArchive();\n$zip->open('file.zip', ZipArchive::CREATE | ZipArchive::OVERWRITE);\n\n// Initialize empty \"delete list\"\n$filesToDelete = array();\n\n// Create recursive directory iterator\n/** @var SplFileInfo[] $files */\n$files = new RecursiveIteratorIterator(\n    new RecursiveDirectoryIterator($rootPath),\n    RecursiveIteratorIterator::LEAVES_ONLY\n);\n\nforeach ($files as $file)\n{\n    // Skip directories (they would be added automatically)\n    if (!$file->isDir())\n    {\n        // Get real and relative path for current file\n        $filePath = $file->getRealPath();\n        $relativePath = substr($filePath, strlen($rootPath) + 1);\n\n        // Add current file to archive\n        $zip->addFile($filePath, $relativePath);\n\n        // Add current file to \"delete list\"\n        // delete it later cause ZipArchive create archive only after calling close function and ZipArchive lock files until archive created)\n        if ($file->getFilename() != 'important.txt')\n        {\n            $filesToDelete[] = $filePath;\n        }\n    }\n}\n\n// Zip archive will be created only after closing object\n$zip->close();\n\n// Delete all files from \"delete list\"\nforeach ($filesToDelete as $file)\n{\n    unlink($file);\n}\n\n```"}
{"instruction": "How to create a zip file in Java", "output": "Look at this example:\n\n```\nStringBuilder sb = new StringBuilder();\nsb.append(\"Test String\");\n\nFile f = new File(\"d:\\\\test.zip\");\nZipOutputStream out = new ZipOutputStream(new FileOutputStream(f));\nZipEntry e = new ZipEntry(\"mytext.txt\");\nout.putNextEntry(e);\n\nbyte[] data = sb.toString().getBytes();\nout.write(data, 0, data.length);\nout.closeEntry();\n\nout.close();\n\n```\n\nThis will create a zip in the root of D: named test.zip which will contain one single file called mytext.txt. Of course you can add more zip entries and also specify a subdirectory like this:\n\n```\nZipEntry e = new ZipEntry(\"folderName/mytext.txt\");\n\n```\n\nYou can find more information about compression with Java here."}
{"instruction": "Need to ZIP an entire directory using Node.js", "output": "I ended up using archiver lib. Works great.\nExample\n\n```\nvar file_system = require('fs');\nvar archiver = require('archiver');\n\nvar output = file_system.createWriteStream('target.zip');\nvar archive = archiver('zip');\n\noutput.on('close', function () {\n    console.log(archive.pointer() + ' total bytes');\n    console.log('archiver has been finalized and the output file descriptor has closed.');\n});\n\narchive.on('error', function(err){\n    throw err;\n});\n\narchive.pipe(output);\n\n// append files from a sub-directory, putting its contents at the root of archive\narchive.directory(source_dir, false);\n\n// append files from a sub-directory and naming it `new-subdir` within the archive\narchive.directory('subdir/', 'new-subdir');\n\narchive.finalize();\n\n```"}
{"instruction": "How to use Python&#39;s pip to download and keep the zipped files for a package?", "output": "pip install --download is deprecated. Starting from version 8.0.0 you should use pip download command:\n\n```\n pip download <package-name>\n\n```"}
{"instruction": "Download Returned Zip file from URL", "output": "Most people recommend using requests if it is available, and the requests documentation recommends this for downloading and saving raw data from a url:\n\n```\nimport requests \n\ndef download_url(url, save_path, chunk_size=128):\n    r = requests.get(url, stream=True)\n    with open(save_path, 'wb') as fd:\n        for chunk in r.iter_content(chunk_size=chunk_size):\n            fd.write(chunk)\n\n```\n\nSince the answer asks about downloading and saving the zip file, I haven't gone into details regarding reading the zip file. See one of the many answers below for possibilities.\nIf for some reason you don't have access to requests, you can use urllib.request instead. It may not be quite as robust as the above.\n\n```\nimport urllib.request\n\ndef download_url(url, save_path):\n    with urllib.request.urlopen(url) as dl_file:\n        with open(save_path, 'wb') as out_file:\n            out_file.write(dl_file.read())\n\n```\n\nFinally, if you are using Python 2 still, you can use urllib2.urlopen.\n\n```\nfrom contextlib import closing\n\ndef download_url(url, save_path):\n    with closing(urllib2.urlopen(url)) as dl_file:\n        with open(save_path, 'wb') as out_file:\n            out_file.write(dl_file.read())\n\n```"}
{"instruction": "Using R to download zipped data file, extract, and import data", "output": "Zip archives are actually more a 'filesystem' with content metadata etc. See help(unzip) for details.  So to do what you sketch out above you need to \n\nCreate a temp. file name (eg tempfile())\nUse download.file() to fetch the file into the temp. file\nUse unz() to extract the target file from temp. file\nRemove the temp file via unlink()\n\nwhich in code (thanks for basic example, but this is simpler) looks like\n\n```\ntemp <- tempfile()\ndownload.file(\"http://www.newcl.org/data/zipfiles/a1.zip\",temp)\ndata <- read.table(unz(temp, \"a1.dat\"))\nunlink(temp)\n\n```\n\nCompressed (.z) or gzipped (.gz) or bzip2ed (.bz2) files are just the file and those you can read directly from a connection.  So get the data provider to use that instead :)"}
{"instruction": "How do I export my project as a .zip of git repository?", "output": "git archive --format=zip --output /full/path/to/zipfile.zip master"}
{"instruction": "Python: Open file in zip without temporarily extracting it", "output": "Vincent Povirk's answer won't work completely;\n\n```\nimport zipfile\narchive = zipfile.ZipFile('images.zip', 'r')\nimgfile = archive.open('img_01.png')\n...\n\n```\n\nYou have to change it in:\n\n```\nimport zipfile\narchive = zipfile.ZipFile('images.zip', 'r')\nimgdata = archive.read('img_01.png')\n...\n\n```\n\nFor details read the ZipFile docs here."}
{"instruction": "How does one make a Zip bomb?", "output": "Citing from the Wikipedia page:\n\nOne example of a Zip bomb is the file\n  45.1.zip which was 45.1 kilobytes of compressed data, containing nine\n  layers of nested zip files in sets of\n  10, each bottom layer archive\n  containing a 1.30 gigabyte file for a\n  total of 1.30 exabytes of uncompressed\n  data.\n\nSo all you need is one single 1.3GB file full of zeroes, compress that into a ZIP file, make 10 copies, pack those into a ZIP file, and repeat this process 9 times. \nThis way, you get a file which, when uncompressed completely, produces an absurd amount of data without requiring you to start out with that amount. \nAdditionally, the nested archives make it much harder for programs like virus scanners (the main target of these \"bombs\") to be smart and refuse to unpack archives that are \"too large\", because until the last level the total amount of data is not that much, you don't \"see\" how large the files at the lowest level are until you have reached that level, and each individual file is not \"too large\" - only the huge number is problematic."}
{"instruction": "Zip with list output instead of tuple", "output": "If you are zipping more than 2 lists (or even only 2, for that matter), a readable way would be:\n\n```\n[list(a) for a in zip([1,2,3], [4,5,6], [7,8,9])]\n\n```\n\nThis uses a list comprehension to apply list to each element (tuple) in the list, converting them into lists."}
{"instruction": "Download multiple files as a zip-file using php", "output": "You can use the ZipArchive class to create a ZIP file and stream it to the client. Something like:\n\n```\n$files = array('readme.txt', 'test.html', 'image.gif');\n$zipname = 'file.zip';\n$zip = new ZipArchive;\n$zip->open($zipname, ZipArchive::CREATE);\nforeach ($files as $file) {\n  $zip->addFile($file);\n}\n$zip->close();\n\n```\n\nand to stream it:\n\n```\nheader('Content-Type: application/zip');\nheader('Content-disposition: attachment; filename='.$zipname);\nheader('Content-Length: ' . filesize($zipname));\nreadfile($zipname);\n\n```\n\nThe second line forces the browser to present a download box to the user and prompts the name filename.zip. The third line is optional but certain (mainly older) browsers have issues in certain cases without the content size being specified."}
{"instruction": "Python in-memory zip library", "output": "According to the Python docs:\n\n```\nclass zipfile.ZipFile(file[, mode[, compression[, allowZip64]]])\n\n  Open a ZIP file, where file can be either a path to a file (a string) or a file-like object. \n\n```\n\nSo, to open the file in memory, just create a file-like object (perhaps using BytesIO).\n\n```\nfile_like_object = io.BytesIO(my_zip_data)\nzipfile_ob = zipfile.ZipFile(file_like_object)\n\n```"}
{"instruction": "How to update one file in a zip archive", "output": "From zip(1):\n\nWhen given the name of an existing zip archive, zip will replace identically named entries in the zip archive or add entries for new names.\n\nSo just use the zip command as you normally would to create a new .zip file containing only that one file, except the .zip filename you specify will be the existing archive."}
{"instruction": "Simplest way to download and unzip files in Node.js cross-platform?", "output": "Node has builtin support for gzip and deflate via the zlib module:\n\n```\nvar zlib = require('zlib');\n\nzlib.gunzip(gzipBuffer, function(err, result) {\n    if(err) return console.error(err);\n\n    console.log(result);\n});\n\n```\n\nEdit: You can even pipe the data directly through e.g. Gunzip (using request):\n\n```\nvar request = require('request'),\n    zlib = require('zlib'),\n    fs = require('fs'),\n    out = fs.createWriteStream('out');\n\n// Fetch http://example.com/foo.gz, gunzip it and store the results in 'out'\nrequest('http://example.com/foo.gz').pipe(zlib.createGunzip()).pipe(out);\n\n```\n\nFor tar archives, there is Isaacs' tar module, which is used by npm.\nEdit 2: Updated answer as zlib doesn't support the zip format. This will only work for gzip."}
{"instruction": "I didn&#39;t find &quot;ZipFile&quot; class in the &quot;System.IO.Compression&quot; namespace", "output": "You need an extra reference for this; the most convenient way to do this is via the NuGet package System.IO.Compression.ZipFile\n\n```\n<!-- Version here correct at time of writing, but please check for latest -->\n<PackageReference Include=\"System.IO.Compression.ZipFile\" Version=\"4.3.0\" />\n\n```\n\nIf you are working on .NET Framework without NuGet, you need to add a dll reference to the assembly, \"System.IO.Compression.FileSystem.dll\" - and ensure you are using at least .NET 4.5 (since it doesn't exist in earlier frameworks).\nFor info, you can find the assembly and .NET version(s) from MSDN"}
{"instruction": "How to download/checkout a project from Google Code in Windows?", "output": "If you don't want to install anything but do want to download an SVN or GIT repository, then you can use this: http://downloadsvn.codeplex.com/\nI have nothing to do with this project, but I just used it now and it saved me a few minutes. Maybe it will help someone."}
{"instruction": "Pairs from single list", "output": "My favorite way to do it:\n\n```\ndef pairwise(t):\n    it = iter(t)\n    return zip(it,it)\n\n# for \"pairs\" of any length\ndef chunkwise(t, size=2):\n    it = iter(t)\n    return zip(*[it]*size)\n\n```\n\nWhen you want to pair all elements you obviously might need a fillvalue:\n\n```\nfrom itertools import izip_longest\ndef blockwise(t, size=2, fillvalue=None):\n    it = iter(t)\n    return izip_longest(*[it]*size, fillvalue=fillvalue)\n\n```\n\nWith Python 3, itertools.izip is now simply zip .. to work with an older Python, use\n\n```\nfrom itertools import izip as zip\n\n```"}
{"instruction": "How to [recursively] Zip a directory in PHP?", "output": "Here is a simple function that can compress any file or directory recursively, only needs the zip extension to be loaded.\n\n```\nfunction Zip($source, $destination)\n{\n    if (!extension_loaded('zip') || !file_exists($source)) {\n        return false;\n    }\n\n    $zip = new ZipArchive();\n    if (!$zip->open($destination, ZIPARCHIVE::CREATE)) {\n        return false;\n    }\n\n    $source = str_replace('\\\\', '/', realpath($source));\n\n    if (is_dir($source) === true)\n    {\n        $files = new RecursiveIteratorIterator(new RecursiveDirectoryIterator($source), RecursiveIteratorIterator::SELF_FIRST);\n\n        foreach ($files as $file)\n        {\n            $file = str_replace('\\\\', '/', $file);\n\n            // Ignore \".\" and \"..\" folders\n            if( in_array(substr($file, strrpos($file, '/')+1), array('.', '..')) )\n                continue;\n\n            $file = realpath($file);\n\n            if (is_dir($file) === true)\n            {\n                $zip->addEmptyDir(str_replace($source . '/', '', $file . '/'));\n            }\n            else if (is_file($file) === true)\n            {\n                $zip->addFromString(str_replace($source . '/', '', $file), file_get_contents($file));\n            }\n        }\n    }\n    else if (is_file($source) === true)\n    {\n        $zip->addFromString(basename($source), file_get_contents($source));\n    }\n\n    return $zip->close();\n}\n\n```\n\nCall it like this:\n\n```\nZip('/folder/to/compress/', './compressed.zip');\n\n```"}
{"instruction": "How to read data from a zip file without having to unzip the entire file", "output": "DotNetZip is your friend here.\nAs easy as:\n\n```\nusing (ZipFile zip = ZipFile.Read(ExistingZipFile))\n{\n  ZipEntry e = zip[\"MyReport.doc\"];\n  e.Extract(OutputStream);\n}\n\n```\n\n(you can also extract to a file or other destinations).\nReading the zip file's table of contents is as easy as:\n\n```\nusing (ZipFile zip = ZipFile.Read(ExistingZipFile))\n{\n  foreach (ZipEntry e in zip)\n  {\n    if (header)\n    {\n      System.Console.WriteLine(\"Zipfile: {0}\", zip.Name);\n      if ((zip.Comment != null) && (zip.Comment != \"\")) \n        System.Console.WriteLine(\"Comment: {0}\", zip.Comment);\n      System.Console.WriteLine(\"\\n{1,-22} {2,8}  {3,5}   {4,8}  {5,3} {0}\",\n                               \"Filename\", \"Modified\", \"Size\", \"Ratio\", \"Packed\", \"pw?\");\n      System.Console.WriteLine(new System.String('-', 72));\n      header = false;\n    }\n    System.Console.WriteLine(\"{1,-22} {2,8} {3,5:F0}%   {4,8}  {5,3} {0}\",\n                             e.FileName,\n                             e.LastModified.ToString(\"yyyy-MM-dd HH:mm:ss\"),\n                             e.UncompressedSize,\n                             e.CompressionRatio,\n                             e.CompressedSize,\n                             (e.UsesEncryption) ? \"Y\" : \"N\");\n\n  }\n}\n\n```\n\n\nEdited To Note: DotNetZip used to live at Codeplex. Codeplex has been shut down. The old archive is still available at Codeplex. It looks like the code has migrated to Github:\n\nhttps://github.com/DinoChiesa/DotNetZip. Looks to be the original author's repo.\nhttps://github.com/haf/DotNetZip.Semverd. This looks to be the currently maintained version. It's also packaged up an available via Nuget at https://www.nuget.org/packages/DotNetZip/"}
{"instruction": "Zip including hidden files", "output": "EDIT: The correct way is zip -r 1.zip .\nThe commands shown in my previous answer below are incorrect because they also include the parent directory.\n\nHave you tried this:\n\n```\nzip yourfile.zip sourcedir/* .*\n\n```\n\nor you in your case\n\n```\nzip 1.zip * .[^.]*\n\n```\n\nIt should include all hidden files also."}
{"instruction": "get file list of files contained in a zip file", "output": "What you need is ZipFile.namelist() that will give you a list of all the contents of the archive, you can then do a zip.open('filename_you_discover') to get the contents of that file."}
{"instruction": "How can I list the files in a zip archive without decompressing it?", "output": "Perreal's answer is right, but I recommend installing atool (look for it in your distribution's package manager). Then, for any kind of archive file, bzip2, gzip, tar... you have just one command to remember :\n\n```\nals archive_name\n\n```"}
{"instruction": "How to eliminate absolute path in zip archive if absolute paths for files are provided?", "output": "The zipfile write() method supports an extra argument (arcname) which is the archive name to be stored in the zip file, so you would only need to change your code with:\n\n```\nfrom os.path import basename\n...\nzip.write(first_path, basename(first_path))\nzip.write(second_path, basename(second_path))\nzip.close()\n\n```\n\nWhen you have some spare time reading the documentation for zipfile will be helpful."}
{"instruction": "Can I zip more than two lists together in Scala?", "output": "I don't believe it's possible to generate a list of tuples of arbitrary size, but the transpose function does exactly what you need if you don't mind getting a list of lists instead."}
{"instruction": "Can Git treat ZIP files as directories and files inside the ZIP as blobs?", "output": "This doesn't exist, but it could easily exist in the current framework. Just as Git acts differently with displaying binary or ASCII files when performing a diff, it could be told to offer special treatment to certain file types through the configuration interface.\nIf you don't want to change the code base (although this is kind of a cool idea you've got), you could also script it for yourself by using pre-commit and post-checkout hooks to unzip and store the files, then return them to their .zip state on checkout. You would have to restrict actions to only those files blobs / indexes that are specified by git add.\nEither way is a bit of work -- it's just a question of whether the other Git commands are aware of what's going on and play nicely."}
{"instruction": "Read Content from Files which are inside Zip file", "output": "If you're wondering how to get the file content from each ZipEntry it's actually quite simple. Here's a sample code:\n\n```\npublic static void main(String[] args) throws IOException {\n    ZipFile zipFile = new ZipFile(\"C:/test.zip\");\n\n    Enumeration<? extends ZipEntry> entries = zipFile.entries();\n\n    while(entries.hasMoreElements()){\n        ZipEntry entry = entries.nextElement();\n        InputStream stream = zipFile.getInputStream(entry);\n    }\n}\n\n```\n\nOnce you have the InputStream you can read it however you want."}
{"instruction": "Extracting a zipfile to memory?", "output": "extractall extracts to the file system, so you won't get what you want. To extract a file in memory, use the ZipFile.read() method.\nIf you really need the full content in memory, you could do something like:\n\n```\ndef extract_zip(input_zip):\n    input_zip=ZipFile(input_zip)\n    return {name: input_zip.read(name) for name in input_zip.namelist()}\n\n```"}
{"instruction": "Install php-zip on php 5.6 on Ubuntu", "output": "Try either \n\nsudo apt-get install php-zip or\nsudo apt-get install php5.6-zip\n\nThen, you might have to restart your web server.\n\nsudo service apache2 restart or\nsudo service nginx restart\n\nIf you are installing on centos or fedora OS then use yum in place of apt-get. example:-\nsudo yum install php-zip or\nsudo yum install php5.6-zip and \nsudo service httpd restart"}
{"instruction": "Unzipping files", "output": "I wrote an unzipper in Javascript.  It works.  \nIt relies on Andy G.P. Na's binary file reader and some RFC1951 inflate logic from notmasteryet.  I added the ZipFile class.  \nworking example:\nhttp://cheeso.members.winisp.net/Unzip-Example.htm (dead link)\nThe source:\nhttp://cheeso.members.winisp.net/srcview.aspx?dir=js-unzip (dead link)\nNB: the links are dead; I'll find a new host soon. \nIncluded in the source is a ZipFile.htm demonstration page, and 3 distinct scripts, one for the zipfile class, one for the inflate class, and one for a binary file reader class. The demo also depends on jQuery and jQuery UI.  If you just download the js-zip.zip file, all of the necessary source is there.  \n\nHere's what the application code looks like in Javascript:\n\n```\n// In my demo, this gets attached to a click event.\n// it instantiates a ZipFile, and provides a callback that is\n// invoked when the zip is read.  This can take a few seconds on a\n// large zip file, so it's asynchronous. \nvar readFile = function(){\n    $(\"#status\").html(\"<br/>\");\n    var url= $(\"#urlToLoad\").val();\n    var doneReading = function(zip){\n        extractEntries(zip);\n    };\n\n    var zipFile = new ZipFile(url, doneReading);\n};\n\n\n// this function extracts the entries from an instantiated zip\nfunction extractEntries(zip){\n    $('#report').accordion('destroy');\n\n    // clear\n    $(\"#report\").html('');\n\n    var extractCb = function(id) {\n        // this callback is invoked with the entry name, and entry text\n        // in my demo, the text is just injected into an accordion panel.\n        return (function(entryName, entryText){\n            var content = entryText.replace(new RegExp( \"\\\\n\", \"g\" ), \"<br/>\");\n            $(\"#\"+id).html(content);\n            $(\"#status\").append(\"extract cb, entry(\" + entryName + \")  id(\" + id + \")<br/>\");\n            $('#report').accordion('destroy');\n            $('#report').accordion({collapsible:true, active:false});\n        });\n    }\n\n    // for each entry in the zip, extract it. \n    for (var i=0; i<zip.entries.length;  i++) {\n        var entry = zip.entries[i];\n\n        var entryInfo = \"<h4><a>\" + entry.name + \"</a></h4>\\n<div>\";\n\n        // contrive an id for the entry, make it unique\n        var randomId = \"id-\"+ Math.floor((Math.random() * 1000000000));\n\n        entryInfo += \"<span class='inputDiv'><h4>Content:</h4><span id='\" + randomId +\n            \"'></span></span></div>\\n\";\n\n        // insert the info for one entry as the last child within the report div\n        $(\"#report\").append(entryInfo);\n\n        // extract asynchronously\n        entry.extract(extractCb(randomId));\n    }\n}\n\n```\n\n\nThe demo works in a couple of steps:  The readFile fn is triggered by a click, and instantiates a ZipFile object, which reads the zip file. There's an asynchronous callback for when the read completes (usually happens in less than a second for reasonably sized zips) - in this demo the callback is held in the doneReading local variable, which simply calls extractEntries, which \njust blindly unzips all the content of the provided zip file.  In a real app you would probably choose some of the entries to extract (allow the user to select, or choose one or more entries programmatically, etc). \nThe extractEntries fn iterates over all entries, and calls extract() on each one, passing a callback.  Decompression of an entry takes time, maybe 1s or more for each entry in the zipfile, which means asynchrony is appropriate. The extract callback simply adds the extracted content to an jQuery accordion on the page. If the content is binary, then it gets formatted as such (not shown). \n\nIt works, but I think that the utility is somewhat limited. \nFor one thing: It's very slow. Takes ~4 seconds to unzip the 140k AppNote.txt file from PKWare. The same uncompress can be done in less than .5s in a .NET program. EDIT: The Javascript ZipFile unpacks considerably faster than this now, in IE9 and in Chrome. It is still slower than a compiled program, but it is plenty fast for normal browser usage.\nFor another: it does not do streaming. It basically slurps in the entire contents of the zipfile into memory. In a \"real\" programming environment you could read in only the metadata of a zip file (say, 64 bytes per entry) and then read and decompress the other data as desired.  There's no way to do IO like that in javascript, as far as I know, therefore the only option is to read the entire zip into memory and do random access in it.  This means it will place unreasonable demands on system memory for large zip files. Not so much a problem for a smaller zip file. \nAlso: It doesn't handle the \"general case\" zip file - there are lots of zip options that I didn't bother to implement in the unzipper - like ZIP encryption, WinZip encryption, zip64, UTF-8 encoded filenames, and so on. (EDIT - it handles UTF-8 encoded filenames now). The ZipFile class handles the basics, though. Some of these things would not be hard to implement.  I have an AES encryption class in Javascript; that could be integrated to support encryption. Supporting Zip64 would probably useless for most users of Javascript, as it is intended to support >4gb zipfiles - don't need to extract those in a browser. \nI also did not test the case for unzipping binary content. Right now it unzips text.  If you have a zipped binary file, you'd need to edit the ZipFile class to handle it properly. I didn't figure out how to do that cleanly.   It does binary files now, too. \n\nEDIT - I updated the JS unzip library and demo.  It now does binary files, in addition to text. I've made it more resilient and more general - you can now specify the encoding to use when reading text files.  Also the demo is expanded - it shows unzipping an XLSX file in the browser, among other things. \nSo, while I think it is of limited utility and interest, it works. I guess it would work in Node.js."}
{"instruction": "how do I zip a whole folder tree in unix, but only certain files?", "output": "Switch into the website folder, then run\n\n```\nzip -R foo.zip '*.php' '*.html' '*.js' '*.css' \n\n```\n\nYou can also run this from outside the website folder:\n\n```\nzip -r foo.zip website_folder -i '*.php' '*.html' '*.js' '*.css'\n\n```"}
{"instruction": "Why is *.tar.gz still much more common than *.tar.xz?", "output": "\"Lowest Common Denominator\". The extra space saved is rarely worth the loss of interoperability. Many embedded Linux systems have gzip, but not xz. Many old system as well. Gnu Tar which is the industry standard supports flags -z to process through gzip, but many  systems, especially based around Busybox, don't support the -J flag for xz, meaning it requires 2-step operation (and a lot of extra diskspace for uncompressed .tar unless you use the syntax of |tar xf - - which many people don't know about.) Also, uncompressing the full filesystem of some 10MB from tar.gz on a small embedded ARM takes some 2 minutes and isn't really a problem. No clue about xz but bzip2 takes around 10-15 minutes. Definitely not worth the bandwidth saved."}
{"instruction": "how to zip a folder itself using java", "output": "It can be easily solved by package java.util.Zip no need any extra Jar files \nJust copy the following code and run it with your IDE \n\n```\n//Import all needed packages\npackage general;\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.zip.ZipEntry;\nimport java.util.zip.ZipOutputStream;\n\npublic class ZipUtils {\n\n    private List <String> fileList;\n    private static final String OUTPUT_ZIP_FILE = \"Folder.zip\";\n    private static final String SOURCE_FOLDER = \"D:\\\\Reports\"; // SourceFolder path\n\n    public ZipUtils() {\n        fileList = new ArrayList < String > ();\n    }\n\n    public static void main(String[] args) {\n        ZipUtils appZip = new ZipUtils();\n        appZip.generateFileList(new File(SOURCE_FOLDER));\n        appZip.zipIt(OUTPUT_ZIP_FILE);\n    }\n\n    public void zipIt(String zipFile) {\n        byte[] buffer = new byte[1024];\n        String source = new File(SOURCE_FOLDER).getName();\n        FileOutputStream fos = null;\n        ZipOutputStream zos = null;\n        try {\n            fos = new FileOutputStream(zipFile);\n            zos = new ZipOutputStream(fos);\n\n            System.out.println(\"Output to Zip : \" + zipFile);\n            FileInputStream in = null;\n\n            for (String file: this.fileList) {\n                System.out.println(\"File Added : \" + file);\n                ZipEntry ze = new ZipEntry(source + File.separator + file);\n                zos.putNextEntry(ze);\n                try {\n                    in = new FileInputStream(SOURCE_FOLDER + File.separator + file);\n                    int len;\n                    while ((len = in .read(buffer)) > 0) {\n                        zos.write(buffer, 0, len);\n                    }\n                } finally {\n                    in.close();\n                }\n            }\n\n            zos.closeEntry();\n            System.out.println(\"Folder successfully compressed\");\n\n        } catch (IOException ex) {\n            ex.printStackTrace();\n        } finally {\n            try {\n                zos.close();\n            } catch (IOException e) {\n                e.printStackTrace();\n            }\n        }\n    }\n\n    public void generateFileList(File node) {\n        // add file only\n        if (node.isFile()) {\n            fileList.add(generateZipEntry(node.toString()));\n        }\n\n        if (node.isDirectory()) {\n            String[] subNote = node.list();\n            for (String filename: subNote) {\n                generateFileList(new File(node, filename));\n            }\n        }\n    }\n\n    private String generateZipEntry(String file) {\n        return file.substring(SOURCE_FOLDER.length() + 1, file.length());\n    }\n}\n\n```\n\nRefer mkyong..I changed the code for the requirement of current question"}
{"instruction": "How do I attach the Android Support Library source in Eclipse?", "output": "I just want to add yet another method of attaching sources for the support library. It requires ADT in version 20 or later. Supposedly this method works for all JARs for which setting source/javadoc location is disabled by the container. Here's what you need to do:\n\nThe android-support-v4.jar library lies in the libs directory of your project. In that same directory create a regular Java properties file named exactly like the JAR in question, but with appended .properties extension. So, for our support library it'll be: android-support-v4.jar.properties.\nOpen created properties file and set value of property named src to the location where sources for that library can be found. Your file should have one line like:\n\n```\nsrc=c:/apps/adt-bundle-windows-64bit/sdk/extras/android/support/v4/src\n\n```\n\nSave the file.\nClose and re-open your android project.\nTry browsing to one of the support classes. The source attachment should work now.\n\nWorked perfectly in my case.\nOne thing to note: if src is not an absolute path, it will be resolved starting in the parent directory of the JAR file. Taking support library as an example - if src=support/src, ADT will assume that the class sources are located in libs/support/src.\nShort description of this feature written by its author can be found here.\nIf anyone is interested in how exactly this .properties file is processed, I recommend reading patch set #4, esp. changes in eclipse/plugins/com.android.ide.eclipse.adt/src/com/android/ide/eclipse/adt/ internal/project/LibraryClasspathContainerInitializer.java :)\nEdit\nPlease also see a fine comment by WindRider about adding sources properly in a project with multiple referenced libraries."}
{"instruction": "Unzipping files in Python", "output": "```\nimport zipfile\nwith zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n    zip_ref.extractall(directory_to_extract_to)\n\n```\n\nThat's pretty much it!"}
{"instruction": "Unzip All Files In A Directory", "output": "This works in bash, according to this link:\n\nunzip \\*.zip"}
{"instruction": "Unzip files programmatically in .net", "output": "We have used SharpZipLib successfully on many projects. I know it's a third party tool, but source code is included and could provide some insight if you chose to reinvent the wheel here."}
{"instruction": "What is a good Java library to zip/unzip files?", "output": "I know its late and there are lots of answers but this zip4j is one of the best libraries for zipping I have used. Its simple (no boiler code) and can easily handle password protected files.\n\n```\nimport net.lingala.zip4j.exception.ZipException;\nimport net.lingala.zip4j.core.ZipFile;\n\n\npublic static void unzip(){\n    String source = \"some/compressed/file.zip\";\n    String destination = \"some/destination/folder\";\n    String password = \"password\";\n\n    try {\n         ZipFile zipFile = new ZipFile(source);\n         if (zipFile.isEncrypted()) {\n            zipFile.setPassword(password);\n         }\n         zipFile.extractAll(destination);\n    } catch (ZipException e) {\n        e.printStackTrace();\n    }\n}\n\n```\n\nThe Maven dependency is:\n\n```\n<dependency>\n    <groupId>net.lingala.zip4j</groupId>\n    <artifactId>zip4j</artifactId>\n    <version>1.3.2</version>\n</dependency>\n\n```"}
{"instruction": "Unzip a file with php", "output": "I can only assume your code came from a tutorial somewhere online? In that case, good job trying to figure it out by yourself. On the other hand, the fact that this code could actually be published online somewhere as the correct way to unzip a file is a bit frightening.\nPHP has built-in extensions for dealing with compressed files. There should be no need to use system calls for this. ZipArchivedocs is one option.\n\n```\n$zip = new ZipArchive;\n$res = $zip->open('file.zip');\nif ($res === TRUE) {\n  $zip->extractTo('/myzips/extract_path/');\n  $zip->close();\n  echo 'woot!';\n} else {\n  echo 'doh!';\n}\n\n```\n\nAlso, as others have commented, $HTTP_GET_VARS has been deprecated since version 4.1 ... which was a reeeeeally long time ago. Don't use it. Use the $_GET superglobal instead.\nFinally, be very careful about accepting whatever input is passed to a script via a $_GET variable.\nALWAYS SANITIZE USER INPUT.\n\nUPDATE\nAs per your comment, the best way to extract the zip file into the same directory in which it resides is to determine the hard path to the file and extract it specifically to that location. So, you could do:\n\n```\n// assuming file.zip is in the same directory as the executing script.\n$file = 'file.zip';\n\n// get the absolute path to $file\n$path = pathinfo(realpath($file), PATHINFO_DIRNAME);\n\n$zip = new ZipArchive;\n$res = $zip->open($file);\nif ($res === TRUE) {\n  // extract it to the path we determined above\n  $zip->extractTo($path);\n  $zip->close();\n  echo \"WOOT! $file extracted to $path\";\n} else {\n  echo \"Doh! I couldn't open $file\";\n}\n\n```"}
{"instruction": "Downloading and unzipping a .zip file without writing to disk", "output": "My suggestion would be to use a StringIO object. They emulate files, but reside in memory. So you could do something like this:\n\n```\n# get_zip_data() gets a zip archive containing 'foo.txt', reading 'hey, foo'\n\nimport zipfile\nfrom StringIO import StringIO\n\nzipdata = StringIO()\nzipdata.write(get_zip_data())\nmyzipfile = zipfile.ZipFile(zipdata)\nfoofile = myzipfile.open('foo.txt')\nprint foofile.read()\n\n# output: \"hey, foo\"\n\n```\n\nOr more simply (apologies to Vishal):\n\n```\nmyzipfile = zipfile.ZipFile(StringIO(get_zip_data()))\nfor name in myzipfile.namelist():\n    [ ... ]\n\n```\n\nIn Python 3 use BytesIO instead of StringIO:\n\n```\nimport zipfile\nfrom io import BytesIO\n\nfilebytes = BytesIO(get_zip_data())\nmyzipfile = zipfile.ZipFile(filebytes)\nfor name in myzipfile.namelist():\n    [ ... ]\n\n```"}
{"instruction": "How do you recursively unzip archives in a directory and its subdirectories from the Unix command-line?", "output": "If you want to extract the files to the respective folder you can try this\n\n```\nfind . -name \"*.zip\" | while read filename; do unzip -o -d \"`dirname \"$filename\"`\" \"$filename\"; done;\n\n```\n\nA multi-processed version for systems that can handle high I/O:\n\n```\nfind . -name \"*.zip\" | xargs -P 5 -I fileName sh -c 'unzip -o -d \"$(dirname \"fileName\")/$(basename -s .zip \"fileName\")\" \"fileName\"'\n\n```"}
{"instruction": "Unzipping files", "output": "I wrote an unzipper in Javascript.  It works.  \nIt relies on Andy G.P. Na's binary file reader and some RFC1951 inflate logic from notmasteryet.  I added the ZipFile class.  \nworking example:\nhttp://cheeso.members.winisp.net/Unzip-Example.htm (dead link)\nThe source:\nhttp://cheeso.members.winisp.net/srcview.aspx?dir=js-unzip (dead link)\nNB: the links are dead; I'll find a new host soon. \nIncluded in the source is a ZipFile.htm demonstration page, and 3 distinct scripts, one for the zipfile class, one for the inflate class, and one for a binary file reader class. The demo also depends on jQuery and jQuery UI.  If you just download the js-zip.zip file, all of the necessary source is there.  \n\nHere's what the application code looks like in Javascript:\n\n```\n// In my demo, this gets attached to a click event.\n// it instantiates a ZipFile, and provides a callback that is\n// invoked when the zip is read.  This can take a few seconds on a\n// large zip file, so it's asynchronous. \nvar readFile = function(){\n    $(\"#status\").html(\"<br/>\");\n    var url= $(\"#urlToLoad\").val();\n    var doneReading = function(zip){\n        extractEntries(zip);\n    };\n\n    var zipFile = new ZipFile(url, doneReading);\n};\n\n\n// this function extracts the entries from an instantiated zip\nfunction extractEntries(zip){\n    $('#report').accordion('destroy');\n\n    // clear\n    $(\"#report\").html('');\n\n    var extractCb = function(id) {\n        // this callback is invoked with the entry name, and entry text\n        // in my demo, the text is just injected into an accordion panel.\n        return (function(entryName, entryText){\n            var content = entryText.replace(new RegExp( \"\\\\n\", \"g\" ), \"<br/>\");\n            $(\"#\"+id).html(content);\n            $(\"#status\").append(\"extract cb, entry(\" + entryName + \")  id(\" + id + \")<br/>\");\n            $('#report').accordion('destroy');\n            $('#report').accordion({collapsible:true, active:false});\n        });\n    }\n\n    // for each entry in the zip, extract it. \n    for (var i=0; i<zip.entries.length;  i++) {\n        var entry = zip.entries[i];\n\n        var entryInfo = \"<h4><a>\" + entry.name + \"</a></h4>\\n<div>\";\n\n        // contrive an id for the entry, make it unique\n        var randomId = \"id-\"+ Math.floor((Math.random() * 1000000000));\n\n        entryInfo += \"<span class='inputDiv'><h4>Content:</h4><span id='\" + randomId +\n            \"'></span></span></div>\\n\";\n\n        // insert the info for one entry as the last child within the report div\n        $(\"#report\").append(entryInfo);\n\n        // extract asynchronously\n        entry.extract(extractCb(randomId));\n    }\n}\n\n```\n\n\nThe demo works in a couple of steps:  The readFile fn is triggered by a click, and instantiates a ZipFile object, which reads the zip file. There's an asynchronous callback for when the read completes (usually happens in less than a second for reasonably sized zips) - in this demo the callback is held in the doneReading local variable, which simply calls extractEntries, which \njust blindly unzips all the content of the provided zip file.  In a real app you would probably choose some of the entries to extract (allow the user to select, or choose one or more entries programmatically, etc). \nThe extractEntries fn iterates over all entries, and calls extract() on each one, passing a callback.  Decompression of an entry takes time, maybe 1s or more for each entry in the zipfile, which means asynchrony is appropriate. The extract callback simply adds the extracted content to an jQuery accordion on the page. If the content is binary, then it gets formatted as such (not shown). \n\nIt works, but I think that the utility is somewhat limited. \nFor one thing: It's very slow. Takes ~4 seconds to unzip the 140k AppNote.txt file from PKWare. The same uncompress can be done in less than .5s in a .NET program. EDIT: The Javascript ZipFile unpacks considerably faster than this now, in IE9 and in Chrome. It is still slower than a compiled program, but it is plenty fast for normal browser usage.\nFor another: it does not do streaming. It basically slurps in the entire contents of the zipfile into memory. In a \"real\" programming environment you could read in only the metadata of a zip file (say, 64 bytes per entry) and then read and decompress the other data as desired.  There's no way to do IO like that in javascript, as far as I know, therefore the only option is to read the entire zip into memory and do random access in it.  This means it will place unreasonable demands on system memory for large zip files. Not so much a problem for a smaller zip file. \nAlso: It doesn't handle the \"general case\" zip file - there are lots of zip options that I didn't bother to implement in the unzipper - like ZIP encryption, WinZip encryption, zip64, UTF-8 encoded filenames, and so on. (EDIT - it handles UTF-8 encoded filenames now). The ZipFile class handles the basics, though. Some of these things would not be hard to implement.  I have an AES encryption class in Javascript; that could be integrated to support encryption. Supporting Zip64 would probably useless for most users of Javascript, as it is intended to support >4gb zipfiles - don't need to extract those in a browser. \nI also did not test the case for unzipping binary content. Right now it unzips text.  If you have a zipped binary file, you'd need to edit the ZipFile class to handle it properly. I didn't figure out how to do that cleanly.   It does binary files now, too. \n\nEDIT - I updated the JS unzip library and demo.  It now does binary files, in addition to text. I've made it more resilient and more general - you can now specify the encoding to use when reading text files.  Also the demo is expanded - it shows unzipping an XLSX file in the browser, among other things. \nSo, while I think it is of limited utility and interest, it works. I guess it would work in Node.js."}
{"instruction": "Extract files from zip without keeping the structure using python ZipFile?", "output": "This opens file handles of members of the zip archive, extracts the filename and copies it to a target file (that's how ZipFile.extract works, without taking care of subdirectories).\n\n```\nimport os\nimport shutil\nimport zipfile\n\nmy_dir = r\"D:\\Download\"\nmy_zip = r\"D:\\Download\\my_file.zip\"\n\nwith zipfile.ZipFile(my_zip) as zip_file:\n    for member in zip_file.namelist():\n        filename = os.path.basename(member)\n        # skip directories\n        if not filename:\n            continue\n    \n        # copy file (taken from zipfile's extract)\n        source = zip_file.open(member)\n        target = open(os.path.join(my_dir, filename), \"wb\")\n        with source, target:\n            shutil.copyfileobj(source, target)\n\n```"}
{"instruction": "Create a dedicated folder for every zip files in a directory and extract zip files", "output": "\"extract here\" is merely a feature of whatever unzip wrapper you are using.  unzip will only extract what actually is in the archive.  There is probably no simpler way than a shell script.  But sed, awk etc. are not needed for this if you have a POSIX-compliant shell:\n\n```\nfor f in *.zip; do unzip -d \"${f%*.zip}\" \"$f\"; done\n\n```\n\n(You MUST NOT escape the * or pathname expansion will not take place.)  Be aware that if the ZIP archive contains a directory, such as with Eclipse archives (which always contain eclipse/), you would end up with ./eclipse*/eclipse/eclipse.ini in any case.  Add echo before unzip for a dry run."}
{"instruction": "How to download and unzip a zip file in memory in NodeJs?", "output": "You need a library that can handle buffers. The latest version of adm-zip will do:\n\n```\nnpm install adm-zip\n\n```\n\nMy solution uses the http.get method, since it returns Buffer chunks. \nCode:\n\n```\nvar file_url = 'http://notepad-plus-plus.org/repository/7.x/7.6/npp.7.6.bin.x64.zip';\n\nvar AdmZip = require('adm-zip');\nvar http = require('http');\n\nhttp.get(file_url, function(res) {\n  var data = [], dataLen = 0; \n\n  res.on('data', function(chunk) {\n    data.push(chunk);\n    dataLen += chunk.length;\n\n  }).on('end', function() {\n    var buf = Buffer.alloc(dataLen);\n\n    for (var i = 0, len = data.length, pos = 0; i < len; i++) { \n      data[i].copy(buf, pos); \n      pos += data[i].length; \n    } \n\n    var zip = new AdmZip(buf);\n    var zipEntries = zip.getEntries();\n    console.log(zipEntries.length)\n\n    for (var i = 0; i < zipEntries.length; i++) {\n      if (zipEntries[i].entryName.match(/readme/))\n        console.log(zip.readAsText(zipEntries[i]));\n    }\n  });\n});\n\n```\n\nThe idea is to create an array of buffers and concatenate them into a new one at the end. This is due to the fact that buffers cannot be resized.\nUpdate\nThis is a simpler solution that uses the request module to obtain the response in a buffer, by setting encoding: null in the options. It also follows redirects and resolves http/https automatically.\n\n```\nvar file_url = 'https://github.com/mihaifm/linq/releases/download/3.1.1/linq.js-3.1.1.zip';\n\nvar AdmZip = require('adm-zip');\nvar request = require('request');\n\nrequest.get({url: file_url, encoding: null}, (err, res, body) => {\n  var zip = new AdmZip(body);\n  var zipEntries = zip.getEntries();\n  console.log(zipEntries.length);\n\n  zipEntries.forEach((entry) => {\n    if (entry.entryName.match(/readme/i))\n      console.log(zip.readAsText(entry));\n  });\n});\n\n```\n\nThe body of the response is a buffer that can be passed directly to AdmZip, simplifying the whole process."}
{"instruction": "How to unzip a piped zip file (from &quot;wget -qO-&quot;)?", "output": "```\nwget -q -O tmp.zip http://downloads.wordpress.org/plugin/akismet.2.5.3.zip && unzip tmp.zip && rm tmp.zip\n\n```"}
{"instruction": "zcat won&#39;t unzip files properly", "output": "Your school's system still has the old \"compress\" style utilities rather than the newer GNU \"gzip\" based ones.\nYou need to use gzcat rather than zcat, assuming that it's available."}
{"instruction": "How can unrar a file with python", "output": "Late, but I wasn't satisfied with any of the answers.\n\n```\npip install patool\nimport patoolib\npatoolib.extract_archive(\"foo_bar.rar\", outdir=\"path here\")\n\n```\n\nWorks on Windows and linux without any other libraries needed."}
{"instruction": "gradle - download and unzip file from url", "output": "There isn't currently a Gradle API for downloading from a URL. You can implement this using Ant, Groovy, or, if you do want to benefit from Gradle's dependency resolution/caching features, by pretending it's an Ivy repository with a custom artifact URL. The unzipping can be done in the usual Gradle way (copy method or Copy task)."}
{"instruction": "Simple way to unzip a .zip file using zlib", "output": "zlib handles the deflate compression/decompression algorithm, but there is more than that in a ZIP file. \nYou can try libzip. It is free, portable and easy to use.\nUPDATE: Here I attach quick'n'dirty example of libzip, with all the error controls ommited:\n\n```\n#include <zip.h>\n\nint main()\n{\n    //Open the ZIP archive\n    int err = 0;\n    zip *z = zip_open(\"foo.zip\", 0, &err);\n\n    //Search for the file of given name\n    const char *name = \"file.txt\";\n    struct zip_stat st;\n    zip_stat_init(&st);\n    zip_stat(z, name, 0, &st);\n\n    //Alloc memory for its uncompressed contents\n    char *contents = new char[st.size];\n\n    //Read the compressed file\n    zip_file *f = zip_fopen(z, name, 0);\n    zip_fread(f, contents, st.size);\n    zip_fclose(f);\n\n    //And close the archive\n    zip_close(z);\n\n    //Do something with the contents\n    //delete allocated memory\n    delete[] contents;\n}\n\n```"}
{"instruction": "easy way to unzip file", "output": "Slight rework of the OP's solution to create the containing directory dest if it doesn't exist, and to wrap the file extraction/writing in a closure to eliminate stacking of defer .Close() calls per @Nick Craig-Wood's comment:\n\n```\nfunc Unzip(src, dest string) error {\n    r, err := zip.OpenReader(src)\n    if err != nil {\n        return err\n    }\n    defer func() {\n        if err := r.Close(); err != nil {\n            panic(err)\n        }\n    }()\n\n    os.MkdirAll(dest, 0755)\n\n    // Closure to address file descriptors issue with all the deferred .Close() methods\n    extractAndWriteFile := func(f *zip.File) error {\n        rc, err := f.Open()\n        if err != nil {\n            return err\n        }\n        defer func() {\n            if err := rc.Close(); err != nil {\n                panic(err)\n            }\n        }()\n\n        path := filepath.Join(dest, f.Name)\n\n        // Check for ZipSlip (Directory traversal)\n        if !strings.HasPrefix(path, filepath.Clean(dest) + string(os.PathSeparator)) {\n            return fmt.Errorf(\"illegal file path: %s\", path)\n        }\n\n        if f.FileInfo().IsDir() {\n            os.MkdirAll(path, f.Mode())\n        } else {\n            os.MkdirAll(filepath.Dir(path), f.Mode())\n            f, err := os.OpenFile(path, os.O_WRONLY|os.O_CREATE|os.O_TRUNC, f.Mode())\n            if err != nil {\n                return err\n            }\n            defer func() {\n                if err := f.Close(); err != nil {\n                    panic(err)\n                }\n            }()\n\n            _, err = io.Copy(f, rc)\n            if err != nil {\n                return err\n            }\n        }\n        return nil\n    }\n\n    for _, f := range r.File {\n        err := extractAndWriteFile(f)\n        if err != nil {\n            return err\n        }\n    }\n\n    return nil\n}\n\n```\n\nNote: Updated to include Close() error handling as well (if we're looking for best practices, may as well follow ALL of them)."}
{"instruction": "How to check file size before opening?", "output": "Use file.info()\n\n```\nfile.info(\"data/ullyses.txt\")\n\n                    size isdir mode               mtime               ctime               atime  uid  gid\ndata/ullyses.txt 1573151 FALSE  664 2015-06-01 15:25:55 2015-06-01 15:25:55 2015-06-01 15:25:55 1008 1008\n\n```\n\nThen extract the column called size:\n\n```\nfile.info(\"data/ullyses.txt\")$size\n[1] 1573151\n\n```"}
{"instruction": "Unzip all zipped files in a folder to that same folder using Python 2.7.5", "output": "Below is the code that worked for me:\n\n```\nimport os, zipfile\n\ndir_name = 'C:\\\\SomeDirectory'\nextension = \".zip\"\n\nos.chdir(dir_name) # change directory from working dir to dir with files\n\nfor item in os.listdir(dir_name): # loop through items in dir\n    if item.endswith(extension): # check for \".zip\" extension\n        file_name = os.path.abspath(item) # get full path of files\n        zip_ref = zipfile.ZipFile(file_name) # create zipfile object\n        zip_ref.extractall(dir_name) # extract file to dir\n        zip_ref.close() # close file\n        os.remove(file_name) # delete zipped file\n\n```\n\nLooking back at the code I had amended, the directory was getting confused with the directory of the script.\nThe following also works while not ruining the working directory. First remove the line \n\n```\nos.chdir(dir_name) # change directory from working dir to dir with files\n\n```\n\nThen assign file_name as\n\n```\nfile_name = dir_name + \"/\" + item\n\n```"}
{"instruction": "How to extract file from zip without maintaining directory structure in Python?", "output": "You can use zipfile.ZipFile.open:\n\n```\nimport shutil\nimport zipfile\n\nwith zipfile.ZipFile('/path/to/my_file.apk') as z:\n    with z.open('/res/drawable/icon.png') as zf, open('temp/icon.png', 'wb') as f:\n        shutil.copyfileobj(zf, f)\n\n```\n\nOr use zipfile.ZipFile.read:\n\n```\nimport zipfile\n\nwith zipfile.ZipFile('/path/to/my_file.apk') as z:\n    with open('temp/icon.png', 'wb') as f:\n        f.write(z.read('/res/drawable/icon.png'))\n\n```"}
{"instruction": "How to read from a zip file within zip file in Python?", "output": "When you use the .open() call on a ZipFile instance you indeed get an open file handle. However, to read a zip file, the ZipFile class needs a little more. It needs to be able to seek on that file, and the object returned by .open() is not seekable in your case. Only Python 3 (3.2 and up) produces a ZipExFile object that supports seeking (provided the underlying file handle for the outer zip file is seekable, and nothing is trying to write to the ZipFile object).\nThe workaround is to read the whole zip entry into memory using .read(), store it in a BytesIO object (an in-memory file that is seekable) and feed that to ZipFile: \n\n```\nfrom io import BytesIO\n\n# ...\n        zfiledata = BytesIO(zfile.read(name))\n        with zipfile.ZipFile(zfiledata) as zfile2:\n\n```\n\nor, in the context of your example:\n\n```\nimport zipfile\nfrom io import BytesIO\n\nwith zipfile.ZipFile(\"parent.zip\", \"r\") as zfile:\n    for name in zfile.namelist():\n        if re.search(r'\\.zip$', name) is not None:\n            # We have a zip within a zip\n            zfiledata = BytesIO(zfile.read(name))\n            with zipfile.ZipFile(zfiledata) as zfile2:\n                for name2 in zfile2.namelist():\n                    # Now we can extract\n                    logging.info( \"Found internal internal file: \" + name2)\n                    print \"Processing code goes here\"\n\n```"}
{"instruction": "unzip a tar.gz file?", "output": "```\nfn <- \"http://s.wordpress.org/resources/survey/wp2011-survey.tar.gz\"\ndownload.file(fn,destfile=\"tmp.tar.gz\")\nuntar(\"tmp.tar.gz\",list=TRUE)  ## check contents\nuntar(\"tmp.tar.gz\")\n## or, if you just want to extract the target file:\nuntar(\"tmp.tar.gz\",files=\"wp2011-survey/anon-data.csv\")\nX <- read.csv(\"wp2011-survey/anon-data.csv\")\n\n```\n\nTom Wenseleers points out that the archive package can help with this:\n\n```\nlibrary(archive)\nlibrary(readr)\nread_csv(archive_read(\"tmp.tar.gz\", file = 3), col_types = cols())\n\n```\n\nand that archive::archive_extract(\"tmp.tar.gz\", files=\"wp2011-survey/anon-data.csv\") is quite a bit faster than the in-built base R untar (especially for large archives) It supports 'tar', 'ZIP', '7-zip', 'RAR', 'CAB', 'gzip', 'bzip2', 'compress', 'lzma' and 'xz' formats."}
{"instruction": "Unpack inner zips in zip with Maven", "output": "You can unzip any files using ant task runner plugin:\n\n```\n<plugin>\n    <groupId>org.apache.maven.plugins</groupId>\n    <artifactId>maven-antrun-plugin</artifactId>\n    <version>1.6</version>\n    <executions>\n        <execution>\n            <id>prepare</id>\n            <phase>validate</phase>\n            <configuration>\n                <tasks>\n                    <echo message=\"prepare phase\" />\n                    <unzip src=\"zips/archive.zip\" dest=\"output/\" />\n                    <unzip src=\"output/inner.zip\" dest=\"output/\" />\n                    <unzip dest=\"output\">\n                      <fileset dir=\"archives\">\n                        <include name=\"prefix*.zip\" />\n                      </fileset>\n                    </unzip>\n                </tasks>\n            </configuration>\n            <goals>\n                <goal>run</goal>\n            </goals>\n        </execution>\n    </executions>\n</plugin>\n\n```"}
{"instruction": "How to zip and unzip the files?", "output": "Take a look at java.util.zip.* classes for zip functionality.  I've done some basic zip/unzip code, which I've pasted below.  Hope it helps.\n\n```\npublic static void zip(String[] files, String zipFile) throws IOException {\n    BufferedInputStream origin = null;\n    ZipOutputStream out = new ZipOutputStream(new BufferedOutputStream(new FileOutputStream(zipFile)));\n    try { \n        byte data[] = new byte[BUFFER_SIZE];\n\n        for (int i = 0; i < files.length; i++) {\n            FileInputStream fi = new FileInputStream(files[i]);    \n            origin = new BufferedInputStream(fi, BUFFER_SIZE);\n            try {\n                ZipEntry entry = new ZipEntry(files[i].substring(files[i].lastIndexOf(\"/\") + 1));\n                out.putNextEntry(entry);\n                int count;\n                while ((count = origin.read(data, 0, BUFFER_SIZE)) != -1) {\n                    out.write(data, 0, count);\n                }\n            }\n            finally {\n                origin.close();\n            }\n        }\n    }\n    finally {\n        out.close();\n    }\n}\n\npublic static void unzip(String zipFile, String location) throws IOException {\n    try {\n        File f = new File(location);\n        if(!f.isDirectory()) {\n            f.mkdirs();\n        }\n        ZipInputStream zin = new ZipInputStream(new FileInputStream(zipFile));\n        try {\n            ZipEntry ze = null;\n            while ((ze = zin.getNextEntry()) != null) {\n                String path = location + ze.getName();\n\n                if (ze.isDirectory()) {\n                    File unzipFile = new File(path);\n                    if(!unzipFile.isDirectory()) {\n                        unzipFile.mkdirs();\n                    }\n                }\n                else {\n                    FileOutputStream fout = new FileOutputStream(path, false);\n                    try {\n                        for (int c = zin.read(); c != -1; c = zin.read()) {\n                            fout.write(c);\n                        }\n                        zin.closeEntry();\n                    }\n                    finally {\n                        fout.close();\n                    }\n                }\n            }\n        }\n        finally {\n            zin.close();\n        }\n    }\n    catch (Exception e) {\n        Log.e(TAG, \"Unzip exception\", e);\n    }\n}\n\n```"}
{"instruction": "How to programmatically extract / unzip a .7z (7-zip) file with R", "output": "If you have 7z executable in your path, you can simple use system command\nsystem('7z e -o <output_dir> <archive_name>')"}
{"instruction": "unzip (zip, tar, tag.gz) files with ruby", "output": "To extract files from a .tar.gz file you can use the following methods from packages distributed with Ruby:\n\n```\nrequire 'rubygems/package'\nrequire 'zlib'\ntar_extract = Gem::Package::TarReader.new(Zlib::GzipReader.open('Path/To/myfile.tar.gz'))\ntar_extract.rewind # The extract has to be rewinded after every iteration\ntar_extract.each do |entry|\n  puts entry.full_name\n  puts entry.directory?\n  puts entry.file?\n  # puts entry.read\nend\ntar_extract.close\n\n```\n\nEach entry of type Gem::Package::TarReader::Entry points to a file or directory within the .tar.gz file.\nSimilar code can be used (replace Reader with Writer) to write files to a .tar.gz file."}
{"instruction": "JavaScript: Decompress / inflate /unzip /ungzip strings", "output": "Take a look at this Stack Overflow question, the answers there contains references to multiple compressing engines implemented in javascript. Most of these are based on LZ77."}
{"instruction": "How to speed up unzipping time in Java / Android?", "output": "I don't know if unzipping on Android is slow, but copying byte for byte in a loop is surely slowing it down even more. Try using BufferedInputStream and BufferedOutputStream - it might be a bit more complicated, but in my experience it is worth it in the end.\n\n```\nBufferedInputStream in = new BufferedInputStream(zin);\nBufferedOutputStream out = new BufferedOutputStream(fout);\n\n```\n\nAnd then you can write with something like that:\n\n```\nbyte b[] = new byte[1024];\nint n;\nwhile ((n = in.read(b,0,1024)) >= 0) {\n  out.write(b,0,n);\n}\n\n```"}
{"instruction": "open failed: EBUSY (Device or resource busy)", "output": "I have a big Answer!!\nThe Problem comes from the Android System or/and the FAT32 system. \nI can not explain how the system gets the error, it has something to do with deleting files and the FAT32 System.\nBut the solution is really easy:\nBefore you delete a Directory or File: rename it!\nCode to rename:\n\n```\nfinal File to = new File(file.getAbsolutePath() + System.currentTimeMillis());\nfile.renameTo(to);\nto.delete();\n\n```\n\nThat's it, if you rename the folder or file before you delete it, there is no chance for the system to try to open an existing file again or an open file which you want to save again (or something like this)."}
{"instruction": "Unzip Archive with Groovy", "output": "AFAIK, there isn't a native way. But check out the article Powerful Groovy on how you'd add a .zip(...) method to File, which would be very close to what you're looking for. You'd just need to make an .unzip(...) method."}
{"instruction": "How to unzip specific folder from a .zip with Python", "output": "Check zipfile module.\nFor your case:\n\n```\nimport zipfile\n\narchive = zipfile.ZipFile('archive.zip')\n\nfor file in archive.namelist():\n    if file.startswith('foo/'):\n        archive.extract(file, 'destination_path')\n\n```"}
{"instruction": "Unzip files in folders automatically [mac os x]", "output": "find ./ -name \\*.zip -exec unzip {} \\; maybe?"}
{"instruction": "How do I write a BASH script to download and unzip file on a Mac?", "output": "OSX uses the same GNU sh/bash as Linux\n\n```\n#!/bin/sh\n\nmkdir /tmp/some_tmp_dir                         && \\\ncd /tmp/some_tmp_dir                            && \\\ncurl -sS http://foo.bar/filename.zip > file.zip && \\\nunzip file.zip                                  && \\\nrm file.zip\n\n```\n\nthe first line #!/bin/sh is so called \"shebang\" line and is mandatory"}
{"instruction": "Unzipping directory structure with python", "output": "The extract and extractall methods are great if you're on Python 2.6.  I have to use Python 2.5 for now, so I just need to create the directories if they don't exist.  You can get a listing of directories with the namelist() method.  The directories will always end with a forward slash (even on Windows) e.g.,\n\n```\nimport os, zipfile\n\nz = zipfile.ZipFile('myfile.zip')\nfor f in z.namelist():\n    if f.endswith('/'):\n        os.makedirs(f)\n\n```\n\nYou probably don't want to do it exactly like that (i.e., you'd probably want to extract the contents of the zip file as you iterate over the namelist), but you get the idea."}
{"instruction": "Is it possible to unzip a compressed file with multiple threads?", "output": "In short: No, unzipping with multiple cores is not available.\nThe decompression normally has lower CPU-intensity than the compression (where multiple cores are often involved).\nYou wouldn't have much of an advantage anyway as the read/write-operations are more of the bottlenecks during decompression."}
{"instruction": "Unzip .gz file using c#", "output": "The following example from MSDN shows how to use the GZipStream class to compress and decompress a directory of files.\n\n```\nnamespace zip\n{\n    public class Program\n    {\n        public static void Main()\n        {\n            string directoryPath = @\"c:\\users\\public\\reports\";\n\n            DirectoryInfo directorySelected = new DirectoryInfo(directoryPath);\n\n            foreach (FileInfo fileToCompress in directorySelected.GetFiles())\n            {\n                Compress(fileToCompress);\n            }\n\n            foreach (FileInfo fileToDecompress in directorySelected.GetFiles(\"*.gz\"))\n            {\n                Decompress(fileToDecompress);\n            }\n        }\n\n        public static void Compress(FileInfo fileToCompress)\n        {\n            using (FileStream originalFileStream = fileToCompress.OpenRead())\n            {\n                if ((File.GetAttributes(fileToCompress.FullName) & FileAttributes.Hidden) != FileAttributes.Hidden & fileToCompress.Extension != \".gz\")\n                {\n                    using (FileStream compressedFileStream = File.Create(fileToCompress.FullName + \".gz\"))\n                    {\n                        using (GZipStream compressionStream = new GZipStream(compressedFileStream, CompressionMode.Compress))\n                        {\n                            originalFileStream.CopyTo(compressionStream);\n                            Console.WriteLine(\"Compressed {0} from {1} to {2} bytes.\",\n                                fileToCompress.Name, fileToCompress.Length.ToString(), compressedFileStream.Length.ToString());\n                        }\n                    }\n                }\n            }\n        }\n\n        public static void Decompress(FileInfo fileToDecompress)\n        {\n            using (FileStream originalFileStream = fileToDecompress.OpenRead())\n            {\n                string currentFileName = fileToDecompress.FullName;\n                string newFileName = currentFileName.Remove(currentFileName.Length - fileToDecompress.Extension.Length);\n\n                using (FileStream decompressedFileStream = File.Create(newFileName))\n                {\n                    using (GZipStream decompressionStream = new GZipStream(originalFileStream, CompressionMode.Decompress))\n                    {\n                        decompressionStream.CopyTo(decompressedFileStream);\n                        Console.WriteLine(\"Decompressed: {0}\", fileToDecompress.Name);\n                    }\n                }\n            }\n        }\n    }\n}\n\n```"}
{"instruction": "Unzipping downloaded files in iOS", "output": "I've used ZipArchive with success in the past. It's pretty ligthweight and simple to use, supports password protection, multiple files inside a ZIP, as well as compress & decompress.\nThe basic usage is:\n\n```\nNSString *filepath = [[NSBundle mainBundle] pathForResource:@\"ZipFileName\" ofType:@\"zip\"];\nZipArchive *zipArchive = [[ZipArchive alloc] init];\n[zipArchive UnzipOpenFile:filepath Password:@\"xxxxxx\"];\n[zipArchive UnzipFileTo:{pathToDirectory} overWrite:YES];\n[zipArchive UnzipCloseFile];\n[zipArchive release];\n\n```\n\nmore examples about this package here\nI have also tried SSZipArchive in some projects.\nBelow line would unzip your zip file. \n\n```\n[SSZipArchive unzipFileAtPath:path toDestination:destination];\n\n```"}
{"instruction": "How to get a list of directories in a zip?", "output": "To list only directories:\n\n```\nunzip -l foo.zip \"*/\"\n\n```\n\nOutput (e.g.):\n\n```\n\nArchive:  foo.zip\n  Length      Date    Time    Name\n---------  ---------- -----   ----\n        0  2015-09-10 20:10   work/\n        0  2015-08-31 10:45   work/test1/\n        0  2015-08-31 10:50   work/test1/cc/\n        0  2015-08-31 10:45   work/test1/dd/\n        0  2015-08-31 10:45   work/test1/aa/\n        0  2015-08-31 10:45   work/test1/bb/\n        0  2015-09-09 21:17   work/tmp/\n        0  2015-08-23 18:49   work/tmp/work/\n        0  2015-09-08 19:33   work/tmp/work/loop/\n        0  2015-08-15 16:00   work/tmp/work/1/\n        0  2015-08-15 16:00   work/1/\n        0  2015-08-24 18:40   work/dir/\n        0  2015-09-05 18:07   work/rename/\n---------                     -------\n        0                     13 files\n\n```\n\nor use\n\n```\nzipinfo -1 foo.zip \"*/\"\n\n```\n\nOutput (e.g.):\n\n```\n\nwork/\nwork/test1/\nwork/test1/cc/\nwork/test1/dd/\nwork/test1/aa/\nwork/test1/bb/\nwork/tmp/\nwork/tmp/work/\nwork/tmp/work/loop/\nwork/tmp/work/1/\nwork/1/\nwork/dir/\nwork/rename/\n\n```"}
{"instruction": "Unzip buffer with Python?", "output": "Is it possible to use zlib \n\nNo, zlib is not designed to operate on ZIP files. \n\nand how can I avoid using a temporary file?\n\nUse io.BytesIO:\n\n```\nimport zipfile\nimport io\n\nbuffer = b'PK\\x03\\x04\\n\\x00\\x00\\x00\\x00\\x00\\n\\\\\\x88Gzzo\\xed\\x03\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x07\\x00\\x1c\\x00foo.txtUT\\t\\x00\\x03$\\x14gV(\\x14gVux\\x0b\\x00\\x01\\x041\\x04\\x00\\x00\\x041\\x04\\x00\\x00hi\\nPK\\x01\\x02\\x1e\\x03\\n\\x00\\x00\\x00\\x00\\x00\\n\\\\\\x88Gzzo\\xed\\x03\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x07\\x00\\x18\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\xb4\\x81\\x00\\x00\\x00\\x00foo.txtUT\\x05\\x00\\x03$\\x14gVux\\x0b\\x00\\x01\\x041\\x04\\x00\\x00\\x041\\x04\\x00\\x00PK\\x05\\x06\\x00\\x00\\x00\\x00\\x01\\x00\\x01\\x00M\\x00\\x00\\x00D\\x00\\x00\\x00\\x00\\x00'\n\nz = zipfile.ZipFile(io.BytesIO(buffer))\n\n# The following three lines are alternatives. Use one of them\n# according to your need:\nfoo = z.read('foo.txt')        # Reads the data from \"foo.txt\"\nfoo2 = z.read(z.infolist()[0]) # Reads the data from the first file\nz.extractall()                 # Copies foo.txt to the filesystem\n\nz.close()\n\n\nprint foo\nprint foo2\n\n```"}
{"instruction": "How to open gzip text files in Gvim without unzipping?", "output": "Solution mentioned in VimDocs as answered by therefromwhere\n\n```\naugroup gzip\n autocmd!\n autocmd BufReadPre,FileReadPre *.gz set bin\n autocmd BufReadPost,FileReadPost   *.gz '[,']!gunzip\n autocmd BufReadPost,FileReadPost   *.gz set nobin\n autocmd BufReadPost,FileReadPost   *.gz execute \":doautocmd BufReadPost \" . expand(\"%:r\")\n autocmd BufWritePost,FileWritePost *.gz !mv <afile> <afile>:r\n autocmd BufWritePost,FileWritePost *.gz !gzip <afile>:r\n autocmd FileAppendPre      *.gz !gunzip <afile>\n autocmd FileAppendPre      *.gz !mv <afile>:r <afile>\n autocmd FileAppendPost     *.gz !mv <afile> <afile>:r\n autocmd FileAppendPost     *.gz !gzip <afile>:r\naugroup END\n\n```"}
{"instruction": "How to quickly check if a zip file is corrupted?", "output": "Section 4.3.7 of this page says that the compressed size is 4 bytes starting from byte 18. You could try reading that and comparing it to the size to the file.\nHowever, I think it's pretty much useless for checking if the zip file is corrupted for two reasons:\n\nSome zip files contain more bytes than just the zip part. For example, self-extracting archives have an executable part yet they're still valid zip.\nThe file can be corrupted without changing its size.\n\nSo, I suggest calculating the CRC for a guaranteed method of checking for corruption."}
{"instruction": "How do you unzip very large files in python?", "output": "Here's an outline of decompression of large files.\n\n```\nimport zipfile\nimport zlib\nimport os\n\nsrc = open( doc, \"rb\" )\nzf = zipfile.ZipFile( src )\nfor m in  zf.infolist():\n\n    # Examine the header\n    print m.filename, m.header_offset, m.compress_size, repr(m.extra), repr(m.comment)\n    src.seek( m.header_offset )\n    src.read( 30 ) # Good to use struct to unpack this.\n    nm= src.read( len(m.filename) )\n    if len(m.extra) > 0: ex= src.read( len(m.extra) )\n    if len(m.comment) > 0: cm= src.read( len(m.comment) ) \n\n    # Build a decompression object\n    decomp= zlib.decompressobj(-15)\n\n    # This can be done with a loop reading blocks\n    out= open( m.filename, \"wb\" )\n    result= decomp.decompress( src.read( m.compress_size ) )\n    out.write( result )\n    result = decomp.flush()\n    out.write( result )\n    # end of the loop\n    out.close()\n\nzf.close()\nsrc.close()\n\n```"}
{"instruction": "How to unzip a zip file using scala?", "output": "Well since were are using some utilities from java, here is a version basen on this, translated to scala, maybe this should be more functional, but it is useful\n\n```\npackage zip\n\nimport java.io.{ IOException, FileOutputStream, FileInputStream, File }\nimport java.util.zip.{ ZipEntry, ZipInputStream }\n\n/**\n * Created by anquegi on 04/06/15.\n */\nobject Unzip extends App {\n\n  val INPUT_ZIP_FILE: String = \"src/main/resources/my-zip.zip\";\n  val OUTPUT_FOLDER: String = \"src/main/resources/my-zip\";\n\n  def unZipIt(zipFile: String, outputFolder: String): Unit = {\n\n    val buffer = new Array[Byte](1024)\n\n    try {\n\n      //output directory\n      val folder = new File(OUTPUT_FOLDER);\n      if (!folder.exists()) {\n        folder.mkdir();\n      }\n\n      //zip file content\n      val zis: ZipInputStream = new ZipInputStream(new FileInputStream(zipFile));\n      //get the zipped file list entry\n      var ze: ZipEntry = zis.getNextEntry();\n\n      while (ze != null) {\n\n        val fileName = ze.getName();\n        val newFile = new File(outputFolder + File.separator + fileName);\n\n        System.out.println(\"file unzip : \" + newFile.getAbsoluteFile());\n\n        //create folders\n        new File(newFile.getParent()).mkdirs();\n\n        val fos = new FileOutputStream(newFile);\n\n        var len: Int = zis.read(buffer);\n\n        while (len > 0) {\n\n          fos.write(buffer, 0, len)\n          len = zis.read(buffer)\n        }\n\n        fos.close()\n        ze = zis.getNextEntry()\n      }\n\n      zis.closeEntry()\n      zis.close()\n\n    } catch {\n      case e: IOException => println(\"exception caught: \" + e.getMessage)\n    }\n\n  }\n\n  Unzip.unZipIt(INPUT_ZIP_FILE, OUTPUT_FOLDER)\n\n}\n\n```"}
{"instruction": "Extracting zipped files using JSZIP in javascript", "output": "It took a bit of digging in their documentation but they have an example that shows how to read the file contents from a ZIP.\nYou are getting the object that describes the ZIP contents but not the actual content. Here is an adjusted version:\n\n```\nvar JSZip = require('JSZip');\n\nfs.readFile(filePath, function(err, data) {\n    if (!err) {\n        var zip = new JSZip();\n        zip.loadAsync(data).then(function(contents) {\n            Object.keys(contents.files).forEach(function(filename) {\n                zip.file(filename).async('nodebuffer').then(function(content) {\n                    var dest = path + filename;\n                    fs.writeFileSync(dest, content);\n                });\n            });\n        });\n    }\n});\n\n```"}
{"instruction": "print content of more than one file in a zip archive", "output": "Use the -p option of unzip to pipe the output.  Multiple files are concatenated.  The -c option does the same thing, but includes the file name in front of each file."}
{"instruction": "linux unzip excluding everything in the folder and underneath", "output": "You need to quote the exclude pattern so that it is passed to unzip. Otherwise it will be expanded by the shell before being passed to unzip.\nTry:\n\n```\nunzip patch2.zip -x \"EXCLUDE/*\"\n\n```"}
{"instruction": "I want to extract all .zip files in a given directory in temp using powershell", "output": "```\nGet-ChildItem 'path to folder' -Filter *.zip | Expand-Archive -DestinationPath 'path to extract' -Force\n\n```\n\nrequires ps v5"}
{"instruction": "What is a good way to test a file to see if its a zip file?", "output": "The zip file format is defined by PKWARE. You can find their file specification here.\nNear the top you will find the header specification:\n\nA.  Local file header:\n\n```\n    local file header signature     4 bytes  (0x04034b50)\n    version needed to extract       2 bytes\n    general purpose bit flag        2 bytes\n    compression method              2 bytes\n    last mod file time              2 bytes\n    last mod file date              2 bytes\n    crc-32                          4 bytes\n    compressed size                 4 bytes\n    uncompressed size               4 bytes\n    file name length                2 bytes\n    extra field length              2 bytes\n\n    file name (variable size)\n    extra field (variable size)\n\n```\n\n\nFrom this you can see that the first 4 bytes of the header should be the file signature which should be the hex value 0x04034b50. Byte order in the file is the other way round  - PKWARE specify that \"All values are stored in little-endian byte order unless otherwise specified.\", so if you use a hex editor to view the file you will see 50 4b 03 04 as the first 4 bytes.\nYou can use this to check if your file is a zip file. If you open the file in notepad, you will notice that the first two bytes (50 and 4b) are the ASCII characters PK."}
{"instruction": "Zip and Unzip File in Powershell 4", "output": "Write-Zip seems to be part of http://pscx.codeplex.com/ that require a separate installation before you can use it. \nHowever, if you just want to create a Zip archive from a folder, you could just run\n\n```\n$source = \"c:\\temp\\source\"\n$archive = \"c:\\temp\\archive.zip\"\n\nAdd-Type -assembly \"system.io.compression.filesystem\"\n[io.compression.zipfile]::CreateFromDirectory($source, $archive)\n\n```\n\nThis utilizes the CreateFromDirectory method from the .NET Framework class ZipFile. It creates a zip archive from the files located inside the $source folder and creates an archive as defined in the $archive variable. Note, ZipFile class was introduced in .NET Framework 4.5"}
{"instruction": "Check if a directory exists in a zip file with Python", "output": "Just check the filename with \"/\" at the end of it.\n\n```\nimport zipfile\n\ndef isdir(z, name):\n    return any(x.startswith(\"%s/\" % name.rstrip(\"/\")) for x in z.namelist())\n\nf = zipfile.ZipFile(\"sample.zip\", \"r\")\nprint isdir(f, \"a\")\nprint isdir(f, \"a/b\")\nprint isdir(f, \"a/X\")\n\n```\n\nYou use this line\n\n```\nany(x.startswith(\"%s/\" % name.rstrip(\"/\")) for x in z.namelist())\n\n```\n\nbecause it is possible that archive contains no directory explicitly; just a path with a directory name.\nExecution result:\n\n```\n$ mkdir -p a/b/c/d\n$ touch a/X\n$ zip -r sample.zip a\nadding: a/ (stored 0%)\nadding: a/X (stored 0%)\nadding: a/b/ (stored 0%)\nadding: a/b/c/ (stored 0%)\nadding: a/b/c/d/ (stored 0%)\n\n$ python z.py\nTrue\nTrue\nFalse\n\n```"}
{"instruction": "How is Docker different from a virtual machine?", "output": "Docker originally used LinuX Containers (LXC), but later switched to runC (formerly known as libcontainer), which runs in the same operating system as its host. This allows it to share a lot of the host operating system resources. Also, it uses a layered filesystem (AuFS) and manages networking.\nAuFS is a layered file system, so you can have a read only part and a write part which are merged together. One could have the common parts of the operating system as read only (and shared amongst all of your containers) and then give each container its own mount for writing.\nSo, let's say you have a 1¬†GB container image; if you wanted to use a full VM, you would need to have 1¬†GB x number of VMs you want. With Docker and AuFS you can share the bulk of the 1¬†GB between all the containers and if you have 1000 containers you still might only have a little over 1¬†GB of space for the containers OS (assuming they are all running the same OS image).\nA full virtualized system gets its own set of resources allocated to it, and does minimal sharing. You get more isolation, but it is much heavier (requires more resources). With Docker you get less isolation, but the containers are lightweight (require fewer resources). So you could easily run thousands of containers on a host, and it won't even blink. Try doing that with Xen, and unless you have a really big host, I don't think it is possible.\nA full virtualized system usually takes minutes to start, whereas Docker/LXC/runC containers take seconds, and often even less than a second.\nThere are pros and cons for each type of virtualized system. If you want full isolation with guaranteed resources, a full VM is the way to go. If you just want to isolate processes from each other and want to run a ton of them on a reasonably sized host, then Docker/LXC/runC seems to be the way to go.\nFor more information, check out this set of blog posts which do a good job of explaining how LXC works.\n\nWhy is deploying software to a docker image (if that's the right term) easier than simply deploying to a consistent production environment?\n\nDeploying a consistent production environment is easier said than done. Even if you use tools like Chef and Puppet, there are always OS updates and other things that change between hosts and environments.\nDocker gives you the ability to snapshot the OS into a shared image, and makes it easy to deploy on other Docker hosts. Locally, dev, qa, prod, etc.: all the same image. Sure you can do this with other tools, but not nearly as easily or fast.\nThis is great for testing; let's say you have thousands of tests that need to connect to a database, and each test needs a pristine copy of the database and will make changes to the data. The classic approach to this is to reset the database after every test either with custom code or with tools like Flyway - this can be very time-consuming and means that tests must be run serially. However, with Docker you could create an image of your database and run up one instance per test, and then run all the tests in parallel since you know they will all be running against the same snapshot of the database. Since the tests are running in parallel and in Docker containers they could run all on the same box at the same time and should finish much faster. Try doing that with a full VM.\nFrom comments...\n\nInteresting! I suppose I'm still confused by the notion of \"snapshot[ting] the OS\". How does one do that without, well, making an image of the OS?\n\nWell, let's see if I can explain. You start with a base image, and then make your changes, and commit those changes using docker, and it creates an image. This image contains only the differences from the base. When you want to run your image, you also need the base, and it layers your image on top of the base using a layered file system: as mentioned above, Docker uses AuFS. AuFS merges the different layers together and you get what you want; you just need to run it. You can keep adding more and more images (layers) and it will continue to only save the diffs. Since Docker typically builds on top of ready-made images from a registry, you rarely have to \"snapshot\" the whole OS yourself."}
{"instruction": "From inside of a Docker container, how do I connect to the localhost of the machine?", "output": "If you are using Docker-for-mac or Docker-for-Windows 18.03+, connect to your MySQL service using the host host.docker.internal (instead of the 127.0.0.1 in your connection string).\nIf you are using Docker-for-Linux 20.10.0+, you can also use the host host.docker.internal if you started your Docker container with the --add-host host.docker.internal:host-gateway option, or added the following snippet in your docker-compose.yml file:\n\n```\nextra_hosts:\n    - \"host.docker.internal:host-gateway\"\n\n```\n\nOtherwise, read below\n\nTLDR\nUse --network=\"host\" in your docker run command, then 127.0.0.1 in your Docker container will point to your Docker host.\nNote: This mode only works on Docker for Linux, per the documentation.\n\nNote on Docker container networking modes\nDocker offers different networking modes when running containers. Depending on the mode you choose you would connect to your MySQL database running on the Docker host differently.\ndocker run --network=\"bridge\" (default)\nDocker creates a bridge named docker0 by default. Both the Docker host and the Docker containers have an IP address on that bridge.\nOn the Docker host, type sudo ip addr show docker0 you will have an output looking like:\n\n```\n[vagrant@docker:~] $ sudo ip addr show docker0\n4: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default\n    link/ether 56:84:7a:fe:97:99 brd ff:ff:ff:ff:ff:ff\n    inet 172.17.42.1/16 scope global docker0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::5484:7aff:fefe:9799/64 scope link\n       valid_lft forever preferred_lft forever\n\n```\n\nSo here my Docker host has the IP address 172.17.42.1 on the docker0 network interface.\nNow start a new container and get a shell on it: docker run --rm -it ubuntu:trusty bash and within the container type ip addr show eth0 to discover how its main network interface is set up:\n\n```\nroot@e77f6a1b3740:/# ip addr show eth0\n863: eth0: <BROADCAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\n    link/ether 66:32:13:f0:f1:e3 brd ff:ff:ff:ff:ff:ff\n    inet 172.17.1.192/16 scope global eth0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::6432:13ff:fef0:f1e3/64 scope link\n       valid_lft forever preferred_lft forever\n\n```\n\nHere my container has the IP address 172.17.1.192. Now look at the routing table:\n\n```\nroot@e77f6a1b3740:/# route\nKernel IP routing table\nDestination     Gateway         Genmask         Flags Metric Ref    Use Iface\ndefault         172.17.42.1     0.0.0.0         UG    0      0        0 eth0\n172.17.0.0      *               255.255.0.0     U     0      0        0 eth0\n\n```\n\nSo the IP address of the Docker host 172.17.42.1 is set as the default route and is accessible from your container.\n\n```\nroot@e77f6a1b3740:/# ping 172.17.42.1\nPING 172.17.42.1 (172.17.42.1) 56(84) bytes of data.\n64 bytes from 172.17.42.1: icmp_seq=1 ttl=64 time=0.070 ms\n64 bytes from 172.17.42.1: icmp_seq=2 ttl=64 time=0.201 ms\n64 bytes from 172.17.42.1: icmp_seq=3 ttl=64 time=0.116 ms\n\n```\n\ndocker run --network=\"host\"\nAlternatively you can run a Docker container with network settings set to host. Such a container will share the network stack with the Docker host and from the container point of view, localhost (or 127.0.0.1) will refer to the Docker host.\nBe aware that any port opened in your Docker container would be opened on the Docker host. And this without requiring the -p or -P docker run option.\nIP configuration on my Docker host:\n\n```\n[vagrant@docker:~] $ ip addr show eth0\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\n    link/ether 08:00:27:98:dc:aa brd ff:ff:ff:ff:ff:ff\n    inet 10.0.2.15/24 brd 10.0.2.255 scope global eth0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::a00:27ff:fe98:dcaa/64 scope link\n       valid_lft forever preferred_lft forever\n\n```\n\nAnd from a Docker container in host mode:\n\n```\n[vagrant@docker:~] $ docker run --rm -it --network=host ubuntu:trusty ip addr show eth0\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\n    link/ether 08:00:27:98:dc:aa brd ff:ff:ff:ff:ff:ff\n    inet 10.0.2.15/24 brd 10.0.2.255 scope global eth0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::a00:27ff:fe98:dcaa/64 scope link\n       valid_lft forever preferred_lft forever\n\n```\n\nAs you can see, both the Docker host and Docker container share the exact same network interface and as such have the same IP address.\n\nConnecting to MySQL from containers\nBridge mode\nTo access MySQL running on the Docker host from containers in bridge mode, you need to make sure the MySQL service is listening for connections on the 172.17.42.1 IP address.\nTo do so, make sure you have either bind-address = 172.17.42.1 or bind-address = 0.0.0.0 in your MySQL configuration file (my.cnf).\nIf you need to set an environment variable with the IP address of the gateway, you can run the following code in a container:\n\n```\nexport DOCKER_HOST_IP=$(route -n | awk '/UG[ \\t]/{print $2}')\n\n```\n\nThen in your application, use the DOCKER_HOST_IP environment variable to open the connection to MySQL.\nNote: if you use bind-address = 0.0.0.0, your MySQL server will listen for connections on all network interfaces. That means your MySQL server could be reached from the Internet; make sure to set up firewall rules accordingly.\nNote 2: if you use bind-address = 172.17.42.1 your MySQL server won't listen for connections made to 127.0.0.1. Processes running on the Docker host that would want to connect to MySQL would have to use the 172.17.42.1 IP address.\nHost mode\nTo access MySQL running on the docker host from containers in host mode, you can keep bind-address = 127.0.0.1 in your MySQL configuration and connect to 127.0.0.1 from your containers:\n\n```\n[vagrant@docker:~] $ docker run --rm -it --network=host mysql mysql -h 127.0.0.1 -uroot -p\nEnter password:\nWelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 36\nServer version: 5.5.41-0ubuntu0.14.04.1 (Ubuntu)\n\nCopyright (c) 2000, 2014, Oracle and/or its affiliates. All rights reserved.\n\nOracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners.\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n\nmysql>\n\n```\n\nNote: Do use mysql -h 127.0.0.1 and not mysql -h localhost; otherwise the MySQL client would try to connect using a Unix socket."}
{"instruction": "What is the difference between the &#39;COPY&#39; and &#39;ADD&#39; commands in a Dockerfile?", "output": "You should check the ADD and COPY documentation for a more detailed description of their behaviors, but in a nutshell, the major difference is that ADD can do more than COPY:\n\nADD allows <src> to be a URL\nReferring to comments below, the ADD documentation states that:\n\n\nIf  is a local tar archive in a recognized compression format (identity, gzip, bzip2 or xz) then it is unpacked as a directory. Resources from remote URLs are not decompressed.\n\nNote that the Best practices for writing Dockerfiles suggests using COPY where the magic of ADD is not required. Otherwise, you (since you had to look up this answer) are likely to get surprised someday when you mean to copy keep_this_archive_intact.tar.gz into your container, but instead, you spray the contents onto your filesystem."}
{"instruction": "Copying files from Docker container to host", "output": "In order to copy a file from a container to the host, you can use the command\n\n```\ndocker cp <containerId>:/file/path/within/container /host/path/target\n\n```\n\nHere's an example:\n\n```\n$ sudo docker cp goofy_roentgen:/out_read.jpg .\n\n```\n\nHere goofy_roentgen is the container name I got from the following command:\n\n```\n$ sudo docker ps\n\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS                                            NAMES\n1b4ad9311e93        bamos/openface      \"/bin/bash\"         33 minutes ago      Up 33 minutes       0.0.0.0:8000->8000/tcp, 0.0.0.0:9000->9000/tcp   goofy_roentgen\n\n```\n\nYou can also use (part of) the Container ID. The following command is equivalent to the first\n\n```\n$ sudo docker cp 1b4a:/out_read.jpg .\n\n```"}
{"instruction": "What is the difference between CMD and ENTRYPOINT in a Dockerfile?", "output": "Docker has a default entrypoint which is /bin/sh -c but does not have a default command.\nWhen you run docker like this:\ndocker run -i -t ubuntu bash\nthe entrypoint is the default /bin/sh -c, the image is ubuntu and the command is bash.\nThe command is run via the entrypoint. i.e., the actual thing that gets executed is /bin/sh -c bash. This allowed Docker to implement RUN quickly by relying on the shell's parser.\nLater on, people asked to be able to customize this, so ENTRYPOINT and --entrypoint were introduced.\nEverything after the image name, ubuntu in the example above, is the command and is passed to the entrypoint. When using the CMD instruction, it is exactly as if you were executing\ndocker run -i -t ubuntu <cmd>\nThe parameter of the entrypoint is <cmd>.\nYou will also get the same result if you instead type this command docker run -i -t ubuntu: a bash shell will start in the container because in the ubuntu Dockerfile a default CMD is specified:\nCMD [\"bash\"].\nAs everything is passed to the entrypoint, you can have a very nice behavior from your images. @Jiri example is good, it shows how to use an image as a \"binary\". When using [\"/bin/cat\"] as entrypoint and then doing docker run img /etc/passwd, you get it, /etc/passwd is the command and is passed to the entrypoint so the end result execution is simply /bin/cat /etc/passwd.\nAnother example would be to have any cli as entrypoint. For instance, if you have a redis image, instead of running docker run redisimg redis -H something -u toto get key, you can simply have ENTRYPOINT [\"redis\", \"-H\", \"something\", \"-u\", \"toto\"] and then run like this for the same result: docker run redisimg get key."}
{"instruction": "How to copy files from host to Docker container?", "output": "The cp command can be used to copy files.\nOne specific file can be copied TO the container like:\n\n```\ndocker cp foo.txt container_id:/foo.txt\n\n```\n\nOne specific file can be copied FROM the container like:\n\n```\ndocker cp container_id:/foo.txt foo.txt\n\n```\n\nFor emphasis, container_id is a container ID, not an image ID. (Use docker ps to view listing which includes container_ids.)\nMultiple files contained by the folder src can be copied into the target folder using:\n\n```\ndocker cp src/. container_id:/target\ndocker cp container_id:/src/. target\n\n```\n\nReference: Docker CLI docs for cp\nIn Docker versions prior to 1.8 it was only possible to copy files from a container to the host. Not from the host to a container."}
{"instruction": "How do I get into a Docker container&#39;s shell?", "output": "docker attach will let you connect to your Docker container, but this isn't really the same thing as ssh.  If your container is running a webserver, for example, docker attach will probably connect you to the stdout of the web server process.  It won't necessarily give you a shell.\nThe docker exec command is probably what you are looking for; this will let you run arbitrary commands inside an existing container.  For example, to run bash inside a container:\n\n```\ndocker exec -it <mycontainer> sh\n\n```\n\nOf course, whatever command you are running must exist in the container filesystem; if your container doesn't have sh, this will fail with something like:\n\n```\nOCI runtime exec failed: exec failed: unable to start container process:\nexec: \"sh\": executable file not found in $PATH: unknown\n\n```\n\n[If your container doesn't have sh -- which is a common case for minimal images -- you may need to investigate other ways to explore the container filesystem.]\nIn the above command <mycontainer> is the name or ID of the target container.  It doesn't matter whether or not you're using docker compose; just run docker ps and use either the ID (a hexadecimal string displayed in the first column) or the name (displayed in the final column).  E.g., given:\n\n```\n$ docker ps\nd2d4a89aaee9        larsks/mini-httpd   \"mini_httpd -d /cont   7 days ago          Up 7 days                               web                 \n\n```\n\nI can run:\n\n```\n$ docker exec -it web ip addr\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN \n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host \n       valid_lft forever preferred_lft forever\n18: eth0: <BROADCAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP \n    link/ether 02:42:ac:11:00:03 brd ff:ff:ff:ff:ff:ff\n    inet 172.17.0.3/16 scope global eth0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::42:acff:fe11:3/64 scope link \n       valid_lft forever preferred_lft forever\n\n```\n\nI could accomplish the same thing by running:\n\n```\n$ docker exec -it d2d4a89aaee9 ip addr\n\n```\n\nSimilarly, I could start a shell in the container;\n\n```\n$ docker exec -it web sh\n/ # echo This is inside the container.\nThis is inside the container.\n/ # exit\n$\n\n```\n\n\nIn commands shown in this answer, the -i and -t options (combined as -it) are necessary to get an interactive shell:\n\n-i keeps stdin connected; if you don't specify -i, the shell will simply exit.\n\n-t allocates a tty device; if you don't specify -t, you won't have a very pleasant interactive experience (there will be no shell prompt or job control, for example).\n\n\n\nIf you're specifically using docker compose, there is a convenience docker compose exec command that works very much like the docker exec command, except:\n\nIt defaults to the behavior of -i and -t\nIt allows you to refer to containers by their service name in your compose.yaml file.\n\nFor example, if you have a compose.yaml like this:\n\n```\nservices:\n  web:\n    image: docker.io/alpinelinux/darkhttpd\n\n```\n\nThen you can run:\n\n```\ndocker compose exec web sh\n\n```\n\nThe equivalent docker exec command would be something like:\n\n```\ndocker exec -it myproject-web-1 sh\n\n```"}
{"instruction": "How to copy Docker images from one host to another without using a repository", "output": "You will need to save the Docker image as a tar file:\n\n```\ndocker save -o <path for generated tar file> <image name>\n\n```\n\nThen copy your image to a new system with regular file transfer tools such as cp, scp, or rsync (preferred for big files). After that you will have to load the image into Docker:\n\n```\ndocker load -i <path to image tar file>\n\n```\n\nYou should add filename (not just directory) with -o, for example:\n\n```\ndocker save -o c:/myfile.tar centos:16\n\n```\n\nyour image syntax may need the repository prefix (:latest tag is default)\n\n```\ndocker save -o C:\\path\\to\\file.tar repository/imagename\n\n```\n\n\nPS: You may need to sudo all commands."}
{"instruction": "How to get a Docker container&#39;s IP address from the host", "output": "This solution only works if the container is connected with a single network. The --format option of inspect comes to the rescue.\nModern Docker client syntax is:\n\n```\ndocker inspect \\\n  -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' container_name_or_id\n\n```\n\nOld Docker client syntax is:\n\n```\ndocker inspect \\\n  --format '{{ .NetworkSettings.IPAddress }}' container_name_or_id\n\n```\n\nThese commands will return the Docker container's IP address.\nAs mentioned in the comments: if you are on Windows, use double quotes \" instead of single quotes ' around the curly braces."}
{"instruction": "Should I use Vagrant or Docker for creating an isolated environment?", "output": "If your purpose is the isolation, I think Docker is what you want.\nVagrant is a virtual machine manager. It allows you to script the virtual machine configuration as well as the provisioning. However, it is still a virtual machine depending on VirtualBox (or others) with a huge overhead. It requires you to have a hard drive file that can be huge, it takes a lot of ram, and performance may be not very good.\nDocker on the other hand uses kernel cgroup and namespacing via LXC. It means that you are using the same kernel as the host and the same file system.\nYou can use Dockerfile with the docker build command in order to handle the provisioning and configuration of your container. You have an example at docs.docker.com on how to make your Dockerfile; it is very intuitive.\nThe only reason you could want to use Vagrant is if you need to do BSD, Windows or other non-Linux development on your Ubuntu box. Otherwise, go for Docker."}
{"instruction": "How to force Docker for a clean build of an image", "output": "There's a --no-cache option:\n\n```\ndocker build --no-cache -t u12_core -f u12_core .\n\n```\n\nIn older versions of Docker you needed to pass --no-cache=true, but this is no longer the case."}
{"instruction": "How do I pass environment variables to Docker containers?", "output": "You can pass environment variables to your containers with the -e (alias --env) flag.\ndocker run -e xx=yy\nAn example from a startup script:\n\n```\nsudo docker run -d -t -i -e REDIS_NAMESPACE='staging' \\ \n-e POSTGRES_ENV_POSTGRES_PASSWORD='foo' \\\n-e POSTGRES_ENV_POSTGRES_USER='bar' \\\n-e POSTGRES_ENV_DB_NAME='mysite_staging' \\\n-e POSTGRES_PORT_5432_TCP_ADDR='docker-db-1.hidden.us-east-1.rds.amazonaws.com' \\\n-e SITE_URL='staging.mysite.com' \\\n-p 80:80 \\\n--link redis:redis \\  \n--name container_name dockerhub_id/image_name\n\n```\n\nOr, if you don't want to have the value on the command-line where it will be displayed by ps, etc., -e can pull in the value from the current environment if you just give it without the =:\n\n```\nsudo PASSWORD='foo' docker run  [...] -e PASSWORD [...]\n\n```\n\nIf you have many environment variables and especially if they're meant to be secret, you can use an env-file:\n\n```\n$ docker run --env-file ./env.list ubuntu bash\n\n```\n\n\nThe --env-file flag takes a filename as an argument and expects each line to be in the VAR=VAL format, mimicking the argument passed to --env. Comment lines need only be prefixed with #"}
{"instruction": "How to remove old Docker containers", "output": "Since Docker 1.13.x you can use Docker container prune:\n\n```\ndocker container prune\n\n```\n\nThis will remove all stopped containers and should work on all platforms the same way.\nThere is also a Docker system prune:\n\n```\ndocker system prune\n\n```\n\nwhich will clean up all unused containers, networks, images (both dangling and unreferenced), and optionally, volumes, in one command.\n\nFor older Docker versions, you can string Docker commands together with other Unix commands to get what you need. Here is an example on how to clean up old containers that are weeks old:\n\n```\n$ docker ps --filter \"status=exited\" | grep 'weeks ago' | awk '{print $1}' | xargs --no-run-if-empty docker rm\n\n```\n\nTo give credit, where it is due, this example is from https://twitter.com/jpetazzo/status/347431091415703552."}
{"instruction": "How to fix Docker: Permission denied", "output": "If you want to run Docker as a non-root user, then you need to add your user to the docker group.\n\nCreate the docker group if it does not exist:\n\n\n```\n$ sudo groupadd docker\n\n```\n\n\nAdd your user to the docker group:\n\n\n```\n$ sudo usermod -aG docker $USER\n\n```\n\n\nLog in to the new docker group (to avoid having to log out and log in again; but if not enough, try to reboot):\n\n\n```\n$ newgrp docker\n\n```\n\n\nCheck if Docker can be run without root:\n\n\n```\n$ docker run hello-world\n\n```\n\nReboot if you still get an error:\n\n```\n$ reboot\n\n```\n\n\nFrom the official Docker documentation \"Manage Docker as a non-root user\":\n‚ö†Ô∏è Warning\n\nThe docker group grants root-level privileges to the user. For details on how this impacts security in your system, see Docker Daemon Attack Surface."}
{"instruction": "What is the difference between a Docker image and a container?", "output": "An instance of an image is called a container. You have an image, which is a set of layers as you describe. If you start this image, you have a running container of this image. You can have many running containers of the same image.\nYou can see all your images with docker images whereas you can see your running containers with docker ps (and you can see all containers with docker ps -a).\nSo a running instance of an image is a container."}
{"instruction": "How to remove old and unused Docker images", "output": "(see below for original answer)\n\nUpdate Sept. 2016: Docker 1.13: PR 26108 and commit 86de7c0 introduce a few new commands to help facilitate visualizing how much space the docker daemon data is taking on disk and allowing for easily cleaning up \"unneeded\" excess.\ndocker system prune will delete all dangling data (containers, networks, and images). You can remove all unused volumes with the --volumes option and remove all unused images (not just dangling) with the -a option.\nYou also have:\n\ndocker container prune\ndocker image prune\ndocker network prune\ndocker volume prune\n\nFor unused images, use docker image prune -a (for removing dangling and ununsed images).\nWarning: 'unused' means \"images not referenced by any container\": be careful before using -a.\nAs illustrated in A L's answer, docker system prune --all will remove all unused images not just dangling ones... which can be a bit too much.\nCombining docker xxx prune with the --filter option can be a great way to limit the pruning (docker SDK API 1.28 minimum, so docker 17.04+)\n\nThe currently supported filters are:\n\n\nuntil (<timestamp>) - only remove containers, images, and networks created before given timestamp\nlabel (label=<key>, label=<key>=<value>, label!=<key>, or label!=<key>=<value>) - only remove containers, images, networks, and volumes with (or without, in case label!=... is used) the specified labels.\n\nSee \"Prune images\" for an example.\n\nWarning: there is no \"preview\" or \"--dry-run\" option for those docker xxx prune commands.\nThis is requested with moby/moby issue 30623 since 2017, but seems tricky to be implemented (Aug. 2022)\n\nHaving a more representative overview of what will be pruned will be quite complicated, for various reasons;\n\nrace conditions (can be resolved by documenting the limitations);\nA container/image/volume/network may not be in use at the time that \"dry run\" is used, but may be in use the moment the actual prune is executed (or vice-versa), so dry run will always be an \"approximation\" of what will be pruned.\nthe more difficult part is due to how objects (containers, images, networks etc.) depend on each other.\nFor example, an image can be deleted if it no longer has references to it (no more tags, no more containers using it); this is the reason that docker system prune deletes objects in a specific order (first remove all unused containers, then remove unused images).\nIn order to replicate the same flow for \"dry-run\", it will be needed to temporarily construct representation of all objects and where they're referenced based on that (basically; duplicate all reference-counters, and then remove references from that \"shadow\" representation).\nFinally; with the work being done on integrating the containerd snapshotter (image and layer store), things may change more;\nFor example, images can now be multi-arch, and (to be discussed), \"pruning\" could remove unused variants (architectures) from an image to clean up space, which brings another dimension to calculating \"what can be removed\".\n\n\n\nOriginal answer (Sep. 2016)\nI usually do:\n\n```\ndocker rmi $(docker images --filter \"dangling=true\" -q --no-trunc)\n\n```\n\nI have an [alias for removing those dangling images: drmi]13\n\nThe dangling=true filter finds unused images\n\nThat way, any intermediate image no longer referenced by a labelled image is removed.\nI do the same first for exited processes (containers)\n\n```\nalias drmae='docker rm $(docker ps -qa --no-trunc --filter \"status=exited\")'\n\n```\n\nAs haridsv points out in the comments:\n\nTechnically, you should first clean up containers before cleaning up images, as this will catch more dangling images and less errors.\n\n\nJess Frazelle (jfrazelle) has the bashrc function:\n\n```\ndcleanup(){\n    docker rm -v $(docker ps --filter status=exited -q 2>/dev/null) 2>/dev/null\n    docker rmi $(docker images --filter dangling=true -q 2>/dev/null) 2>/dev/null\n}\n\n```\n\n\nTo remove old images, and not just \"unreferenced-dangling\" images, you can consider docker-gc:\n\n\nA simple Docker container and image garbage collection script.\n\nContainers that exited more than an hour ago are removed.\nImages that don't belong to any remaining container after that are removed."}
{"instruction": "How can I delete all local Docker images?", "output": "Unix\nTo delete all containers including its volumes use,\n\n```\ndocker rm -vf $(docker ps -aq)\n\n```\n\nTo delete all the images,\n\n```\ndocker rmi -f $(docker images -aq)\n\n```\n\nRemember, you should remove all the containers before removing all the images from which those containers were created.\nWindows - Powershell\n\n```\ndocker images -a -q | % { docker image rm $_ -f }\n\n```\n\nWindows - cmd.exe\n\n```\nfor /F %i in ('docker images -a -q') do docker rmi -f %i\n\n```"}
{"instruction": "Exploring Docker container&#39;s file system", "output": "Here are a couple different methods...\nA) Use docker exec (easiest)\nDocker version 1.3 or newer supports the command exec that behave similar to nsenter. This command can run new process in already running container (container must have PID 1 process running already). You can run /bin/bash to explore container state:\n\n```\ndocker exec -t -i mycontainer /bin/bash\n\n```\n\nsee Docker command line documentation\nB) Use Snapshotting\nYou can evaluate container filesystem this way:\n\n```\n# find ID of your running container:\ndocker ps\n\n# create image (snapshot) from container filesystem\ndocker commit 12345678904b5 mysnapshot\n\n# explore this filesystem using bash (for example)\ndocker run -t -i mysnapshot /bin/bash\n\n```\n\nThis way, you can evaluate filesystem of the running container in the precise time moment. Container is still running, no future changes are included.\nYou can later delete snapshot using (filesystem of the running container is not affected!):\n\n```\ndocker rmi mysnapshot\n\n```\n\nC) Use ssh\nIf you need continuous access, you can install sshd to your container and run the sshd daemon:\n\n```\ndocker run -d -p 22 mysnapshot /usr/sbin/sshd -D\n \n# you need to find out which port to connect:\ndocker ps\n\n```\n\nThis way, you can run your app using ssh (connect and execute what you want).\nD) Use nsenter\nUse nsenter, see Why you don't need to run SSHd in your Docker containers\n\nThe short version is: with nsenter, you can get a shell into an\nexisting container, even if that container doesn‚Äôt run SSH or any kind\nof special-purpose daemon"}
{"instruction": "How to deal with persistent storage (e.g. databases) in Docker", "output": "Docker 1.9.0 and above\nUse volume API\n\n```\ndocker volume create --name hello\ndocker run -d -v hello:/container/path/for/volume container_image my_command\n\n```\n\nThis means that the data-only container pattern must be abandoned in favour of the new volumes.\nActually the volume API is only a better way to achieve what was the data-container pattern.\nIf you create a container with a -v volume_name:/container/fs/path Docker will automatically create a named volume for you that can:\n\nBe listed through the docker volume ls\nBe identified through the docker volume inspect volume_name\nBacked up as a normal directory\nBacked up as before through a --volumes-from connection\n\nThe new volume API adds a useful command that lets you identify dangling volumes:\n\n```\ndocker volume ls -f dangling=true\n\n```\n\nAnd then remove it through its name:\n\n```\ndocker volume rm <volume name>\n\n```\n\nAs @mpugach underlines in the comments, you can get rid of all the dangling volumes with a nice one-liner:\n\n```\ndocker volume rm $(docker volume ls -f dangling=true -q)\n# Or using 1.13.x\ndocker volume prune\n\n```\n\nDocker 1.8.x and below\nThe approach that seems to work best for production is to use a data only container.\nThe data only container is run on a barebones image and actually does nothing except exposing a data volume.\nThen you can run any other container to have access to the data container volumes:\n\n```\ndocker run --volumes-from data-container some-other-container command-to-execute\n\n```\n\n\nHere you can get a good picture of how to arrange the different containers.\nHere there is a good insight on how volumes work.\n\nIn this blog post there is a good description of the so-called container as volume pattern which clarifies the main point of having data only containers.\nDocker documentation has now the DEFINITIVE description of the container as volume/s pattern.\nFollowing is the backup/restore procedure for Docker 1.8.x and below.\nBACKUP:\n\n```\nsudo docker run --rm --volumes-from DATA -v $(pwd):/backup busybox tar cvf /backup/backup.tar /data\n\n```\n\n\n--rm: remove the container when it exits\n--volumes-from DATA: attach to the volumes shared by the DATA container\n-v $(pwd):/backup: bind mount the current directory into the container; to write the tar file to\nbusybox: a small simpler image - good for quick maintenance\ntar cvf /backup/backup.tar /data: creates an uncompressed tar file of all the files in the /data directory\n\nRESTORE:\n\n```\n# Create a new data container\n$ sudo docker run -v /data -name DATA2 busybox true\n# untar the backup files into the new container·æøs data volume\n$ sudo docker run --rm --volumes-from DATA2 -v $(pwd):/backup busybox tar xvf /backup/backup.tar\ndata/\ndata/sven.txt\n# Compare to the original container\n$ sudo docker run --rm --volumes-from DATA -v `pwd`:/backup busybox ls /data\nsven.txt\n\n```\n\nHere is a nice article from the excellent Brian Goff explaining why it is good to use the same image for a container and a data container."}
{"instruction": "Where are Docker images stored on the host machine?", "output": "The contents of the /var/lib/docker directory vary depending on the driver Docker is using for storage. \nBy default this will be aufs but can fall back to overlay, overlay2, btrfs, devicemapper or zfs depending on your kernel support. In most places this will be aufs but the RedHats went with devicemapper.\nYou can manually set the storage driver with the -s or --storage-driver= option to the Docker daemon. \n\n/var/lib/docker/{driver-name} will contain the driver specific storage for contents of the images. \n/var/lib/docker/graph/<id> now only contains metadata about the image, in the json and layersize files.\n\nIn the case of aufs:\n\n/var/lib/docker/aufs/diff/<id> has the file contents of the images.\n/var/lib/docker/repositories-aufs is a JSON file containing local image information. This can be viewed with the command docker images.\n\nIn the case of devicemapper:\n\n/var/lib/docker/devicemapper/devicemapper/data stores the images\n/var/lib/docker/devicemapper/devicemapper/metadata the metadata\nNote these files are thin provisioned \"sparse\" files so aren't as big as they seem."}
{"instruction": "Docker Compose - How to execute multiple commands?", "output": "Figured it out, use bash -c.\nExample:\n\n```\ncommand: bash -c \"python manage.py migrate && python manage.py runserver 0.0.0.0:8000\"\n\n```\n\nSame example in multilines:\n\n```\ncommand: >\n    bash -c \"python manage.py migrate\n    && python manage.py runserver 0.0.0.0:8000\"\n\n```\n\nOr:\n\n```\ncommand: bash -c \"\n    python manage.py migrate\n    && python manage.py runserver 0.0.0.0:8000\n  \"\n\n```"}
{"instruction": "Error &quot;The input device is not a TTY&quot;", "output": "Remove the -it from your cli to make it non interactive and remove the TTY. If you don't need either, e.g. running your command inside of a Jenkins or cron script, you should do this.\nOr you can change it to -i if you have input piped into the docker command that doesn't come from a TTY. If you have something like xyz | docker ... or docker ... <input in your command line, do this.\nOr you can change it to -t if you want TTY support but don't have it available on the input device. Do this for apps that check for a TTY to enable color formatting of the output in your logs, or for when you later attach to the container with a proper terminal.\nOr if you need an interactive terminal and aren't running in a terminal on Linux or MacOS, use a different command line interface. PowerShell is reported to include this support on Windows.\n\nWhat is a TTY? It's a terminal interface that supports escape sequences, moving the cursor around, etc, that comes from the old days of dumb terminals attached to mainframes. Today it is provided by the Linux command terminals and ssh interfaces. See the wikipedia article for more details.\nTo see the difference of running a container with and without a TTY, run a container without one: docker run --rm -i ubuntu bash. From inside that container, install vim with apt-get update; apt-get install vim. Note the lack of a prompt. When running vim against a file, try to move the cursor around within the file."}
{"instruction": "Docker how to change repository name or rename image?", "output": "```\ndocker image tag server:latest myname/server:latest\n\n```\n\nor\n\n```\ndocker image tag d583c3ac45fd myname/server:latest\n\n```\n\nTags are just human-readable aliases for the full image name (d583c3ac45fd...). \nSo you can have as many of them associated with the same image as you like. If you don't like the old name you can remove it after you've retagged it:\n\n```\ndocker rmi server\n\n```\n\nThat will just remove the alias/tag. Since d583c3ac45fd has other names, the actual image won't be deleted."}
{"instruction": "How to list containers in Docker", "output": "To show only running containers run:\n\n```\ndocker ps\n\n```\n\nTo show all containers run:\n\n```\ndocker ps -a\n\n```\n\nTo show the latest created container (includes all states) run:\n\n```\ndocker ps -l\n\n```\n\nTo show n last created containers (includes all states) run:\n\n```\ndocker ps -n=-1\n\n```\n\nTo display total file sizes run:\n\n```\ndocker ps -s\n\n```\n\nThe content presented above is from docker.com.\nIn the new version of Docker, commands are updated, and some management commands are added:\n\n```\ndocker container ls\n\n```\n\nIt is used to list all the running containers includes all states.\n\n```\ndocker container ls -a\n\n```\n\nAnd then, if you want to clean them all,\n\n```\ndocker rm $(docker ps -aq)\n\n```\n\nIt is used to list all the containers created irrespective of its state.\nAnd to stop all the Docker containers (force)\n\n```\ndocker rm -f $(docker ps -a -q)  \n\n```\n\nHere the container is the management command."}
{"instruction": "How to see docker image contents", "output": "If the image contains a shell, you can run an interactive shell container using that image and explore whatever content that image has.  If sh is not available, the busybox ash shell might be.\nFor instance:\n\n```\ndocker run -it image_name sh\n\n```\n\nOr following for images with an entrypoint\n\n```\ndocker run -it --entrypoint sh image_name\n\n```\n\nOr if you want to see how the image was built, meaning the steps in its Dockerfile, you can:\n\n```\ndocker image history --no-trunc image_name > image_history\n\n```\n\nThe steps will be logged into the image_history file."}
{"instruction": "How to include files outside of Docker&#39;s build context?", "output": "The best way to work around this is to specify the Dockerfile independently of the build context, using -f.\nFor instance, this command will give the ADD command access to anything in your current directory.\n\n```\ndocker build -f docker-files/Dockerfile .\n\n```\n\nUpdate: Docker now allows having the Dockerfile outside the build context (fixed in 18.03.0-ce). So you can also do something like\n\n```\ndocker build -f ../Dockerfile .\n\n```"}
{"instruction": "How does one remove a Docker image?", "output": "Try docker rmi node. That should work. \nSeeing all created containers is as simple as docker ps -a. \nTo remove all existing containers (not images!) run docker rm $(docker ps -aq)"}
{"instruction": "How do I edit a file after I shell to a Docker container?", "output": "As in the comments, there's no default editor set - strange - the $EDITOR environment variable is empty. You can log in into a container with:\n\n```\ndocker exec -it <container> bash\n\n```\n\nAnd run:\n\n```\napt-get update\napt-get install vim\n\n```\n\nOr use the following Dockerfile:\n\n```\nFROM  confluent/postgres-bw:0.1\n\nRUN [\"apt-get\", \"update\"]\nRUN [\"apt-get\", \"install\", \"-y\", \"vim\"]\n\n```\n\nDocker images are delivered trimmed to the bare minimum - so no editor is installed with the shipped container. That's why there's a need to install it manually.\nEDIT\nI also encourage you to read my post about the topic."}
{"instruction": "docker push error: denied: requested access to the resource is denied", "output": "You may need to switch your docker repo to private before docker push.\nThanks to the answer provided by Dean Wu and this comment by ses, before pushing, remember to log out, then log in from the command line to your docker hub account\n\n```\n# you may need log out first `docker logout` ref. https://stackoverflow.com/a/53835882/248616\ndocker login\n\n```\n\nAccording to the docs:\n\n```\nYou need to include the namespace for Docker Hub to associate it with your account.\nThe namespace is the same as your Docker Hub account name.\nYou need to rename the image to YOUR_DOCKERHUB_NAME/docker-whale.\n\n```\n\nSo, this means you have to tag your image before pushing:\n\n```\ndocker tag firstimage YOUR_DOCKERHUB_NAME/firstimage\n\n```\n\nand then you should be able to push it.\n\n```\ndocker push YOUR_DOCKERHUB_NAME/firstimage\n\n```"}
{"instruction": "Run a Docker image as a container", "output": "The specific way to run it depends on whether you gave the image a tag/name or not.\n\n```\n$ docker images\nREPOSITORY          TAG                 ID                  CREATED             SIZE\nubuntu              12.04               8dbd9e392a96        4 months ago        131.5 MB (virtual 131.5 MB)\n\n```\n\nWith a name (let's use Ubuntu):\n\n```\n$ docker run -i -t ubuntu:12.04 /bin/bash\n\n```\n\nWithout a name, just using the ID:\n\n```\n$ docker run -i -t 8dbd9e392a96 /bin/bash\n\n```\n\nPlease see Docker run reference for more information."}
{"instruction": "How do I assign a port mapping to an existing Docker container?", "output": "You can change the port mapping by directly editing the hostconfig.json file at\n/var/lib/docker/containers/[hash_of_the_container]/hostconfig.json or /var/snap/docker/common/var-lib-docker/containers/[hash_of_the_container]/hostconfig.json if you installed Docker as a snap.\nYou can determine the [hash_of_the_container] via the docker inspect <container_name> command and the value of the \"Id\" field is the hash.\n\nStop the container (docker stop <container_name>).\nStop docker service (per Tacsiazuma's comment)\nChange the file.\nRestart your docker engine (to flush/clear config caches).\nStart the container (docker start <container_name>).\n\nSo you don't need to create an image with this approach. You can also change the restart flag here.\nP.S. You may visit https://docs.docker.com/engine/admin/ to learn how to correctly restart your docker engine as per your host machine. I used sudo systemctl restart docker to restart my docker engine that is running on Ubuntu 16.04.\nAdding ports: As mentioned in the comments you might also want to export the port from the (containered) machine. (For example when a new port appears in docker inspect but not in docker container ls)\nIn this case you also have to edit the config.v2.json file (next to hostconfig-file).\nThe syntax is like \"ExposedPorts\":{\"8080/tcp\":{}} (if you wish to expose port 8080 from the container)"}
{"instruction": "What is the runtime performance cost of a Docker container?", "output": "An excellent 2014 IBM research paper ‚ÄúAn Updated Performance Comparison of Virtual Machines and Linux Containers‚Äù by Felter et¬†al. provides a comparison between bare metal, KVM, and Docker containers. The general result is: Docker is nearly identical to native performance and faster than KVM in every category.\nThe exception to this is Docker‚Äôs NAT‚Ää‚Äî‚Äâif you use port mapping (e.g., docker run -p 8080:8080), then you can expect a minor hit in latency, as shown below. However, you can now use the host network stack (e.g., docker run --net=host) when launching a Docker container, which will perform identically to the Native column (as shown in the Redis latency results lower down). \n\nThey also ran latency tests on a few specific services, such as Redis. You can see that above 20 client threads, highest latency overhead goes Docker NAT, then KVM, then a rough tie between Docker host/native. \n\nJust because it‚Äôs a really useful paper, here are some other figures. Please download it for full access. \nTaking a look at Disk I/O:\n\nNow looking at CPU overhead: \n\nNow some examples of memory (read the paper for details, memory can be extra tricky):"}
{"instruction": "Docker - Ubuntu - bash: ping: command not found", "output": "Docker images are pretty minimal, but you can install ping in your official ubuntu docker image via:\n\n```\napt-get update -y\napt-get install -y iputils-ping\n\n```\n\nChances are you don't need ping on your image, and just want to use it for testing purposes. Above example will help you out.\nBut if you need ping to exist on your image, you can create a Dockerfile or commit the container you ran the above commands into a new image.\nCommit:\n\n```\ndocker commit -m \"Installed iputils-ping\" --author \"Your Name <name@domain.com>\" ContainerNameOrId yourrepository/imagename:tag\n\n```\n\nDockerfile:\n\n```\nFROM ubuntu\nRUN apt-get update && apt-get install -y iputils-ping\nCMD bash\n\n```\n\nPlease note there are best practices on creating docker images, like clearing apt cache files afterwards and etc."}
{"instruction": "How to access host port from docker container", "output": "When running Docker natively on Linux, you can access host services using the IP address of the docker0 interface.  From inside the container, this will be your default route.\nFor example, on my system:\n\n```\n$ ip addr show docker0\n7: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default \n    link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff\n    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::f4d2:49ff:fedd:28a0/64 scope link \n       valid_lft forever preferred_lft forever\n\n```\n\nAnd inside a container:\n\n```\n# ip route show\ndefault via 172.17.0.1 dev eth0 \n172.17.0.0/16 dev eth0  src 172.17.0.4 \n\n```\n\nIt's fairly easy to extract this IP address using a simple shell\nscript:\n\n```\n#!/bin/sh\n\nhostip=$(ip route show | awk '/default/ {print $3}')\necho $hostip\n\n```\n\nYou may need to modify the iptables rules on your host to permit\nconnections from Docker containers. Something like this will do the\ntrick:\n\n```\n# iptables -A INPUT -i docker0 -j ACCEPT\n\n```\n\nThis would permit access to any ports on the host from Docker\ncontainers.  Note that:\n\niptables rules are ordered, and this rule may or may not do the\nright thing depending on what other rules come before it.\n\nyou will only be able to access host services that are either (a)\nlistening on INADDR_ANY (aka 0.0.0.0) or that are explicitly\nlistening on the docker0 interface.\n\n\n\nIf you are using Docker on MacOS or Windows 18.03+, you can connect to the magic hostname host.docker.internal.\n\nLastly, under Linux you can run your container in the host network namespace by setting --net=host; in this case localhost on your host is the same as localhost inside the container, so containerized service will act like non-containerized services and will be accessible without any additional configuration."}
{"instruction": "In a Dockerfile, How to update PATH environment variable?", "output": "You can use Environment Replacement in your Dockerfile as follows:\n\n```\nENV PATH=\"$PATH:/opt/gtk/bin\"\n\n```\n\n(please note that when using ${PATH}, this might use the host's PATH instead of the Container)"}
{"instruction": "What is the difference between &quot;expose&quot; and &quot;publish&quot; in Docker?", "output": "Basically, you have three (four) options:\n\nNeither specify EXPOSE nor -p\nOnly specify EXPOSE\nSpecify EXPOSE and -p\nOnly specify -p which implicitly does EXPOSE\n\n\nIf you specify neither EXPOSE nor -p, the service in the container will only be accessible from inside the container itself.\n\nIf you EXPOSE a port, the service in the container is not accessible from outside Docker, but from inside other Docker containers. So this is good for inter-container communication.\n\nIf you EXPOSE and -p a port, the service in the container is accessible from anywhere, even outside Docker.\n\nIf you do -p, but do not EXPOSE, Docker does an implicit EXPOSE. This is because if a port is open to the public, it is automatically also open to other Docker containers. Hence -p includes EXPOSE. This is effectively same as 3).\n\n\nThe reason why both are separated is IMHO because:\n\nchoosing a host port depends on the host and hence does not belong to the Dockerfile (otherwise it would be depending on the host),\nand often it's enough if a service in a container is accessible from other containers.\n\nThe documentation explicitly states:\n\nThe EXPOSE instruction exposes ports for use within links.\n\nIt also points you to how to link containers (legacy feature), which basically is the inter-container communication I talked about."}
{"instruction": "How can I expose more than 1 port with Docker?", "output": "To expose just one port, this is what you need to do:\n\n```\ndocker run -p <host_port>:<container_port>\n\n```\n\nTo expose multiple ports, simply provide multiple -p arguments:\n\n```\ndocker run -p <host_port1>:<container_port1> -p <host_port2>:<container_port2>\n\n```"}
{"instruction": "How to push a docker image to a private repository", "output": "You need to tag your image correctly first with your registryhost:\n\n```\ndocker tag [OPTIONS] IMAGE[:TAG] [REGISTRYHOST/][USERNAME/]NAME[:TAG]\n\n```\n\nThen docker push using that same tag.\n\n```\ndocker push NAME[:TAG]\n\n```\n\nExample:\n\n```\ndocker tag 518a41981a6a myRegistry.com/myImage\ndocker push myRegistry.com/myImage\n\n```"}
{"instruction": "Difference between RUN and CMD in a Dockerfile", "output": "RUN is an image build step, the state of the container after a RUN command will be committed to the container image. A Dockerfile can have many RUN steps that layer on top of one another to build the image.\nCMD is the command the container executes by default when you launch the built image. A Dockerfile will only use the final CMD defined. The CMD can be overridden when starting a container with docker run $image $other_command.\nENTRYPOINT is also closely related to CMD and can modify the way a CMD is interpreted when a container is started from an image."}
{"instruction": "How to enter in a Docker container already running with a new TTY", "output": "With docker 1.3, there is a new command docker exec. This allows you to enter a running container:\n\n```\ndocker exec -it [container-id] bash\n\n```\n\nNote: this assumes bash is installed on your container. You may run sh or whatever interactive shell is installed on the container."}
{"instruction": "Communication between multiple docker-compose projects", "output": "You just need to make sure that the containers you want to talk to each other are on the same network. Networks are a first-class docker construct, and not specific to compose.\n\n```\n# front/docker-compose.yml\nservices:\n  front:\n    ...\n    networks:\n      - some-net\nnetworks:\n  some-net:\n    driver: bridge\n\n```\n\n...\n\n```\n# api/docker-compose.yml\nservices:\n  api:\n    ...\n    networks:\n      - front_some-net\nnetworks:\n  front_some-net:\n    external: true\n\n```\n\n\nNote: Your app‚Äôs network is given a name based on the ‚Äúproject name‚Äù, which is based on the name of the directory it lives in, in this case a prefix front_ was added\n\nThey can then talk to each other using the service name. From front you can do ping api and vice versa."}
{"instruction": "kubectl apply vs kubectl create?", "output": "Those are two different approaches:\nImperative Management\nkubectl create is what we call Imperative Management. On this approach you tell the Kubernetes API what you want to create, replace or delete, not how you want your K8s cluster world to look like.\nDeclarative Management\nkubectl apply is part of the Declarative Management approach, where changes that you may have applied to a live object (i.e. through scale) are \"maintained\" even if you apply other changes to the object.\n\nYou can read more about imperative and declarative management in the Kubernetes Object Management documentation.\n\ntl:dr\nThey do different things. If the resource exists, kubectl create will error out and kubectl apply will not error out."}
{"instruction": "Kubernetes pod gets recreated when deleted", "output": "You need to delete the deployment, which should in turn delete the pods and the replica sets https://github.com/kubernetes/kubernetes/issues/24137\nTo list all deployments:\n\n```\nkubectl get deployments --all-namespaces\n\n```\n\nThen to delete the deployment:\n\n```\nkubectl delete -n NAMESPACE deployment DEPLOYMENT\n\n```\n\nWhere NAMESPACE is the namespace it's in, and DEPLOYMENT is the name of the deployment. If NAMESPACE is default, leave off the -n option altogether.\nIn some cases it could also be running due to a job or daemonset.\nCheck the following and run their appropriate delete command.\n\n```\nkubectl get jobs\n\nkubectl get daemonsets.app --all-namespaces\n\nkubectl get daemonsets.extensions --all-namespaces\n\n```"}
{"instruction": "kubectl logs - continuously", "output": "```\nkubectl logs -f <pod-id>\n\n```\n\nYou can use the -f flag:\n-f, --follow=false: Specify if the logs should be streamed.\nhttps://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#logs"}
{"instruction": "Kubernetes API - Get Pods on Specific Nodes", "output": "As mentioned in the accepted answer the PR is now merged and you can get pods by node as follows:\n\n```\nkubectl get pods --all-namespaces -o wide --field-selector spec.nodeName=<node>\n\n```"}
{"instruction": "Kubernetes: How do I delete clusters and contexts from kubectl config?", "output": "kubectl config unset takes a dot-delimited path. You can delete cluster/context/user entries by name. E.g.\n\n```\nkubectl config unset users.gke_project_zone_name\n\nkubectl config unset contexts.aws_cluster1-kubernetes\n\nkubectl config unset clusters.foobar-baz\n\n```\n\nSide note, if you teardown your cluster using cluster/kube-down.sh (or gcloud if you use Container Engine), it will delete the associated kubeconfig entries. There is also a planned kubectl config rework for a future release to make the commands more intuitive/usable/consistent."}
{"instruction": "How does kubectl port-forward create a connection?", "output": "kubectl port-forward makes a specific Kubernetes API request.  That means the system running it needs access to the API server, and any traffic will get tunneled over a single HTTP connection.\nHaving this is really useful for debugging (if one specific pod is acting up you can connect to it directly; in a microservice environment you can talk to a back-end service you wouldn't otherwise expose) but it's not an alternative to setting up service objects.  When I've worked with kubectl port-forward it's been visibly slower than connecting to a pod via a service, and I've found seen the command just stop after a couple of minutes.  Again these aren't big problems for debugging, but they're not what I'd want for a production system."}
{"instruction": "Listing all resources in a namespace", "output": "Based on this comment , the supported way to list all resources is to iterate through all the api versions listed by kubectl api-resources:\n\nkubectl api-resources enumerates the resource types available in your cluster.\nthis means you can combine it with kubectl get to actually list every instance of every resource type in a namespace:\n\n\n```\nkubectl api-resources --verbs=list --namespaced -o name \\\n  | xargs -n 1 kubectl get --show-kind --ignore-not-found -l <label>=<value> -n <namespace>\n\n```"}
{"instruction": "Restart container within pod", "output": "Is it possible to restart a single container\n\nNot through kubectl, although depending on the setup of your cluster you can \"cheat\" and docker kill the-sha-goes-here, which will cause kubelet to restart the \"failed\" container (assuming, of course, the restart policy for the Pod says that is what it should do)\n\nhow do I restart the pod\n\nThat depends on how the Pod was created, but based on the Pod name you provided, it appears to be under the oversight of a ReplicaSet, so you can just kubectl delete pod test-1495806908-xn5jn and kubernetes will create a new one in its place (the new Pod will have a different name, so do not expect kubectl get pods to return test-1495806908-xn5jn ever again)"}
{"instruction": "kubectl get events only for a pod", "output": "You can use the event command of kubectl.\nTo filter for a specific pod you can use a field-selector:\n\n```\nkubectl get event --namespace abc-namespace --field-selector involvedObject.name=my-pod-zl6m6\n\n```\n\nTo see what fields are possible you can use kubectl describe on any event."}
{"instruction": "How can I update a secret on Kubernetes when it is generated from a file?", "output": "This should work:\n\n```\nkubectl create secret generic production-tls \\\n--save-config \\\n--dry-run=client \\\n--from-file=./tls.key --from-file=./tls.crt \\\n-o yaml | \\\nkubectl apply -f -\n\n```"}
{"instruction": "Kubernetes sort pods by age", "output": "Pods have status, which you can use to find out startTime.\nI guess something like kubectl get po --sort-by=.status.startTime should work.\nYou could also try:\n\nkubectl get po --sort-by='{.firstTimestamp}'.\nkubectl get pods --sort-by=.metadata.creationTimestamp Thanks @chris\n\nAlso apparently in Kubernetes 1.7 release, sort-by is broken.\nhttps://github.com/kubernetes/kubectl/issues/43\nHere's the bug report : https://github.com/kubernetes/kubernetes/issues/48602\nHere's the PR: https://github.com/kubernetes/kubernetes/pull/48659/files"}
{"instruction": "How to pull environment variables with Helm charts", "output": "You can export the variable and use it while running helm install.\nBefore that, you have to modify your chart so that the value can be set while installation. \nSkip this part, if you already know, how to setup template fields.\n\nAs you don't want to expose the data, so it's better to have it saved as secret in kubernetes. \nFirst of all, add this two lines in your Values file, so that these two values can be set from outside.\n\n```\nusername: root\npassword: password\n\n```\n\nNow, add a secret.yaml file inside your template folder. and, copy this code snippet into that file. \n\n```\napiVersion: v1\nkind: Secret\nmetadata:\n  name: {{ .Release.Name }}-auth\ndata:\n  password: {{ .Values.password | b64enc }}\n  username: {{ .Values.username | b64enc }}\n\n```\n\nNow tweak your deployment yaml template and make changes in env section, like this\n\n```\n...\n...\n    spec:\n      restartPolicy: Always\n      containers:\n        - name: sample-app\n          image: \"sample-app:latest\"\n          imagePullPolicy: Always\n          env:          \n          - name: \"USERNAME\"\n            valueFrom:\n              secretKeyRef:\n                key:  username\n                name: {{ .Release.Name }}-auth\n          - name: \"PASSWORD\"\n            valueFrom:\n              secretKeyRef:\n                key:  password\n                name: {{ .Release.Name }}-auth\n...\n...\n\n```\n\n\nIf you have modified your template correctly for --set flag,\nyou can set this using environment variable.\n\n```\n$ export USERNAME=root-user\n\n```\n\nNow use this variable while running helm install,\n\n```\n$ helm install --set username=$USERNAME ./mychart\n\n```\n\nIf you run this helm install in dry-run mode, you can verify the changes,\n\n```\n$ helm install --dry-run --set username=$USERNAME --debug ./mychart\n[debug] Created tunnel using local port: '44937'\n\n[debug] SERVER: \"127.0.0.1:44937\"\n\n[debug] Original chart version: \"\"\n[debug] CHART PATH: /home/maruf/go/src/github.com/the-redback/kubernetes-yaml-drafts/helm-charts/mychart\n\nNAME:   irreverant-meerkat\nREVISION: 1\nRELEASED: Fri Apr 20 03:29:11 2018\nCHART: mychart-0.1.0\nUSER-SUPPLIED VALUES:\nusername: root-user\n\nCOMPUTED VALUES:\npassword: password\nusername: root-user\n\nHOOKS:\nMANIFEST:\n\n---\n# Source: mychart/templates/secret.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: irreverant-meerkat-auth\ndata:\n  password: password\n  username: root-user\n---\n# Source: mychart/templates/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: irreverant-meerkat\n  labels:\n    app: irreverant-meerkat\nspec:\n  replicas: 1\n  template:\n    metadata:\n      name: irreverant-meerkat\n      labels:\n        app: irreverant-meerkat\n    spec:\n      containers:\n      - name: irreverant-meerkat\n        image: alpine\n        env:\n        - name: \"USERNAME\"\n          valueFrom:\n            secretKeyRef:\n              key:  username\n              name: irreverant-meerkat-auth\n        - name: \"PASSWORD\"\n          valueFrom:\n            secretKeyRef:\n              key:  password\n              name: irreverant-meerkat-auth\n\n        imagePullPolicy: IfNotPresent\n      restartPolicy: Always\n  selector:\n    matchLabels:\n      app: irreverant-meerkat\n\n```\n\nYou can see that the data of username in secret has changed to root-user.\nI have added this example into github repo.\nThere is also some discussion in kubernetes/helm repo regarding this. You can see this issue to know about all other ways to use environment variables."}
{"instruction": "Get error &quot;unknown field &quot;serviceName&quot; in io.k8s.api.networking.v1.IngressBackend&quot; when switch from v1beta1 to v1 in Kubernetes Ingress", "output": "I think that this PR contains the change you're asking about.\n\n```\n`Ingress` and `IngressClass` resources have graduated to `networking.k8s.io/v1`. Ingress and IngressClass types in the `extensions/v1beta1` and `networking.k8s.io/v1beta1` API versions are deprecated and will no longer be served in 1.22+. Persisted objects can be accessed via the `networking.k8s.io/v1` API. Notable changes in v1 Ingress objects (v1beta1 field names are unchanged):\n* `spec.backend` -> `spec.defaultBackend`\n* `serviceName` -> `service.name`\n* `servicePort` -> `service.port.name` (for string values)\n* `servicePort` -> `service.port.number` (for numeric values)\n* `pathType` no longer has a default value in v1; \"Exact\", \"Prefix\", or \"ImplementationSpecific\" must be specified\nOther Ingress API updates:\n* backends can now be resource or service backends\n* `path` is no longer required to be a valid regular expression\n\n```\n\nIf you look in the 1.19 Ingress doc, it looks like the new syntax would be:\n\n```\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: minimal-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  rules:\n  - http:\n      paths:\n      - path: /testpath\n        pathType: Prefix\n        backend:\n          service:\n            name: test\n            port:\n              number: 80\n\n```\n\nI unfortunately don't have a 1.19 cluster to test myself, but I think this is what you're running into."}
{"instruction": "did you specify the right host or port? error on Kubernetes", "output": "The issue is that your kubeconfig is not right.\nTo auto-generate it run:\n\n```\ngcloud container clusters get-credentials \"CLUSTER NAME\"\n\n```\n\nThis worked for me."}
{"instruction": "Tailing few lines from huge logs of kubectl logs -f", "output": "In a cluster best practices are to gather all logs in a single point through an aggregator and analyze them with a dedicated tool. For that reason in K8S, log command is quite basic.\nAnyway kubectl logs -h shows some options useful for you:\n\n```\n# Display only the most recent 20 lines of output in pod nginx\nkubectl logs --tail=20 nginx\n\n# Show all logs from pod nginx written in the last hour\nkubectl logs --since=1h nginx\n\n```\n\nSome tools with your requirements (and more) are available on github, some of which are:\n\nhttps://github.com/boz/kail\n\nhttps://github.com/stern/stern\n\nhttps://github.com/rogosprojects/klogs"}
{"instruction": "Tell when Job is Complete", "output": "Since version 1.11, you can do:\n\n```\nkubectl wait --for=condition=complete job/myjob\n\n```\n\nand you can also set a timeout:\n\n```\nkubectl wait --for=condition=complete --timeout=30s job/myjob\n\n```"}
{"instruction": "Execute bash command in pod with kubectl?", "output": "The double dash symbol \"--\" is used to separate the command you want to run inside the container from the kubectl arguments.\nSo the correct way is:\n\n```\nkubectl exec -it --namespace=tools mongo-pod -- bash -c \"mongo\"\n\n```\n\nYou forgot a space between \"--\" and \"bash\".\nTo execute multiple commands you may want:\n\nto create a script and mount it as a volume in your pod and execute it\n\nto launch a side container with the script and run it"}
{"instruction": "How to configure kubectl with cluster information from a .conf file?", "output": "Here are the official documentation for how to configure kubectl\nhttps://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/\nYou have a few options, specifically to this question, you can just copy your admin.conf to ~/.kube/config"}
{"instruction": "What is command to find detailed information about Kubernetes master(s) using kubectl?", "output": "kubectl version also shows the apiserver version. For example, this is the output when I run it:\n\n```\n$ kubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"2\", GitVersion:\"v1.2.4\", GitCommit:\"3eed1e3be6848b877ff80a93da3785d9034d0a4f\", GitTreeState:\"clean\"}\nServer Version: version.Info{Major:\"1\", Minor:\"2\", GitVersion:\"v1.2.4\", GitCommit:\"3eed1e3be6848b877ff80a93da3785d9034d0a4f\", GitTreeState:\"clean\"}\n\n```\n\nThe second line (\"Server Version\") contains the apiserver version. There isn't a way to get the master's kubelet version if it isn't registered as one of the nodes (which it isn't if it isn't showing up in kubectl get nodes), but in most deployments it'll be the same version as the apiserver."}
{"instruction": "kubectl how to list all contexts", "output": "You can view all contexts via the config command\n\n```\n$ kubectl config get-contexts\n\n```"}
{"instruction": "kubectl unable to connect to server: x509: certificate signed by unknown authority", "output": "Sorry I wasn't able to provide this earlier, I just realized the cause:\nSo on the master node we're running a kubectl proxy\n\n```\nkubectl proxy --address 0.0.0.0 --accept-hosts '.*'\n\n```\n\nI stopped this and voila the error was gone.\nI'm now able to do\n\n```\n\n    kubectl get nodes\nNAME                    STATUS    AGE       VERSION\ncentos-k8s2             Ready     3d        v1.7.5\nlocalhost.localdomain   Ready     3d        v1.7.5\n\n```\n\nI hope this helps those who stumbled upon this scenario."}
{"instruction": "What&#39;s the meaning of &quot;READY=2/2&quot; output by command &quot;kubectl get pod $yourpod&quot;", "output": "The \"Ready\" column shows how many containers in a pod are considered ready.\nYou can have some containers starting faster then others or having their readiness checks not yet fulfilled (or still in initial delay). In such cases there will be fewer containers ready in the pod than the total number (e.g. 1/2) hence the whole pod will not be considered ready."}
{"instruction": "Get current image of kubernetes deployment", "output": "From kubectl 1.6 the -o wide option does this, so\n\n```\nkubectl get deployments -o wide\n\n```\n\nwill show the current image in the output."}
{"instruction": "Is there a way to kubectl apply all the files in a directory?", "output": "You can apply all files in a folder with\n\n```\nkubectl apply -f <folder> \n\n```\n\nYou may also be interested in parameterization of your manifest files using Kustomize e.g. use more replicas in a prod-namespace than in a test-namespace. You can apply parameterized manifest files with\n\n```\nkubectl apply -k <folder>\n\n```"}
{"instruction": "how to check whether RBAC is enabled, using kubectl", "output": "You can check this by executing the command kubectl api-versions; if RBAC is enabled you should see the API version .rbac.authorization.k8s.io/v1.\nIn AKS, the best way is to check the cluster's resource details at resources.azure.com.\nIf you can spot \"enableRBAC\": true, your cluster has RBAC enabled.\nPlease note that existing non-RBAC enabled AKS clusters cannot currently be updated for RBAC use. (thanks @DennisAmeling for the clarification)"}
{"instruction": "What is the difference between kubectl apply and kubectl replace", "output": "I have written up a thorough explanation of the differences between apply, replace, and patch: Kubernetes Apply vs. Replace vs. Patch (update 2024-10-31: the Atomist blog no longer exists, use this Internet Archive link). It includes an explanation that the current top-ranked answer to this question is wrong.\nBriefly, kubectl apply uses the provided spec to create a resource if it does not exist and update, i.e., patch, it if it does.  The spec provided to apply need only contain the required parts of a spec, when creating a resource the API will use defaults for the rest and when updating a resource it will use its current values.\nThe kubectl replace completely replaces the existing resource with the one defined by the provided spec. replace wants a complete spec as input, including read-only properties supplied by the API like .metadata.resourceVersion, .spec.nodeName for pods, .spec.clusterIP for services, and .secrets for service accounts. kubectl has some internal tricks to help you get that right, but typically the use case for replace is getting a resource spec, changing a property, and then using that changed, complete spec to replace the existing resource.\nThe kubectl replace command has a --force option which actually does not use the replace, i.e., PUT, API endpoint. It forcibly deletes (DELETE) and then recreates, (POST) the resource using the provided spec."}
{"instruction": "How to pass image pull secret while using &#39;kubectl run&#39; command?", "output": "You can use the overrides if you specify it right, it's an array in the end, that took me a bit to figure out, the below works on Kubernetes of at least 1.6:\n--overrides='{ \"spec\": { \"template\": { \"spec\": { \"imagePullSecrets\": [{\"name\": \"your-registry-secret\"}] } } } }'\nfor example\n\n```\nkubectl run -i -t hello-world --rm --generator=run-pod/v1 \\\n--image=eu.gcr.io/your-registry/hello-world \\\n--image-pull-policy=\"IfNotPresent\" \\\n--overrides='{ \"spec\": { \"template\": { \"spec\": { \"imagePullSecrets\": [{\"name\": \"your-registry-secret\"}] } } } }'\n\n```"}
{"instruction": "How to Add Users to Kubernetes (kubectl)?", "output": "For a full overview on Authentication, refer to the official Kubernetes docs on Authentication and Authorization\nFor users, ideally you use an Identity provider for Kubernetes (OpenID Connect).\nIf you are on GKE / ACS you integrate with respective Identity and Access Management frameworks\nIf you self-host kubernetes (which is the case when you use kops), you may use coreos/dex to integrate with LDAP / OAuth2 identity providers - a good reference is this detailed 2 part SSO for Kubernetes article.\nkops (1.10+) now has built-in authentication support which eases the integration with AWS IAM as identity provider if you're on AWS.\nfor Dex there are a few open source cli clients as follows:\n\nNordstrom/kubelogin\npusher/k8s-auth-example\n\nIf you are looking for a quick and easy (not most secure and easy to manage in the long run) way to get started, you may abuse serviceaccounts - with 2 options for specialised Policies to control access. (see below)\nNOTE since 1.6  Role Based Access Control is strongly recommended! this answer does not cover RBAC setup\nEDIT: Great, but outdated (2017-2018), guide by Bitnami on User setup with RBAC is also available.\nSteps to enable service account access are (depending on if your cluster configuration includes RBAC or ABAC policies, these accounts may have full Admin rights!):\nEDIT: Here is a bash script to automate Service Account creation - see below steps\n\nCreate service account for user Alice\n\n```\nkubectl create sa alice\n\n```\n\n\nGet related secret\n\n```\nsecret=$(kubectl get sa alice -o json | jq -r .secrets[].name)\n\n```\n\n\nGet ca.crt from secret (using OSX base64 with -D flag for decode)\n\n```\nkubectl get secret $secret -o json | jq -r '.data[\"ca.crt\"]' | base64 -D > ca.crt\n\n```\n\n\nGet service account token from secret\n\n```\nuser_token=$(kubectl get secret $secret -o json | jq -r '.data[\"token\"]' | base64 -D)\n\n```\n\n\nGet information from your kubectl config (current-context, server..)\n\n```\n# get current context\nc=$(kubectl config current-context)\n\n# get cluster name of context\nname=$(kubectl config get-contexts $c | awk '{print $3}' | tail -n 1)\n\n# get endpoint of current context \nendpoint=$(kubectl config view -o jsonpath=\"{.clusters[?(@.name == \\\"$name\\\")].cluster.server}\")\n\n```\n\n\nOn a fresh machine, follow these steps (given the ca.cert and $endpoint information retrieved above:\n\nInstall kubectl\n\n```\n brew install kubectl\n\n```\n\n\nSet cluster (run in directory where ca.crt is stored)\n\n```\n kubectl config set-cluster cluster-staging \\\n   --embed-certs=true \\\n   --server=$endpoint \\\n   --certificate-authority=./ca.crt\n\n```\n\n\nSet user credentials\n\n```\n kubectl config set-credentials alice-staging --token=$user_token\n\n```\n\n\nDefine the combination of alice user with the staging cluster\n\n```\n kubectl config set-context alice-staging \\\n   --cluster=cluster-staging \\\n   --user=alice-staging \\\n   --namespace=alice\n\n```\n\n\nSwitch current-context to alice-staging for the user\n\n```\n kubectl config use-context alice-staging\n\n```\n\n\n\n\n\nTo control user access with policies (using ABAC), you need to create a policy file (for example):\n\n```\n{\n  \"apiVersion\": \"abac.authorization.kubernetes.io/v1beta1\",\n  \"kind\": \"Policy\",\n  \"spec\": {\n    \"user\": \"system:serviceaccount:default:alice\",\n    \"namespace\": \"default\",\n    \"resource\": \"*\",\n    \"readonly\": true\n  }\n}\n\n```\n\nProvision this policy.json on every master node and add --authorization-mode=ABAC --authorization-policy-file=/path/to/policy.json flags to API servers\nThis would allow Alice (through her service account) read only rights to all resources in default namespace only."}
{"instruction": "kubectl port forwarding timeout issue", "output": "Seems there is a 5 minute timeout that can be overridden with kubelet parameters:\nhttps://github.com/kubernetes/kubernetes/issues/19231\n\nIf you want to pass something higher than 5 minutes (or unlimited) into your kubelets, you can specify the streaming-connection-idle-timeout. E.g.\n  --streaming-connection-idle-timeout=4h to set it to 4 hours. Or:\n  --streaming-connection-idle-timeout=0 to make it unlimited.\n  (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)"}
{"instruction": "Kubectl error: the object has been modified; please apply your changes to the latest version and try again", "output": "I am able to reproduce the issue in my test environment. Steps to reproduce:\n\nCreate a deployment from Kubernetes Engine > Workloads > Deploy\nInput your Application Name, Namespace, Labels\nSelect cluster or create new cluster\n\nYou are able to view the YAML file here and here is the sample:\n\n```\n---\napiVersion: \"apps/v1\"\nkind: \"Deployment\"\nmetadata:\n  name: \"nginx-1\"\n  namespace: \"default\"\n  labels:\n    app: \"nginx-1\"\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: \"nginx-1\"\n  template:\n    metadata:\n      labels:\n        app: \"nginx-1\"\n    spec:\n      containers:\n      - name: \"nginx\"\n        image: \"nginx:latest\"\n---\napiVersion: \"autoscaling/v2beta1\"\nkind: \"HorizontalPodAutoscaler\"\nmetadata:\n  name: \"nginx-1-hpa\"\n  namespace: \"default\"\n  labels:\n    app: \"nginx-1\"\nspec:\n  scaleTargetRef:\n    kind: \"Deployment\"\n    name: \"nginx-1\"\n    apiVersion: \"apps/v1\"\n  minReplicas: 1\n  maxReplicas: 5\n  metrics:\n  - type: \"Resource\"\n    resource:\n      name: \"cpu\"\n      targetAverageUtilization: 80\n\n```\n\nAfter deployment if you go to Kubernetes Engine > Workloads > nginx-1 (click on it)  \na.) You will get Deployment details (Overview, Details, Revision history, events, YAML)\n  b.) click on YAML and copy the content from YAML tab\n  c.) create new YAML file and paste the content and save the file\n  d.) Now if you run the command $kubectl apply -f newyamlfile.yaml, it will shows you the below error:  \n\n```\nWarning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply\nError from server (Conflict): error when applying patch:\n{\"metadata\":{\"annotations\":{\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"apiVersion\\\":\\\"apps/v1\\\",\\\"kind\\\":\\\"Deployment\\\",\\\"metadata\\\":{\\\"annotations\\\":{\\\"deployment.kubernetes.io/revision\\\":\\\"1\\\"},\\\"creationTimestamp\\\":\\\"2019-09-17T21:34:39Z\\\",\\\"generation\\\":1,\\\"labels\\\":{\\\"app\\\":\\\"nginx-1\\\"},\\\"name\\\":\\\"nginx-1\\\",\\\"namespace\\\":\\\"default\\\",\\\"resourceVersion\\\":\\\"218884\\\",\\\"selfLink\\\":\\\"/apis/apps/v1/namespaces/default/deployments/nginx-1\\\",\\\"uid\\\":\\\"f41c5b6f-d992-11e9-9adc-42010a80023b\\\"},\\\"spec\\\":{\\\"progressDeadlineSeconds\\\":600,\\\"replicas\\\":3,\\\"revisionHistoryLimit\\\":10,\\\"selector\\\":{\\\"matchLabels\\\":{\\\"app\\\":\\\"nginx-1\\\"}},\\\"strategy\\\":{\\\"rollingUpdate\\\":{\\\"maxSurge\\\":\\\"25%\\\",\\\"maxUnavailable\\\":\\\"25%\\\"},\\\"type\\\":\\\"RollingUpdate\\\"},\\\"template\\\":{\\\"metadata\\\":{\\\"creationTimestamp\\\":null,\\\"labels\\\":{\\\"app\\\":\\\"nginx-1\\\"}},\\\"spec\\\":{\\\"containers\\\":[{\\\"image\\\":\\\"nginx:latest\\\",\\\"imagePullPolicy\\\":\\\"Always\\\",\\\"name\\\":\\\"nginx\\\",\\\"resources\\\":{},\\\"terminationMessagePath\\\":\\\"/dev/termination-log\\\",\\\"terminationMessagePolicy\\\":\\\"File\\\"}],\\\"dnsPolicy\\\":\\\"ClusterFirst\\\",\\\"restartPolicy\\\":\\\"Always\\\",\\\"schedulerName\\\":\\\"default-scheduler\\\",\\\"securityContext\\\":{},\\\"terminationGracePeriodSeconds\\\":30}}},\\\"status\\\":{\\\"availableReplicas\\\":3,\\\"conditions\\\":[{\\\"lastTransitionTime\\\":\\\"2019-09-17T21:34:47Z\\\",\\\"lastUpdateTime\\\":\\\"2019-09-17T21:34:47Z\\\",\\\"message\\\":\\\"Deployment has minimum availability.\\\",\\\"reason\\\":\\\"MinimumReplicasAvailable\\\",\\\"status\\\":\\\"True\\\",\\\"type\\\":\\\"Available\\\"},{\\\"lastTransitionTime\\\":\\\"2019-09-17T21:34:39Z\\\",\\\"lastUpdateTime\\\":\\\"2019-09-17T21:34:47Z\\\",\\\"message\\\":\\\"ReplicaSet \\\\\\\"nginx-1-7b4bb7fbf8\\\\\\\" has successfully progressed.\\\",\\\"reason\\\":\\\"NewReplicaSetAvailable\\\",\\\"status\\\":\\\"True\\\",\\\"type\\\":\\\"Progressing\\\"}],\\\"observedGeneration\\\":1,\\\"readyReplicas\\\":3,\\\"replicas\\\":3,\\\"updatedReplicas\\\":3}}\\n\"},\"generation\":1,\"resourceVersion\":\"218884\"},\"spec\":{\"replicas\":3},\"status\":{\"availableReplicas\":3,\"observedGeneration\":1,\"readyReplicas\":3,\"replicas\":3,\"updatedReplicas\":3}}\nto:\nResource: \"apps/v1, Resource=deployments\", GroupVersionKind: \"apps/v1, Kind=Deployment\"\nName: \"nginx-1\", Namespace: \"default\"\nObject: &{map[\"apiVersion\":\"apps/v1\" \"metadata\":map[\"name\":\"nginx-1\" \"namespace\":\"default\" \"selfLink\":\"/apis/apps/v1/namespaces/default/deployments/nginx-1\" \"uid\":\"f41c5b6f-d992-11e9-9adc-42010a80023b\" \"generation\":'\\x02' \"labels\":map[\"app\":\"nginx-1\"] \"annotations\":map[\"deployment.kubernetes.io/revision\":\"1\"] \"resourceVersion\":\"219951\" \"creationTimestamp\":\"2019-09-17T21:34:39Z\"] \"spec\":map[\"replicas\":'\\x01' \"selector\":map[\"matchLabels\":map[\"app\":\"nginx-1\"]] \"template\":map[\"metadata\":map[\"labels\":map[\"app\":\"nginx-1\"] \"creationTimestamp\":<nil>] \"spec\":map[\"containers\":[map[\"imagePullPolicy\":\"Always\" \"name\":\"nginx\" \"image\":\"nginx:latest\" \"resources\":map[] \"terminationMessagePath\":\"/dev/termination-log\" \"terminationMessagePolicy\":\"File\"]] \"restartPolicy\":\"Always\" \"terminationGracePeriodSeconds\":'\\x1e' \"dnsPolicy\":\"ClusterFirst\" \"securityContext\":map[] \"schedulerName\":\"default-scheduler\"]] \"strategy\":map[\"type\":\"RollingUpdate\" \"rollingUpdate\":map[\"maxUnavailable\":\"25%\" \"maxSurge\":\"25%\"]] \"revisionHistoryLimit\":'\\n' \"progressDeadlineSeconds\":'\\u0258'] \"status\":map[\"observedGeneration\":'\\x02' \"replicas\":'\\x01' \"updatedReplicas\":'\\x01' \"readyReplicas\":'\\x01' \"availableReplicas\":'\\x01' \"conditions\":[map[\"message\":\"Deployment has minimum availability.\" \"type\":\"Available\" \"status\":\"True\" \"lastUpdateTime\":\"2019-09-17T21:34:47Z\" \"lastTransitionTime\":\"2019-09-17T21:34:47Z\" \"reason\":\"MinimumReplicasAvailable\"] map[\"lastTransitionTime\":\"2019-09-17T21:34:39Z\" \"reason\":\"NewReplicaSetAvailable\" \"message\":\"ReplicaSet \\\"nginx-1-7b4bb7fbf8\\\" has successfully progressed.\" \"type\":\"Progressing\" \"status\":\"True\" \"lastUpdateTime\":\"2019-09-17T21:34:47Z\"]]] \"kind\":\"Deployment\"]}\nfor: \"test.yaml\": Operation cannot be fulfilled on deployments.apps \"nginx-1\": the object has been modified; please apply your changes to the latest version and try again\n\n```\n\nTo solve the problem, you need to find the exact yaml file and then edit it as per your requirement, after that you can run $kubectl apply -f nginx-1.yaml \nHope this information finds you well."}
{"instruction": "Install Google Cloud components error from gcloud command", "output": "The Cloud SDK component manager only works if you don't install the SDK through another package manager.  If you want to use the component manager, you can install using one of these methods:\nhttps://cloud.google.com/sdk/downloads#versioned\nhttps://cloud.google.com/sdk/downloads#interactive\nAdditional packages are available in our deb and yum repos so all the same components are available, you just need to use your existing package manager to install them:\nhttps://cloud.google.com/sdk/downloads#apt-get\nhttps://cloud.google.com/sdk/downloads#yum"}
{"instruction": "How to view logs of failed jobs with kubectl?", "output": "One other approach:\n\nkubectl describe job $JOB\nPod name is shown under \"Events\"\nkubectl logs $POD"}
{"instruction": "Kubernetes / kubectl - &quot;A container name must be specified&quot; but seems like it is?", "output": "If a pod has more than 1 containers then you need to provide the name of the specific container. \nin your case, There is a pod (postgres-operator-49202276-bjtf4) which has 2 containers (apiserver and postgres-operator ).\nfollowing commands will provide logs for the specific containers \n\n```\nkubectl logs deployment/postgres-operator -c apiserver\n\n\nkubectl logs deployment/postgres-operator -c postgres-operator\n\n```"}
{"instruction": "Kubernetes create deployment unexpected SchemaError", "output": "After installing kubectl with brew you should run: \n\nrm /usr/local/bin/kubectl\nbrew link --overwrite kubernetes-cli\n\nAnd also optionally:\nbrew link --overwrite --dry-run kubernetes-cli."}
{"instruction": "Can I set a default namespace in Kubernetes?", "output": "Yes, you can set the namespace as per the docs like so:\n\n```\n$ kubectl config set-context --current --namespace=NAMESPACE\n\n```\n\nAlternatively, you can use kubectx for this."}
{"instruction": "How to format the output of kubectl describe to JSON", "output": "kubectl describe doesn't support -o or equivalent. It's meant to be human-readable rather than script-friendly. You can achieve what you described with kubectl get pods -l <selector_of_your_rc> -o <output_format>, for example:\n\n```\n$ kubectl get pods -l app=guestbook,tier=frontend -o name\npod/frontend-a4kjz\npod/frontend-am1ua\npod/frontend-yz2dq\n\n```"}
{"instruction": "Kubernetes POD delete with Pattern Match or Wildcard", "output": "Am I missing something to delete POD using Pattern Match or with Wilcard?\n\nWhen using Kubernetes it is more common to use labels and selectors. E.g. if you deployed an application, you usually set a label on the pods e.g. app=my-app and you can then get the pods with e.g. kubectl get pods -l app=my-app.\nUsing this aproach, it is easier to delete the pods you are interested in, with e.g.\n\n```\nkubectl delete pods -l app=my-app\n\n```\n\nor with namespaces\n\n```\nkubectl delete pods -l app=my-app -n default\n\n```\n\nSee more on Kubernetes Labels and Selectors\nSet-based selector \n\nI have some pod's running in the name of \"superset-react\" and \"superset-graphql\" and I want to search my wildcard superset and delete both of them in one command\n\nI suggest that those pods has labels app=something-react and app=something-graphql. If you want to classify those apps, e.g. if your \"superset\" varies, you could add a label app-type=react and app-type=graphql to all those type of apps.\nThen you can delete pods for both app types with this command:\n\n```\nkubectl delete pods -l 'app-type in (react, graphql)'\n\n```"}
{"instruction": "Kubernetes describe pod - Error from server (NotFound)", "output": "You've not specified the namespace in your describe pod command.\nYou did kubectl get all -n minio-operator, which gets all resources in the minio-operator namespace, but your kubectl describe has no namespace, so it's looking in the default namespace for a pod that isn't there.\n\n```\nkubectl describe pod -n minio-operator <pod name>\n\n```\n\nShould work OK.\nMost resources in kubernetes are namespaced, so will require the -n <namespace> argument unless you switch namespaces."}
{"instruction": "how to pass environment variable in kubectl deployment?", "output": "I used envsubst (https://www.gnu.org/software/gettext/manual/html_node/envsubst-Invocation.html) for this. Create a deployment.yaml\n\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: $NAME\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n\n```\n\nThen:\n\n```\nexport NAME=my-test-nginx\nenvsubst < deployment.yaml | kubectl apply -f -\n\n```\n\nNot sure what OS are you using to run this. On macOS, envsubst installed like:\n\n```\nbrew install gettext\nbrew link --force gettext \n\n```"}
{"instruction": "kubectl how to rename a context", "output": "Renaming contexts is easy!\n\n```\n$ kubectl config rename-context old-name new-name\n\n```\n\nConfirm the change by \n\n```\n$ kubectl config get-contexts \n\n```"}
{"instruction": "Kubectl error: memcache.go:265] couldn‚Äôt get current server API group list: Get", "output": "Try\n\n```\nkubectl get nodes -v=10\n\n```\n\nand look for the errors."}
{"instruction": "Define size for /dev/shm on container engine", "output": "Mounting an emptyDir to /dev/shm and setting the medium to Memory did the trick!\n\n```\nspec:\n  volumes:\n  - name: dshm\n    emptyDir:\n      medium: Memory\n  containers:\n  - image: gcr.io/project/image\n    volumeMounts:\n      - mountPath: /dev/shm\n        name: dshm\n\n```"}
{"instruction": "Kubectl command to list pods of a deployment in Kubernetes", "output": "There's a label in the pod for the selector in the deployment. That's how a deployment manages its pods. For example for the label or selector app=http-svc you can do something like that this and avoid using grep and listing all the pods (this becomes useful as your number of pods becomes very large)\nHere are some examples command line:\n\n```\n# single label\nkubectl get pods -l=app=http-svc\nkubectl get pods --selector=app=http-svc\n# multiple labels\nkubectl get pods --selector key1=value1,key2=value2\n\n```\n\nTo get the label used by your deployment:\n\n```\nkubectl describe deploy/DEPLOYMENT_NAME | grep Selector\n\n```"}
{"instruction": "How to create a kubectl config file for serviceaccount", "output": "```\n# your server name goes here\nserver=https://localhost:8443\n# the name of the secret containing the service account token goes here\nname=default-token-sg96k\n\nca=$(kubectl get secret/$name -o jsonpath='{.data.ca\\.crt}')\ntoken=$(kubectl get secret/$name -o jsonpath='{.data.token}' | base64 --decode)\nnamespace=$(kubectl get secret/$name -o jsonpath='{.data.namespace}' | base64 --decode)\n\necho \"\napiVersion: v1\nkind: Config\nclusters:\n- name: default-cluster\n  cluster:\n    certificate-authority-data: ${ca}\n    server: ${server}\ncontexts:\n- name: default-context\n  context:\n    cluster: default-cluster\n    namespace: default\n    user: default-user\ncurrent-context: default-context\nusers:\n- name: default-user\n  user:\n    token: ${token}\n\" > sa.kubeconfig\n\n```"}
{"instruction": "Wait for kubernetes job to complete on either failure/success using command line", "output": "kubectl wait --for=condition=<condition name is waiting for a specific condition, so afaik it can not specify multiple conditions at the moment.\nMy workaround is using oc get --wait, --wait is closed the command if the target resource is updated. I will monitor status section of the job using oc get --wait until status is updated. Update of status section is meaning the Job is complete with some status conditions.\nIf the job complete successfully, then status.conditions.type is updated immediately as Complete. But if the job is failed then the job pod will be restarted automatically regardless restartPolicy is OnFailure or Never. But we can deem the job is Failed status if not to updated as Complete after first update.\nLook the my test evidence as follows.\n\nJob yaml for testing successful complete\n\n\n```\n\n    # vim job.yml\n    apiVersion: batch/v1\n    kind: Job\n    metadata:\n      name: pi\n    spec:\n      parallelism: 1\n      completions: 1\n      template:\n        metadata:\n          name: pi\n        spec:\n          containers:\n          - name: pi\n            image: perl\n            command: [\"perl\",  \"-wle\", \"exit 0\"]\n          restartPolicy: Never\n\n```\n\n\nIt will show you Complete if it complete the job successfully.\n\n\n```\n\n    # oc create -f job.yml &&\n      oc get job/pi -o=jsonpath='{.status}' -w &&\n      oc get job/pi -o=jsonpath='{.status.conditions[*].type}' | grep -i -E 'failed|complete' || echo \"Failed\" \n\n    job.batch/pi created\n    map[startTime:2019-03-09T12:30:16Z active:1]Complete\n\n```\n\n\nJob yaml for testing failed complete\n\n\n```\n\n    # vim job.yml\n    apiVersion: batch/v1\n    kind: Job\n    metadata:\n      name: pi\n    spec:\n      parallelism: 1\n      completions: 1\n      template:\n        metadata:\n          name: pi\n        spec:\n          containers:\n          - name: pi\n            image: perl\n            command: [\"perl\",  \"-wle\", \"exit 1\"]\n          restartPolicy: Never\n\n```\n\n\nIt will show you Failed if the first job update is not Complete. Test if after delete the existing job resource.\n\n\n```\n\n    # oc delete job pi\n    job.batch \"pi\" deleted\n\n    # oc create -f job.yml &&\n      oc get job/pi -o=jsonpath='{.status}' -w &&\n      oc get job/pi -o=jsonpath='{.status.conditions[*].type}' | grep -i -E 'failed|complete' || echo \"Failed\" \n\n    job.batch/pi created\n    map[active:1 startTime:2019-03-09T12:31:05Z]Failed\n\n```\n\nI hope it help you. :)"}
{"instruction": "How to see logs of terminated pods", "output": "Running kubectl logs -p will fetch logs from existing resources at API level. This means that terminated pods' logs will be unavailable using this command.\nAs mentioned in other answers, the best way is to have your logs centralized via logging agents or directly pushing these logs into an external service.\nAlternatively and given the logging architecture in Kubernetes, you might be able to fetch the logs directly from the log-rotate files in the node hosting the pods. However, this option might depend on the Kubernetes implementation as log files might be deleted when the pod eviction is triggered."}
{"instruction": "kubeadm init shows kubelet isn&#39;t running or healthy", "output": "Unfortunately, swap was still enabled.\nThe error was fixed by\n\n```\nsudo swapoff -a\nsudo sed -i '/ swap / s/^/#/' /etc/fstab\n\n```\n\nreboot the machine."}
{"instruction": "Create kubernetes pod with volume using kubectl run", "output": "Your JSON override is specified incorrectly. Unfortunately kubectl run just ignores fields it doesn't understand.\n\n```\nkubectl run -i --rm --tty ubuntu --overrides='\n{\n  \"apiVersion\": \"batch/v1\",\n  \"spec\": {\n    \"template\": {\n      \"spec\": {\n        \"containers\": [\n          {\n            \"name\": \"ubuntu\",\n            \"image\": \"ubuntu:14.04\",\n            \"args\": [\n              \"bash\"\n            ],\n            \"stdin\": true,\n            \"stdinOnce\": true,\n            \"tty\": true,\n            \"volumeMounts\": [{\n              \"mountPath\": \"/home/store\",\n              \"name\": \"store\"\n            }]\n          }\n        ],\n        \"volumes\": [{\n          \"name\":\"store\",\n          \"emptyDir\":{}\n        }]\n      }\n    }\n  }\n}\n'  --image=ubuntu:14.04 --restart=Never -- bash\n\n```\n\nTo debug this issue I ran the command you specified, and then in another terminal ran:\n\n```\nkubectl get job ubuntu -o json\n\n```\n\nFrom there you can see that the actual job structure differs from your json override (you were missing the nested template/spec, and volumes, volumeMounts, and containers need to be arrays)."}
{"instruction": "kubectl wait --for=condition=complete --timeout=30s", "output": "This totally looks like you are running kubectl wait --for=condition=complete on a Pod as described in your output rather than a Job.\nA pod doesn't have the --for=condition=complete option. Exactly, what I get when I run it on a pod:\n\n```\n$ kubectl wait --for=condition=complete pod/mypod-xxxxxxxxxx-xxxxx\nerror: .status.conditions accessor error: Failure is of the type string, expected map[string]interface{}\n\n```"}
{"instruction": "How to view members of subject with Group kind", "output": "Admittedly, late to the party here.\nHave a read through the Kubernetes 'Authenticating' docs. Kubernetes does not have an in-built mechanism for defining and controlling users (as distinct from ServiceAccounts which are used to provide a cluster identity for Pods, and therefore services running on them).\nThis means that Kubernetes does not therefore have any internal DB to reference, to determine and display group membership.\nIn smaller clusters, x509 certificates are typically used to authenticate users. The API server is configured to trust a CA for the purpose, and then users are issued certificates signed by that CA. As you had noticed, if the subject contains an 'Organisation' field, that is mapped to a Kubernetes group. If you want a user to be a member of more than one group, then you specify multiple 'O' fields. (As an aside, to my mind it would have made more sense to use the 'OU' field, but that is not the case)\nIn answer to your question, it appears that in the case of a cluster where users are authenticated by certificates, your only route is to have access to the issued certs, and to check for the presence of the 'O' field in the subject. I guess in more advanced cases, Kubernetes would be integrated with a centralised tool such as AD, which could be queried natively for group membership."}
{"instruction": "Should I use Vagrant or Docker for creating an isolated environment?", "output": "If your purpose is the isolation, I think Docker is what you want.\nVagrant is a virtual machine manager. It allows you to script the virtual machine configuration as well as the provisioning. However, it is still a virtual machine depending on VirtualBox (or others) with a huge overhead. It requires you to have a hard drive file that can be huge, it takes a lot of ram, and performance may be not very good.\nDocker on the other hand uses kernel cgroup and namespacing via LXC. It means that you are using the same kernel as the host and the same file system.\nYou can use Dockerfile with the docker build command in order to handle the provisioning and configuration of your container. You have an example at docs.docker.com on how to make your Dockerfile; it is very intuitive.\nThe only reason you could want to use Vagrant is if you need to do BSD, Windows or other non-Linux development on your Ubuntu box. Otherwise, go for Docker."}
{"instruction": "Vagrant stuck connection timeout retrying", "output": "I solved this problem, and will answer in case anyone else has a similar issue.\nWhat I did was: I enabled the GUI of Virtual box to see that it was waiting for input on startup to select whether I wanted to boot directly to ubuntu or safemode etc.\nTo turn on the GUI you have to put this in your vagrant config Vagrantfile:\n\n```\nconfig.vm.provider :virtualbox do |vb|\n  vb.gui = true\nend\n\n```"}
{"instruction": "Easiest way to copy a single file from host to Vagrant guest?", "output": "Instead of using a shell provisioner to copy the file, you can also use a Vagrant file provisioner.\n\nProvisioner name: \"file\"\nThe file provisioner allows you to upload a file from the host machine to the guest machine.\n\n\n```\nVagrant.configure(\"2\") do |config|\n  # ... other configuration\n\n  config.vm.provision \"file\", source: \"~/.gitconfig\", destination: \".gitconfig\"\nend\n\n```"}
{"instruction": "Where does Vagrant download its .box files to?", "output": "As mentioned in the docs, boxes are stored at: \n\nMac OS X and Linux: ~/.vagrant.d/boxes\nWindows: C:/Users/USERNAME/.vagrant.d/boxes"}
{"instruction": "Vagrant error : Failed to mount folders in Linux guest", "output": "The plugin vagrant-vbguest   solved my problem: \n\n```\n$ vagrant plugin install vagrant-vbguest\n\n```\n\nOutput:\n\n```\n$ vagrant reload\n==> default: Attempting graceful shutdown of VM...\n...\n==> default: Machine booted and ready!\nGuestAdditions 4.3.12 running --- OK.\n==> default: Checking for guest additions in VM...\n==> default: Configuring and enabling network interfaces...\n==> default: Exporting NFS shared folders...\n==> default: Preparing to edit /etc/exports. Administrator privileges will be required...\n==> default: Mounting NFS shared folders...\n==> default: VM already provisioned. Run `vagrant provision` or use `--provision` to force it\n\n```\n\nJust make sure you are running the latest version of VirtualBox"}
{"instruction": "How to change Vagrant &#39;default&#39; machine name?", "output": "I found the multiple options confusing, so I decided to test all of them to see exactly what they do. \nI'm using VirtualBox 4.2.16-r86992 and Vagrant 1.3.3.\nI created a directory called nametest and ran \n\n```\nvagrant init precise64 http://files.vagrantup.com/precise64.box\n\n```\n\nto generate a default Vagrantfile.  Then I opened the VirtualBox GUI so I could see what names the boxes I create would show up as.\n\nDefault Vagrantfile\n\n```\nVagrant.configure('2') do |config|\n    config.vm.box = \"precise64\"\n    config.vm.box_url = \"http://files.vagrantup.com/precise64.box\"\nend\n\n```\n\nVirtualBox GUI Name:  \"nametest_default_1386347922\"\nComments:  The name defaults to the format DIRECTORY_default_TIMESTAMP.\nDefine VM\n\n```\nVagrant.configure('2') do |config|\n    config.vm.box = \"precise64\"\n    config.vm.box_url = \"http://files.vagrantup.com/precise64.box\"\n    config.vm.define \"foohost\"\nend\n\n```\n\nVirtualBox GUI Name:  \"nametest_foohost_1386347922\"\nComments:  If you explicitly define a VM, the name used replaces the token 'default'. This is the name vagrant outputs on the console. Simplifying based on zook's (commenter) input\nSet Provider Name\n\n```\nVagrant.configure('2') do |config|\n    config.vm.box = \"precise64\"\n    config.vm.box_url = \"http://files.vagrantup.com/precise64.box\"\n    config.vm.provider :virtualbox do |vb|\n        vb.name = \"foohost\"\n    end\nend\n\n```\n\nVirtualBox GUI Name:  \"foohost\"\nComments:  If you set the name attribute in a provider configuration block, that name will become the entire name displayed in the VirtualBox GUI.\nCombined Example: Define VM -and- Set Provider Name\n\n```\nVagrant.configure('2') do |config|\n    config.vm.box = \"precise64\"\n    config.vm.box_url = \"http://files.vagrantup.com/precise64.box\"\n    config.vm.define \"foohost\"\n    config.vm.provider :virtualbox do |vb|\n        vb.name = \"barhost\"\n    end\nend\n\n```\n\nVirtualBox GUI Name:  \"barhost\"\nComments:  If you use both methods at the same time, the value assigned to name in the provider configuration block wins. Simplifying based on zook's (commenter) input\nSet hostname (BONUS)\n\n```\nVagrant.configure(VAGRANTFILE_API_VERSION) do |config|\n  config.vm.hostname = \"buzbar\"\nend\n\n```\n\nComments: This sets the hostname inside the VM. This would be the output of hostname command in the VM and also this is what's visible in the prompt like vagrant@<hostname>, here it will look like vagrant@buzbar\n\nFinal Code\n\n```\n    Vagrant.configure('2') do |config|\n        config.vm.box = \"precise64\"\n        config.vm.box_url = \"http://files.vagrantup.com/precise64.box\"\n        config.vm.hostname = \"buzbar\"\n        config.vm.define \"foohost\"\n        config.vm.provider :virtualbox do |vb|\n            vb.name = \"barhost\"\n        end\n    end\n\n```\n\n\nSo there it is.  You now know 3 different options you can set and the effects they have.  I guess it's a matter of preference at this point?  (I'm new to Vagrant, so I can't speak to best practices yet.)"}
{"instruction": "Psql could not connect to server: No such file or directory, 5432 error?", "output": "I've had this same issue, related to the configuration of my pg_hba.conf file (located in /etc/postgresql/9.6/main). Please note that 9.6 is the postgresql version I am using.\nThe error itself is related to a misconfiguration of postgresql, which causes the server to crash before it starts.\nI would suggest following these instructions:\n\nCertify that postgresql service is running, using sudo service postgresql start \nRun pg_lsclusters from your terminal\nCheck what is the cluster you are running, the output should be something like:\nVersion - Cluster   Port Status Owner    Data directory \n9.6  -------  main --   5432 online postgres /var/lib/postgresql/9.6/main\n\nDisregard the '---' signs, as they are being used there only for alignment.\n  The important information are the version and the cluster. You can also check whether the server is running or not in the status column.\n\nCopy the info from the version and the cluster, and use like so: \npg_ctlcluster <version> <cluster> start, so in my case, using version 9.6 and cluster 'main', it would be pg_ctlcluster 9.6 main start\nIf something is wrong, then postgresql will generate a log, that can be accessed on /var/log/postgresql/postgresql-<version>-main.log, so in my case, the full command would be sudo nano /var/log/postgresql/postgresql-9.6-main.log.\nThe output should show what is the error. \n\n\n2017-07-13 16:53:04 BRT [32176-1] LOG:  invalid authentication method \"all\"\n 2017-07-13 16:53:04 BRT [32176-2] CONTEXT:  line 90 of configuration file \"/etc/postgresql/9.5/main/pg_hba.conf\"\n 2017-07-13 16:53:04 BRT [32176-3] FATAL:  could not load pg_hba.conf\n\nFix the errors and restart postgresql service through sudo service postgresql restart and it should be fine.\n\nI have searched a lot to find this, credit goes to this post.\nBest of luck!"}
{"instruction": "Failed to open/create the internal network Vagrant on Windows10", "output": "I found a solution\n\nOpen Windows Network Connections\nRight click on VirtualBox Host only adapter that created\nChoose properties\nCheck \"VirtualBox NDIS6 Bridged Networking driver\"\ndisable and Enable the adapter"}
{"instruction": "How to disable &quot;Cannot Render Console from...&quot; on Rails", "output": "You need to specifically allow the 10.0.2.2 network space in the Web Console config.\nSo you'll want something like this:\n\n```\nclass Application < Rails::Application\n  config.web_console.permissions = '10.0.2.2'\nend\n\n```\n\nRead here for more information.\nAs pointed out by pguardiario, this wants to go into config/environments/development.rb rather than config/application.rb so it is only applied in your development environment."}
{"instruction": "How to ssh to vagrant without actually running &quot;vagrant ssh&quot;?", "output": "I've had to re-implement \"vagrant ssh\" because it's -c option didn't pass on arguments properly. This is basically what it does (there might be more, but it works fine this way)\n\n```\n#!/bin/sh\nPORT=$(vagrant ssh-config | grep Port | grep -o '[0-9]\\+')\nssh -q \\\n    -o UserKnownHostsFile=/dev/null \\\n    -o StrictHostKeyChecking=no \\\n    -i ~/.vagrant.d/insecure_private_key \\\n    vagrant@localhost \\\n    -p $PORT \\\n    \"$@\"\n\n```\n\nAs a one-liner (with thanks to kgadek):\n\n```\nssh $(vagrant ssh-config | awk 'NR>1 {print \" -o \"$1\"=\"$2}') localhost\n\n```\n\nTo account for when you have more than one vagrant host, this will select the desired host, as well as cull blank lines from the config (using sed):\n\n```\nHOST=name-of-my-host\nssh $(vagrant ssh-config $HOST | sed '/^[[:space:]]*$/d' |  awk 'NR>1 {print \" -o \"$1\"=\"$2}') localhost\n\n```"}
{"instruction": "Vagrant ssh authentication failure", "output": "Make sure your first network interface is NAT. The other second network interface can be anything you want when you're building box. Don't forget the Vagrant user, as discussed in the Google thread. \nGood luck."}
{"instruction": "SSH to Vagrant box in Windows?", "output": "I use PuTTY to connect to my Vagrant boxes on Windows7.\nMake sure you\n\nconvert the %USERPROFILE%\\.vagrant.d\\insecure_private_key to .ppk using PuTTYGen\nuse the .ppk key in your PuTTY session - configured in Connection > SSH > Auth > Private key file\nuse host 127.0.0.1\nuse port 2222 instead of 22\nyou can set the default username (vagrant) under Connection > SSH > Auth > Private key for authentication"}
{"instruction": "VBoxManage: error: Failed to create the host-only adapter", "output": "I had the same problem today. The reason was that I had another VM running in VirtualBox.\nSolution:\n\nOpen VirtualBox and shut down every VM running\nGo to System Preferences > Security & Privacy Then hit the \"Allow\" button to let Oracle (VirtualBox) load.\nRestart VirtualBox\n\n\n```\nsudo \"/Library/Application Support/VirtualBox/LaunchDaemons/VirtualBoxStartup.sh\" restart\n\n```\n\nYou should now be able to run vagrant up or vagrant reload and have your new host configured.\n\nAs mentioned in this answer, recent versions of macOS can block VirtualBox.\n\nSolution:\nGo to System Preferences > Security & Privacy Then hit the \"Allow\" button to let Oracle (VirtualBox) load.\n(thanks to @pazhyn, @lsimonetti & @dave-beauchesne for clarifications)"}
{"instruction": "How to set host_key_checking=false in ansible inventory file?", "output": "Due to the fact that I answered this in 2014, I have updated my answer to account for more recent versions of ansible.\nYes, you can do it at the host/inventory level (Which became possible on newer ansible versions) or global level:\ninventory:\nAdd the following.\n\n```\nansible_ssh_common_args='-o StrictHostKeyChecking=no'\n\n```\n\nhost:\nAdd the following.\n\n```\nansible_ssh_extra_args='-o StrictHostKeyChecking=no'\n\n```\n\nhosts/inventory options will work with connection type ssh and not paramiko. Some people may strongly argue that inventory and hosts is more secure because the scope is more limited.\nglobal:\nAnsible User Guide - Host Key Checking\n\nYou can do it either in the /etc/ansible/ansible.cfg or ~/.ansible.cfg file:\n\n```\n  [defaults]\n  host_key_checking = False\n\n```\n\n\nOr you can setup and env variable (this might not work on newer ansible versions):\n\n```\n  export ANSIBLE_HOST_KEY_CHECKING=False\n\n```\n\n\n\nKubernetes:\n\nYou can also run a ansible head node as a Kubernetes pod like described here.\n\nIn this case you can use any of the global options above."}
{"instruction": "How to add a downloaded .box file to Vagrant?", "output": "Solution:\n\n```\nvagrant box add my-box file:///d:/path/to/file.box\n\n```\n\nHas to be in a URL format."}
{"instruction": "How do I associate a Vagrant project directory with an existing VirtualBox VM?", "output": "WARNING: The solution below works for Vagrant 1.0.x but not Vagrant 1.1+.\nVagrant uses the \".vagrant\" file in the same directory as your \"Vagrantfile\" to track the UUID of your VM. This file will not exist if a VM does not exist. The format of the file is JSON. It looks like this if a single VM exists:\n\n```\n{\n   \"active\":{\n      \"default\":\"02f8b71c-75c6-4f33-a161-0f46a0665ab6\"\n   }\n}\n\n```\n\ndefault is the name of the default virtual machine (if you're not using multi-VM setups). \nIf your VM has somehow become disassociated, what you can do is do VBoxManage list vms which will list every VM that VirtualBox knows about by its name and UUID. Then manually create a .vagrant file in the same directory as your Vagrantfile and fill in the contents properly.\nRun vagrant status to ensure that Vagrant picked up the proper changes.\nNote: This is not officially supported by Vagrant and Vagrant may change the format of .vagrant at any time. But this is valid as of Vagrant 0.9.7 and will be valid for Vagrant 1.0."}
{"instruction": "Using Laravel Homestead: &#39;no input file specified&#39;", "output": "Instead of reinstalling try\n\n```\nvagrant up --provision\n\n```\n\nor\n\n```\nhomestead up --provision\n\n```"}
{"instruction": "Vagrant Not Starting Up. User that created VM doesn&#39;t match current user", "output": "I ran into the same problem today.\nI edited my UID by opening the file .vagrant/machines/default/virtualbox/creator_uid and changing the 501 to a 0. \nAfter I saved the file, the command vagrant up worked like a champ.\nNB: the .vagrant folder is in the same directory as your Vagrantfile, where you ran vagrant up"}
{"instruction": "A VirtualBox machine with the name &#39;homestead&#39; already exists", "output": "I solved by using vboxmanage to get the ID of the VM.\n\n```\n$ vboxmanage list vms\n\"my-vm\" {c700b8b6-b766-4638-871b-736b44b7db18}\n\n```\n\nCopy the ID of the desired VM (the c700‚Ä¶db18 string) into the contents of ~/.vagrant/machines/default/virtualbox/id. Save the file then run vagrant up to get the vm working without having to destroy it."}
{"instruction": "Vagrant reverse port forwarding?", "output": "When you run vagrant ssh, it's actually using this underlying command:\nssh -p 2222 -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o LogLevel=ERROR -o IdentitiesOnly=yes -i ~/.vagrant.d/insecure_private_key vagrant@127.0.0.1\nSSH supports forwarding ports in the direction you want with the -R guestport:host:hostport option. So, if you wanted to connect to port 12345 on the guest and have it forwarded to localhost:80, you would use this command:\nssh -p 2222 -R 12345:localhost:80 -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o LogLevel=ERROR -o IdentitiesOnly=yes -i ~/.vagrant.d/insecure_private_key vagrant@127.0.0.1\nAs Eero correctly comments, you can also use the command vagrant ssh -- -R 12345:localhost:80, which has the same effect in a much more concise command."}
{"instruction": "How can I kill whatever process is using port 8080 so that I can vagrant up?", "output": "This might help\n\n```\nlsof -n -i4TCP:8080 \n\n```\n\nThe PID is the second field in the output.\nOr try:\n\n```\nlsof -i -P\n\n```"}
{"instruction": "How to export a Vagrant virtual machine to transfer it", "output": "You have two ways to do this, I'll call it dirty way and clean way:\n1. The dirty way\nCreate a box from your current virtual environment, using vagrant package command:\nhttp://docs.vagrantup.com/v2/cli/package.html\nThen copy the box to the other pc, add it using vagrant box add and run it using vagrant up as usual.\nKeep in mind that files in your working directory (the one with the Vagrantfile) are shared when the virtual machine boots, so you need to copy it to the other pc as well.\n2. The clean way\nTheoretically it should never be necessary to do export/import with Vagrant. If you have the foresight to use provisioning for configuring the virtual environment (chef, puppet, ansible), and a version control system like git for your working directory, copying an environment would be at this point simple as running:\n\n```\ngit clone <your_repo>\nvagrant up\n\n```"}
{"instruction": "Is there a default password to connect to vagrant when using `homestead ssh` for the first time?", "output": "After trying a lot of passwords and becoming totally confused why my public key password is not working I found out that I have to use vagrant as password.\nMaybe this info helps someone else too - that's because I've written it down here.\nEdit:\nAccording to the Vagrant documentation, there is usually a default password for the user vagrant which is vagrant.\nRead more on here: official website\nIn recent versions however, they have moved to generating keypairs for each machine. If you would like to find out where that key is, you can run vagrant ssh -- -v. This will show the verbose output of the ssh login process. You should see a line like\n\n```\ndebug1: Trying private key: /home/aaron/Documents/VMs/.vagrant/machines/default/virtualbox/private_key\n\n```"}
{"instruction": "Removing list of vms in vagrant cache", "output": "You should use the following command to remove invalid entries from the global index:\n\n```\nvagrant global-status --prune\n\n```"}
{"instruction": "How do I add my own public key to Vagrant VM?", "output": "Copying the desired public key would fall squarely into the provisioning phase. The exact answer depends on what provisioning you fancy to use (shell, Chef, Puppet etc). The most trivial would be a file provisioner for the key, something along this:\n\n```\nconfig.vm.provision \"file\", source: \"~/.ssh/id_rsa.pub\", destination: \"~/.ssh/me.pub\"\n\n```\n\nWell, actually you need to append to authorized_keys. Use the the shell provisioner, like so:\n\n```\nVagrant.configure(2) do |config|\n  # ... other config\n  config.vm.provision \"shell\", inline: <<-SHELL\n    cat /home/vagrant/.ssh/me.pub >> /home/vagrant/.ssh/authorized_keys\n  SHELL\n  # ... other config\nend\n\n```\n\nYou can also use a true provisioner, like Puppet. For example see Managing SSH Authorized Keys with Puppet."}
{"instruction": "Vagrant&#39;s port forwarding not working", "output": "I'll make this an actual answer instead of just more comments.\nFirst thing: try curl 'http://localhost:80' from within the VM.  If that doesn't work, then it's definitely not the port forwarding.\nNext: try curl -v 'http://localhost:4567/' from your host machine.  Curl might give you a better error message than Safari.\nI'd check that there are no firewalls set up restricting access to port 80.  The default Vagrant VM (Ubuntu) doesn't come with a firewall set up, but you said you're using something else, so it might be worth it to check.\nIf that's not it, try making something other than Apache listed on port 80.  Python ships with a simple HTTP server you can use -- go to the folder with index.html and run sudo python -m SimpleHTTPServer 80, then try hitting that with curl from both boxes.  If that works, then it's probably an Apache configuration issue.  I don't have enough experience with Apache to help if that's the case (I use nginx)."}
{"instruction": "Vagrant was unable to mount VirtualBox shared folders", "output": "Update\n\nFixed in VirtualBox 5.1.22.\n\n(https://www.virtualbox.org/ticket/16670)\nOriginal answer\nIn my case a symlink inside the VM was wrong.\nLog into the VM:\n\n```\n$ vagrant ssh\n\n```\n\nThen:\n\n```\n$ ls -lh /sbin/mount.vboxsf \nlrwxrwxrwx 1 root root 49 Apr 19 14:05 /sbin/mount.vboxsf -> /opt/VBoxGuestAdditions-5.1.20/other/mount.vboxsf\n\n```\n\nThis link is broken. When you look inside /opt/VBoxGuestAdditions-5.1.20/ you see that the subfolder \"other\" doesn't exists.\nThis line fixes that:\n\n```\n$ sudo ln -sf /opt/VBoxGuestAdditions-*/lib/VBoxGuestAdditions/mount.vboxsf /sbin/mount.vboxsf\n\n```\n\nNow logout of the VM:\n\n```\n$ exit\n\n```\n\nAnd check if it works for you:\n\n```\n$ vagrant reload\n\n```\n\nBut i can't fix the link because /sbin/mount.vboxsf does not exist inside my box!\nIf the link /sbin/mount.vboxsf does not exists in the first place, than the VBoxGuestAdditions couldn't be installed. This can happen if you have not downloaded the matching VirtualBox Extension Pack after you updated VirtualBox (v5.2.18 and up should download it automatically during Virtualbox updates). You can do that now:\n\nHalt all your running VMs.\nOpen the VirtualBox program.\nStart the download of the VirtualBox Extension Pack and install it.\nIf you are not being asked if you want to download the VirtualBox Extension Pack, open ->File ->Check for Update, to trigger the download dialog.\n\nIf there was no update for the VirtualBox Extension, than it's possible the VBoxGuestAdditions can't be installed because the installer shows a message and the vagrant plugin vbguest can not prompt an answer (this is \"fixed\" in vbguest v0.15.0). You can see that in the output of \"vagrant up\". There should be a line similar to this one:\n\n```\nDo you wish to continue? [yes or no]\n\n```\n\nIf that is the case you can run the installer manually. To do so, start the box:\n\n```\n$ vagrant up\n\n```\n\nThen trigger vbguest to try the installation again but leave the VBoxGuestAdditions ISO mounted afterwards:\n\n```\n$ vagrant vbguest --do install --no-cleanup\n\n```\n\nLog into the box:\n\n```\n$ vagrant ssh\n\n```\n\nRan the installer manually:\n\n```\n$ sudo /mnt/VBoxLinuxAdditions.run\n\n```\n\nA last hope: DIY!\nIf nothing of the above worked, try to install the guest additions manually:\n\n```\ncd /opt\nsudo wget -c http://download.virtualbox.org/virtualbox/5.1.28/VBoxGuestAdditions_5.1.28.iso -O VBoxGuestAdditions_5.1.28.iso\nsudo mount VBoxGuestAdditions_5.1.28.iso -o loop /mnt\nsudo sh /mnt/VBoxLinuxAdditions.run\n\n```"}
{"instruction": "vagrant login as root by default", "output": "Solution: \nAdd the following to your Vagrantfile:\n\n```\nconfig.ssh.username = 'root'\nconfig.ssh.password = 'vagrant'\nconfig.ssh.insert_key = 'true'\n\n```\n\nWhen you vagrant ssh henceforth, you will login as root and should expect the following:\n\n```\n==> mybox: Waiting for machine to boot. This may take a few minutes...\n    mybox: SSH address: 127.0.0.1:2222\n    mybox: SSH username: root\n    mybox: SSH auth method: password\n    mybox: Warning: Connection timeout. Retrying...\n    mybox: Warning: Remote connection disconnect. Retrying...\n==> mybox: Inserting Vagrant public key within guest...\n==> mybox: Key inserted! Disconnecting and reconnecting using new SSH key...\n==> mybox: Machine booted and ready!\n\n```\n\nUpdate 23-Jun-2015:\nThis works for version 1.7.2 as well. Keying security has improved since 1.7.0; this technique overrides back to the previous method which uses a known private key. This solution is not intended to be used for a box that is accessible publicly without proper security measures done prior to publishing.\nReference:\n\nhttps://docs.vagrantup.com/v2/vagrantfile/ssh_settings.html"}
{"instruction": "Symbolic links and synced folders in Vagrant", "output": "Virtualbox does not allow symlinks on shared folders for security reasons. To enable symlinks the following line needs to be added to the vm provider config block in the Vagrantfile:\n\n```\nconfig.vm.provider \"virtualbox\" do |v|\n    v.customize [\"setextradata\", :id, \"VBoxInternal2/SharedFoldersEnableSymlinksCreate/v-root\", \"1\"]\nend\n\n```\n\nAdditionally, on windows vagrant up needs to be executed in a shell with admin rights. \nNo workarounds necessary."}
{"instruction": "How do I increase the RAM and set up host-only networking in Vagrant?", "output": "You can modify various VM properties by adding the following configuration (see the Vagrant docs for a bit more info):\n\n```\n  # Configure VM Ram usage\n  config.vm.customize [\n                        \"modifyvm\", :id,\n                        \"--name\", \"Test_Environment\",\n                        \"--memory\", \"1024\"\n                      ]\n\n```\n\nYou can obtain the properties that you want to change from the documents for VirtualBox command-line options:\n\nhttp://www.virtualbox.org/manual/ch08.html#vboxmanage-modifyvm\n\nThe vagrant documentation has the section on how to change IP address:\n\n```\nVagrant::Config.run do |config|\n  config.vm.network :hostonly, \"192.168.50.4\"\nend\n\n```\n\nAlso you can restructure the configuration like this, ending is do with end without nesting it.\nThis is simpler.\n\n```\nconfig.vm.define :web do |web_config|\n    web_config.vm.box = \"lucid32\"\n    web_config.vm.forward_port 80, 8080\nend\nweb_config.vm.provision :puppet do |puppet|\n    puppet.manifests_path = \"manifests\"\n    puppet.manifest_file = \"lucid32.pp\"\nend\n\n```"}
{"instruction": "Where is Vagrant saving changes to the VM?", "output": "Vagrant imports the base box which is located at ~/.vagrant.d/boxes/, like you said.\nThis is where the base boxes are kept. It uses it to start a VM from the clean state. When importing a VM, the responsibility of where to store data files and VM state is up to VirtualBox itself. This is a configurable location but typically defaults to ~/VirtualBox\\ VMS for Mac OS X and Linux. In Windows the boxes are kept in %userprofile%\\.vagrant.d\\boxes\nIt is easy to find the place where they are getting created, regardless of what platform you happen to be using.\n\n```\n1. Start VirtualBox. \n2. Go to the VM that shows as running instance. \n3. Check out the Settings ->Storage. \n4. You can find the path to location where VMs are created and stored in your file system.\n\n```"}
{"instruction": "How to use vagrant in a proxy environment?", "output": "Install proxyconf: \n\n```\nvagrant plugin install vagrant-proxyconf\n\n```\n\nConfigure your Vagrantfile:\n\n```\nconfig.proxy.http     = \"http://yourproxy:8080\"\nconfig.proxy.https    = \"http://yourproxy:8080\"\nconfig.proxy.no_proxy = \"localhost,127.0.0.1\"\n\n```"}
{"instruction": "What exactly do the Vagrant commands do?", "output": "I'm not sure when it changed, but the current version (1.6.3) has a proper list of commands, and running vagrant list-commands gives an even more complete list:\n\n```\nbox             manages boxes: installation, removal, etc.\nconnect         connect to a remotely shared Vagrant environment\ndestroy         stops and deletes all traces of the vagrant machine\ndocker-logs     outputs the logs from the Docker container\ndocker-run      run a one-off command in the context of a container\nglobal-status   outputs status Vagrant environments for this user\nhalt            stops the vagrant machine\nhelp            shows the help for a subcommand\ninit            initializes a new Vagrant environment by creating a Vagrantfile\nlist-commands   outputs all available Vagrant subcommands, even non-primary ones\nlogin           log in to Vagrant Cloud\npackage         packages a running vagrant environment into a box\nplugin          manages plugins: install, uninstall, update, etc.\nprovision       provisions the vagrant machine\nrdp             connects to machine via RDP\nreload          restarts vagrant machine, loads new Vagrantfile configuration\nresume          resume a suspended vagrant machine\nrsync           syncs rsync synced folders to remote machine\nrsync-auto      syncs rsync synced folders automatically when files change\nshare           share your Vagrant environment with anyone in the world\nssh             connects to machine via SSH\nssh-config      outputs OpenSSH valid configuration to connect to the machine\nstatus          outputs status of the vagrant machine\nsuspend         suspends the machine\nup              starts and provisions the vagrant environment\nversion         prints current and latest Vagrant version\n\n```\n\nThe only commands left out from the complete list when running vagrant by itself are the docker and rsync ones. On my system, anyway.\nThat seems to be the definitive answer, now."}
{"instruction": "Vagrant error: NFS is reporting that your exports file is invalid", "output": "Try using the known good versions of VirtualBox and Vagrant noted in Discourse as Your First Rails App:\n\nVagrant 1.1.2\nVirtualBox 4.2.10\n\nI was having the same issue on Mac (OS¬†X 10.9 (Mavericks)), but rolling back to these versions seemed to fix it for me."}
{"instruction": "Vagrant for a Java project: should you compile in the VM or on the host?", "output": "After much thought and experimentation, I've decided on where to use Vagrant and how it integrates with the Java development workflow.\nFor JavaEE / deployed applications, configuring a web server and a database server are definitely things that have \"enough\" complexity to warrant the use of Vagrant. With two servers and the myriad ways to configure them, it's easy for configuration to get out of sync from one developer to another, bringing about the \"works on my machine\" syndrome. For this kind of software, it would work best to edit and compile the code on the host, and deploy to a Vagrant VM that mimics your production environment. The deployment folder for the web server could even be symlinked to a compile target on the host, removing the need to manually redeploy. So Vagrant could be an important part of your development lifecycle, but the cycle time for code/compile/deploy from the host and run on the VM with Java would be longer than the cycle time for code on the host and run on the VM that we see with PHP/Ruby/Node/etc.\nFor standalone Java applications (like libraries or desktop applications) the story changes a bit. In this case it makes the most sense to edit, compile, and run on the host machine, eschewing the use of Vagrant altogether. If you're using one of the big Java IDE's (Eclipse, Netbeans, IntelliJ...), you already have Java installed on the machine. At that point there is very little advantage compared to the overhead of using Vagrant, and only serves to put an extra layer of complexity in your development process. This is because by the time you are able to edit Java with an IDE you are able to run everything on the host anyway. One issue is that the version of Java required for the project may not match the version running the IDE on the host. In general (hopefully) this is not too much of a problem; as of this writing JDK6 is end-of-lifed and JDK8 is not yet released (guess where that leaves us). But if you did need to run multiple versions, you should be able to set JAVA_HOME on the host as needed. Though this does introduce extra complexity, it is less complexity than maintaining a Vagrant runtime just to work with projects using different versions of Java.\nThe interesting question is what to do with containerless web applications. Should the web server (in this case internal to the application) be run inside the VM as we did for the external web server? Or run on the host as we did for the standalone application? For containerless web applications, there is no external web server to worry about, but there is still likely a database. In this situation we can take a hybrid approach. Running a containerless web app is essentially the same as running a standalone application, so it would be effective to compile and run your code on the host machine. But with a database involved there is still enough complexity and configuration there that it makes sense to have the database server be on its own Vagrant VM.\nHopefully this gives Java developers who are interested in Vagrant some context about how to use it."}
{"instruction": "How to connect to Mysql Server inside VirtualBox Vagrant?", "output": "Make sure MySQL binds to 0.0.0.0 and not 127.0.0.1 or it will not be accessible from outside the machine\nYou can ensure this by editing the /etc/mysql/my.conf file and looking for the bind-address item--you want it to look like bind-address = 0.0.0.0.  Then save this and restart MySQL:\n\n```\nsudo service mysql restart\n\n```\n\nIf you are doing this on a production server, you want to be aware of the security implications, discussed here: https://serverfault.com/questions/257513/how-bad-is-setting-mysqls-bind-address-to-0-0-0-0"}
{"instruction": "How to pass parameter on &#39;vagrant up&#39; and have it in the scope of Vagrantfile?", "output": "You cannot pass any parameter to vagrant. The only way is to use environment variables\n\n```\nMY_VAR='my value' vagrant up\n\n```\n\nAnd use ENV['MY_VAR'] in recipe."}
{"instruction": "vagrant.d outside of the home folder", "output": "By default Vagrant uses ~/.vagrant.d. Fortunately, vagrant provides an environment variable called VAGRANT_HOME by which you can set vagrant home. \nJust do the following to change the vagrant home (this only works in the current session)\nexport VAGRANT_HOME=/path/to/vagrant\nTo make it permanent, add this to your ~/.bash_profile (for login shell).\nUpdate: VAGRANT_HOME has been added to the documentation - Environmental Variables\n\nVAGRANT_HOME can be set to change the directory where Vagrant stores global state. By default, this is set to ~/.vagrant.d. The Vagrant home directory is where things such as boxes are stored, so it can actually become quite large on disk."}
{"instruction": "Pass environment variables to vagrant shell provisioner", "output": "It's not ideal, but I got this to work for now:\n\n```\nconfig.vm.provision \"shell\" do |s|\n    s.inline = \"VAR1 is $1 and VAR2 is $2\"\n    s.args   = \"#{ENV['VAR1']} #{ENV['VAR2']}\"\nend\n\n```"}
{"instruction": "How do I remove a Vagrant box from global-status, after deleting that box from the filesystem?", "output": "To discard old boxes listed in vagrant global-status (eg. you deleted the folder containing the .vagrant dir from the filesystem) you just need to run:\n\n```\nvagrant global-status --prune\n\n```\n\nYou might need to remove the Virtual Machine directly from your provider (VMWare, Virtualbox, ..) control interface."}
{"instruction": "execute commands as user after Vagrant provisioning", "output": "You should be able to do this using the Vagrant Shell provisioner, e.g.\n\n```\nVagrant.configure(\"2\") do |config|\n  $script = <<-SCRIPT\n  rbenv install 2.0.0-p353\n  rbenv global 2.0.0-p353\n  gem update --system\n  yes | gem update\n  gem install rdoc\n  gem install rails pg\n  SCRIPT\n\n  config.vm.provision \"shell\", inline: $script, privileged: false\nend\n\n```\n\nThe key is to specify privileged: false so that it will use the default user and not root."}
{"instruction": "Vagrant box could not be found or could not be accessed in the remote catalog - incompatible curl version", "output": "There seems to be an issue with vagrant 1.8.7 and the embedded curl version vs the mac os binary (shipped by default on mac os Sierra and others) - remove the embedded one \n\n```\nsudo rm /opt/vagrant/embedded/bin/curl\n\n```\n\nNote: you also need to remove the embedded curl when you add a vagrant box (remotely or locally) so if you get the same error when running vagrant box add .... just remove the curl from vagrant and it will work"}
{"instruction": "How to combine Vagrant with Jenkins for the perfect Continuous Integration Environment?", "output": "it is a good solution for build system, my suggestion:\n\nYour current jenkins works as master CI (probably started by user jenkins)\nCreate another user in same machine or another machine to work as jenkins slave mode\n\n\njenkins slave can be invoked from jenkins master, and it can use different user like vagrant who had permission and environment for vagrant, therefore it will not interfere the original jenkins master server\ncreate your base vagrant box, then it can be reused to speedup for your deployment\n\nMost of the installation information (packages) could be managed by puppet (or chef) to be loaded into your vm box.\n\nProbably you can take a look at veewee, which can create vagrant box on fly.\nHere is the Make CI easier with Jenkins CI and Vagrant for my guideline for this suggestion."}
{"instruction": "Demand a Vagrant plugin within the Vagrantfile?", "output": "2019 Update: Vagrant now has functionality to require plugins through the Vagrantfile via:\n\n```\nVagrant.configure(\"2\") do |config|\n  config.vagrant.plugins = \"vagrant-some-plugin\"\n\n  # or as array:\n  config.vagrant.plugins = [\"vagrant-some-plugin\", \"vagrant-some-other-plugin\"]\n\n  # or as hash\n  config.vagrant.plugins = {\"vagrant-some-plugin\" => {\"version\" => \"1.0.0\"}}\nend\n\n```\n\nIf Vagrant detects there are plugins not already installed it will prompt the user to install them itself:\n\n```\n$ vagrant up\nVagrant has detected project local plugins configured for this\nproject which are not installed.\n\n  vagrant-some-plugin\nInstall local plugins (Y/N) [N]: y\nInstalling the 'vagrant-some-plugin' plugin. This can take a few minutes...\nFetching vagrant-some-plugin-1.0.0.gem\nInstalled the plugin 'vagrant-some-plugin (1.0.0)'!\n\n\nVagrant has completed installing local plugins for the current Vagrant\nproject directory. Please run the requested command again.\n\n```\n\nSee https://www.vagrantup.com/docs/vagrantfile/vagrant_settings.html"}
{"instruction": "How can I create a VM in Vagrant with VirtualBox with two CPUs?", "output": "Add vb.customize [\"modifyvm\", :id, \"--ioapic\", \"on\"] to the config.vm.provider block inside your Vagrantfile.\nLooking at the VirtualBox documentation it mentions:\n\n\"Note Enabling the I/O APIC is required for 64-bit guest operating\n  systems, especially Windows Vista; it is also required if you want to\n  use more than one virtual CPU in a virtual machine.\""}
{"instruction": "Vagrant, how to specify the disk size?", "output": "I have used the vagrant plugin vagrant-disksize to resize the disk. \nIt worked. It can also help to specify the initial disk size. \nRun the following at the command line:\n\n```\nvagrant plugin install vagrant-disksize\n\n```\n\nand use the following in your Vagrantfile:\n\n```\nvagrant.configure('2') do |config|\n    config.vm.box = 'ubuntu/xenial64'\n    config.disksize.size = '50GB'\nend\n\n```"}
{"instruction": "How do I delete a virtualbox machine in the GURU_MEDITATION error state?", "output": "Kill the VBoxHeadless process and run \"vagrant destroy\"\nDestroying vagrant and sending the kill signal with the \"killall\" command looks like:\nkillall -9 VBoxHeadless && vagrant destroy"}
{"instruction": "Vagrant insecure by default?", "output": "The short answer is YES.\nWhy?\nWhen building Vagrant base boxes (manually or using tools like Veewee to automate), builders follow the vagrant base boxes specifications which defines the following:\n\nUser root and vagrant use vagrant as password\nPublic key authentication (password-less) for the user vagrant.\n\nVagrant project provides an insecure key pair for SSH Public Key Authentication so that vagrant ssh works.\nBecause everyone has access to the private key, anyone can use the private key to login to your VMs (suppose they know your IP of the host machine, port is by default 2222 as forwarding rules in place.)\nIt is NOT secure OOTB. However, you can remove the trusted key from ~vagrant/.ssh/authorized_keys and add your own, change password for vagrant and root, then it's considered relatively safe.\nUpdate\nSince Vagrant 1.2.3, by default SSH forwarded port binds to 127.0.0.1 so only local connections are allowed [GH-1785].\nIMPORTANT Update\nSince Vagrant 1.7.0 (PR #4707) Vagrant will replace the default insecure ssh keypair with randomly generated keypair on first vagrant up.\nSee in the CHANGELOG: the default insecure keypair is used, Vagrant will automatically replace it with a randomly generated keypair on first vagrant up. GH-2608"}
{"instruction": "How to find the Vagrant IP?", "output": "run:\n\n```\nvagrant ssh-config > .ssh.config\n\n```\n\nand then in config/deploy.rb\n\n```\nrole :web, \"default\"     \nrole :app, \"default\"\nset :use_sudo, true\nset :user, 'root'\nset :run_method, :sudo\n\n# Must be set for the password prompt from git to work\ndefault_run_options[:pty] = true\nssh_options[:forward_agent] = true\nssh_options[:config] = '.ssh.config'\n\n```"}
{"instruction": "How to upgrade to VirtualBox Guest Additions on VM box?", "output": "You can check out the following plugin, it should suit your needs:\nhttps://github.com/dotless-de/vagrant-vbguest\nFor Vagrant ‚â• 1.1\nvagrant plugin install vagrant-vbguest\nVagrant 1.0 and older\nvagrant gem install vagrant-vbguest"}
{"instruction": "How do I exit Vim?", "output": "Hit the Esc key to enter \"Normal mode\". Then you can type : to enter \"Command-line mode\". A colon (:) will appear at the bottom of the screen and you can type in one of the following commands. To execute a command, press the Enter key.\n\n:q  to quit (short for :quit)\n:q! to quit without saving (short for :quit!)\n:wq to write and quit\n:wq! to write and quit, attempting to force the write if the file lacks write permission\n:x to write and quit; like :wq but writes only if modified (short for :exit)\n:qa to quit all (short for :quitall)\n:cq to quit, without saving, with a nonzero exit code to indicate failure (short for :cquit)\n\nYou can also quit Vim directly from \"Normal mode\" by typing ZZ to save and quit (same as :x) or ZQ to just quit (same as :q!). (Note that case is important here. ZZ and zz do not mean the same thing.)\nVim has extensive help - that you can access with the :help command - where you can find answers to all your questions and a tutorial for beginners."}
{"instruction": "How do I make git use the editor of my choice for editing commit messages?", "output": "Setting the default editor for Git\nPick one:\n\nSet core.editor in your Git config:\n\n```\ngit config --global core.editor \"vim\"\n\n```\n\n\nSet the GIT_EDITOR environment variable:\n\n```\nexport GIT_EDITOR=vim\n\n```\n\n\n\n\nSetting the default editor for all programs\nSet the standardized VISUAL and EDITOR environment variables*:\n\n```\nexport VISUAL=vim\nexport EDITOR=\"$VISUAL\"\n\n```\n\nNOTE: Setting both is not necessarily needed, but some programs may not use the more-correct VISUAL. See VISUAL vs. EDITOR.\n\nFixing compatibility issues\nSome editors require a --wait flag, or they will open a blank page. For example:\n\nSublime Text (if correctly set up; or use the full path to the executable in place of subl):\n\n```\nexport VISUAL=\"subl --wait\"\n\n```\n\n\nVS Code (after adding the shell command):\n\n```\nexport VISUAL=\"code --wait\"\n\n```"}
{"instruction": "How to replace a character by a newline in Vim", "output": "Use \\r instead of \\n.\nSubstituting by \\n inserts a null character into the text. To get a newline, use \\r. When searching for a newline, you‚Äôd still use \\n, however. This asymmetry is due to the fact that \\n and \\r do slightly different things:\n\\n matches an end of line (newline), whereas \\r matches a carriage return. On the other hand, in substitutions \\n inserts a null character whereas \\r inserts a newline (more precisely, it‚Äôs treated as the input CR). Here‚Äôs a small, non-interactive example to illustrate this, using the Vim command line feature (in other words, you can copy and paste the following into a terminal to run it). xxd shows a hexdump of the resulting file.\n\n```\necho bar > test\n(echo 'Before:'; xxd test) > output.txt\nvim test '+s/b/\\n/' '+s/a/\\r/' +wq\n(echo 'After:'; xxd test) >> output.txt\nmore output.txt\n\n```\n\n\n```\nBefore:\n0000000: 6261 720a                                bar.\nAfter:\n0000000: 000a 720a                                ..r.\n\n```\n\nIn other words, \\n has inserted the byte 0x00 into the text; \\r has inserted the byte 0x0a."}
{"instruction": "Indent multiple lines quickly in vi", "output": "Use the > command. To indent five lines, 5>>. To mark a block of lines and indent it, Vjj> to indent three lines (Vim only). To indent a curly-braces block, put your cursor on one of the curly braces and use >% or from anywhere inside block use >iB.\nIf you‚Äôre copying blocks of text around and need to align the indent of a block in its new location, use ]p instead of just p. This aligns the pasted block with the surrounding text.\nAlso, the shiftwidth setting allows you to control how many spaces to indent."}
{"instruction": "Vim clear last search highlighting", "output": "To turn off highlighting until the next search:\n\n```\n:noh\n\n```\n\nOr turn off highlighting completely:\n\n```\nset nohlsearch\n\n```\n\nOr, to toggle it:\n\n```\nset hlsearch!\n\nnnoremap <F3> :set hlsearch!<CR>\n\n```"}
{"instruction": "How to do case insensitive search in Vim", "output": "You can use the \\c escape sequence anywhere in the pattern. For example:\n/\\ccopyright or /copyright\\c or even /copyri\\cght\nTo do the inverse (case sensitive matching), use \\C (capital C) instead."}
{"instruction": "How to duplicate a whole line in Vim?", "output": "First:\nyy or Y to copy the line (mnemonic: yank)\nor\ndd to delete the line (Vim copies what you deleted into a clipboard-like \"register\", like a cut operation)\nthen:\np to paste the copied or deleted text after the current line\nor\nShift + P to paste the copied or deleted text before the current line"}
{"instruction": "How does the Vim &quot;write with sudo&quot; trick work?", "output": "In :w !sudo tee %...\n% means \"the current file\"\nAs eugene y pointed out, % does indeed mean \"the current file name\", which is passed to tee so that it knows which file to overwrite.\n(In substitution commands, it's slightly different; as :help :% shows, it's equal to 1,$ (the entire file) (thanks to @Orafu for pointing out that this does not evaluate to the filename). For example, :%s/foo/bar means \"in the current file, replace occurrences of foo with bar.\" If you highlight some text before typing :s, you'll see that the highlighted lines take the place of % as your substitution range.)\n:w isn't updating your file\nOne confusing part of this trick is that you might think :w is modifying your file, but it isn't. If you opened and modified file1.txt, then ran :w file2.txt, it would be a \"save as\"; file1.txt wouldn't be modified, but the current buffer contents would be sent to file2.txt.\nInstead of file2.txt, you can substitute a shell command to receive the buffer contents. For instance, :w !cat will just display the contents.\nIf Vim wasn't run with sudo access, its :w can't modify a protected file, but if it passes the buffer contents to the shell, a command in the shell can be run with sudo. In this case, we use tee.\nUnderstanding tee\nAs for tee, picture the tee command as a T-shaped pipe in a normal bash piping situation: it directs output to specified file(s) and also sends it to standard output, which can be captured by the next piped command. \nFor example, in ps -ax | tee processes.txt | grep 'foo', the list of processes will be written to a text file and passed along to grep.\n\n```\n     +-----------+    tee     +------------+\n     |           |  --------  |            |\n     | ps -ax    |  --------  | grep 'foo' |\n     |           |     ||     |            |\n     +-----------+     ||     +------------+\n                       ||   \n               +---------------+\n               |               |\n               | processes.txt |\n               |               |\n               +---------------+\n\n```\n\n(Diagram created with Asciiflow.)\nSee the tee man page for more info.\nTee as a hack\nIn the situation your question describes, using tee is a hack because we're ignoring half of what it does. sudo tee writes to our file and also sends the buffer contents to standard output, but we ignore standard output. We don't need to pass anything to another piped command in this case; we're just using tee as an alternate way of writing a file and so that we can call it with sudo.\nMaking this trick easy\nYou can add this to your .vimrc to make this trick easy-to-use: just type :w!!.\n\n```\n\" Allow saving of files as sudo when I forgot to start vim using sudo.\ncmap w!! w !sudo tee > /dev/null %\n\n```\n\nThe > /dev/null part explicitly throws away the standard output, since, as I said, we don't need to pass anything to another piped command."}
{"instruction": "What&#39;s a quick way to comment/uncomment lines in Vim?", "output": "I use the NERD Commenter script. It lets you easily comment, uncomment or toggle comments in your code.\nAs mentioned in the comments:\n\nfor anyone who is confused by the usage, default leader is \"\\\" so 10\\cc will comment ten lines and 10\\cu will uncomment those ten lines"}
{"instruction": "Turning off auto indent when pasting text into vim", "output": "Update: Better answer here: https://stackoverflow.com/a/38258720/62202\nTo turn off autoindent when you paste code, there's a special \"paste\" mode.\nType \n\n```\n:set paste\n\n```\n\nThen paste your code.  Note that the text in the tooltip now says -- INSERT (paste) --.\nAfter you pasted your code, turn off the paste-mode, so that auto-indenting when you type works correctly again.\n\n```\n:set nopaste\n\n```\n\nHowever, I always found that cumbersome.  That's why I map <F3> such that it can switch between paste and nopaste modes while editing the text!  I add this to .vimrc\n\n```\nset pastetoggle=<F3>\n\n```"}
{"instruction": "What is the difference between the remap, noremap, nnoremap and vnoremap mapping commands in Vim?", "output": "remap is an option that makes mappings work recursively. By default, it is on and I'd recommend you leave it that way. The rest are mapping commands, described below:\n:map and :noremap are recursive and non-recursive versions of the various mapping commands. For example, if we run:\n\n```\n:map j gg           (moves the cursor to the first line)\n:map Q j            (moves the cursor to the first line)\n:noremap W j        (moves the cursor down one line)\n\n```\n\nThen:\n\nj will be mapped to gg.\nQ will also be mapped to gg, because j will be expanded for the recursive mapping.\nW will be mapped to j (and not to gg) because j will not be expanded for the non-recursive mapping.\n\nNow remember that Vim is a modal editor. It has a normal mode, visual mode and other modes.\nFor each of these sets of mappings, there is a mapping that works in normal, visual, select and operator modes (:map and :noremap), one that works in normal mode (:nmap and :nnoremap), one in visual mode (:xmap and :xnoremap) and so on.\nFor more guidance on this, see:\n\n```\n:help :map\n:help :noremap\n:help recursive_mapping\n:help :map-modes\n\n```"}
{"instruction": "How do I move to end of line in Vim?", "output": "Just the $ (dollar sign) key.  You can use A to move to the end of the line and switch to editing mode (Append).  To jump to the last non-blank character, you can press g then _ keys.\nThe opposite of A is I (Insert mode at beginning of line), as an aside.  Pressing just the ^ will place your cursor at the first non-white-space character of the line."}
{"instruction": "Tab key == 4 spaces and auto-indent after curly braces in Vim", "output": "As has been pointed out in a couple of other answers, the preferred method now is NOT to use smartindent, but instead use the following (in your .vimrc):\n\n```\nfiletype plugin indent on\n\" show existing tab with 4 spaces width\nset tabstop=4\n\" when indenting with '>', use 4 spaces width\nset shiftwidth=4\n\" On pressing tab, insert 4 spaces\nset expandtab\n\n```\n\n\n\nIn your [.vimrc:][1] file:\n\n```\nset smartindent\nset tabstop=4\nset shiftwidth=4\nset expandtab\n\n```\n\nThe help files take a bit of time to get used to, but the more you read, the better Vim gets:\n\n```\n:help smartindent\n\n```\n\nEven better, you can embed these settings in your source for portability:\n\n```\n:help auto-setting\n\n```\n\nTo see your current settings:\n\n```\n:set all\n\n```\n\nAs graywh points out in the comments, smartindent has been replaced by cindent which \"Works more cleverly\", although still mainly for languages with C-like syntax:\n\n```\n:help C-indenting\n\n```"}
{"instruction": "Redefine tab as 4 spaces", "output": "It depends on what you mean.\n\nDo you want tab characters in your file to appear 4 character cells wide?\nOr do you want the Tab key to generate an indent consisting of 4 space characters?\n\nDepending on which behavior you need, one of the following sets of settings should work:\nIf you want tab characters in your file to appear 4 character cells wide:\n\n```\nset tabstop=4\n\n```\n\nIf your code requires use of actual tab characters these settings prevent unintentional insertion of spaces (these are the defaults, but you may want to set them defensively):\n\n```\nset softtabstop=0 noexpandtab\n\n```\n\nIf you also want to use tab characters for indentation, you should also set shiftwidth to be the same as tabstop:\n\n```\nset shiftwidth=4\n\n```\n\nTo make any of these settings permanent add them to your vimrc file.\nIf you want pressing the Tab key to indent with 4 space characters:\nFirst, tell vim to use 4-space indents, and to intelligently use the Tab key for indentation instead of for inserting tab characters (when at the beginning of a line):\n\n```\nset shiftwidth=4 smarttab\n\n```\n\nIf you'd also like vim to only use space characters, never tab characters:\n\n```\nset expandtab\n\n```\n\nFinally, I also recommend setting tab stops to be different from the indentation width, in order to reduce the chance of tab characters masquerading as proper indents:\n\n```\nset tabstop=8 softtabstop=0\n\n```\n\nTo make any of these settings permanent add them to your vimrc.\nMore Details\nIn case you need to make adjustments, or would simply like to understand what these options all mean, here's a breakdown of what each option means, along with links their the full documentation.\n\ntabstop\nThe width of a hard tabstop measured in \"spaces\" ‚Äî effectively the (maximum) width of an actual tab character.\nshiftwidth\nThe size of an \"indent\". It's measured in spaces, so if your codebase indents with tab characters then you want shiftwidth to equal tabstop. This is also used by commands like =, > and <.\nexpandtab\nEnabling this will make the Tab key (in insert mode) insert spaces instead of tab characters. This also affects the behavior of the :retab command.\nIf you have this enabled you can enter a literal tab character by typing ctrlV followed by Tab. (mnemonic \"verbatim tab\")\nsmarttab\nEnabling this will cause the Tab key (in insert mode) go to the next indent (as set by 'shiftwidth') instead of the next tab stop (or soft tabstop), but only when the cursor is at the beginning of a line (i.e. the only preceding characters are whitespace). Spaces and/or tabs will be used depending on the values of other options.\nsofttabstop\nSetting this to a non-zero value other than tabstop will make the tab key (in insert mode) insert a combination of spaces (and possibly tabs) to simulate tab stops at this width. Codebases where setting this to something other than 0 is desired are extremely rare.\n\nFor further details on any of these you can also use :help 'optionname' in vim (e.g. :help 'tabstop')."}
{"instruction": "What is the &lt;leader&gt; in a .vimrc file?", "output": "The <Leader> key is mapped to \\ by default.  So if you have a map of <Leader>t, you can execute it by default with \\+t.  For more detail or re-assigning it using the mapleader variable, see\n:help leader\n\n```\n\nTo define a mapping which uses the \"mapleader\" variable, the special string\n\"<Leader>\" can be used.  It is replaced with the string value of \"mapleader\".\nIf \"mapleader\" is not set or empty, a backslash is used instead.  \nExample:\n    :map <Leader>A  oanother line <Esc>\nWorks like:\n    :map \\A  oanother line <Esc>\nBut after:\n    :let mapleader = \",\"\nIt works like:\n    :map ,A  oanother line <Esc>\n\nNote that the value of \"mapleader\" is used at the moment the mapping is\ndefined.  Changing \"mapleader\" after that has no effect for already defined\nmappings.\n\n\n\n```"}
{"instruction": "How to effectively work with multiple files in Vim", "output": "Why not use tabs (introduced in Vim 7)?\nYou can switch between tabs with :tabn and :tabp,\nWith :tabe <filepath> you can add a new tab; and with a regular :q or :wq you close a tab.\nIf you map :tabn and :tabp to your F7/F8 keys you can easily switch between files.\nIf there are not that many files or you don't have Vim 7 you can also split your screen in multiple files: :sp <filepath>. Then you can switch between splitscreens with Ctrl+W and then an arrow key in the direction you want to move (or instead of arrow keys, w for next and W for previous splitscreen)"}
{"instruction": "How to copy to clipboard in Vim?", "output": "The * register will do this. In Windows, + and * are equivalent. In unix there is a subtle difference between + and *:\n\nUnder Windows, the * and + registers\n  are equivalent. For X11 systems,\n  though, they differ. For X11 systems,\n  * is the selection, and + is the cut buffer (like clipboard). \n  http://vim.wikia.com/wiki/Accessing_the_system_clipboard\n\n* is probably what you want most of the time, so I use * because it functions as I expect it to in both environments.\nIn Linux distros you have to install vim-gtk (aka gvim) first to gain clipboard functionality. This is because non-gtk vim is typically compiled without X11 support. This is to allow it to run on console only machines (often servers).\nAnd for those confused about how to use registers when yanking or putting, you merely write \" then the name of the register. So for copying something to the clipboard register you type \"*y and then to put you type \"*p (credit: Kyle Mathews)"}
{"instruction": "How can I make Vim paste from (and copy to) the system&#39;s clipboard?", "output": "TL;DR\nTry using \"*yy or \"+yy to copy a line to your system's clipboard.\nFull answer\nBe aware that copying/pasting from the system clipboard will not work if :echo has('clipboard') returns 0. In this case, Vim is not compiled with the +clipboard feature and you'll have to install a different version or recompile it. Some Linux distros supply a minimal Vim installation by default, but if you install the vim-gtk or vim-gtk3 package you can get the extra features nonetheless.\nThe \"* and \"+ registers are for the system's clipboard (:help registers). Depending on your system, they may do different things. For instance, on systems that don't use X11 like OS X or Windows, the \"* register is used to read from and write to the system clipboard. On X11 systems, both registers can be used. See :help x11-selection for more details, but basically the \"* is analogous to X11's _PRIMARY_ selection (which usually copies things you select with the mouse and pastes with the middle mouse button) and \"+ is analogous to X11's _CLIPBOARD_ selection (which is the clipboard proper).\nIf all that went over your head, try using \"*yy or \"+yy to copy a line to your system's clipboard. Assuming you have the appropriate compile options, one or the other should work.\nYou might like to remap this to something more convenient for you. For example, you could put vnoremap <C-c> \"*y in your ~/.vimrc so that you can visually select and press Ctrl+c to yank to your system's clipboard.\nYou also may want to have a look at the 'clipboard' option described in :help cb. In this case, you can :set clipboard=unnamed or :set clipboard=unnamedplus to make all yanking/deleting operations automatically copy to the system clipboard. This could be an inconvenience in some cases where you are storing something else in the clipboard as it will overwrite it.\nTo paste you can use \"+p or \"*p (again, depending on your system and/or desired selection) or you can map these to something else. I type them explicitly, but I often find myself in insert mode. If you're in insert mode you can still paste them with proper indentation by using <C-r><C-p>* or <C-r><C-p>+. See :help i_CTRL-R_CTRL-P.\nVim's paste option (:help paste) is also worth mentioning: This puts Vim into a special \"paste mode\" that disables several other options, allowing you to easily paste into Vim using your terminal emulator's or multiplexer's familiar paste shortcut. (Simply type :set paste to enable it, paste your content and then type :set nopaste to disable it.) Alternatively, you can use the pastetoggle option to set a keycode that toggles the mode (:help pastetoggle).\nI recommend using registers instead of these options, but if they are still too scary, this can be a convenient workaround while you're perfecting your Vim chops.\nSee :help clipboard for more detailed information."}
{"instruction": "How do I use vim registers?", "output": "Registers in Vim let you run actions or commands on text stored within them.  To access a register, you type \"a before a command, where a is the name of a register.  If you want to copy the current line into register k, you can type\n\n```\n\"kyy\n\n```\n\nOr you can append to a register by using a capital letter\n\n```\n\"Kyy\n\n```\n\nYou can then move through the document and paste it elsewhere using\n\n```\n\"kp\n\n```\n\nTo paste from system clipboard on Linux\n\n```\n\"+p\n\n```\n\nTo paste from system clipboard on Windows (or from \"mouse highlight\" clipboard on Linux)\n\n```\n\"*p\n\n```\n\nTo access all currently defined registers type\n\n```\n:reg\n\n```"}
{"instruction": "What is your most productive shortcut with Vim?", "output": "Your problem with Vim is that  you don't grok vi.\nYou mention cutting with yy and complain that you almost never want to cut whole lines.  In fact programmers, editing source code, very often want to work on whole lines, ranges of lines and blocks of code.  However, yy is only one of many way to yank text into the anonymous copy buffer (or \"register\" as it's called in vi).\nThe \"Zen\" of vi is that you're speaking a language.  The initial y is a verb.  The statement yy is a synonym for y_. The y is doubled up to make it easier to type, since it is such a common operation.\nThis can also be expressed as dd P (delete the current line and paste a copy back into place; leaving a copy in the anonymous register as a side effect).  The y and d \"verbs\" take any movement as their \"subject.\"  Thus yW is \"yank from here (the cursor) to the end of the current/next (big) word\" and y'a is \"yank from here to the line containing the mark named 'a'.\"\nIf you only understand basic up, down, left, and right cursor movements then vi will be no more productive than a copy of \"notepad\" for you.  (Okay, you'll still have syntax highlighting and the ability to handle files larger than a piddling ~45KB or so; but work with me here).\nvi has 26 \"marks\" and 26 \"registers.\"  A mark is set to any cursor location using the m command.  Each mark is designated by a single lower case letter.  Thus ma sets the 'a' mark to the current location, and mz sets the 'z' mark.  You can move to the line containing a mark using the ' (single quote) command.  Thus 'a moves to the beginning of the line containing the 'a' mark.  You can move to the precise location of any mark using the ` (backquote) command.  Thus  `z will move directly to the exact location of the 'z' mark.\nBecause these are \"movements\" they can also be used as subjects for other \"statements.\"\nSo, one way to cut an arbitrary selection of text would be to drop a mark (I usually use 'a' as my \"first\" mark, 'z' as my next mark, 'b' as another, and 'e' as yet another (I don't recall ever having interactively used more than four marks in 15 years of using vi; one creates one's own conventions regarding how marks and registers are used by macros that don't disturb one's interactive context).  Then we go to the other end of our desired text; we can start at either end, it doesn't matter.  Then we can simply use d`a to cut or y`a to copy.  Thus the whole process has a 5 keystrokes overhead (six if we started in \"insert\" mode and needed to Esc out command mode).  Once we've cut or copied then pasting in a copy is a single keystroke: p.\nI say that this is one way to cut or copy text.  However, it is only one of many.  Frequently we can more succinctly describe the range of text without moving our cursor around and dropping a mark.  For example if I'm in a paragraph of text I can use { and } movements to the beginning or end of the paragraph respectively.  So, to move a paragraph of text I cut it using { d} (3 keystrokes).  (If I happen to already be on the first or last line of the paragraph I can then simply use d} or d{ respectively.\nThe notion of \"paragraph\" defaults to something which is usually intuitively reasonable.  Thus it often works for code as well as prose.\nFrequently we know some pattern (regular expression) that marks one end or the other of the text in which we're interested.  Searching forwards or backwards are movements in vi.  Thus they can also be used as \"subjects\" in our \"statements.\"  So I can use d/foo to cut from the current line to the next line containing the string \"foo\" and y?bar to copy from the current line to the most recent (previous) line containing \"bar.\"  If I don't want whole lines I can still use the search movements (as statements of their own), drop my mark(s) and use the  `x commands as described previously.\nIn addition to \"verbs\" and \"subjects\" vi also has \"objects\" (in the grammatical sense of the term).  So far I've only described the use of the anonymous register.  However, I can use any of the 26 \"named\" registers by prefixing the \"object\" reference with \" (the double quote modifier).  Thus if I use \"add I'm cutting the current line into the 'a' register and if I use \"by/foo then I'm yanking a copy of the text from here to the next line containing \"foo\" into the 'b' register.  To paste from a register I simply prefix the paste with the same modifier sequence: \"ap pastes a copy of the 'a' register's contents into the text after the cursor and \"bP pastes a copy from 'b' to before the current line.\nThis notion of \"prefixes\" also adds the analogs of grammatical \"adjectives\" and \"adverbs'  to our text manipulation \"language.\"  Most commands (verbs) and movement (verbs or objects, depending on context) can also take numeric prefixes. Thus 3J means \"join the next three lines\" and d5} means \"delete from the current line through the end of the fifth paragraph down from here.\"\nThis is all intermediate level vi.  None of it is Vim specific and there are far more advanced tricks in vi if you're ready to learn them.  If you were to master just these intermediate concepts then you'd probably find that you rarely need to write any macros because the text manipulation language is sufficiently concise and expressive to do most things easily enough using the editor's \"native\" language.\n\nA sampling of more advanced tricks:\nThere are a number of : commands, most notably the :% s/foo/bar/g global substitution technique.  (That's not advanced but other : commands can be).  The whole : set of commands was historically inherited by vi's previous incarnations as the ed (line editor) and later the ex (extended line editor) utilities.  In fact vi is so named because it's the visual interface to ex.\n: commands normally operate over lines of text.  ed and ex were written in an era when terminal screens were uncommon and many terminals were \"teletype\" (TTY) devices.  So it was common to work from printed copies of the text, using commands through an extremely terse interface (common connection speeds were 110 baud, or, roughly, 11 characters per second -- which is slower than a fast typist; lags were common on multi-user interactive sessions; additionally there was often some motivation to conserve paper).\nSo the syntax of most : commands includes an address or range of addresses (line number) followed by a command.  Naturally one could use literal line numbers: :127,215 s/foo/bar to change the first occurrence of \"foo\" into \"bar\" on each line between 127 and 215.  One could also use some abbreviations such as . or $ for current and last lines respectively.  One could also use relative prefixes + and - to refer to offsets after or before the curent line, respectively.  Thus: :.,$j meaning \"from the current line to the last line, join them all into one line\".  :% is synonymous with :1,$ (all the lines).\nThe :... g and :... v commands bear some explanation as they are incredibly powerful.  :... g is a prefix for \"globally\" applying a subsequent command to all lines which match a pattern (regular expression) while :... v applies such a command to all lines which do NOT match the given pattern (\"v\" from \"conVerse\").  As with other ex commands these can be prefixed by addressing/range references.  Thus :.,+21g/foo/d means \"delete any lines containing the string \"foo\" from the current one through the next 21 lines\" while :.,$v/bar/d means \"from here to the end of the file, delete any lines which DON'T contain the string \"bar.\"\nIt's interesting that the common Unix command grep was actually inspired by this ex command (and is named after the way in which it was documented).  The ex command :g/re/p (grep) was the way they documented how to \"globally\" \"print\" lines containing a \"regular expression\" (re).  When ed and ex were used, the :p command was one of the first that anyone learned and often the first one used when editing any file.  It was how you printed the current contents (usually just one page full at a time using :.,+25p or some such).\nNote that :% g/.../d or (its reVerse/conVerse counterpart: :% v/.../d are the most common usage patterns.  However there are  couple of other ex commands which are worth remembering:\nWe can use m to move lines around, and j to join lines.  For example if you have a list and you want to separate all the stuff matching (or conversely NOT matching some pattern) without deleting them, then you can use something like: :% g/foo/m$ ... and all the \"foo\" lines will have been moved to the end of the file.  (Note the other tip about using the end of your file as a scratch space).  This will have preserved the relative order of all the \"foo\" lines while having extracted them from the rest of the list.  (This would be equivalent to doing something like: 1G!GGmap!Ggrep foo<ENTER>1G:1,'a g/foo'/d (copy the file to its own tail, filter the tail through grep, and delete all the stuff from the head).\nTo join lines usually I can find a pattern for all the lines which need to be joined to their predecessor (all the lines which start with \"^   \" rather than \"^ * \" in some bullet list, for example).  For that case I'd use: :% g/^   /-1j (for every matching line, go up one line and join them). (BTW: for bullet lists trying to search for the bullet lines and join to the next doesn't work for a couple reasons ... it can join one bullet line to another, and it won't join any bullet line to all of its continuations; it'll only work pairwise on the matches).\nAlmost needless to mention you can use our old friend s (substitute) with the g and v (global/converse-global) commands.  Usually you don't need to do so.  However, consider some case where you want to perform a substitution only on lines matching some other pattern.  Often you can use a complicated pattern with captures and use back references to preserve the portions of the lines that you DON'T want to change.  However, it will often be easier to separate the match from the substitution: :% g/foo/s/bar/zzz/g -- for every line containing \"foo\" substitute all \"bar\" with \"zzz.\"  (Something like :% s/\\(.*foo.*\\)bar\\(.*\\)/\\1zzz\\2/g would only work for the cases those instances of \"bar\" which were PRECEDED by \"foo\" on the same line; it's ungainly enough already, and would have to be mangled further to catch all the cases where \"bar\" preceded \"foo\")\nThe point is that there are more than just p, s, and d lines in the ex command set.\nThe : addresses can also refer to marks.  Thus you can use: :'a,'bg/foo/j to join any line containing the string foo to its subsequent line, if it lies between the lines between the 'a' and 'b' marks.  (Yes, all of the preceding ex command examples can be limited to subsets of the file's lines by prefixing with these sorts of addressing expressions).\nThat's pretty obscure (I've only used something like that a few times in the last 15 years). However, I'll freely admit that I've often done things iteratively and interactively that could probably have been done more efficiently if I'd taken the time to think out the correct incantation.\nAnother very useful vi or ex command is :r to read in the contents of another file.  Thus: :r foo inserts the contents of the file named \"foo\" at the current line.\nMore powerful is the :r! command.  This reads the results of a command.  It's the same as suspending the vi session, running a command, redirecting its output to a temporary file, resuming your vi session, and reading in the contents from the temp. file.\nEven more powerful are the ! (bang) and :... ! (ex bang) commands.  These also execute external commands and read the results into the current text. However, they also filter selections of our text through the command!  This we can sort all the lines in our file using 1G!Gsort (G is the vi \"goto\" command; it defaults to going to the last line of the file, but can be prefixed by a line number, such as 1, the first line).  This is equivalent to the ex variant :1,$!sort.  Writers often use ! with the Unix fmt or fold utilities for reformating or \"word wrapping\" selections of text.  A very common macro is {!}fmt (reformat the current paragraph).  Programmers sometimes use it to run their code, or just portions of it, through indent or other code reformatting tools.\nUsing the :r! and ! commands means that any external utility or filter can be treated as an extension of our editor.  I have occasionally used these with scripts that pulled data from a database, or with wget or lynx commands that pulled data off a website, or ssh commands that pulled data from remote systems.\nAnother useful ex command is :so (short for :source).  This reads the contents of a file as a series of commands.  When you start vi it normally, implicitly, performs a :source on ~/.exinitrc file (and Vim usually does this on ~/.vimrc, naturally enough).  The use of this is that you can change your editor profile on the fly by simply sourcing in a new set of macros, abbreviations, and editor settings.  If you're sneaky you can even use this as a trick for storing sequences of ex editing commands to apply to files on demand.\nFor example I have a seven line file (36 characters) which runs a file through wc, and inserts a C-style comment at the top of the file containing that word count data.  I can apply that \"macro\" to a file by using a command like: vim +'so mymacro.ex' ./mytarget\n(The + command line option to vi and Vim is normally used to start the editing session at a given line number.  However it's a little known fact that one can follow the + by any valid ex command/expression, such as a \"source\" command as I've done here; for a simple example I have scripts which invoke: vi +'/foo/d|wq!' ~/.ssh/known_hosts to remove an entry from my SSH known hosts file non-interactively while I'm re-imaging a set of servers).\nUsually it's far easier to write such \"macros\" using Perl, AWK, sed (which is, in fact, like grep a utility inspired by the ed command).\nThe @ command is probably the most obscure vi command.  In occasionally teaching advanced systems administration courses for close to a decade I've met very few people who've ever used it.  @ executes the contents of a register as if it were a vi or ex command.\nExample: I often use: :r!locate ... to find some file on my system and read its name into my document.  From there I delete any extraneous hits, leaving only the full path to the file I'm interested in.  Rather than laboriously Tab-ing through each component of the path (or worse, if I happen to be stuck on a machine without Tab completion support in its copy of vi) I just use:\n\n0i:r (to turn the current line into a valid :r command),\n\"cdd (to delete the line into the \"c\" register) and\n@c execute that command.\n\nThat's only 10 keystrokes (and the expression \"cdd @c is effectively a finger macro for me, so I can type it almost as quickly as any common six letter word).\n\nA sobering thought\nI've only scratched to surface of vi's power and none of what I've described here is even part of the \"improvements\" for which vim is named!  All of what I've described here should work on any old copy of vi from 20 or 30 years ago.\nThere are people who have used considerably more of vi's power than I ever will."}
{"instruction": "What is Vim recording and how can it be disabled?", "output": "You start recording by q<letter>\nand you can end it by typing q again.\nRecording is a really useful feature of Vim.\nIt records everything you type. You can then replay it simply by typing @<letter>.\nRecord search, movement, replacement...\nOne of the best feature of Vim IMHO."}
{"instruction": "How to move screen without moving cursor in Vim?", "output": "zz - move current line to the middle\nof the screen (warning: ZZ is save and exit, so watch out for caps lock)\nzt - move current line\nto the top of the screen\nzb - move\ncurrent line to the bottom of the\nscreen"}
{"instruction": "Convert DOS/Windows line endings to Linux line endings in Vim", "output": "dos2unix is a commandline utility that will do this.\nIn Vim, :%s/^M//g will if you use Ctrl-v Ctrl-m to input the ^M (On Windows, use Ctrl-q Ctrl-m instead).\nVim may not show the ^M characters if the file is loaded with a dos (Windows) file format. In this case you can :set ff=unix and then save the file with :w or :w! and Vim will reformat the file as it saves it for you.\nThere is documentation on the fileformat setting, and the Vim wiki has a comprehensive page on line ending conversions.\nAlternately, if you move files back and forth a lot, you might not want to convert them, but rather to do :set ff=dos, so Vim will know it's a DOS file and use DOS conventions for line endings."}
{"instruction": "Copy all the lines to clipboard", "output": "You should yank the text to the * or + registers:\ngg\"*yG\nExplanation: \n\ngg to get the cursor to the first character of the file\n\"*y to start a yank command to the register * from the first line, until...\nG to go the end of the file"}
{"instruction": "Vim delete blank lines", "output": "```\n:g/^$/d\n\n```\n\n:g will execute a command on lines which match a regex. The regex is 'blank line' and the command is :d (delete)"}
{"instruction": "Differences between Emacs and Vim", "output": "(the text below is my opinion, it should not be taken as fact or an insult)\nWith Emacs you are expected to have it open 24/7 and live inside the program, almost everything you do can be done from there. You write your own extensions, use it for note-taking, organization, games, programming, shell access, file access, listening to music, web browsing. It takes weeks and weeks till you will be happy with it and then you will learn new stuff all the time. You will be annoyed when you don't have access to it and constantly change your config. You won't be able to use other peoples emacs versions easily and it won't just be installed. It uses Lisp, which is great. You can make it into anything you want it to be. (anything, at all)\nWith Vim, it's almost always pre-installed. It's fast. You open up a file do a quick edit and then quit. You can work with the basic setup if you are on someone else's machine. It's not quite so editable, but it's still far better than most text editors. It recognizes that most of the time you are reading/editing not typing and makes that portion faster. You don't suffer from emacs pinkie. It's not so infuriating. It's easier to learn. \nEven though I use Emacs all day every day (and love it) unless you intend to spend a lot of time in the program you choose I would pick vim"}
{"instruction": "How do I do redo (i.e. &quot;undo undo&quot;) in Vim?", "output": "Ctrl+r"}
{"instruction": "How do I fix the indentation of an entire file in Vi?", "output": "=, the indent command can take motions. So, gg to get the start of the file, = to indent, G to the end of the file, gg=G."}
{"instruction": "How to paste yanked text into the Vim command line", "output": "Yes. Hit Ctrl-R then \". If you have literal control characters in what you have yanked, use Ctrl-R, Ctrl-O, \".\nHere is an explanation of what you can do with registers. What you can do with registers is extraordinary, and once you know how to use them you cannot live without them.\nRegisters are basically storage locations for strings. Vim has many registers that work in different ways:\n\n0 (yank register: when you use y in normal mode, without specifying a register, yanked text goes there and also to the default register),\n1 to 9 (shifting delete registers, when you use commands such as c or d, what has been deleted goes to register 1, what was in register 1 goes to register 2, etc.),\n\" (default register, also known as unnamed register. This is where the \" comes in Ctrl-R, \"),\na to z for your own use (capitalized A to Z are for appending to corresponding registers).\n_ (acts like /dev/null (Unix) or NUL (Windows), you can write to it but it's discarded and when you read from it, it is always empty),\n- (small delete register),\n/ (search pattern register, updated when you look for text with /, ?, * or # for instance; you can also write to it to dynamically change the search pattern),\n: (stores last VimL typed command via Q or :, readonly),\n+ and * (system clipboard registers, you can write to them to set the clipboard and read the clipboard contents from them)\n\nSee :help registers for the full reference.\nYou can, at any moment, use :registers to display the contents of all registers. Synonyms and shorthands for this command are :display, :reg and :di.\nIn Insert or Command-line mode, Ctrl-R plus a register name, inserts the contents of this register. If you want to insert them literally (no auto-indenting, no conversion of control characters like 0x08 to backspace, etc), you can use Ctrl-R, Ctrl-O, register name.\nSee :help i_CTRL-R and following paragraphs for more reference.\nBut you can also do the following (and I probably forgot many uses for registers).\n\nIn normal mode, hit \":p. The last command you used in vim is pasted into your buffer.\nLet's decompose: \" is a Normal mode command that lets you select what register is to be used during the next yank, delete or paste operation. So \": selects the colon register (storing last command). Then p is a command you already know, it pastes the contents of the register.\ncf. :help \", :help quote_:\n\nYou're editing a VimL file (for instance your .vimrc) and would like to execute a couple of consecutive lines right now: yj:@\"Enter.\nHere, yj yanks current and next line (this is because j is a linewise motion but this is out of scope of this answer) into the default register (also known as the unnamed register). Then the :@ Ex command plays Ex commands stored in the register given as argument, and \" is how you refer to the unnamed register. Also see the top of this answer, which is related.\nDo not confuse \" used here (which is a register name) with the \" from the previous example, which was a Normal-mode command.\ncf. :help :@ and :help quote_quote\n\nInsert the last search pattern into your file in Insert mode, or into the command line, with Ctrl-R, /.\ncf. :help quote_/, help i_CTRL-R\nCorollary: Keep your search pattern but add an alternative: / Ctrl-R, / \\|alternative.\n\nYou've selected two words in the middle of a line in visual mode, yanked them with y, they are in the unnamed register. Now you want to open a new line just below where you are, with those two words: :pu. This is shorthand for :put \".¬†The :put command, like many Ex commands, works only linewise.\ncf. :help :put\nYou could also have done: :call setreg('\"', @\", 'V') then p. The setreg function sets the register of which the name is given as first argument (as a string), initializes it with the contents of the second argument (and you can use registers as variables with the name @x where x is the register name in VimL), and turns it into the mode specified in the third argument, V for linewise, nothing for characterwise and literal ^V for blockwise.\ncf. :help setreg(). The reverse functions are getreg() and getregtype().\n\nIf you have recorded a macro with qa...q, then :echo @a will tell you what you have typed, and @a will replay the macro (probably you knew that one, very useful in order to avoid repetitive tasks)\ncf. :help q, help @\nCorollary from the previous example: If you have 8go in the clipboard, then @+ will play the clipboard contents as a macro, and thus go to the 8th byte of your file. Actually this will work with almost every register. If your last inserted string was dd in Insert mode, then @. will (because the . register contains the last inserted string) delete a line. (Vim documentation is wrong in this regard, since it states that the registers #, %, : and . will only work with p, P, :put and Ctrl-R).\ncf. :help @\nDon't confuse :@ (command that plays Vim commands from a register) and @ (normal-mode command that plays normal-mode commands from a register).\nNotable exception is @:. The command register does not contain the initial colon neither does it contain the final carriage return. However in Normal mode, @: will do what you expect, interpreting the register as an Ex command, not trying to play it in Normal mode. So if your last command was :e, the register contains e but @: will reload the file, not go to end of word.\ncf. :help @:\n\nShow what you will be doing in Normal mode before running it: @='dd' Enter. As soon as you hit the = key, Vim switches to expression evaluation: as you enter an expression and hit Enter, Vim computes it, and the result acts as a register content. Of course the register = is read-only, and one-shot. Each time you start using it, you will have to enter a new expression.\ncf. :help quote_=\nCorollary: If you are editing a command, and you realize that you should need to insert into your command line some line from your current buffer: don't press Esc! Use Ctrl-R =getline(58) Enter. After that you will be back to command line editing, but it has inserted the contents of the 58th line.\n\nDefine a search pattern manually: :let @/ = 'foo'\ncf. :help :let\nNote that doing that, you needn't to escape / in the pattern. However you need to double all single quotes of course.\n\nCopy all lines beginning with foo, and afterwards all lines containing bar to clipboard, chain these commands: qaq (resets the a register storing an empty macro inside it), :g/^foo/y A, :g/bar/y A, :let @+ = @a.\nUsing a capital register name makes the register work in append mode\nBetter, if Q has not been remapped by mswin.vim, start Ex mode with Q, chain those ‚Äúcolon commands‚Äù which are actually better called ‚ÄúEx commands‚Äù, and go back to Normal mode by typing visual.\ncf. :help :g, :help :y, :help Q\n\nDouble-space your file: :g/^/put _.¬†This puts the contents of the black hole register (empty when reading, but writable, behaving like /dev/null) linewise, after each line (because every line has a beginning!).\n\nAdd a line containing foo before each line: :g/^/-put ='foo'. This is a clever use of the expression register. Here, - is a synonym for .-1 (cf. :help :range). Since :put puts the text after the line, you have to explicitly tell it to act on the previous one.\n\nCopy the entire buffer to the system clipboard: :%y+.\ncf. :help :range (for the % part) and :help :y.\n\nIf you have misrecorded a macro, you can type :let @a=' Ctrl-R =replace(@a,\"'\",\"''\",'g') Enter ' and edit it. This will modify the contents of the macro stored in register a, and it's shown here how you can use the expression register to do that. Another, simpler, way of modifying a macro is to paste it in a buffer (\"ap), edit it, and put it again into the register, by selecting it and \"ay.\n\nIf you did dddd, you might do uu in order to undo. With p you could get the last deleted line. But actually you can also recover up to 9 deletes with the registers @1 through @9.\nEven better, if you do \"1P, then . in Normal mode will play \"2P, and so on.\ncf. :help . and :help quote_number\n\nIf you want to insert the current date in Insert mode: Ctrl-R=strftime('%y%m%d')Enter.\ncf. :help strftime()\n\n\nOnce again, what can be confusing:\n\n:@ is a command-line command that interprets the contents of a register as vimscript and sources it\n\n@  in normal mode command that interprets the contents of a register as normal-mode keystrokes (except when you use : register, that contains last played command without the initial colon: in this case it replays the command as if you also re-typed the colon and the final return key).\n\n\"  in normal mode command that helps you select a register for yank, paste, delete, correct, etc.\n\n\"  is also a valid register name (the default, or unnamed, register) and therefore can be passed as an arguments for commands that expect register names"}
{"instruction": "To switch from vertical split to horizontal split fast in Vim", "output": "Vim mailing list says (re-formatted for better readability):\n\nTo change two vertically split\n  windows to horizonally split\nCtrl-w t Ctrl-w K\nHorizontally to vertically:\nCtrl-w t Ctrl-w H\nExplanations:\nCtrl-w t     makes the first (topleft) window current\nCtrl-w K     moves the current window to full-width at the very top\nCtrl-w H     moves the current window to full-height at far left\n\nNote that the t is lowercase, and the K and H are uppercase.\nAlso, with only two windows, it seems like you can drop the Ctrl-w t part because if you're already in one of only two windows, what's the point of making it current?"}
{"instruction": "&quot;Find next&quot; in Vim", "output": "It is n for next and N for previous.\nAnd if you use reverse search with ? (for example, ?cake) instead of /, it is the other way round.\nIf it is installed on your system, you should try to run vimtutor command from your terminal, which will start a tutorial of the basic Vim commands.\nRob Wells advice about * and # is also very pertinent."}
{"instruction": "What does the ^M character mean in Vim?", "output": "Unix uses 0xA for a newline character. Windows uses a combination of two characters: 0xD 0xA. 0xD is the carriage return character. ^M happens to be the way vim displays 0xD (0x0D = 13, M is the 13th letter in the English alphabet).\nYou can remove all the ^M characters by running the following: \n\n```\n:%s/^M//g\n\n```\n\nWhere ^M is entered by holding down Ctrl and typing v followed by m, and then releasing Ctrl. This is sometimes abbreviated as ^V^M, but note that you must enter it as described in the previous sentence, rather than typing it out literally.\nThis expression will replace all occurrences of ^M with the empty string (i.e. nothing). I use this to get rid of ^M in files copied from Windows to Unix (Solaris, Linux, OSX)."}
{"instruction": "Changing case in Vim", "output": "Visually select the text, then U for uppercase or u for lowercase. To swap all casing in a visual selection, press ~ (tilde).\nWithout using a visual selection, gU<motion> will make the characters in motion uppercase, or use gu<motion> for lowercase.\nFor more of these, see section 3 in Vim's change.txt help file."}
{"instruction": "Find and replace strings in vim on multiple lines", "output": "The :&& command repeats the last substitution with the same flags. You can supply the additional range(s) to it (and concatenate as many as you like):\n\n```\n:6,10s/<search_string>/<replace_string>/g | 14,18&&\n\n```\n\nIf you have many ranges though, I'd rather use a loop:\n\n```\n:for range in split('6,10 14,18')| exe range 's/<search_string>/<replace_string>/g' | endfor\n\n```"}
{"instruction": "Where is my .vimrc file?", "output": "You need to create it. In most installations I've used it hasn't been created by default.\nYou usually create it as ~/.vimrc."}
{"instruction": "How do I close a single buffer (out of many) in Vim?", "output": "More from manuals:\n:bd\n\nUnload buffer [N] (default: current\nbuffer) and delete it from\nthe buffer list.  If the buffer was changed, this fails,\nunless when [!] is specified, in which case changes are\nlost.\nThe file remains unaffected.\n\nIf you know what you‚Äôre doing, you can also use¬†:bw\n:bw\n\nLike |:bdelete|, but really delete the\nbuffer.\n\nA word of caution: ‚Äúthe w¬†in¬†bw does not stand for write but for wipeout!‚Äù"}
{"instruction": "How can I make my match non greedy in vim?", "output": "Instead of .* use .\\{-}.\n%s/style=\".\\{-}\"//g\nAlso, see :help non-greedy"}
{"instruction": "In Vim is there a way to delete without putting text in the register?", "output": "To delete something without saving it in a register, you can use the \"black hole register\":\n\n```\n\"_d\n\n```\n\nOf course you could also use any of the other registers that don't hold anything you are interested in."}
{"instruction": "How to get the list of all installed color schemes in Vim?", "output": "Type\n:colorscheme then Space followed by TAB.\nor as Peter said,\n:colorscheme then Space followed by CTRLd\nThe short version of the command is :colo so you can use it in the two previous commands, instead of using the \"long form\"."}
{"instruction": "Move cursor to end of file in vim", "output": "No need to explicitly go to the end of line before doing a, use A;\nAppend text at the end of line [count] times\n<ESC>GA"}
{"instruction": "Convert ^M (Windows) line breaks to normal line breaks", "output": "This is the only thing that worked for me:\n\n:e ++ff=dos\n\nFound it at: http://vim.wikia.com/wiki/File_format"}
{"instruction": "How to insert text at beginning of a multi-line selection in vi/Vim", "output": "Press Esc to enter 'command mode'\nUse Ctrl+V to enter visual block mode (try Ctrl+Q, if V doesn't work)\nMove Up/Down to select the columns of text in the lines you want to\ncomment.\nThen hit Shift+i and type the text you want to insert (or type x to delete)\nThen hit Esc, wait 1 second and the inserted text will appear on every line.\n\nFor further information and reading, check out \"Inserting text in multiple lines\" in the Vim Tips Wiki."}
{"instruction": "Vim: insert the same characters across multiple lines", "output": "Move the cursor to the n in name.\nEnter visual block mode (Ctrlv).\nPress j three times (or 3j) to jump down by 3 lines; G (capital g) to jump to the last line\nPress I (capital i).\nType in vendor_. Note: It will only update the screen in the first line - until Esc is pressed (6.), at which point all lines will be updated.\nPress Esc.\n\n\nAn uppercase I must be used rather than a lowercase i, because the lowercase i is interpreted as the start of a text object, which is rather useful on its own, e.g. for selecting inside a tag block (it):"}
{"instruction": "Renaming the current file in Vim", "output": "There's a little plugin that lets you do this."}
{"instruction": "How to comment in Vim&#39;s config files: &quot;.vimrc&quot;?", "output": "Put a double quote to the left of the text you want to comment:\n\n```\n\"   this is how a comment looks like in ~/.vimrc\n\n```\n\n(You don't need a closing quote.)\nThis comment syntax is documented at :help comment, part of docs on Ex command-lines, which begins:\n\n```\n'\"' at the start of a line causes the whole line to be ignored.  '\"'\nafter a command causes the rest of the line to be ignored.  This can be used\nto add comments. ...\n\n```\n\n(Note, per :help vimrc, that \"Each line in a vimrc file is executed as an Ex command line.\")"}
{"instruction": "Vim and Ctags tips and tricks", "output": "Ctrl+] - go to definition\nCtrl+T - Jump back from the definition.\nCtrl+W Ctrl+] - Open the definition in a horizontal split  \nAdd these lines in vimrc\nmap <C-\\> :tab split<CR>:exec(\"tag \".expand(\"<cword>\"))<CR>\nmap <A-]> :vsp <CR>:exec(\"tag \".expand(\"<cword>\"))<CR> \nCtrl+\\ - Open the definition in a new tab\nAlt+] - Open the definition in a vertical split\nAfter the tags are generated. You can use the following keys to tag into and tag out of functions:\nCtrl+Left MouseClick - Go to definition\nCtrl+Right MouseClick - Jump back from definition"}
{"instruction": "Move entire line up and down in Vim", "output": "Put the following to your .vimrc to do the job\n\n```\nnoremap <c-s-up> :call feedkeys( line('.')==1 ? '' : 'ddkP' )<CR>\nnoremap <c-s-down> ddp\n\n```\n\nDisappearing of the line looks like a Vim bug. I put a hack to avoid it. Probably there is some more accurate solution.\nUpdate\nThere are a lot of unexplained difficulties with just using Vim combinations. These are line missing and extra line jumping.\nSo here is the scripting solution which can be placed either inside .vimrc or ~/.vim/plugin/swap_lines.vim\n\n```\nfunction! s:swap_lines(n1, n2)\n    let line1 = getline(a:n1)\n    let line2 = getline(a:n2)\n    call setline(a:n1, line2)\n    call setline(a:n2, line1)\nendfunction\n\nfunction! s:swap_up()\n    let n = line('.')\n    if n == 1\n        return\n    endif\n\n    call s:swap_lines(n, n - 1)\n    exec n - 1\nendfunction\n\nfunction! s:swap_down()\n    let n = line('.')\n    if n == line('$')\n        return\n    endif\n\n    call s:swap_lines(n, n + 1)\n    exec n + 1\nendfunction\n\nnoremap <silent> <c-s-up> :call <SID>swap_up()<CR>\nnoremap <silent> <c-s-down> :call <SID>swap_down()<CR>\n\n```"}
{"instruction": "Replace tabs with spaces in vim", "output": "IIRC, something like:\n\n```\nset tabstop=2 shiftwidth=2 expandtab\n\n```\n\nshould do the trick. If you already have tabs, then follow it up with a nice global RE to replace them with double spaces.\nIf you already have tabs you want to replace,\n\n```\n:retab\n\n```"}
{"instruction": "What are the benefits of learning Vim?", "output": "I've been using vi and vim also for some 20 years, and I'm still learning new things.\nDavid Rayner's Best of Vim Tips site is an excellent list, though it's probably more useful once you have some familiarity with vim.\nI also want to mention the ViEmu site which has some great info on vi/vim tips and especially the article Why, oh WHY, do those nutheads use vi? (archived version)"}
{"instruction": "Paste in insert mode?", "output": "While in insert mode hit CTRL-R {register} \nExamples:    \n\nCTRL-R * will insert in the contents of the clipboard    \nCTRL-R \" (the unnamed register) inserts the last delete or yank.  \n\nTo find this in vim's help type :h i_ctrl-r"}
{"instruction": "How do I make git use the editor of my choice for editing commit messages?", "output": "Setting the default editor for Git\nPick one:\n\nSet core.editor in your Git config:\n\n```\ngit config --global core.editor \"vim\"\n\n```\n\n\nSet the GIT_EDITOR environment variable:\n\n```\nexport GIT_EDITOR=vim\n\n```\n\n\n\n\nSetting the default editor for all programs\nSet the standardized VISUAL and EDITOR environment variables*:\n\n```\nexport VISUAL=vim\nexport EDITOR=\"$VISUAL\"\n\n```\n\nNOTE: Setting both is not necessarily needed, but some programs may not use the more-correct VISUAL. See VISUAL vs. EDITOR.\n\nFixing compatibility issues\nSome editors require a --wait flag, or they will open a blank page. For example:\n\nSublime Text (if correctly set up; or use the full path to the executable in place of subl):\n\n```\nexport VISUAL=\"subl --wait\"\n\n```\n\n\nVS Code (after adding the shell command):\n\n```\nexport VISUAL=\"code --wait\"\n\n```"}
{"instruction": "Differences between Emacs and Vim", "output": "(the text below is my opinion, it should not be taken as fact or an insult)\nWith Emacs you are expected to have it open 24/7 and live inside the program, almost everything you do can be done from there. You write your own extensions, use it for note-taking, organization, games, programming, shell access, file access, listening to music, web browsing. It takes weeks and weeks till you will be happy with it and then you will learn new stuff all the time. You will be annoyed when you don't have access to it and constantly change your config. You won't be able to use other peoples emacs versions easily and it won't just be installed. It uses Lisp, which is great. You can make it into anything you want it to be. (anything, at all)\nWith Vim, it's almost always pre-installed. It's fast. You open up a file do a quick edit and then quit. You can work with the basic setup if you are on someone else's machine. It's not quite so editable, but it's still far better than most text editors. It recognizes that most of the time you are reading/editing not typing and makes that portion faster. You don't suffer from emacs pinkie. It's not so infuriating. It's easier to learn. \nEven though I use Emacs all day every day (and love it) unless you intend to spend a lot of time in the program you choose I would pick vim"}
{"instruction": "How can I reload .emacs after changing it?", "output": "You can use the command load-file (M-x load-file, and then press Return twice to accept the default filename, which is the current file being edited).\nYou can also just move the point to the end of any sexp and press C-x, C-e to execute just that sexp. Usually it's not necessary to reload the whole file if you're just changing a line or two."}
{"instruction": "How to set the font size in Emacs?", "output": "```\n(set-face-attribute 'default nil :height 100)\n\n```\n\nThe value is in 1/10pt, so 100 will give you 10pt, etc."}
{"instruction": "Why should I use an IDE?", "output": "It really depends on what language you're using, but in C# and Java I find IDEs beneficial for:\n\nQuickly navigating to a type without needing to worry about namespace, project etc\nNavigating to members by treating them as hyperlinks\nAutocompletion when you can't remember the names of all members by heart\nAutomatic code generation\nRefactoring (massive one)\nOrganise imports (automatically adding appropriate imports in Java, using directives in C#)\nWarning-as-you-type (i.e. some errors don't even require a compile cycle)\nHovering over something to see the docs\nKeeping a view of files, errors/warnings/console/unit tests etc and source code all on the screen at the same time in a useful way\nEase of running unit tests from the same window\nIntegrated debugging\nIntegrated source control\nNavigating to where a compile-time error or run-time exception occurred directly from the error details.\nEtc!\n\nAll of these save time. They're things I could do manually, but with more pain: I'd rather be coding."}
{"instruction": "How do I control how Emacs makes backup files?", "output": "If you've ever been saved by an Emacs backup file, you\nprobably want more of them, not less of them.  It is annoying\nthat they go in the same directory as the file you're editing,\nbut that is easy to change.  You can make all backup files go\ninto a directory by putting something like the following in your\n.emacs.\n\n```\n(setq backup-directory-alist `((\".\" . \"~/.saves\")))\n\n```\n\nThere are a number of arcane details associated with how Emacs\nmight create your backup files.  Should it rename the original\nand write out the edited buffer?  What if the original is linked?\nIn general, the safest but slowest bet is to always make backups\nby copying.\n\n```\n(setq backup-by-copying t)\n\n```\n\nIf that's too slow for some reason you might also have a look at\nbackup-by-copying-when-linked.\nSince your backups are all in their own place now, you might want\nmore of them, rather than less of them.  Have a look at the Emacs\ndocumentation for these variables (with C-h v).\n\n```\n(setq delete-old-versions t\n  kept-new-versions 6\n  kept-old-versions 2\n  version-control t)\n\n```\n\nFinally, if you absolutely must have no backup files:\n\n```\n(setq make-backup-files nil)\n\n```\n\nIt makes me sick to think of it though."}
{"instruction": "How can I replace a character with a newline in Emacs?", "output": "M-x replace-string RET ; RET C-q C-j.\n\nC-q for quoted-insert,\n\nC-j is a newline."}
{"instruction": "How do you &#39;redo&#39; changes after &#39;undo&#39; with Emacs?", "output": "Short version: by undoing the undo. If you undo, and then do a non-editing command such as C-f, then the next undo will undo the undo, resulting in a redo.\nLonger version:\nYou can think of undo as operating on a stack of operations. If you perform some command (even a navigation command such as C-f) after a sequence of undo operations, all the undos are pushed on to the operation stack.  So the next undo undoes the last command.  Suppose you do have an operation sequence that looks like this:\n\nInsert \"foo\"\nInsert \"bar\"\nInsert \"I love spam\"\n\nNow, you undo.  It undoes the last action, resulting in the following list:\n\nInsert \"foo\"\nInsert \"bar\"\n\nIf you do something other than undo at this point - say, C-f, the operation stack looks like this:\n\nInsert \"foo\"\nInsert \"bar\"\nInsert \"I love spam\"\nUndo insert \"I love spam\"\n\nNow, when you undo, the first thing that is undone is the undo.  Resulting in your original stack (and document state):\n\nInsert \"foo\"\nInsert \"bar\"\nInsert \"I love spam\"\n\nIf you do a modifying command to break the undo sequence, that command is added after the undo and is thus the first thing to be undone afterwards.  Suppose you backspaced over \"bar\" instead of hitting C-f.  Then you would have had\n\nInsert \"foo\"\nInsert \"bar\"\nInsert \"I love spam\"\nUndo insert \"I love spam\"\nDelete \"bar\"\n\nThis adding/re-adding happens ad infinitum.  It takes a little getting used to, but it really does give Emacs a highly flexible and powerful undo/redo mechanism."}
{"instruction": "How do I rename an open file in Emacs?", "output": "Try this function from Steve Yegge's .emacs:\n\n```\n\n;; source: http://steve.yegge.googlepages.com/my-dot-emacs-file\n(defun rename-file-and-buffer (new-name)\n  \"Renames both current buffer and file it's visiting to NEW-NAME.\"\n  (interactive \"sNew name: \")\n  (let ((name (buffer-name))\n        (filename (buffer-file-name)))\n    (if (not filename)\n        (message \"Buffer '%s' is not visiting a file!\" name)\n      (if (get-buffer new-name)\n          (message \"A buffer named '%s' already exists!\" new-name)\n        (progn\n          (rename-file filename new-name 1)\n          (rename-buffer new-name)\n          (set-visited-file-name new-name)\n          (set-buffer-modified-p nil))))))\n\n```\n\nTake a look at that page, there's another really useful related function there, called \"move-buffer-file\"."}
{"instruction": "Using Emacs to recursively find and replace in text files not already open", "output": "M-x find-name-dired: you will be prompted for a root directory and a filename pattern.\nPress t to \"toggle mark\" for all files found.\nPress Q for \"Query-Replace in Files...\": you will be prompted for query/substitution regexps.\nProceed as with query-replace-regexp: SPACE to replace and move to next match, n to skip a match, etc.\nPress C-x s to save buffers. (You can then press y for yes, n for no, or ! for yes for all)"}
{"instruction": "Step-by-step debugging with IPython", "output": "You can use IPython's %pdb magic. Just call %pdb in IPython and when an error occurs, you're automatically dropped to ipdb. While you don't have the stepping immediately, you're in ipdb afterwards.\nThis makes debugging individual functions easy, as you can just load a file with %load and then run a function. You could force an error with an assert at the right position.\n%pdb is a line magic. Call it as %pdb on, %pdb 1, %pdb off or %pdb 0. If called without argument it works as a toggle."}
{"instruction": "What are these ^M&#39;s that keep showing up in my files in emacs?", "output": "In git-config, set core.autocrlf to true to make git automatically convert line endings correctly for your platform, e.g. run this command for a global setting:\n\n```\ngit config --global core.autocrlf true\n\n```"}
{"instruction": "How to have Emacs auto-refresh all buffers when files have changed on disk?", "output": "(global-auto-revert-mode t) in your .emacs."}
{"instruction": "How to change size of split screen emacs windows?", "output": "With the mouse, you can drag the window sizes around.\nClick anywhere on the mode line that is not otherwise 'active' (the buffer name is safe, or any unused area to the right hand side), and you can drag up or down.\nSide-to-side dragging requires a very precise click on the spot where the two mode lines join.\nC-x¬†- (shrink-window-if-larger-than-buffer) will shrink a window to fit its content.\nC-x¬†+ (balance-windows) will make windows the same heights and widths.\nC-x¬†^ (enlarge-window) increases the height by 1 line, or the prefix arg value. A negative arg shrinks the window. e.g. C--¬†C-1¬†C-6¬†C-x¬†^ shrinks by 16 rows, as does C-u¬†-¬†1¬†6¬†C-x¬†^.\n(There is no default binding for shrink-window.)\nC-x¬†} (enlarge-window-horizontally) does likewise, horizontally.\nC-x¬†{ (shrink-window-horizontally) is also bound by default.\nFollowing one of these commands with repeat (C-x¬†z to initiate, and just z for continued repetition) makes it pretty easy to get to the exact size you want.\nIf you regularly want to do this with a specific value, you could record a keyboard macro to do it, or use something like\n(global-set-key (kbd \"C-c v\") (kbd \"C-u - 1 6 C-x ^\"))\nOr this:\n(global-set-key (kbd \"C-c v\") (kbd \"C-x o C-x 2 C-x 0 C-u - 1 C-x o\"))\nWhich is a smidgen hacky, so this would be better:\n\n```\n(defun halve-other-window-height ()\n  \"Expand current window to use half of the other window's lines.\"\n  (interactive)\n  (enlarge-window (/ (window-height (next-window)) 2)))\n\n(global-set-key (kbd \"C-c v\") 'halve-other-window-height)\n\n```\n\nTangentially, I also love winner-mode which lets you repeatedly 'undo' any changes to window configurations with C-c¬†left (whether the change is the size/number/arrangement of the windows, or just which buffer is displayed). C-c¬†right returns you to the most recent configuration. Set it globally with (winner-mode 1)"}
{"instruction": "Open a file with su/sudo inside Emacs", "output": "The nice thing about Tramp is that you only pay for that round-trip to SSH when you open the first file.  Sudo then caches your credentials, and Emacs saves a handle, so that subsequent sudo-opened files take much less time.\nI haven't found the extra time it takes to save burdening, either.  It's fast enough, IMO."}
{"instruction": "Set 4 Space Indent in Emacs in Text Mode", "output": "Do not confuse variable tab-width with variable tab-stop-list.\nThe former is used for the display of literal TAB characters.\nThe latter controls what characters are inserted when you press the TAB character in certain modes.\n\n-- GNU Emacs Manual\n\n```\n(customize-variable (quote tab-stop-list))\n\n```\n\nor add tab-stop-list entry to custom-set-variables in .emacs file:\n\n```\n(custom-set-variables\n  ;; custom-set-variables was added by Custom.\n  ;; If you edit it by hand, you could mess it up, so be careful.\n  ;; Your init file should contain only one such instance.\n  ;; If there is more than one, they won't work right.\n '(tab-stop-list (quote (4 8 12 16 20 24 28 32 36 40 44 48 52 56 60 64 68 72 76 80 84 88 92 96 100 104 108 112 116 120))))\n\n```\n\nAnother way to edit the tab behavior is with with M-x edit-tab-stops.\nSee the GNU Emacs Manual on Tab Stops for more information on edit-tab-stops."}
{"instruction": "Updating packages in Emacs", "output": "In order to automatically update the list of packages, only if there is no package list already, use the following:\n\n```\n(when (not package-archive-contents)\n    (package-refresh-contents))\n\n```\n\nIn order to update all installed packages, type package-list-packages, which will take you to the *Packages* buffer (and also update the list of packages), and then type U x.\npackage-refresh-contents unconditionally tries to download a package list from all repos you've added to package-archives; package-archive-contents is non nil if you have already downloaded the package list.\n\nELPA is the original.  I don't think it's really maintained anymore, but I'm not sure.  I don't use it.\nGNU is \"official\".  It's maintained along with Emacs, which means things should always work but updates and new packages don't come very often.\nMarmalade is basically a website where you can upload a complete package, and it will be added to the marmalade repo.  You don't just submit a link to the package's upstream, and it doesn't quite automate the creation of the package completely.  I think this is the Right Thing, because you don't necessarily want to track upstream.  Unfortunately, it has been unmaintained for a while, but someone recently took it over so it should be back and better at some point.\nMelpa takes a URL to e.g. the EmacsWiki lisp area or a github repo, and builds a package automatically from it.  Thus it is usually at most a day behind whatever it is tracking.  Although it tracks upstream, I've never had a problem in practice, and this is where most of my packages are from.  There is also Melpa Stable, which is like Melpa but grabs tagged revisions of the upstream repo instead of the latest revision.  Melpa stable has fewer packages than Melpa.  Probably Melpa should be preferred over Melpa Stable.\nOrg mode has its own package.el repo (http://orgmode.org/elpa/).\nAll of the package repos work the same, you just add them to your package-archives.\nHere's a more in-depth blog post about this subject, which I mostly agree with.\n\nI'm not sure, but I think if a package is duplicated in different repos, the order the repos appear in in package-archives determines precedence.  I don't know if higher precedence is at the beginning or end of the list.\n\n\nUpdate: In Emacs 25, there is a variable package-archive-priorities that you can use to prioritize your package repos (e.g. prefer ELPA over MELPA).\n\nHere is the relevant section of my init.el, if you're interested:\n\n```\n(setq jpk-packages\n      '(\n        ac-dabbrev\n        ...\n        yasnippet\n        ))\n\n(package-initialize)\n(add-to-list 'package-archives\n             '(\"melpa\" . \"http://melpa.org/packages/\"))\n(add-to-list 'package-archives\n             '(\"org\" . \"http://orgmode.org/elpa/\"))\n\n;; install any packages in jpk-packages, if they are not installed already\n(let ((refreshed nil))\n  (when (not package-archive-contents)\n    (package-refresh-contents)\n    (setq refreshed t))\n  (dolist (pkg jpk-packages)\n    (when (and (not (package-installed-p pkg))\n             (assoc pkg package-archive-contents))\n      (unless refreshed\n        (package-refresh-contents)\n        (setq refreshed t))\n      (package-install pkg))))\n\n(defun package-list-unaccounted-packages ()\n  \"Like `package-list-packages', but shows only the packages that\n  are installed and are not in `jpk-packages'.  Useful for\n  cleaning out unwanted packages.\"\n  (interactive)\n  (package-show-package-list\n   (remove-if-not (lambda (x) (and (not (memq x jpk-packages))\n                            (not (package-built-in-p x))\n                            (package-installed-p x)))\n                  (mapcar 'car package-archive-contents))))\n\n```"}
{"instruction": "Using Emacs as an IDE", "output": "You'll have to be specific as to what you mean by \"the rest\".  Except for the object inspector (that I\"m aware of), emacs does all the above quite easily:\n\neditor (obvious)\ncompiler - just run M-x compile and enter your compile command.  From there on, you can just M-x compile and use the default.  Emacs will capture C/C++ compiler errors (works best with GCC) and help you navigate to lines with warnings or errors.\nDebugging - similarly, when you want to debug, type M-x gdb and it will create a gdb buffer with special bindings\nDocumentation Lookup - emacs has excellent CScope bindings for code navigation.  For other documentation: Emacs also has a manpage reader, and for everything else, there's the web and books.\nversion control - there are lots of Emacs bindings for various VCS backends (CVS, SCCS, RCS, SVN, GIT all come to mind)\n\nEdit: I realize my answer about documentation lookup really pertained to code navigation.  Here's some more to-the-point info:  \n\nLooking up manpages, info manuals, and Elisp documentation from within emacs\nLooking up Python documentation from within Emacs.  \n\nGoogle searching will no doubt reveal further examples.\nAs the second link shows, looking up functions (and whatever) in other documentation can be done, even if not supported out of the box."}
{"instruction": "Who originally invented this type of syntax: -*- coding: utf-8 -*-", "output": "This way of specifying the encoding of a Python file comes from PEP 0263 - Defining Python Source Code Encodings.\nIt is also recognized by GNU Emacs (see Python Language Reference, 2.1.4 Encoding declarations), though I don't know if it was the first program to use that syntax."}
{"instruction": "How do I duplicate a whole line in Emacs?", "output": "I use \n\n```\nC-a C-SPACE C-n M-w C-y\n\n```\n\nwhich breaks down to\n\nC-a: move cursor to start of line\nC-SPACE: begin a selection (\"set mark\")\nC-n: move cursor to next line\nM-w: copy region\nC-y: paste (\"yank\")\n\nThe aforementioned\n\n```\nC-a C-k C-k C-y C-y\n\n```\n\namounts to the same thing (TMTOWTDI)\n\nC-a: move cursor to start of line\nC-k: cut (\"kill\") the line\nC-k: cut the newline\nC-y: paste (\"yank\") (we're back at square one)\nC-y: paste again (now we've got two copies of the line)\n\nThese are both embarrassingly verbose compared to C-d in your editor, but in Emacs there's always a customization. C-d is bound to delete-char by default, so how about C-c C-d? Just add the following to your .emacs:\n\n```\n(global-set-key \"\\C-c\\C-d\" \"\\C-a\\C- \\C-n\\M-w\\C-y\")\n\n```\n\n(@Nathan's elisp version is probably preferable, because it won't break if any of the key bindings are changed.)\nBeware: some Emacs modes may reclaim C-c C-d to do something else."}
{"instruction": "Re-open *scratch* buffer in Emacs?", "output": "GNU Emacs default bindings:\n\nC-xb *scratch* RET\n\nor, more verbosely\n\nM-x switch-to-buffer *scratch* RET\n\nThe *scratch* buffer is the buffer selected upon startup, and has the major mode Lisp Interaction.  Note: the mode for the *scratch* buffer is controlled by the variable initial-major-mode.\nIn general you can create as many \"scratch\" buffers as you want, and name them however you choose.\n\nC-xb NAME RET\n\nswitches to a buffer NAME, creating it if it doesn't exist. A new buffer is not associated with a file on disk until you use C-xC-w (or M-x write-file RET) to choose a file where it should be saved.\n\nM-x text-mode RET\n\nchanges the current buffer's major mode to Text mode.  To find all the modes available (that is, without requiring any new packages), you can get a list by typing:\n\nM-x apropos-command -mode$ RET"}
{"instruction": "Emacs - Multiple columns one buffer", "output": "See follow-mode.\nExcerpt:\n\nFollow mode is a minor mode that makes two windows, both showing the same buffer, scroll as a single tall ‚Äúvirtual window.‚Äù To use Follow mode, go to a frame with just one window, split it into two side-by-side windows using C-x 3, and then type M-x follow-mode. From then on, you can edit the buffer in either of the two windows, or scroll either one; the other window follows it.\n\nIn Follow mode, if you move point outside the portion visible in one window and into the portion visible in the other window, that selects the other window‚Äîagain, treating the two as if they were parts of one large window."}
{"instruction": "How to run multiple shells on Emacs", "output": "C-u M-x shell will do it.\nIt will prompt for a name for the new shell, just hit return for the default (which will be something like *shell*<2>.\nAlso works with eshell.\nAnother trick, if you use eshell: just as M-x eshell takes you back to *eshell* (rather than starting a new eshell), if you use a numeric prefix argument it will take you to that eshell buffer. For instance, C-3M-xeshell will take you to *eshell*<3>. Sadly if you use shell (rather than eshell), this trick doesn't seem to work (in my Emacs 24.0.50.1 at least.)"}
{"instruction": "Is there a (repeat-last-command) in Emacs?", "output": "with regards to 'describe-last-function':\nThere's a variable last-command which is set to a symbol representative of the last thing you did. So this elisp snippet - (describe-function last-command) - ought to bring up the documentation for the thing that immediately happened.\nSo you could make a trivial working describe-last-function like so\n\n```\n(defun describe-last-function() \n  (interactive) \n  (describe-function last-command))\n\n```\n\nPut that elisp in .emacs or equivalent, and you'll have a M-x describe-last-function.\nIf you've banged on a few keys or done something that modified last-command since the thing you're interested in, the command-history function might be of interest. You can get that by M-x command-history"}
{"instruction": "How do I create a directory from within Emacs?", "output": "to create the directory dir/to/create, type:\n\n```\nM-x make-directory RET dir/to/create RET\n\n```\n\nto create directories dir/parent1/node and dir/parent2/node, type:\n\n```\nM-! mkdir -p dir/parent{1,2}/node RET\n\n```\n\nIt assumes that Emacs's inferior shell is bash/zsh or other compatible shell.\nor in a Dired mode \n\n```\n+\n\n```\n\nIt doesn't create nonexistent parent directories.\nExample:\n\n```\nC-x d *.py RET ; shows python source files in the CWD in `Dired` mode\n+ test RET     ; create `test` directory in the CWD\n\n```\n\nCWD stands for Current Working Directory.\nor just create a new file with non-existing parent directories using C-x C-f and type:\n\n```\nM-x make-directory RET RET\n\n```\n\n\nEmacs asks to create the parent directories automatically while saving a new file in recent Emacs versions. For older version, see How to make Emacs create intermediate dirs - when saving a file?"}
{"instruction": "How to open Emacs inside Bash", "output": "Emacs takes many launch options. The one that you are looking for is\nemacs -nw. This will open Emacs inside the terminal disregarding the DISPLAY environment variable even if it is set.\nThe long form of this flag is emacs --no-window-system.\nMore information about Emacs launch options can be found in the manual."}
{"instruction": "How to achieve code folding effects in Emacs?", "output": "Folding is generally unnecessary with emacs, as it has tools that explicitly implement the actions people do manually when folding code.\nMost people have good success with simple incremental searches.  See \"foo\" mentioned somewhere? Type C-sfoo, find the definition, press enter, read it, and then press C-x C-x to go back to where you were.  Simple and very useful.\nMost modes support imenu.  M-ximenu will let you jump to a function definition (etc.) by name.  You can also bind it to a mouse click to get a menu of functions (or add it to the menubar; see the Info page for more detail).  It provides data for which-function-mode, which will let you see which function you are currently inside in the modeline.  (Why are your functions this long, though?)\nThere is also speedbar, which displays the imenu information (and other things) graphically.\nIf you want to get an overview of your file, try M-xoccur\".  Given a regex, it will create a new buffer with each match in the current buffer.  You can search for \"(defun\" to get an overview of the functions the current file implements.  Clicking on the result will move you to that position in the file.\nSo anyway, think about what you really want to achieve, and Emacs probably implements that. Don't fight with imperfect tools that make you fold and unfold things constantly."}
{"instruction": "How can I delete the current line in Emacs?", "output": "```\nC-a # Go to beginning of line\nC-k # Kill line from current point\n\n```\n\nThere is also\n\n```\nC-S-backspace   # Ctrl-Shift-Backspace\n\n```\n\nwhich invokes M-x kill-whole-line. \nIf you'd like to set a different global key binding, you'd put this in ~/.emacs:\n\n```\n(global-set-key \"\\C-cd\" 'kill-whole-line)     # Sets `C-c d` to `M-x kill-whole-line`\n\n```\n\n\nIf you want to delete a number of whole lines, you can prefix the command with a number:\n\n```\nC-u 5 C-S-backspace    # deletes 5 whole lines\nM-5 C-S-backspace      # deletes 5 whole lines\n\nC-u C-S-backspace      # delete 4 whole lines. C-u without a number defaults to 4\n\nC-u -5 C-S-backspace   # deletes previous 5 whole lines\nM--5 C-S-backspace     # deletes previous 5 whole lines\n\n```\n\nSometimes I also find C-x z helpful:\n\n```\nC-S-backspace         # delete 1 whole line\nC-x z                 # repeat last command\nz                     # repeat last command again. \n                      # Press z as many times as you wish. \n                      # Any other key acts normally, and ends the repeat command.\n\n```"}
{"instruction": "How do I byte-compile everything in my .emacs.d directory?", "output": "C-u 0 M-x byte-recompile-directory\nwill compile all the .el files in the directory and in all subdirectories below.\nThe C-u 0 part is to make it not ask about every .el file that does not have a .elc counterpart."}
{"instruction": "Emacs bulk indent for Python", "output": "If you are programming Python using Emacs, then you should probably be using python-mode. With python-mode, after marking the block of code,\nC-c > or C-c C-l  shifts the region 4 spaces to the right\nC-c < or C-c C-r  shifts the region 4 spaces to the left\nIf you need to shift code by two levels of indention, or some arbitary amount you can prefix the command with an argument:\nC-u 8 C-c >     shifts the region 8 spaces to the right\nC-u 8 C-c <     shifts the region 8 spaces to the left\nAnother alternative is to use M-x indent-rigidly which is bound to C-x TAB:\nC-u 8 C-x TAB   shifts the region 8 spaces to the right\nC-u -8 C-x TAB  shifts the region 8 spaces to the left\nAlso useful are the rectangle commands that operate on rectangles of text instead of lines of text. \nFor example, after marking a rectangular region,\nC-x r o inserts blank space to fill the rectangular region (effectively shifting code to the right)\nC-x r k kills  the rectangular region (effectively shifting code to the left)\nC-x r t prompts for a string to replace the rectangle with.  Entering C-u 8 <space> will then enter 8 spaces.\nPS. With Ubuntu, to make python-mode the default mode for all .py files, simply install the python-mode package."}
{"instruction": "Inline code in org-mode", "output": "You can enclose the text within = or ~ signs to have it typeset in monospaced font and export it verbatim (which means it is not processed for org-specific syntax):\n\n```\nThis is =verbatim text= or ~code~.\n\n```\n\nYou'll find all information about org-mode markup elements in the relevant section of the manual."}
{"instruction": "How to change the indentation width in emacs javascript mode", "output": "js-indent-level can be used in the default javascript-mode, which is included by default starting in emacs 23.2. \n\n```\n(setq js-indent-level 2)\n\n```\n\nshould do what you're looking for. If you're using an older version of emacs, you may be in java-mode.  I think this mode responds to c-basic-offset, however I may be wrong."}
{"instruction": "How to automatically install Emacs packages by specifying a list of package names?", "output": "Based on comments by Profpatsch and answers below:\n\n```\n(defun ensure-package-installed (&rest packages)\n  \"Assure every package is installed, ask for installation if it‚Äôs not.\n\nReturn a list of installed packages or nil for every skipped package.\"\n  (mapcar\n   (lambda (package)\n     ;; (package-installed-p 'evil)\n     (if (package-installed-p package)\n         nil\n       (if (y-or-n-p (format \"Package %s is missing. Install it? \" package))\n           (package-install package)\n         package)))\n   packages))\n\n;; make sure to have downloaded archive description.\n;; Or use package-archive-contents as suggested by Nicolas Dudebout\n(or (file-exists-p package-user-dir)\n    (package-refresh-contents))\n\n(ensure-package-installed 'iedit 'magit) ;  --> (nil nil) if iedit and magit are already installed\n\n;; activate installed packages\n(package-initialize)\n\n```"}
{"instruction": "How do I change read/write mode for a file using Emacs?", "output": "M-x read-only-mode\n\nin very old versions of Emacs, the command was:\n\nM-x toggle-read-only\n\nOn my Windows box, that amounts to Alt-x to bring up the meta prompt and typing \"read-only-mode\" to call the correct elisp function.\nIf you are using the default keyboard bindings,\n\nC-x C-q\n\n(which you read aloud as \"Control-X Control-Q\") will have the same effect.  Remember, however, given that emacs is essentially infinitely re-configurable, your mileage may vary.\nFollowing up from the commentary: you should note that the writeable status of the buffer does not change the writeable permission of the file.  If you try to write out to a read only file, you'll see a confirmation message.  However, if you own the file, you can write out your changes without changing the permissions on the file.\nThis is very convenient if you'd like to make a quick change to a file without having to go through the multiple steps of add write permission, write out changes, remove write permission.  I tend to forget that last step, leaving potentially critical files open for accidental changes later on."}
{"instruction": "Open file via SSH and Sudo with Emacs", "output": "As of Emacs 24.3, an analog of the old multi: syntax has been layered on top of the modern tramp-default-proxies-alist approach, meaning that you can once again perform multi-hops without any prior configuration. For details, see:\nC-hig (tramp)Ad-hoc multi-hops RET\nWith the new syntax, each 'hop' is separated by |. The example in the manual is:\nC-xC-f /ssh:bird@bastion|ssh:you@remotehost:/path RET\nWhich connects firstly as bird@bastion, and from there to you@remotehost:/path\n/su: or /sudo: on remote hosts\nYou can also use this syntax to sudo/su to root (or of course any other user) on a remote host:\nC-xC-f /ssh:you@remotehost|sudo:remotehost:/path/to/file RET\nImportant: be sure to specify the hostname explicitly: sudo:remotehost: rather than sudo:: (see below).\nAs this still uses the proxy mechanism underneath, tramp-default-proxies-alist should now include the value (\"remotehost\" \"root\" \"/ssh:you@remotehost:\")\nMeaning that the proxy /ssh:you@remotehost: is going to be used whenever you request a file as root@remotehost.\nroot is the default user for these methods, but you can of course also change to a non-root user with:\nC-xC-f /ssh:you@remotehost|sudo:them@remotehost:/path/to/file RET\nAlways specify the remote hostname explicitly\nYou are probably used to using sudo:: or su:: and omitting the hostname. If you are staying on the localhost then this is still fine, but if you are hopping to a remote server then you must specify the hostname for every hop -- even if it is the same as for the previous hop. Always use sudo:hostname: or su:hostname: with remote hosts.\nThe trap here is that sudo:: does actually appear to work -- however when you do that the HOST for the dynamic proxy entry will be the hostname you originated from rather than the host you connected to. This will not only look confusing (as the wrong host will be displayed in the file paths), but it will also mean that any subsequent attempt to use sudo:: on your localhost will instead be proxied to the remote server! (and the proxy would also presumably be clobbered if you did the same thing on a second server, causing further issues).\nIn short, don't use :: when you multi-hop!\nEmacs 27+\nStarting from Emacs 27.1 (or Tramp 2.4.2, if using the GNU ELPA package) the :: case works intuitively, such that /ssh:you@remotehost|sudo:: will re-use remotehost rather than your own local host, and so you won't end up with a bad proxy entry.\nIn addition, the likes of /ssh:you@remotehost|sudo:localhost: are detected and flagged as user errors.\nIf you are liable to use a mixture of Emacs versions including versions earlier than 27 (or you are advising someone else who may be using an older version), then it would be safest to continue to treat :: as unsafe when multi-hopping, to avoid potential mishap.  (I.e. specifying the correct remote host explicitly will remain the safest approach if the Tramp version is unknown.)"}
{"instruction": "Emacs Ruby autocomplete almost working", "output": "Here's a thought: The macro binds a flet function (rails-project:root) one time to the value that (rails-project:root) has just before the body executes. (That's how it claims a performance increase: Apparently the outer (rails-project:root) is expensive, so calling once and caching the value seems like a good idea.)\nUnfortunately, if there is code inside the body that has a side effect meant intentionally to change the value that (rails-project:root) returns, it's going to have no effect.  That change will be invisible even to other code called within the body because Emacs lisp has dynamic binding of flet names."}
{"instruction": "emacs/elisp: What is the hash (pound, number sign, octothorp) symbol used for?", "output": "There is no difference:\n\n```\n(eq 'my-add #'my-add)\n\n```\n\nyields t\nThe # can be used in front of a lambda expression indicating to the byte-compiler that the following expression can be byte compiled, see the docs for Anonymous Functions.  But there's nothing to compile in the case of a symbol.\nIn general, it is used in the printed representation along with the left angle bracket (<) to indicate that the object printed is a description (but cannot be read).  For example:\n\n```\n#<buffer foo.txt>\n\n```\n\nIt is also used in constructs by the reader to represent circular structures.  See the docs for Read Syntax for Circular Objects.\nAnd then you have its use for denoting the base for integers, e.g. #x2c -> 44.\nPlus more I'm sure."}
{"instruction": "How to copy text from Emacs to another application on Linux", "output": "Insert the following into your .emacs file:\n\n```\n(setq x-select-enable-clipboard t)\n\n```"}
{"instruction": "Given an emacs command name, how would you find key-bindings ? (and vice versa)", "output": "To just find key bindings for a command, you can use emacs help's \"where-is\" feature\n\n```\nC-h w command-name\n\n```\n\nIf multiple bindings are set for the command they will all be listed.\nFor the inverse, given a key sequence, you can type\n\n```\nC-h k key-sequence\n\n```\n\nTo get the command that would run.  \nYou can get detailed information about a command, also any non-interactive function defined, by typing\n\n```\nC-h f function-name\n\n```\n\nWhich will give you detailed information about a function, including any key bindings for it, and\n\n```\nC-h v variable-name\n\n```\n\nwill give you information about any (bound) variable. Key-maps are kept in variables, however the key codes are stored in a raw format.  Try C-h v isearch-mode-map for an example.\nFor more help on getting help, you can type\n\n```\nC-h ?\n\n```"}
{"instruction": "Where can I find my .emacs file for Emacs running on Windows?", "output": "Copy and pasted from the Emacs FAQ, http://www.gnu.org/software/emacs/windows/:\nWhere do I put my init file?\nOn Windows, the .emacs file may be called _emacs for backward compatibility with DOS and FAT filesystems where filenames could not start with a dot. Some users prefer to continue using such a name, because Windows¬†Explorer cannot create a file with a name starting with a dot, even though the filesystem and most other programs can handle it. In Emacs¬†22 and later, the init file may also be called .emacs.d/init.el. Many of the other files that are created by Lisp packages are now stored in the .emacs.d directory too, so this keeps all your Emacs related files in one place.\nAll the files mentioned above should go in your HOME directory. The HOME directory is determined by following the steps below:\n\nIf the environment variable HOME is set, use the directory it indicates.\nIf the registry entry HKCU\\SOFTWARE\\GNU\\Emacs\\HOME is set, use the directory it indicates.\nIf the registry entry HKLM\\SOFTWARE\\GNU\\Emacs\\HOME is set, use the directory it indicates. Not recommended, as it results in users sharing the same HOME directory.\nIf C:\\.emacs exists, then use C:/. This is for backward compatibility, as previous versions defaulted to C:/ if HOME was not set.\nUse the user's AppData directory, usually a directory called Application Data under the user's profile directory, the location of which varies according to Windows version and whether the computer is part of a domain.\n\nWithin Emacs, ~ at the beginning of a file name is expanded to your HOME directory, so you can always find your .emacs file with C-x C-f ~/.emacs.\nThere's further information at HOME and Startup Directories on MS-Windows."}
{"instruction": "Changing the default folder in Emacs", "output": "You didn't say so, but it sounds like you're starting Emacs from a Windows shortcut.\nThe directory that you see with c-x c-f is the cwd, in Emacs terms, the default-directory (a variable).\nWhen you start Emacs using an MS Windows shortcut, the default-directory is initially the folder (directory) specified in the \"Start In\" field of the shortcut properties.  Right click  the shortcut, select Properties, and type the path to your desktop in the Start In field.\nIf you're using Emacs from the command line, default-directory starts as the directory where you started Emacs (the cwd).\nThis approach is better than editing your .emacs file, since it will allow you to have more than one shortcuts with more than one starting directory, and it lets you have the normal command line behavior of Emacs if you need it.\nCWD = current working directory = PWD = present working directory. It makes a lot more sense at the command line than in a GUI."}
{"instruction": "How do you list the active minor modes in emacs?", "output": "C-h m or M-x describe-mode shows all the active minor modes (and major mode) and a brief description of each."}
{"instruction": "The function to show current file&#39;s full path in mini buffer", "output": "It's the built-in function buffer-file-name that gives you the full path of your file.\nThe best thing to do is to have your emacs window to always show your system-name and the full path of the buffer you're currently editing :\n\n```\n(setq frame-title-format\n      (list (format \"%s %%S: %%j \" (system-name))\n        '(buffer-file-name \"%f\" (dired-directory dired-directory \"%b\"))))\n\n```\n\nYou can also do something like this :\n\n```\n(defun show-file-name ()\n  \"Show the full path file name in the minibuffer.\"\n  (interactive)\n  (message (buffer-file-name)))\n\n(global-set-key [C-f1] 'show-file-name) ; Or any other key you want\n\n```"}
{"instruction": "How to load program reading stdin and taking parameters in gdb?", "output": "If you were doing it from a shell you'd do it like this:\n\n```\n% gdb myprogram\ngdb> run params ... < input.txt\n\n```\n\nThis seems to work within emacs too."}
{"instruction": "Emacs Interactive-Haskell repl non-response if either the cabal or working directory set to project directory", "output": "This is a known bug. and a good catch!\nThe pull request https://github.com/haskell/haskell-mode/pull/1116/files solves it."}
{"instruction": "How to swap the buffers in 2 windows emacs", "output": "I use buffer-move for this. Now if you are working on the buffer on the left side, calling 'buf-move-right' will swap it with the one on the right. I guess this is what you want."}
{"instruction": "Permanently summing a column in an Org-mode table", "output": "Assign a field-name using the ^ mark:\n\n```\n|---+---|\n|   | 1 |\n|   | 2 |\n|   | 3 |\n|---+---|\n|   | 6 |\n| ^ | x |\n|---+---|\n#+TBLFM: $x=vsum(@1..@-1)\n\n```\n\nSee The Org Manual, Section-3.5.9 Advanced Features."}
{"instruction": "View a file&#39;s history in Magit?", "output": "Since magit 2.1: magit-log-buffer-file (as per comment below)\nBefore magit 2.1: magit-file-log is what you are looking for. It will show you all commits for the file in the current buffer in the standard magit log view."}
{"instruction": "The difference between setq and setq-default in Emacs Lisp", "output": "Some variables in Emacs are \"buffer-local\", meaning that each buffer is allowed to have a separate value for that variable that overrides the global default. tab-width is a good example of a buffer-local variable.\nIf a variable is buffer-local, then setq sets its local value in the current buffer and setq-default sets the global default value.\nIf a variable is not buffer-local, then setq and setq-default do the same thing.\nIn your case 2, (setq tab-width 4) set the buffer-local value of tab-width to 4 in the current buffer, leaving the global default value of tab-width still at 8, so when you evaluated tab-width in a different buffer that had no local value, you saw that 8. Then, when you set the default value to 4, that buffer picked it up, since it still had no local value."}
{"instruction": "How do I set the size of Emacs&#39; window?", "output": "If you want to change the size according to resolution you can do something like this (adjusting the preferred width and resolutions according to your specific needs):\n\n```\n(defun set-frame-size-according-to-resolution ()\n  (interactive)\n  (if window-system\n  (progn\n    ;; use 120 char wide window for largeish displays\n    ;; and smaller 80 column windows for smaller displays\n    ;; pick whatever numbers make sense for you\n    (if (> (x-display-pixel-width) 1280)\n           (add-to-list 'default-frame-alist (cons 'width 120))\n           (add-to-list 'default-frame-alist (cons 'width 80)))\n    ;; for the height, subtract a couple hundred pixels\n    ;; from the screen height (for panels, menubars and\n    ;; whatnot), then divide by the height of a char to\n    ;; get the height we want\n    (add-to-list 'default-frame-alist \n         (cons 'height (/ (- (x-display-pixel-height) 200)\n                             (frame-char-height)))))))\n\n(set-frame-size-according-to-resolution)\n\n```\n\nNote that window-system is deprecated in newer versions of emacs. A suitable replacement is (display-graphic-p). See this answer to the question How to detect that emacs is in terminal-mode? for a little more background."}
{"instruction": "jump to line X in nano editor", "output": "In the nano editor\nCtrl+_\nOn opening a file\n\n```\nnano +10 file.txt\n\n```"}
{"instruction": "nano error: Error opening terminal: xterm-256color", "output": "On Red Hat this worked for me:\n\n```\nexport TERM=xterm\n\n```\n\nfurther info here: https://web.archive.org/web/20220331072532/http://www.cloudfarm.it/fix-error-opening-terminal-xterm-256color-unknown-terminal-type/"}
{"instruction": "How Do I Clear The Credentials In AWS Configure?", "output": "just remove ~/.aws/credentials along with ~/.aws/config\nEDIT: Note path references user home directory and specific to each user in system."}
{"instruction": "Unix: what modifier key does &quot;M-&quot; refer to (e.g. M-C)", "output": "M refers to the meta key, which is not present on most keyboards. (Its use in software is for primarily historical reasons.) Usually, the meta key is emulated by another key on your keyboard. On Windows and Linux, it is usually the Alt key. On Mac OS X, that key (aka Option) already has other uses, and so Escape is used instead."}
{"instruction": "Move whole line up/down shortcut in nano (analogue to intelliJ or Visual Studio)", "output": "You can use Ctrl+K to cut a line, move to destination position and press Ctrl+U to paste it."}
{"instruction": "How to commit a change in git when git commit opens Nano?", "output": "That screen is just a text editor (called Nano) and those options at the bottom, represent commands, typically it's the ctrl key + the letter for the command.\nTo make the commit you should write your commit message, then press ctrl+o to save your message, and then ctrl+x to exit the editor.\nTo avoid that screen you could do something like git commit -m 'your commit message', the -m indicates that your commit message goes in the command."}
{"instruction": "nano editor on OS X - How to type M-U for undo?", "output": "There is no \"UNDO\" in OSX GNU nano 2.0.6.\n\nMeta (M-) symbol is the Esc key in OSX. E.g. to cut text with M-T, enter Esc+ T.\nThe Main nano help text says:\n\nEscape-key sequences are notated with the Meta (M-) symbol and can be entered using either the Esc, Alt, or Meta key depending on your keyboard setup.\n\n\nDisplay the help text with all key combinations while in nano by entering ctrl+G.\nHide the help text with the same key combination.\n\nUseful for navigating quickly:\n\nctrl+ Y / ctrl+ V -> Go one screenful up / down\nEsc+ S -> Enable soft line wrapping\nEsc+ M -> Enable Mouse support\nGo to a line number\n\nEsc+ N -> Enable line numbering\n\nEsc+ _ -> go to a line\nYou'll have to type 3 keys for that: Esc+ Shift + -\n\n\n\n\n\nFYI: In debian's GNU nano 5.4, enter Esc+U on an OSX keyboard to \"UNDO\"."}
{"instruction": "How to install nano on boot2docker", "output": "Boot2Docker is based on Tiny Linux which uses tce-load to install packages. The list of packages in the repository can be found here:\nhttp://distro.ibiblio.org/tinycorelinux/tcz_2x.html\nYou'll see that the nano packages is nano.tcz , so you'll run something like:\n\n```\ntce-load -wi nano\n\n```\n\nThis article: http://lowtek.ca/roo/2015/modify-boot2docker-iso/ should also be helpful along with this one: http://www.gerrelt.nl/RaspberryPi/wordpress/tutorial-unpluggable-squeezelite-player-on-tinycore/#Search_and_install_NANO_extension"}
{"instruction": "How to open multiple files and switch between them in nano text editor?", "output": "Put \"set multibuffer\" into ~/.nanorc or start nano with -F. Toggle this inside nano with alt-F. You can now read files into their own buffer with ctrl-R.\nUse alt-comma and alt-period to go forward and backward in the file list."}
{"instruction": "Nano syntax highlighting in Mac OS X 10.7 (Lion)?", "output": "Here are some steps to help you out.\n\nCreate a new directory in /usr/local/share/ called 'nano' like this:\n\n\nmkdir /usr/local/share/nano\n\n\nNow, using nano, make a nano resource file for your C syntax like this:\n\n\nnano /usr/local/share/nano/c.nanorc`\n\n\nNow put your C code highlighting in this file and save it. Here is a link to some possible C syntax highlighting:\n\n\nhttp://code.google.com/p/nanosyntax/source/browse/trunk/syntax-nanorc/c.nanorc\n\n\nSave that file and now open your user‚Äôs nano resource file by typing:\n\n\nnano ~/.nanorc\n\n\nIn this file, add a reference to the c.nanorc file you just made like this:\n\n\ninclude \"/usr/local/share/nano/c.nanorc\"\n\n\nSave your user resource file.\n\nNow, when you open up C files, you should see syntax highlighting. You can add additional syntax highlighting for different types of files using the same method. Just add more lines to your ~/.nanorc file.\nNote that depending on your user permissions, you may have to precede some of the above commands with sudo and then enter your root password."}
{"instruction": "Regex-based matching and sustitution with nano?", "output": "You need to add, or un-comment, the following entry in your global nanorc file (on my machine, it was /etc/nanorc):\n\n```\nset regexp\n\n```\n\nThen fire up a new terminal and press CTRL + / and do your replacements which should now be regex-aware.\nEDIT\n\nSearch for conf->(\\S+):\n\n\nReplace with \\1_conf\n\n\nPress a to replace all occurrences:\n\n\nEnd result:"}
{"instruction": "Python - Saving a File being edited in GNU Nano 2.2.4", "output": "If you press Ctrl-X, for exit, it will ask you whether you want to save the file.\nCtrl-O is for saving file without exiting the editor.\nCtrl-G is for help on key combinations."}
{"instruction": "Error opening terminal: xterm-256color", "output": "Probably due to a Lion upgrade/install. Did you do that recently @Gih?\nPossible duplicate (with fix) at \nnano error: Error opening terminal: xterm-256color\nEDIT:\nEasiest fix (takes 10 seconds)...from Michael:\n\nThere is a solution much easier:\n  http://ricochen.wordpress.com/2011/07/23/mac-os-x-lion-terminal-color-remote-access-problem-fix/"}
{"instruction": "Nano insert newline in search and replace", "output": "set regexp in your config file will:  \n\nDo extended regular expression searches by default.   \n\nYou can also activate it interactively with Meta + R*  \nBut that's it! Search using RegEx, but as replacement Nano accepts only strings, except referenced captured groups \\1 to \\9 .\nNano's RegEx flavour is a POSIX implementation, and for what would interest you it does not compute [:newline:] but accepts \\s, [:cntrl:] (Control characters) and [:space:] (All whitespace characters, including line breaks) also negated POSIX classes: [^[:graph:]]\nAnswer\nIt can't be done, nothing works, tried:  \n\nWhitespace Display Toggle Meta + P - no visible change\nVerbatim Input Meta + Shift + V then pressing Enter would translate into a visible ^M encoding (but works for a Tab input)\n\"Special Functions\" Esc + Esc then a 3 digit sequence - control characters not accepted (but accepts visible chars 013 would actually interpret it like the user pressed Enter)\nUnicode Input Ctrl + Shift + U - control characters not accepted (but accepts visible chars)\n\nASCII codes for some control characters:\n\n\n```\n      DEC   HEX   \n(\\t)    9  0x09   Horizontal Tab\n(\\n)   10  0x0a   New Line\n(\\r)   13  0x0d   Carriage Return\n ( )   32  0x20   Space\n\n```\n\n\nNano commands: \n\nSearch Ctrl + W\nFind Next Meta + W*\nToggle replace while searching Ctrl + R\nSearch & Replace Ctrl + \\\n\n*Meta is usually the Alt key\n\n\nPS: \"sometimes failure is the answer, or someone should prove me wrong\"\nPPS: please"}
{"instruction": "Console editor: something between VIM and nano?", "output": "You can use mcedit. It is much more user friendly than other editors - it even supports mouse over ssh (even in Putty console from Windows).\nSimply install package Midnight Commander as follows:\n\nUbuntu/Debian:\n\n```\nsudo apt-get install mc\n\n```\n\nFedora/Redhat:\n\n```\nsudo yum install mc\n\n```\n\nMacOS:\n\n```\nbrew install mc\n\n```\n\n\nand it will make mcedit available.\nIn local console it even supports Shift+Del, Shift+Ins, Ctrl+Ins shortcuts that we use in Windows.\nIt also has nice ability to copy blocks into arbitrary files, effectively giving you unlimited number of clipboards."}
{"instruction": "What is the benefit of using things such as emacs, vim, and nano over any other IDE or text editor?", "output": "Your question is a tough one. Even if they do run in terminal, it's not their primary advantage. I'm talking about Emacs and Vim right now. To be short, they've been around for at least 20 years (Vim) or more (Emacs), they have a pretty active community, they're scriptable so you can do pretty much anything with them if you know how and they're extremely powerful.\nThey have a pretty big learning curve, so you'll probably end up fumbling around in them for weeks, if not months, before becoming proficient. The primary motivation for learning them is productivity. You can do some pretty amazing things in a minimal amount of keystrokes compared to, let's say, Notepad.\nTry one. If you like it, stick with it for some time, endure the pain and then you'll see the benefits. It's almost like going to the gym."}
{"instruction": "Git commit -a opens GNU nano 2.2.6 How should I change it to open Vim instead?", "output": "You can set it from the command line or in your .gitconfig\n\n```\ngit config --global core.editor vim\n\n```"}
{"instruction": "bash: nano: command not found at Windows git bash", "output": "Little modification of the previous solution (@Simopaa) is worked for me on Windows 10 (without Chocolatey):\n\nDownload nano-git\nMove the nano-git-xxx.exe to (for example) C:\\Program Files\\Git\\bin.\nModify the .gitconfig file with the following (single and double quotes are important):\n\n```\n[core]\neditor = \"winpty '/c/Program Files/Git/bin/nano-git-0d9a7347243.exe'\"\n\n```\n\n(Optional step) Make nano available for editing in general:\nCreate or edit the one of the startup script of bash (e.g. ~/.bash_profile) with the followings:\n\n```\nexport PATH=\"/c/Program Files/Git/bin:$PATH\"\nalias nano=\"winpty nano\"\n\n```"}
{"instruction": "How to turn OFF (or at least override) syntax highlighting in nano via ~/.nanorc?", "output": "To disable syntax highlighting write following lines into ~/.nanorc:\n\n```\nset quiet\nsyntax \"disabled\" \".\"\n\n```\n\nThe first line prevent error reporting. The second line defines a new color syntax.\nYou can also define a single syntax containing your favorite color in your ~/.nanorc:\n\n```\nsyntax \"disabled\" \".\"\ncolor brightgreen,black \".\"\n\n```\n\nI hope this helps."}
{"instruction": "How to set nano up for git commit messages with line length limits", "output": "Nano supports the passing of command line arguments when starting it.\nThis can be leveraged as part of the git editor config setting.\ngit config --global core.editor \"nano -r 72\"\nOr:\nexport GIT_EDITOR='nano -r 72'"}
{"instruction": "Scroll using the mouse in nano editor?", "output": "Nano doesn't support using the mouse wheel to scroll. You can enable nano's (limited) mouse support with alt-M or with --mouse when starting nano (another invocation returns to whatever mouse support your terminal has), but that's it:\n‚àím, ‚àí‚àímouse\nEnable mouse support, if available for your system. When enabled, mouse clicks can be used to place the cursor, set the mark (with a double click), and execute shortcuts. The mouse will work in the X Window System, and on the console when gpm is running. Text can still be selected through dragging by holding down the Shift key.\n(https://www.nano-editor.org/dist/v2.6/nano.1.html)\nBy default, mouse services are provided by the terminal window. The mouse works almost the same as in a GUI editor. You can highlight text, right-click to copy and paste, and use the middle mouse button for pasting text from the buffer. However, when you use the middle mouse button to paste text, the text is pasted at the current cursor location, not at the mouse pointer location. Other limitations exist. For example, you cannot use the mouse to cut or delete text, nor can you use the mouse to scroll through the file.\nNano has its own built-in mouse services, but they are limited. They provide only the ability to move the cursor to the point where you click, and to mark the beginning and end of a section of text. Use the Alt-M key combination to toggle between using the terminal's mouse services and nano's built-in mouse services.\n(https://help.ubuntu.com/community/Nano)"}
{"instruction": "Single line create file with content", "output": "You can use \"echo\" in bash. e.g.:\n\n```\necho \"some text here\" > file.txt\n\n```\n\nIf you don't want a new line character at the end of the file, use the -n argument:\n\n```\necho -n \"some text here\" > file.txt\n\n```"}
{"instruction": "nano editor line breaks", "output": "Ctrl+J and the lines \"broken\"."}
{"instruction": "Nano. Copy (Alt+6) Doesn&#39;t work", "output": "After marking your desired text, copy it by entering (alt+shift+6) or (alt+^)."}
{"instruction": "Nano 2.0.6 - Saving a file in Mac Format", "output": "The command \"M-M\" means \"alt M\". In fact, any command starting with \"M\" means the alt key. Some keyboards gobble up the alt key: in my case, on a Macbook Pro Retina, \"esc\" followed by \"M\" is what worked. More info here.\nHowever, since your screenshot is showing [DOS Format] as the current selection, the key combination you really want is \"alt D\" (or \"esc\" followed by \"D\"). That will toggle off [DOS Format] so that you'll be saving your file with the default line-ending character (ASCII 0xA). Turning on [Mac Format] will just give you a new set of problems."}
{"instruction": "How to disable wrapping lines in nano?", "output": "To change persistent settings for nano, we can edit the user-specific .nanorc file (/home/user/.nanorc). This file contains configuration settings that nano loads on start-up. \nTo disable line-wrapping, we need to specify the nowrap directive. Add this line to .nanorc: \n\n```\nset nowrap \n\n```\n\nWe may need to create this file if it doesn't exist. For more information about the available settings in this file, see the .nanorc man page: \n\n```\n$ man nanorc\n\n```"}
{"instruction": "Launch nano editor passing piped command", "output": "Use process substitution:\n\n```\nnano <(cat /var/log/qmail/current | tai64nlocal)\n\n```\n\nAlso, you don't need to use cat\n\n```\nnano <(tai64nlocal < /var/log/qmail/current)\n\n```"}
{"instruction": "Passing file directly to nano fails with sighup or sigterm", "output": "i found the solution in using find intern exec function\n\n```\n# find -name file.php -exec nano {} \\;\n\n```"}
{"instruction": "I used to use nano to edit text but I switched to vim, which is more powerful. Will I get the same power up if I move to emacs?", "output": "I have only started to use EMACS, but my succession was like that - our local editor (home-MSU-made), then i have used vi/vim for several years or sometimes far editor, and finally 3 years ago I've switched to EMACS. The basic commands are learned quite fast, and then the rest of your life you just master it, every day discovering the faster way to do something. There's quite useful tutorial for the first steps in EMACS. Obtaining basic knowledge of LISP is quite fast too, just to customize. But there's also a user interface in EMACS to change preferences and automatically write it to the options file.\nI must admit that i have not tried the modern capabilities of vim, I've heard that in functionality employed by a programmer vim and emacs are very close. So finally it shall be a matter of taste - which of the two to use. Personally i more like LISP than VIMscript:)\nProbably there will be someone who has changed from vim to EMACS or vice versa - their opinion about pros and contras will be more mature than mine."}
{"instruction": "What is a quick way to edit a remote file on Linux?", "output": "Force Pseudo-TTY Allocation\nYou can force pseudo-tty allocation with one or more -t flags. The SSH(1) man page says:\n\n```\n -t      Force pseudo-tty allocation.  This can be used to execute arbi-\n         trary screen-based programs on a remote machine, which can be\n         very useful, e.g. when implementing menu services.  Multiple -t\n         options force tty allocation, even if ssh has no local tty.\n\n```\n\nExample\nUsing your own example, slightly modified, the following would work if you have a local TTY or PTY:\n\n```\nssh -t user1@computer1 'vim /path/laboratory_notebook_1.md'\n\n```\n\nIt works fine for me with OpenSSH_6.2p2. Your mileage (and operating environment) may vary."}
{"instruction": "How to fix /etc/sudoers: syntax error near line number", "output": "```\nwe have to write -\npkexec visudo\nAnd it will open the file and one need to change as previous if any made.\nThen type Ctrl+X which will ask to save the file. So type: Y and Enter\n\n```"}
{"instruction": "How to change default editor for crontab in CentOS7?", "output": "The export works like you see, but if you use sudo, you're running crontab as other user, in this case as root, then you need set EDITOR as root too.\n(I post as answer because I can't comment)"}
{"instruction": "nano: update a file after selecting it using the file browser", "output": "I have tried your problem and indeed found that nano unable to \"remember\" the filename. My suspicion is that, when you open the editor just by typing \"nano\" in the command line, it only reads the content of that file, and paste it to the current buffer (without also making a new buffer for opening that file).\nSo try to do this: \n1. Open nano just by typing nano in terminal \n2. Then type alt + F to enable nano to open file into separate buffer \n3. Read your file of interest from nano by ctrl + R, then ctrl + T \n4. Make some modification to the file, and try to save it, It should know the filename of the original file now!"}
{"instruction": "bash ps print info about process with name", "output": "You can always use the two-stage approach.\n1.) find the wanted PIDs. For this use the simplest possible ps\n\n```\nps -o pid,comm | grep \"$2\" | cut -f1 -d' '\n\n```\n\nthe ps -o pid,comm prints only two columns, like:\n\n```\n67676 -bash\n71548 -bash\n71995 -bash\n72219 man\n72220 sh\n72221 sh\n72225 sh\n72227 /usr/bin/less\n74364 -bash\n\n```\n\nso grepping it is easy (and noise-less, without false triggers). The cut just extracts the PIDs. E.g. the \n\n```\nps -o pid,comm | grep bash | cut -f1 -d' '\n\n```\n\nprints\n\n```\n67676\n71548\n71995\n74364\n\n```\n\n2.) and now you can feed the found PIDs to the another ps using the -p flag, so the full command is:\n\n```\nps -o uid,pid,ppid,ni,vsz,rss,stat,tty,time,command -p $(ps -o pid,comm | grep bash | cut -f1 -d' ')\n\n```\n\noutput\n\n```\n  UID   PID  PPID NI      VSZ    RSS STAT TTY           TIME COMMAND\n  501 67676 67675  0  2499876   7212 S+   ttys000    0:00.04 -bash\n  501 71548 71547  0  2500900   8080 S    ttys001    0:01.81 -bash\n  501 71995 71994  0  2457892   3616 S    ttys002    0:00.04 -bash\n  501 74364 74363  0  2466084   7176 S+   ttys003    0:00.06 -bash\n\n```\n\ne.g. the solution using the $2 is\n\n```\nps -o uid,pid,ppid,ni,vsz,rss,stat,tty,time,command -p $(ps -o pid,comm | grep \"$2\" | cut -f1 -d' ')\n\n```"}
{"instruction": "Nano syntax color highlighting in windows", "output": "The previous answer is outdated and also wrong.\nMost of the problem with the coloring is due to the fact that the native build windows version doesn't support windows paths, as it's using GNU's glob(). So you need to use forward POSIX style paths in your syntax include statement in the .nanorc file.\nHere is the entire install procedure:\n\nDownload the latest Nano build from here OR here.\n\nDownload the latest syntax highlighter files *.nanorc from here\n\nChose an installation location. For example in C:\\nano\\.\n\nExtract the contents into that directory and move it around so that you have:\n\n\n\n```\nC:\\nano\\bin\\       # For the nano.exe\nC:\\nano\\nanorc\\    # For all the *.nanorc files\nC:\\nano\\doc\\       # For the documentation files\n\n```\n\n\nPut the .nanorc into your home directory in: C:\\Users\\<username>\\.nanorc.\n\nAdd the nano file paths to your System Path:\n\n\n\n```\n# To set and update the Windows (System) \"Path\" in Powershell, use:\n[System.Environment]::SetEnvironmentVariable(\"PATH\", $Env:Path + \";C:\\nano\\bin\", \"Machine\")\n\n```\n\n\nReboot, restart explorer, or install refreshenv from choco.\n\nOptional: Run nano once, to ensure that a filepos_history file is created.\n\nYou probably want to be able to run Nano with both normal or Administrator privileges, but not having to keep track of more edit locations and 2nd config files. To do this, you need to symlink your own versions of the nano config and history setting files, to the admin versions.\n\n\n\n```\n# Link last cursor position files:\nNew-Item -ItemType SymbolicLink -Path \"C:\\ProgramData\\.local\\share\\nano\\filepos_history\" -Target  \"C:\\Users\\<username>\\.local\\share\\nano\\filepos_history\" -Force\n\n# Link .nanorc files:\nNew-Item -ItemType SymbolicLink -Path \"C:\\ProgramData\\.nanorc\" -Target  \"C:\\Users\\<username>\\.nanorc\" -Force\n\n```\n\n\nIMPORTANT!\nEdit your .nanorc to include the correct POSIX paths to all your *.nanorc files.\n\n\n```\n# Why not use nano to edit your .nanorc\ncd ~\nnano .nanorc\n\n# Add the following line(s):\n#include \"C:\\nano\\nanorc\\*.nanorc\"     # Windows paths does NOT work!\ninclude \"/nano/nanorc/*.nanorc\"        # This works!\n\n```\n\nEnjoy!"}
{"instruction": "How to delete several lines containing only whitespaces in nano?", "output": "Use global search and replace with the expression ^\\s+$.\n\nSwitch to search and replace mode with C-\\.\nSwitch to regex mode with Alt-R.\nType the expression ^\\s+$.\nHit Enter to replace with an empty string.\nOn the match, hit A for All."}
{"instruction": "How To Open A File Using Text Edit In Terminal in MacOS", "output": "Here is the solution, I know it's a bit tedious but, you can open the ~/.zshrc using this command :\n\n```\nopen -a TextEdit ~/.zshrc\n\n```"}
{"instruction": "dejustify (unjustify): replace each *single* linefeed with a space, but don&#39;t touch groups of linefeeds. sed, awk, or something else?", "output": "Using any awk:\n\n```\n$ awk -v RS= -v ORS='\\n\\n' -F'\\n' '{$1=$1} 1' file\none two three four five six\n\nseven eight nine ten\n\n\n```\n\nBreaking it down:\n\n-v RS= treat the input as [possibly multi-line] records separated by 1 or more empty lines.\n-v ORS='\\n\\n' put 2 newlines at the end of each output record.\n-F'\\n' set the field separator to a newline so that ONLY newlines get replaced in the next step, otherwise all chains of contiguous white space within each record would be replaced.\n{$1=$1} update the value of a field, $1, thereby causing awk to rebuild the current record replacing all strings that match the FS (a newline) with an OFS (a blank char).\n1 a true condition causing awk to execute it's default action of printing the current record.\n\nThe above will print a blank line at the end of the output, if that's a problem you can always do this instead:\n\n```\n$ awk -v RS= -F'\\n' 'NR>1{print \"\"} {$1=$1} 1' file\none two three four five six\n\nseven eight nine ten\n\n```\n\nwhich prints a blank line before each record except the first instead of printing a blank line after every record:\n\nNR>1{print \"\"} if this is the second or subsequent record then print a blank line before it."}
{"instruction": "nano settings are not adopted from nanorc file", "output": "I was having the same issue after I installed nano using brew (Monterey on M1). It turned out that nano command is by default symlinked to pico editor in /usr/bin, see https://ss64.com/osx/pico.html.\nYou can try checking if that is the case for you by using which nano. In my case it was pointing to /usr/bin/nano which is actually just symlink to pico (you can check this with readlink /usr/bin/nano)\nThis is probably an issue with homebrew, check this post Homebrew: Could not symlink, /usr/local/bin is not writable\nAs a quick fix (to verify if this is actually the case) you can just create a symlink pointing to homebrew installation of nano: ln -s /opt/homebrew/bin/nano /usr/local/bin/nano. \nThis should open nano instead of pico when using nano command and settings in .nanorc should now also be taken into account."}
{"instruction": "difference between DOS Format and MAC Format", "output": "If you just press enter and no option, you save it in the same file-format which was detected. (That is the third option, and the option to prefer for Linux.)\nIf you selected an option you can deselect it again with pressing again \"Alt\" & \"m\" for deselecting [MAC-Format] or \"Alt\" & \"d\" for deselecting [DOS-Format]. If there is no option selected you use the UNIX-Format \"line flyback\" (LF or \\n), which is most likely the format to prefer on Linux. The selected option is written just before the file name in the bottom.\nsee also: https://unix.stackexchange.com/questions/396551/gnu-nano-2-dos-format-or-mac-format-on-linux"}
{"instruction": "Nano on server ignores certain syntax coloring", "output": "I just ran into the same problem, and I fiddled around a bit with the includes to find the error. Surprisingly, turns out that changing the inclusion order fixed the issue:\nThis works:\n\n```\ninclude \"~/.nano/nanorc.nanorc\"\ninclude \"~/.nano/sh.nanorc\"\n# more includes...\n\n```\n\nThis fails to highlight sh files:\n\n```\ninclude \"~/.nano/sh.nanorc\"\ninclude \"~/.nano/nanorc.nanorc\"\n# more includes...\n\n```\n\nSo I guess it's probably a bug (in nano 2.2.2; worked fine in nano 2.1.7)"}
{"instruction": "Special characters not being displayed properly inside text editors on a docker container", "output": "Okay so I've got it to work by adding this line to the Dockerfile, it's to do with the locale not being set properly on Centos7.\nENV LANG en_us.utf-8"}
{"instruction": "Long-line handling from Nano in Vim?", "output": "Vim is a fixed-width text editor. The problem with your suggested visualization is that when moving to a previous / next line (e.g. via k / j), the cursor would \"jump\" horizontally on the screen, instead of going straight up.\nOne could get accustomed to that (the cursor already jumps when the new line is shorter (unless 'virtualedit' is set)), but how would a blockwise visual selection be represented? With jagged edges on both sides?\nThat's why it's unlikely to be accepted, but don't let this discourage anyone from writing the (non-trivial) patch :-)"}
{"instruction": "Nano syntax highlighting rules for D", "output": "I've just made one, for you and me (since I don't believe that anyone else in the world uses nano for D :-)\nHave a look: https://github.com/vinesworth/d-nanorc"}
{"instruction": "How to save file in crontab using Ubuntu", "output": "Try to do as follow:\nUse VI as editor:\n\n```\nexport VISUAL=vi\ncrontab -e\n\n```\n\npress i for write; than press ESC and :wq for save and exit."}
{"instruction": "Why does every text editor write an additional byte (UTF-8)?", "output": "You are seeing a newline character (often expressed in programming languages as \\n, in ASCII it is hex 0a, decimal 10):\n\n```\n$ echo 'foo' > /tmp/test.txt\n$ xxd /tmp/test.txt\n00000000: 666f 6f0a                                foo.\n\n```\n\nThe hex-dump tool xxd shows that the file consists of 4 bytes, hex 66 (ASCII lowercase f), two times hex 65 (lowercase letter o) and the newline.\nYou can use the -n command-line switch to disable adding the newline:\n\n```\n$ echo -n 'foo' > /tmp/test.txt\n$ xxd /tmp/test.txt\n00000000: 666f 6f                                  foo\n\n```\n\nor you can use printf instead (which is more POSIX compliant):\n\n```\n$ printf 'foo' > /tmp/test.txt\n$ xxd /tmp/test.txt\n00000000: 666f 6f                                  foo\n\n```\n\nAlso see 'echo' without newline in a shell script.\nMost text editors will also add a newline to the end of a file; how to prevent this depends on the exact editor (often you can just use delete at the end of the file before saving). There are also various command-line options to remove the newline after the fact, see How can I delete a newline if it is the last character in a file?.\nText editors generally add a newline because they deal with text lines, and the POSIX standard defines that text lines end with a newline:\n\n3.206 Line\n  A sequence of zero or more non- <newline> characters plus a terminating <newline> character.\n\nAlso see Why should text files end with a newline?"}
{"instruction": "Why nano and rnano are different?", "output": "They behave differently because of the argument vector (argv), whose first element (argv[0]) contains the name of the file being executed.\nBecause rnano is a separate file from nano (even though it is just a symbolic link), it has its own, separate argv[0].\nYou can see this check in nano.c's main function:\n\n```\n/* If the executable filename starts with 'r', enable restricted\n * mode.\n */\nif (*(tail(argv[0])) == 'r')\n    SET(RESTRICTED);\n\n```\n\nYou can also test this with a simple shell script. Create a shell script with only one statement, echo $0. Then, create a symbolic link to it with a separate name. Observe the difference."}
{"instruction": "Why does writing to file change the content (hash)?", "output": "Most text editors, including Vim, save the file with a newline at the end when you quit.  That's because according to POSIX, a text file is either empty or ends with a newline, so most users expect and want that behavior.\nAs you've noticed, adding a newline at the end changes the hash, and the hash you're getting is consistent with the sequence foo\\n, where \\n is a newline.\nIf you have a recent enough Vim, you can control the behavior mentioned above by setting nofixeol, which will preserve the existing line ending, or lack thereof, on the last line."}
{"instruction": "Redirect stdout into buffer editor before piping to other commands", "output": "The semi-colon is a perfectly acceptable operator.\n\n```\ngit branch -l > /tmp/branches; nano /tmp/branches; whatever /tmp/branches; rm /tmp/branches\n\n```"}
{"instruction": "Prevent nano from creating save-file when SIGHUP or SIGTERM is received", "output": "According to the man page: https://www.nano-editor.org/dist/v2.2/nano.1.html\n\n‚àíR (‚àí‚àírestricted)\n  Restricted mode: don‚Äôt read or write to any file not specified on the command line; read any nanorc files; allow suspending; allow a file to be appended to, prepended to, or saved under a different name if it already has one; or use backup files or spell checking. Also accessible by invoking nano with any name beginning with ‚Äôr‚Äô (e.g. \"rnano\").\n\nSo one solution is to simply use \"rnano\" or \"nano -R\" instead."}
{"instruction": "How do I list all cron jobs for all users?", "output": "I ended up writing a script (I'm trying to teach myself the finer points of bash scripting, so that's why you don't see something like Perl here). It's not exactly a simple affair, but it does most of what I need. It uses Kyle's suggestion for looking up individual users' crontabs, but also deals with /etc/crontab (including the scripts launched by run-parts in /etc/cron.hourly, /etc/cron.daily, etc.) and the jobs in the /etc/cron.d directory. It takes all of those and merges them into a display something like the following:\n\n```\nmi     h    d  m  w  user      command\n09,39  *    *  *  *  root      [ -d /var/lib/php5 ] && find /var/lib/php5/ -type f -cmin +$(/usr/lib/php5/maxlifetime) -print0 | xargs -r -0 rm\n47     */8  *  *  *  root      rsync -axE --delete --ignore-errors / /mirror/ >/dev/null\n17     1    *  *  *  root      /etc/cron.daily/apt\n17     1    *  *  *  root      /etc/cron.daily/aptitude\n17     1    *  *  *  root      /etc/cron.daily/find\n17     1    *  *  *  root      /etc/cron.daily/logrotate\n17     1    *  *  *  root      /etc/cron.daily/man-db\n17     1    *  *  *  root      /etc/cron.daily/ntp\n17     1    *  *  *  root      /etc/cron.daily/standard\n17     1    *  *  *  root      /etc/cron.daily/sysklogd\n27     2    *  *  7  root      /etc/cron.weekly/man-db\n27     2    *  *  7  root      /etc/cron.weekly/sysklogd\n13     3    *  *  *  archiver  /usr/local/bin/offsite-backup 2>&1\n32     3    1  *  *  root      /etc/cron.monthly/standard\n36     4    *  *  *  yukon     /home/yukon/bin/do-daily-stuff\n5      5    *  *  *  archiver  /usr/local/bin/update-logs >/dev/null\n\n```\n\nNote that it shows the user, and more-or-less sorts by hour and minute so that I can see the daily schedule.\nSo far, I've tested it on Ubuntu, Debian, and Red Hat AS.\n\n```\n#!/bin/bash\n\n# System-wide crontab file and cron job directory. Change these for your system.\nCRONTAB='/etc/crontab'\nCRONDIR='/etc/cron.d'\n\n# Single tab character. Annoyingly necessary.\ntab=$(echo -en \"\\t\")\n\n# Given a stream of crontab lines, exclude non-cron job lines, replace\n# whitespace characters with a single space, and remove any spaces from the\n# beginning of each line.\nfunction clean_cron_lines() {\n    while read line ; do\n        echo \"${line}\" |\n            egrep --invert-match '^($|\\s*#|\\s*[[:alnum:]_]+=)' |\n            sed --regexp-extended \"s/\\s+/ /g\" |\n            sed --regexp-extended \"s/^ //\"\n    done;\n}\n\n# Given a stream of cleaned crontab lines, echo any that don't include the\n# run-parts command, and for those that do, show each job file in the run-parts\n# directory as if it were scheduled explicitly.\nfunction lookup_run_parts() {\n    while read line ; do\n        match=$(echo \"${line}\" | egrep -o 'run-parts (-{1,2}\\S+ )*\\S+')\n\n        if [[ -z \"${match}\" ]] ; then\n            echo \"${line}\"\n        else\n            cron_fields=$(echo \"${line}\" | cut -f1-6 -d' ')\n            cron_job_dir=$(echo  \"${match}\" | awk '{print $NF}')\n\n            if [[ -d \"${cron_job_dir}\" ]] ; then\n                for cron_job_file in \"${cron_job_dir}\"/* ; do  # */ <not a comment>\n                    [[ -f \"${cron_job_file}\" ]] && echo \"${cron_fields} ${cron_job_file}\"\n                done\n            fi\n        fi\n    done;\n}\n\n# Temporary file for crontab lines.\ntemp=$(mktemp) || exit 1\n\n# Add all of the jobs from the system-wide crontab file.\ncat \"${CRONTAB}\" | clean_cron_lines | lookup_run_parts >\"${temp}\" \n\n# Add all of the jobs from the system-wide cron directory.\ncat \"${CRONDIR}\"/* | clean_cron_lines >>\"${temp}\"  # */ <not a comment>\n\n# Add each user's crontab (if it exists). Insert the user's name between the\n# five time fields and the command.\nwhile read user ; do\n    crontab -l -u \"${user}\" 2>/dev/null |\n        clean_cron_lines |\n        sed --regexp-extended \"s/^((\\S+ +){5})(.+)$/\\1${user} \\3/\" >>\"${temp}\"\ndone < <(cut --fields=1 --delimiter=: /etc/passwd)\n\n# Output the collected crontab lines. Replace the single spaces between the\n# fields with tab characters, sort the lines by hour and minute, insert the\n# header line, and format the results as a table.\ncat \"${temp}\" |\n    sed --regexp-extended \"s/^(\\S+) +(\\S+) +(\\S+) +(\\S+) +(\\S+) +(\\S+) +(.*)$/\\1\\t\\2\\t\\3\\t\\4\\t\\5\\t\\6\\t\\7/\" |\n    sort --numeric-sort --field-separator=\"${tab}\" --key=2,1 |\n    sed \"1i\\mi\\th\\td\\tm\\tw\\tuser\\tcommand\" |\n    column -s\"${tab}\" -t\n\nrm --force \"${temp}\"\n\n```"}
{"instruction": "Restarting cron after changing crontab file?", "output": "No.\nFrom the cron man page:\n\n...cron will then examine the modification time on all crontabs \n    and reload those which have changed.  Thus cron need not be restarted \n    whenever a crontab file is modified\n\nBut if you just want to make sure its done anyway,\n\n```\nsudo service cron reload\n\n```\n\nor\n\n```\n/etc/init.d/cron reload\n\n```"}
{"instruction": "How to run a cron job inside a docker container?", "output": "You can copy your crontab into an image, in order for the container launched from said image to run the job.\n\nImportant: as noted in docker-cron issue 3: use LF, not CRLF for your cron file.\n\nSee \"Run a cron job with Docker\" from Julien Boulay in his Ekito/docker-cron:\n\nLet‚Äôs create a new file called \"hello-cron\" to describe our job.\n\n\n```\n# must be ended with a new line \"LF\" (Unix) and not \"CRLF\" (Windows)\n* * * * * echo \"Hello world\" >> /var/log/cron.log 2>&1\n# An empty line is required at the end of this file for a valid cron file.\n\n```\n\nIf you are wondering what is 2>&1, Ayman Hourieh explains.\n\nThe following Dockerfile describes all the steps to build your image\n\n\n```\nFROM ubuntu:latest\nMAINTAINER docker@ekito.fr\n\nRUN apt-get update && apt-get -y install cron\n\n# Copy hello-cron file to the cron.d directory\nCOPY hello-cron /etc/cron.d/hello-cron\n \n# Give execution rights on the cron job\nRUN chmod 0644 /etc/cron.d/hello-cron\n\n# Apply cron job\nRUN crontab /etc/cron.d/hello-cron\n \n# Create the log file to be able to run tail\nRUN touch /var/log/cron.log\n \n# Run the command on container startup\nCMD cron && tail -f /var/log/cron.log\n\n```\n\nBut: if cron dies, the container keeps running.\n(see Gaafar's comment and How do I make apt-get install less noisy?:\napt-get -y install -qq --force-yes cron can work too)\nAs noted by Nathan Lloyd in the comments:\n\nQuick note about a gotcha:\nIf you're adding a script file and telling cron to run it, remember to\nRUN chmod 0744 /the_script\nCron fails silently if you forget.\n\nWarning: as noted in the comments by user8046323\n\nThis config schedules tasks two times.\n\nOne time with crontab and\none time with cron.d\n\nPlease use only one of the ways to evade scheduling your tasks twice\n\nTrue: the problem is with those two lines in the above Dockerfile:\n\n```\nCOPY hello-cron /etc/cron.d/hello-cron\nRUN crontab /etc/cron.d/hello-cron\n\n```\n\n\nBy placing the hello-cron file in the /etc/cron.d directory, you automatically schedule the cron jobs contained in this file. The cron daemon checks this directory for any files containing cron schedules and automatically loads them.\n\nThe crontab command with /etc/cron.d/hello-cron takes the contents of the hello-cron file and loads them into the main crontab. This means the same jobs are now scheduled directly in the crontab as well, effectively duplicating them.\n\n\nyou should choose one method to manage your cron jobs, depending on your specific needs:\n\nIf you prefer using /etc/cron.d (often easier for managing multiple separate cron job files):\n\n```\nCOPY hello-cron /etc/cron.d/hello-cron\nRUN chmod 0644 /etc/cron.d/hello-cron\n\n```\n\n\nIf you prefer using crontab (gives you a consolidated view of all cron jobs and can be easier for a single or a few jobs):\n\n```\nADD hello-cron /etc/cronjob\nRUN crontab /etc/cronjob\n\n```\n\n\n\n\nOR, make sure your job itself redirect directly to stdout/stderr instead of a log file, as described in hugoShaka's answer:\n\n```\n * * * * * root echo hello > /proc/1/fd/1 2>/proc/1/fd/2\n\n```\n\nReplace the last Dockerfile line with\n\n```\nCMD [\"cron\", \"-f\"]\n\n```\n\nBut: it doesn't work if you want to run tasks as a non-root.\nSee also (about cron -f, which is to say cron \"foreground\") \"docker ubuntu cron -f is not working\"\n\nBuild and run it:\n\n```\nsudo docker build --rm -t ekito/cron-example .\nsudo docker run -t -i ekito/cron-example\n\n```\n\n\nBe patient, wait for 2 minutes and your command-line should display:\n\n```\nHello world\nHello world\n\n```\n\n\n\nEric adds in the comments:\n\nDo note that tail may not display the correct file if it is created during image build.\nIf that is the case, you need to create or touch the file during container runtime in order for tail to pick up the correct file.\n\nSee \"Output of tail -f at the end of a docker CMD is not showing\".\n\nSee more in \"Running Cron in Docker\" (Apr. 2021) from Jason Kulatunga, as he commented below\nSee Jason's image AnalogJ/docker-cron based on:\n\nDockerfile installing cronie/crond, depending on distribution.\n\nan entrypoint initializing /etc/environment and then calling\n\n```\ncron -f -l 2\n\n```"}
{"instruction": "How to create a cron job using Bash automatically without the interactive editor?", "output": "You can add to the crontab as follows:\n\n```\n#write out current crontab\ncrontab -l > mycron\n#echo new cron into cron file\necho \"00 09 * * 1-5 echo hello\" >> mycron\n#install new cron file\ncrontab mycron\nrm mycron\n\n```\n\n\nCron line explaination\n\n```\n* * * * * \"command to be executed\"\n- - - - -\n| | | | |\n| | | | ----- Day of week (0 - 7) (Sunday=0 or 7)\n| | | ------- Month (1 - 12)\n| | --------- Day of month (1 - 31)\n| ----------- Hour (0 - 23)\n------------- Minute (0 - 59)\n\n```\n\nSource nixCraft."}
{"instruction": "How to pass in password to pg_dump?", "output": "Create a .pgpass file in the home directory of the account that pg_dump will run as.\nThe format is:\n\n```\nhostname:port:database:username:password\n\n```\n\nThen, set the file's mode to 0600. Otherwise, it will be ignored.\n\n```\nchmod 600 ~/.pgpass\n\n```\n\nSee the Postgresql documentation libpq-pgpass for more details."}
{"instruction": "How do I get a Cron like scheduler in Python?", "output": "If you're looking for something lightweight checkout schedule:\n\n```\nimport schedule\nimport time\n\ndef job():\n    print(\"I'm working...\")\n\nschedule.every(10).minutes.do(job)\nschedule.every().hour.do(job)\nschedule.every().day.at(\"10:30\").do(job)\n\nwhile 1:\n    schedule.run_pending()\n    time.sleep(1)\n\n```\n\nDisclosure: I'm the author of that library."}
{"instruction": "Running a cron every 30 seconds", "output": "You have */30 in the minutes specifier - that means every minute but with a step of 30 (in other words, every half hour). Since cron does not go down to sub-minute resolutions, you will need to find another way.\nOne possibility, though it's a bit of a kludge(a), is to have two jobs, one offset by 30 seconds:\n\n```\n# Need these to run on 30-sec boundaries, keep commands in sync.\n* * * * *              /path/to/executable param1 param2\n* * * * * ( sleep 30 ; /path/to/executable param1 param2 )\n\n```\n\nYou'll see I've added comments and formatted to ensure it's easy to keep them synchronised.\nBoth cron jobs actually run every minute but the latter one will wait half a minute before executing the \"meat\" of the job, /path/to/executable.\nFor other (non-cron-based) options, see the other answers here, particularly the ones mentioning fcron and systemd. These are probably preferable assuming your system has the ability to use them (such as installing fcron or having a distro with systemd in it).\n\nIf you don't want to use the kludgy solution, you can use a loop-based solution with a small modification. You'll still have to manage keeping your process running in some form but, once that's sorted, the following script should work:\n\n```\n#!/bin/env bash\n\n# Debug code to start on minute boundary and to\n# gradually increase maximum payload duration to\n# see what happens when the payload exceeds 30 seconds.\n\n((maxtime = 20))\nwhile [[ \"$(date +%S)\" != \"00\" ]]; do true; done\n\nwhile true; do\n    # Start a background timer BEFORE the payload runs.\n\n    sleep 30 &\n\n    # Execute the payload, some random duration up to the limit.\n    # Extra blank line if excess payload.\n\n    ((delay = RANDOM % maxtime + 1))\n    ((maxtime += 1))\n    echo \"$(date) Sleeping for ${delay} seconds (max ${maxtime}).\"\n    [[ ${delay} -gt 30 ]] && echo\n    sleep ${delay}\n\n    # Wait for timer to finish before next cycle.\n\n    wait\ndone\n\n```\n\nThe trick is to use a sleep 30 but to start it in the background before your payload runs. Then, after the payload is finished, just wait for the background sleep to finish.\nIf the payload takes n seconds (where n <= 30), the wait after the payload will then be 30 - n seconds. If it takes more than 30 seconds, then the next cycle will be delayed until the payload is finished, but no longer.\nYou'll see that I have debug code in there to start on a one-minute boundary to make the output initially easier to follow. I also gradually increase the maximum payload time so you'll eventually see the payload exceed the 30-second cycle time (an extra blank line is output so the effect is obvious).\nA sample run follows (where cycles normally start 30 seconds after the previous cycle):\n\n```\nTue May 26 20:56:00 AWST 2020 Sleeping for 9 seconds (max 21).\nTue May 26 20:56:30 AWST 2020 Sleeping for 19 seconds (max 22).\nTue May 26 20:57:00 AWST 2020 Sleeping for 9 seconds (max 23).\nTue May 26 20:57:30 AWST 2020 Sleeping for 7 seconds (max 24).\nTue May 26 20:58:00 AWST 2020 Sleeping for 2 seconds (max 25).\nTue May 26 20:58:30 AWST 2020 Sleeping for 8 seconds (max 26).\nTue May 26 20:59:00 AWST 2020 Sleeping for 20 seconds (max 27).\nTue May 26 20:59:30 AWST 2020 Sleeping for 25 seconds (max 28).\nTue May 26 21:00:00 AWST 2020 Sleeping for 5 seconds (max 29).\nTue May 26 21:00:30 AWST 2020 Sleeping for 6 seconds (max 30).\nTue May 26 21:01:00 AWST 2020 Sleeping for 27 seconds (max 31).\nTue May 26 21:01:30 AWST 2020 Sleeping for 25 seconds (max 32).\nTue May 26 21:02:00 AWST 2020 Sleeping for 15 seconds (max 33).\nTue May 26 21:02:30 AWST 2020 Sleeping for 10 seconds (max 34).\nTue May 26 21:03:00 AWST 2020 Sleeping for 5 seconds (max 35).\nTue May 26 21:03:30 AWST 2020 Sleeping for 35 seconds (max 36).\n\nTue May 26 21:04:05 AWST 2020 Sleeping for 2 seconds (max 37).\nTue May 26 21:04:35 AWST 2020 Sleeping for 20 seconds (max 38).\nTue May 26 21:05:05 AWST 2020 Sleeping for 22 seconds (max 39).\nTue May 26 21:05:35 AWST 2020 Sleeping for 18 seconds (max 40).\nTue May 26 21:06:05 AWST 2020 Sleeping for 33 seconds (max 41).\n\nTue May 26 21:06:38 AWST 2020 Sleeping for 31 seconds (max 42).\n\nTue May 26 21:07:09 AWST 2020 Sleeping for 6 seconds (max 43).\n\n```\n\nIf you want to avoid the kludgy solution, this is probably better. You'll still need a cron job (or equivalent) to periodically detect if this script is running and, if not, start it. But the script itself then handles the timing.\n\n(a) Some of my workmates would say that kludges are my specialty :-)"}
{"instruction": "Where can I set environment variables that crontab will use?", "output": "Have 'cron' run a shell script that sets the environment before running the command.\nAlways.\n\n```\n#   @(#)$Id: crontab,v 4.2 2007/09/17 02:41:00 jleffler Exp $\n#   Crontab file for Home Directory for Jonathan Leffler (JL)\n#-----------------------------------------------------------------------------\n#Min     Hour    Day     Month   Weekday Command\n#-----------------------------------------------------------------------------\n0        *       *       *       *       /usr/bin/ksh /work1/jleffler/bin/Cron/hourly\n1        1       *       *       *       /usr/bin/ksh /work1/jleffler/bin/Cron/daily\n23       1       *       *       1-5     /usr/bin/ksh /work1/jleffler/bin/Cron/weekday\n2        3       *       *       0       /usr/bin/ksh /work1/jleffler/bin/Cron/weekly\n21       3       1       *       *       /usr/bin/ksh /work1/jleffler/bin/Cron/monthly\n\n```\n\nThe scripts in ~/bin/Cron are all links to a single script, 'runcron', which looks like:\n\n```\n:       \"$Id: runcron.sh,v 2.1 2001/02/27 00:53:22 jleffler Exp $\"\n#\n#       Commands to be performed by Cron (no debugging options)\n\n#       Set environment -- not done by cron (usually switches HOME)\n. $HOME/.cronfile\n\nbase=`basename $0`\ncmd=${REAL_HOME:-/real/home}/bin/$base\n\nif [ ! -x $cmd ]\nthen cmd=${HOME}/bin/$base\nfi\n\nexec $cmd ${@:+\"$@\"}\n\n```\n\n(Written using an older coding standard - nowadays, I'd use a shebang '#!' at the start.)\nThe '~/.cronfile' is a variation on my profile for use by cron - rigorously non-interactive and no echoing for the sake of being noisy.  You could arrange to execute the .profile and so on instead.  (The REAL_HOME stuff is an artefact of my environment - you can pretend it is the same as $HOME.)\nSo, this code reads the appropriate environment and then executes the non-Cron version of the command from my home directory.  So, for example, my 'weekday' command looks like:\n\n```\n:       \"@(#)$Id: weekday.sh,v 1.10 2007/09/17 02:42:03 jleffler Exp $\"\n#\n#       Commands to be done each weekday\n\n# Update ICSCOPE\nn.updics\n\n```\n\nThe 'daily' command is simpler:\n\n```\n:       \"@(#)$Id: daily.sh,v 1.5 1997/06/02 22:04:21 johnl Exp $\"\n#\n#       Commands to be done daily\n\n# Nothing -- most things are done on weekdays only\n\nexit 0\n\n```"}
{"instruction": "Running a cron job at 2:30 AM every day", "output": "```\ncrontab -e\n\n```\n\nadd:\n\n```\n30 2 * * * /your/command\n\n```"}
{"instruction": "How do I schedule jobs in Jenkins?", "output": "By setting the schedule period to 15 13 * * * you tell Jenkins to schedule the build every day of every month of every year at the 15th minute of the 13th hour of the day.\nJenkins used a cron expression (official documentation), and the different fields are:\n\nMINUTES   Minutes in one hour (0-59)\nHOURS     Hours in one day (0-23)\nDAYMONTH  Day in a month (1-31)\nMONTH     Month in a year (1-12)\nDAYWEEK   Day of the week (0-7) where 0 and 7 are sunday\n\nIf you want to schedule your build every 5 minutes, this will do the job : */5 * * * *\nIf you want to schedule your build every day at 8h00, this will do the job : 0 8 * * *\nFor the past few versions (2014), Jenkins have a new parameter, H (extract from the Jenkins code documentation):\n\nTo allow periodically scheduled tasks to produce even load on the system, the symbol H (for ‚Äúhash‚Äù) should be used wherever possible.\nFor example, using 0 0 * * * for a dozen daily jobs will cause a large spike at midnight. In contrast, using H H * * * would still execute each job once a day, but not all at the same time, better using limited resources.\n\nNote also that:\n\nThe H symbol can be thought of as a random value over a range, but it actually is a hash of the job name, not a random function, so that the value remains stable for any given project.\n\nMore example of using 'H'"}
{"instruction": "Run Cron job every N minutes plus offset", "output": "To run a task every 20 minutes starting at 5 past the hour, try this:\n\n```\n 5-59/20 * * * *\n\n```\n\nExplanation\nAn * in the minute field is the same as 0-59/1 where 0-59 is the range and 1 is the step. The command will run at the first minute in the range (0), then at all successive minutes that are distant from the first by step (1), until the last (59).\nWhich is why */20 * * * * will run at 0 minutes, 20 minutes after, and 40 minutes after -- which is the same as every 20 minutes. However, */25 * * * * will run at 0 minutes, 25 minutes after, and 50 minutes after -- which is not the same as every 25 minutes. That's why it's usually desirable to use a step value in the minute field that divides evenly into 60.\nSo to offset the start time, specify the range explicitly and set the first value to the amount of the offset.\nExamples\n5-59/20 * * * * will run at 5 minutes after, 25 minutes after, and 45 minutes after.\n10-59/25 * * * * will run at 10 minutes after and 35 minutes after.\n1-59/2 * * * * will run every odd minute."}
{"instruction": "How to run crontab job every week on Sunday", "output": "Here is an explanation of the crontab format.\n\n```\n# 1. Entry: Minute when the process will be started [0-60]\n# 2. Entry: Hour when the process will be started [0-23]\n# 3. Entry: Day of the month when the process will be started [1-28/29/30/31]\n# 4. Entry: Month of the year when the process will be started [1-12]\n# 5. Entry: Weekday when the process will be started [0-6] [0 is Sunday]\n#\n# all x min = */x\n\n```\n\nSo according to this your 5 8 * * 0 would run 8:05 every Sunday."}
{"instruction": "Cron and virtualenv", "output": "You should be able to do this by using the python in your virtual environment:\n\n```\n/home/my/virtual/bin/python /home/my/project/manage.py command arg\n\n```\n\nEDIT: If your django project isn't in the PYTHONPATH, then you'll need to switch to the right directory:\n\n```\ncd /home/my/project && /home/my/virtual/bin/python ...\n\n```\n\nYou can also try to log the failure from cron:\n\n```\ncd /home/my/project && /home/my/virtual/bin/python /home/my/project/manage.py > /tmp/cronlog.txt 2>&1\n\n```\n\nAnother thing to try is to make the same change in your manage.py script at the very top:\n\n```\n#!/home/my/virtual/bin/python\n\n```"}
{"instruction": "How do I write a bash script to restart a process if it dies?", "output": "Avoid PID-files, crons, or anything else that tries to evaluate processes that aren't their children.\nThere is a very good reason why in UNIX, you can ONLY wait on your children.  Any method (ps parsing, pgrep, storing a PID, ...) that tries to work around that is flawed and has gaping holes in it.  Just say no.\nInstead you need the process that monitors your process to be the process' parent.  What does this mean?  It means only the process that starts your process can reliably wait for it to end.  In bash, this is absolutely trivial.\n\n```\nuntil myserver; do\n    echo \"Server 'myserver' crashed with exit code $?.  Respawning..\" >&2\n    sleep 1\ndone\n\n```\n\nOr to be able to stop it:\n\n```\ntrap 'kill $(jobs -p)' EXIT; until myserver & wait; do\n    echo \"ldap proxy crashed with exit code $?. Respawning..\" >&2\n    sleep 1\ndone\n\n```\n\nThe above piece of bash code runs myserver in an until loop.  The first line starts myserver and waits for it to end.  When it ends, until checks its exit status.  If the exit status is 0, it means it ended gracefully (which means you asked it to shut down somehow, and it did so successfully).  In that case we don't want to restart it (we just asked it to shut down!).  If the exit status is not 0, until will run the loop body, which emits an error message on STDERR and restarts the loop (back to line 1) after 1 second.\nWhy do we wait a second?  Because if something's wrong with the startup sequence of myserver and it crashes immediately, you'll have a very intensive loop of constant restarting and crashing on your hands.  The sleep 1 takes away the strain from that.\nNow all you need to do is start this bash script (asynchronously, probably), and it will monitor myserver and restart it as necessary.  If you want to start the monitor on boot (making the server \"survive\" reboots), you can schedule it in your user's cron(1) with an @reboot rule.  Open your cron rules with crontab:\n\n```\ncrontab -e\n\n```\n\nThen add a rule to start your monitor script:\n\n```\n@reboot /usr/local/bin/myservermonitor\n\n```\n\n\nAlternatively; look at inittab(5) and /etc/inittab.  You can add a line in there to have myserver start at a certain init level and be respawned automatically.\n\nEdit.\nLet me add some information on why not to use PID files.  While they are very popular; they are also very flawed and there's no reason why you wouldn't just do it the correct way.\nConsider this:\n\nPID recycling (killing the wrong process):\n\n/etc/init.d/foo start: start foo, write foo's PID to /var/run/foo.pid\nA while later: foo dies somehow.\nA while later: any random process that starts (call it bar) takes a random PID, imagine it taking foo's old PID.\nYou notice foo's gone: /etc/init.d/foo/restart reads /var/run/foo.pid, checks to see if it's still alive, finds bar, thinks it's foo, kills it, starts a new foo.\n\n\nPID files go stale.  You need over-complicated (or should I say, non-trivial) logic to check whether the PID file is stale, and any such logic is again vulnerable to 1..\n\nWhat if you don't even have write access or are in a read-only environment?\n\nIt's pointless overcomplication; see how simple my example above is.  No need to complicate that, at all.\n\n\nSee also: Are PID-files still flawed when doing it 'right'?\nBy the way; even worse than PID files is parsing ps!  Don't ever do this.\n\nps is very unportable.  While you find it on almost every UNIX system; its arguments vary greatly if you want non-standard output.  And standard output is ONLY for human consumption, not for scripted parsing!\nParsing ps leads to a LOT of false positives.  Take the ps aux | grep PID example, and now imagine someone starting a process with a number somewhere as argument that happens to be the same as the PID you stared your daemon with!  Imagine two people starting an X session and you grepping for X to kill yours.  It's just all kinds of bad.\n\nIf you don't want to manage the process yourself; there are some perfectly good systems out there that will act as monitor for your processes.  Look into runit, for example."}
{"instruction": "Using crontab to execute script every minute and another every 24 hours", "output": "every minute:\n* * * * * /path/to/php /var/www/html/a.php\nevery 24hours (every midnight):\n0 0 * * * /path/to/php /var/www/html/reset.php\nSee this reference for how crontab works: http://adminschoice.com/crontab-quick-reference, and this handy tool to build cron jobx: http://www.htmlbasix.com/crontab.shtml"}
{"instruction": "How to log cron jobs?", "output": "```\n* * * * * myjob.sh >> /var/log/myjob.log 2>&1\n\n```\n\nwill log all output from the cron job to /var/log/myjob.log\nYou might use mail to send emails. Most systems will send unhandled cron job output by email to root or the corresponding user."}
{"instruction": "Crontab Day of the Week syntax", "output": "0 and 7 both stand for Sunday, you can use the one you want, so writing 0-6 or 1-7 has the same result.\nAlso, as suggested by @Henrik, it is possible to replace numbers by shortened name of days, such as MON, THU, etc:\n\n```\n0 - Sun      Sunday\n1 - Mon      Monday\n2 - Tue      Tuesday\n3 - Wed      Wednesday\n4 - Thu      Thursday\n5 - Fri      Friday\n6 - Sat      Saturday\n7 - Sun      Sunday\n\n```\n\nGraphically, * * * * * command to be executed stands for:\n\n\n\nminute\nhour\nday of month\nmonth\nday of week\n\n\n\n\n\n(0-59)\n(0-23)\n(1-31)\n(1-12)\n(1-7)\n\n\n\n*\n*\n*\n*\n*\ncommand to be executed\n\n\n\nOr using the old style:\n\n```\n ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ minute (0 - 59)\n ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ hour (0 - 23)\n ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ day of month (1 - 31)\n ‚îÇ ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ month (1 - 12)\n ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ day of week (0 - 6 => Sunday - Saturday, or\n ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ                1 - 7 => Monday - Sunday)\n ‚Üì ‚Üì ‚Üì ‚Üì ‚Üì\n * * * * * command to be executed\n\n```\n\nFinally, if you want to specify day by day, you can separate days with commas, for example SUN,MON,THU will exectute the command only on sundays, mondays on thursdays.\nYou can read further details in Wikipedia's article about Cron and check a cron expression online with crontab.guru."}
{"instruction": "A cron job for rails: best practices?", "output": "I'm using the rake approach (as supported by heroku)\nWith a file called lib/tasks/cron.rake ..\n\n```\ntask :cron => :environment do\n  puts \"Pulling new requests...\"\n  EdiListener.process_new_messages\n  puts \"done.\"\nend\n\n```\n\nTo execute from the command line, this is just \"rake cron\". This command can then be put on the operating system cron/task scheduler as desired.\nUpdate this is quite an old question and answer! Some new info:\n\nthe heroku cron service I referenced has since been replaced by Heroku Scheduler\nfor frequent tasks (esp. where you want to avoid the Rails environment startup cost) my preferred approach is to use system cron to call a script that will either (a) poke a secure/private webhook API to invoke the required task in the background or (b) directly enqueue a task on your queuing system of choice"}
{"instruction": "What is the Windows version of cron?", "output": "For newer Microsoft OS versions, Windows Server 2012 / Windows 8, look at the schtasks command line utility.\nIf using PowerShell, the Scheduled Tasks Cmdlets in Windows PowerShell are made for scripting.\nFor command-line usage before Windows 8, you can schedule with the AT command.\nFor the original question, asking about Windows XP (and Windows 7): Windows Task Scheduler"}
{"instruction": "How would I get a cron job to run every 30 minutes?", "output": "Do:\n\n```\n0,30 * * * * your_command\n\n```"}
{"instruction": "How to simulate the environment cron executes a script with?", "output": "Add this to your crontab (temporarily):\n\n```\n* * * * * env > ~/cronenv\n\n```\n\nAfter it runs, do this:\n\n```\nenv - `cat ~/cronenv` /bin/sh\n\n```\n\nThis assumes that your cron runs /bin/sh, which is the default regardless of the user's default shell.\nFootnote: if env contains more advanced config, eg PS1=$(__git_ps1 \" (%s)\")$, it will error cryptically env: \": No such file or directory."}
{"instruction": "Test a weekly cron job", "output": "Just do what cron does, run the following as root:\n\n```\nrun-parts -v /etc/cron.weekly\n\n```\n\n... or the next one if you receive the \"Not a directory: -v\" error:\n\n```\nrun-parts /etc/cron.weekly -v\n\n```\n\nOption -v prints the script names before they are run."}
{"instruction": "Crontab - Run in directory", "output": "All jobs are executed by a shell, so start that shell snippet by a command to change the directory.\n\n```\ncd /path/to/directory && ./bin/myapp\n\n```\n\nConcerning the use of && instead of ;: normally it doesn't make a difference, but if the cd command fails (e.g. because the directory doesn't exist) with && the application isn't executed, whereas with ; it's executed (but not in the intended directory)."}
{"instruction": "How do I create a crontab through a script", "output": "Cron jobs usually are stored in a per-user file under /var/spool/cron\nThe simplest thing for you to do is probably just create a text file with the job configured, then copy it to the cron spool folder and make sure it has the right permissions (600)."}
{"instruction": "How to run cron once, daily at 10pm", "output": "It's running every minute of the hour 22 I guess. Try the following to run it every first minute of the hour 22:\n\n```\n0 22 * * * ....\n\n```"}
{"instruction": "How to specify in crontab by what user to run script?", "output": "Instead of creating a crontab to run as the root user, create a crontab for the user that you want to run the script.  In your case, crontab -u www-data -e will edit the crontab for the www-data user.  Just put your full command in there and remove it from the root user's crontab."}
{"instruction": "Run cron job only if it isn&#39;t already running", "output": "I do this for a print spooler program that I wrote, it's just a shell script:\n\n```\n#!/bin/sh\nif ps -ef | grep -v grep | grep doctype.php ; then\n        exit 0\nelse\n        /home/user/bin/doctype.php >> /home/user/bin/spooler.log &\n        #mailing program\n        /home/user/bin/simplemail.php \"Print spooler was not running...  Restarted.\" \n        exit 0\nfi\n\n```\n\nIt runs every two minutes and is quite effective.  I have it email me with special information if for some reason the process is not running."}
{"instruction": "How to get CRON to call in the correct PATHs", "output": "I used /etc/crontab. I used vi and entered in the PATHs I needed into this file and ran it as root. The normal crontab overwrites PATHs that you have set up. A good tutorial on how to do this.\nThe systemwide cron file looks like this:\n\n```\nThis has the username field, as used by /etc/crontab.\n# /etc/crontab: system-wide crontab\n# Unlike any other crontab you don't have to run the `crontab'\n# command to install the new version when you edit this file.\n# This file also has a username field, that none of the other crontabs do.\n\nSHELL=/bin/sh\nPATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin\n\n# m h dom mon dow user   command\n42 6 * * *   root    run-parts --report /etc/cron.daily\n47 6 * * 7   root    run-parts --report /etc/cron.weekly\n52 6 1 * *   root    run-parts --report /etc/cron.monthly\n01 01 * * 1-5 root python /path/to/file.py\n\n```"}
{"instruction": "How to automatically remove completed Kubernetes Jobs created by a CronJob?", "output": "You can now set history limits, or disable history altogether, so that failed or successful CronJobs are not kept around indefinitely.  See my answer here. Documentation is here.\nTo set the history limits:\n\nThe .spec.successfulJobsHistoryLimit and .spec.failedJobsHistoryLimit fields are optional. These fields specify how many completed and failed jobs should be kept. By default, they are set to 3 and 1 respectively. Setting a limit to 0 corresponds to keeping none of the corresponding kind of jobs after they finish.\n\nThe config with 0 limits would look like:\n\n```\napiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  schedule: \"*/1 * * * *\"\n  successfulJobsHistoryLimit: 0\n  failedJobsHistoryLimit: 0\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: hello\n            image: busybox\n            args:\n            - /bin/sh\n            - -c\n            - date; echo Hello from the Kubernetes cluster\n          restartPolicy: OnFailure\n\n```"}
{"instruction": "How to run a script in the background even after I logout SSH?", "output": "Run nohup python bgservice.py & to get the script to ignore the hangup signal and keep running. Output will be put in nohup.out.\nIdeally, you'd run your script with something like supervise so that it can be restarted if (when) it dies."}
{"instruction": "How to write a cron that will run a script every day at midnight?", "output": "Here's a good tutorial on what crontab is and how to use it on Ubuntu. Your crontab line will look something like this:\n\n```\n00 00 * * * ruby path/to/your/script.rb\n\n```\n\n(00 00 indicates midnight--0 minutes and 0 hours--and the *s mean every day of every month.)\n\n```\n\nSyntax: \n  mm hh dd mt wd  command\n\n  mm minute 0-59\n  hh hour 0-23\n  dd day of month 1-31\n  mt month 1-12\n  wd day of week 0-7 (Sunday = 0 or 7)\n  command: what you want to run\n  all numeric values can be replaced by * which means all\n\n```"}
{"instruction": "How to schedule a function to run every hour on Flask?", "output": "You can use BackgroundScheduler() from APScheduler package (v3.5.3):\n\n```\nimport time\nimport atexit\n\nfrom apscheduler.schedulers.background import BackgroundScheduler\n\n\ndef print_date_time():\n    print(time.strftime(\"%A, %d. %B %Y %I:%M:%S %p\"))\n\n\nscheduler = BackgroundScheduler()\nscheduler.add_job(func=print_date_time, trigger=\"interval\", seconds=60)\nscheduler.start()\n\n# Shut down the scheduler when exiting the app\natexit.register(lambda: scheduler.shutdown())\n\n```\n\nNote that two of these schedulers will be launched when Flask is in debug mode. For more information, check out this question."}
{"instruction": "CRON job to run on the last day of the month", "output": "Possibly the easiest way is to simply do three separate jobs:\n\n```\n55 23 30 4,6,9,11        * myjob.sh\n55 23 31 1,3,5,7,8,10,12 * myjob.sh\n55 23 28 2               * myjob.sh\n\n```\n\nThat will run on the 28th of February though, even on leap years so, if that's a problem, you'll need to find another way.\n\nHowever, it's usually both substantially easier and correct to run the job as soon as possible on the first day of each month, with something like:\n\n```\n0 0 1 * * myjob.sh\n\n```\n\nand modify the script to process the previous month's data.\nThis removes any hassles you may encounter with figuring out which day is the last of the month, and also ensures that all data for that month is available, assuming you're processing data. Running at five minutes to midnight on the last day of the month may see you missing anything that happens between then and midnight.\nThis is the usual way to do it anyway, for most end-of-month jobs.\n\nIf you still really want to run it on the last day of the month, one option is to simply detect if tomorrow is the first (either as part of your script, or in the crontab itself).\nSo, something like:\n\n```\n55 23 28-31 * * [[ \"$(date --date=tomorrow +\\%d)\" == \"01\" ]] && myjob.sh\n\n```\n\nshould be a good start, assuming you have a relatively intelligent date program.\nIf your date program isn't quite advanced enough to give you relative dates, you can just put together a very simple program to give you tomorrow's day of the month (you don't need the full power of date), such as:\n\n```\n#include <stdio.h>\n#include <time.h>\n\nint main (void) {\n    // Get today, somewhere around midday (no DST issues).\n\n    time_t noonish = time (0);\n    struct tm *localtm = localtime (&noonish);\n    localtm->tm_hour = 12;\n\n    // Add one day (86,400 seconds).\n\n    noonish = mktime (localtm) + 86400;\n    localtm = localtime (&noonish);\n\n    // Output just day of month.\n\n    printf (\"%d\\n\", localtm->tm_mday);\n\n    return 0;\n}\n\n```\n\nand then use (assuming you've called it tomdom for \"tomorrow's day of month\"):\n\n```\n55 23 28-31 * * [[ \"$(tomdom)\" == \"1\" ]] && myjob.sh\n\n```\n\nThough you may want to consider adding error checking since both time() and mktime() can return -1 if something goes wrong. The code above, for reasons of simplicity, does not take that into account."}
{"instruction": "Spring cron vs normal cron?", "output": "Taking some note from: https://www.baeldung.com/cron-expressions\nA Spring Scheduled tasks is like this:\n\n```\n1 2 3 4 5 6 Index\n- - - - - -\n* * * * * * command to be executed\n- - - - - -\n| | | | | | \n| | | | | ------- Day of week (MON - SUN)\n| | | | --------- Month (1 - 12)\n| | | ----------- Day of month (1 - 31)\n| |-------------- Hour (0 - 23)\n| --------------- Minute (0 - 59)\n----------------- Seconds (0 - 59)\n\n```\n\nFrom: https://www.cyberciti.biz/faq/how-do-i-add-jobs-to-cron-under-linux-or-unix-oses/\nA Linux Cron job is like this:\n\n```\n1 2 3 4 5 Index\n- - - - -\n* * * * * command to be executed\n- - - - -\n| | | | |\n| | | | ----- Day of week (0 - 7) (Sunday=0 or 7)\n| | | ------- Month (1 - 12)\n| | --------- Day of month (1 - 31)\n| ----------- Hour (0 - 23)\n------------- Minute (0 - 59)\n\n```\n\nSide note:\n\nSome article said it is possible to have a 7 optional param which is year , I have tried using latest spring and it show error, so I don't think it is working.\nIf your Linux cron job expression is simple enough, seems like it is possible to just put an 0 in front and it will convert to the spring scheduled tasks expression\n\nE.g. Every 5 minutes\n\n*/5 * * * * Linux cron job\n0 */5 * * * * Spring schedule tasks\n\n\n\n\n\nBonus: Spring Schedule Cron Generator\n\nClick on Show code snippet\nClick on Run Code snippet\nHave fun!\n\n\n\n\n```\n$('.select2').select2({\n  width: '100%'\n});\n\n//// Init ////////////\n$dropdown = $(\"#secondsSelect\");\nfor (let i = 1; i < 60; i++) {\n  $dropdown.append($(\"<option />\").val(i).text(i));\n}\n$dropdown = $(\"#minSelect\");\nfor (let i = 1; i < 60; i++) {\n  $dropdown.append($(\"<option />\").val(i).text(i));\n}\n$dropdown = $(\"#hoursSelect\");\nfor (let i = 1; i < 24; i++) {\n  $dropdown.append($(\"<option />\").val(i).text(i));\n}\n$dropdown = $(\"#dayOfMonthSelect\");\nfor (let i = 1; i < 32; i++) {\n  $dropdown.append($(\"<option />\").val(i).text(i));\n}\n//// Init End ////////////\n\n\n$('.select2').on('select2:select', function(e) {\n  let value = e.params.data.id;\n  let prevValue = $(this).val().length > 0 ? $(this).val()[0] : null;\n\n  if (value != parseInt(value)) {\n    $(this).val(value).change();\n  } else if (prevValue != parseInt(prevValue)) {\n    $(this).val(value).change();\n  }\n  calculateSpringCron();\n});\n\nlet r = function(dropdown) {\n  return dropdown.val().join(\",\");\n\n}\n\nlet calculateSpringCron = function() {\n\n  let result = [\n    r($(\"#secondsSelect\")),\n    r($(\"#minSelect\")),\n    r($(\"#hoursSelect\")),\n    r($(\"#dayOfMonthSelect\")),\n    r($(\"#monthsSelect\")),\n    r($(\"#weekdaySelect\")),\n  ];\n\n  $(\"#result\").val(result.join(\" \"));\n\n  $(\"#result-expand\").html(result.join(\" &nbsp; &nbsp;\"))\n\n}\n\ncalculateSpringCron();\n```\n\n\n```\n.ms-container {\n  display: flex;\n  flex-direction: column;\n  width: 100%;\n  padding-left: 3em;\n  padding-right: 3em;\n  background: none !important;\n  padding-bottom: 5em;\n}\n```\n\n\n```\n<link href=\"https://cdnjs.cloudflare.com/ajax/libs/multi-select/0.9.12/css/multi-select.min.css\" rel=\"stylesheet\" />\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/multi-select/0.9.12/js/jquery.multi-select.min.js\"></script>\n<link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css\" integrity=\"sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T\" crossorigin=\"anonymous\">\n\n<script src=\"https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js\" integrity=\"sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM\" crossorigin=\"anonymous\"></script>\n\n<link href=\"https://cdnjs.cloudflare.com/ajax/libs/select2/4.0.8/css/select2.min.css\" rel=\"stylesheet\" />\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/select2/4.0.8/js/select2.min.js\"></script>\n\n<div class=\"row\">\n  <div class=\"col-12\">\n    <h1>Spring Schedule Cron Generator</h1>\n  </div>\n</div>\n<div class=\"row\">\n  <div class=\"col-4\">\n    Seconds:\n    <select id=\"secondsSelect\" class=\"select2\" name=\"states[]\" multiple=\"multiple\">\n      <option value=\"*\" selected=\"selected\">Every seconds</option>\n      <option value=\"*/2\">Every even seconds</option>\n      <option value=\"1-59/2\">Every odd seconds</option>\n      <option value=\"*/5\">Every 5 seconds</option>\n      <option value=\"*/10\">Every 10 seconds</option>\n      <option value=\"*/15\">Every 15 seconds</option>\n      <option value=\"*/30\">Every 30 seconds</option>\n    </select>\n  </div>\n  <div class=\"col-4\">\n    Minutes:\n    <select id=\"minSelect\" class=\"select2\" name=\"states[]\" multiple=\"multiple\">\n      <option value=\"*\" selected=\"selected\">Every minutes</option>\n      <option value=\"*/2\">Every even minutes</option>\n      <option value=\"1-59/2\">Every odd minutes</option>\n      <option value=\"*/5\">Every 5 minutes</option>\n      <option value=\"*/10\">Every 10 minutes</option>\n      <option value=\"*/15\">Every 15 minutes</option>\n      <option value=\"*/30\">Every 30 minutes</option>\n    </select>\n  </div>\n  <div class=\"col-4\">\n    Hours:\n    <select id=\"hoursSelect\" class=\"select2\" name=\"states[]\" multiple=\"multiple\">\n      <option value=\"*\" selected=\"selected\">Every hour</option>\n      <option value=\"*/2\">Every even hour</option>\n      <option value=\"1-11/2\">Every odd hour</option>\n      <option value=\"*/3\">Every 3 hour</option>\n      <option value=\"*/4\">Every 4 hour</option>\n      <option value=\"*/6\">Every 6 hour</option>\n    </select>\n  </div>\n</div>\n\n<div class=\"row\">\n</div>\n\n<div class=\"row\">\n  <div class=\"col-4\">\n    Days of month:\n    <select id=\"dayOfMonthSelect\" class=\"select2\" name=\"states[]\" multiple=\"multiple\">\n      <option value=\"*\" selected=\"selected\">Every day of month</option>\n      <option value=\"*/2\">Even day of month</option>\n      <option value=\"1-31/2\">Odd day of month</option>\n      <option value=\"*/5\">Every 5 days of month (5,10,15...)</option>\n      <option value=\"*/10\">Every 10 days of month (10,20,30...)</option>\n    </select>\n  </div>\n  <div class=\"col-4\">\n    Months:\n    <select id=\"monthsSelect\" class=\"select2\" name=\"states[]\" multiple=\"multiple\">\n      <option value=\"*\" selected=\"selected\">Every month</option>\n      <option value=\"*/2\">Even months</option>\n      <option value=\"1-11/2\">Odd months</option>\n      <option value=\"*/4\">Every 4 months</option>\n      <option value=\"*/6\">Every 6 months(half year)</option>\n      <option value=\"1\">Jan</option>\n      <option value=\"2\">Feb</option>\n      <option value=\"3\">Mar</option>\n      <option value=\"4\">Apr</option>\n      <option value=\"5\">May</option>\n      <option value=\"6\">Jun</option>\n      <option value=\"7\">Jul</option>\n      <option value=\"8\">Aug</option>\n      <option value=\"9\">Sep</option>\n      <option value=\"10\">Oct</option>\n      <option value=\"11\">Nov</option>\n      <option value=\"12\">Dec</option>\n    </select>\n  </div>\n  <div class=\"col-4\">\n    Weekday:\n    <select id=\"weekdaySelect\" class=\"select2\" name=\"states[]\" multiple=\"multiple\">\n      <option value=\"*\" selected=\"selected\">Every weekday</option>\n      <option value=\"MON-FRI\">Weekdays (MON-FRI)</option>\n      <option value=\"SAT,SUN\">Weekend</option>\n      <option value=\"SUN\">SUN</option>\n      <option value=\"MON\">MON</option>\n      <option value=\"TUE\">TUE</option>\n      <option value=\"WED\">WED</option>\n      <option value=\"THU\">THU</option>\n      <option value=\"FRI\">FRI</option>\n      <option value=\"SAT\">SAT</option>\n    </select>\n  </div>\n</div>\n<div class=\"row\">\n  <div class=\"col-12\">\n    Result:\n    <input id=\"result\" class=\"form-control\" /> With a bit of seperation for better viewing:<br/>\n    <h1 id=\"result-expand\"></h1>\n  </div>\n</div>\n```"}
{"instruction": "How can I programmatically create a new cron job?", "output": "The best way if you're running as root, is to drop a file into /etc/cron.d\nif you use a package manager to package your software, you can simply lay down files in that directory and they are interpreted as if they were crontabs, but with an extra field for the username, e.g.:\nFilename: /etc/cron.d/per_minute\nContent:\n* * * * * root /bin/sh /home/root/script.sh"}
{"instruction": "Cron jobs and random times, within given hours", "output": "If I understand what you're looking for, you'll need to do something a bit messy, like having a cron job that runs a bash script that randomizes the run times...  Something like this:\ncrontab:\n\n```\n0 9 * * * /path/to/bashscript\n\n```\n\nand in /path/to/bashscript:\n\n```\n#!/bin/bash\n\nmaxdelay=$((14*60))  # 14 hours from 9am to 11pm, converted to minutes\nfor ((i=1; i<=20; i++)); do\n    delay=$(($RANDOM%maxdelay)) # pick an independent random delay for each of the 20 runs\n    (sleep $((delay*60)); /path/to/phpscript.php) & # background a subshell to wait, then run the php script\ndone\n\n```\n\nA few notes: this approach it a little wasteful of resources, as it fires off 20 background processes at 9am, each of which waits around for a random number of minutes (up to 14 hours, i.e. 11pm), then launches the php script and exits.  Also, since it uses a random number of minutes (not seconds), the start times aren't quite as random as they could be.  But $RANDOM only goes up to 32,767, and there are 50,400 seconds between 9am and 11pm, it'd be a little more complicated to randomize the seconds as well.  Finally, since the start times are random and independent of each other, it's possible (but not very likely) that two or more instances of the script will be started simultaneously."}
{"instruction": "Running a cron job on Linux every six hours", "output": "You forgot a *, and you've too many fields. It's the hour you need to care about\n\n```\n0 */6 * * * /path/to/mycommand\n\n```\n\nThis means every sixth hour starting from 0, i.e. at hour 0, 6, 12 and 18 which you could write as\n\n```\n0 0,6,12,18 * * * /path/to/mycommand\n\n```"}
{"instruction": "Cron job every three days", "output": "Run it every three days - or less at the end of the month. (It'll run 2 days in a row if the previous month had 31 days.)\n\n```\n0 0 */3 * *\n\n```\n\nHow about that?\nIf you want it to run on specific days of the month, like the 1st, 4th, 7th, etc... then you can just have a conditional in your script that checks for the current day of the month.\n\n```\nif (((date('j') - 1) % 3))\n   exit();\n\n```\n\nor, as @mario points out, you can use date('k') to get the day of the year instead of doing it based on the day of the month."}
{"instruction": "I need a Nodejs scheduler that allows for tasks at different intervals", "output": "I would recommend node-cron. It allows to run tasks using Cron patterns e.g.\n\n```\n'* * * * * *' - runs every second\n'*/5 * * * * *' - runs every 5 seconds\n'10,20,30 * * * * *' - run at 10th, 20th and 30th second of every minute\n'0 * * * * *' - runs every minute\n'0 0 * * * *' - runs every hour (at 0 minutes and 0 seconds)\n\n```\n\nBut also more complex schedules e.g.\n\n```\n'00 30 11 * * 1-5' - Runs every weekday (Monday through Friday) at 11:30:00 AM. It does not run on Saturday or Sunday.\n\n```\n\nSample code: running job every 10 minutes:\n\n\n```\nvar cron = require('cron');\nvar cronJob = cron.job(\"0 */10 * * * *\", function(){\n    // perform operation e.g. GET request http.get() etc.\n    console.info('cron job completed');\n}); \ncronJob.start();\n\n```\n\nYou can find more examples in node-cron wiki\nMore on cron configuration can be found on cron wiki\nI've been using that library in many projects and it does the job. I hope that will help."}
{"instruction": "How do I set a task to run every so often?", "output": "Just use launchd. It is a very powerful launcher system and meanwhile it is the standard launcher system for Mac OS X (current OS X version wouldn't even boot without it). For those who are not familiar with launchd (or with OS X in general), it is like a crossbreed between init, cron, at, SysVinit (init.d), inetd, upstart and systemd. Borrowing concepts of all these projects, yet also offering things you may not find elsewhere.\nEvery service/task is a file. The location of the file depends on the questions: \"When is this service supposed to run?\" and \"Which privileges will the service require?\"\nSystem tasks go to\n\n```\n/Library/LaunchDaemons/\n\n```\n\nif they shall run no matter if any user is logged in to the system or not. They will be started with \"root\" privileges.\nIf they shall only run if any user is logged in, they go to\n\n```\n/Library/LaunchAgents/\n\n```\n\nand will be executed with the privileges of the user that just logged in.\nIf they shall run only if you are logged in, they go to\n\n```\n~/Library/LaunchAgents/\n\n```\n\nwhere ~ is your HOME directory. These task will run with your privileges, just as if you had started them yourself by command line or by double clicking a file in Finder.\nNote that there also exists /System/Library/LaunchDaemons and /System/Library/LaunchAgents, but as usual, everything under /System is managed by OS X. You shall not place any files there, you shall not change any files there, unless you really know what you are doing. Messing around in the Systems folder can make your system unusable (get it into a state where it will even refuse to boot up again). These are the directories where Apple places the launchd tasks that get your system up and running during boot, automatically start services as required, perform system maintenance tasks, and so on.\nEvery launchd task is a file in PLIST format. It should have reverse domain name notation. E.g. you can name your task\n\n```\ncom.example.my-fancy-task.plist\n\n```\n\nThis plist can have various options and settings. Writing one per hand is not for beginners, so you may want to get a tool like LaunchControl (commercial, $18) or Lingon (commercial, $14.99) to create your tasks.\nJust as an example, it could look like this\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE plist PUBLIC \"-//Apple Computer//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n<plist version=\"1.0\">\n<dict>\n    <key>Label</key>\n    <string>com.example.my-fancy-task</string>\n    <key>OnDemand</key>\n    <true/>\n    <key>ProgramArguments</key>\n    <array>\n        <string>/bin/sh</string>\n        <string>/usr/local/bin/my-script.sh</string>\n    </array>\n    <key>StartInterval</key>\n    <integer>1800</integer>\n</dict>\n</plist>\n\n```\n\nThis agent will run the shell script /usr/local/bin/my-script.sh every 1800 seconds (every 30 minutes). You can also have task run on certain dates/times (basically launchd can do everything cron can do) or you can even disable \"OnDemand\" causing launchd to keep the process permanently running (if it quits or crashes, launchd will immediately restart it). You can even limit how much resources a process may use.\nUpdate: Even though OnDemand is still supported, it is deprecated. The new setting is named KeepAlive, which makes much more sense. It can have a boolean value, in which case it is the exact opposite of OnDemand (setting it to false behaves as if OnDemand is true and the other way round). The great new feature is, that it can also have a dictionary value instead of a boolean one. If it has a dictionary value, you have a couple of extra options that give you more fine grain control under which circumstances the task shall be kept alive. E.g. it is only kept alive as long as the program terminated with an exit code of zero, only as long as a certain file/directory on disk exists, only if another task is also alive, or only if the network is currently up.\nAlso you can manually enable/disable tasks via command line:\n\n```\nlaunchctl <command> <parameter>\n\n```\n\ncommand can be load or unload, to load a plist or unload it again, in which case parameter is the path to the file. Or command can be start or stop, to just start or stop such a task, in which case parameter is the label (com.example.my-fancy-task). Other commands and options exist as well.\nUpdate: Even though load, unload, start, and stop do still work, they are legacy now. The new commands are bootstrap, bootout, enable, and disable with slightly different syntax and options. One big difference is that disable is persistent, so once a service has been disabled, it will stay disabled, even across reboots until you enable it again. Also you can use kickstart to run a task immediately, regardless how it has been configured to run.\nThe main difference between the new and the old commands is that they separate tasks by \"domain\". The system has domain and so has every user. So equally labeled tasks may exist in different domains and launchctl can still distinguish them. Even different login and different UI sessions of the same user have their own domain (e.g. the same user may once be logged locally and once remote via SSH and different tasks may run for either session) and so does every single running processes. Thus instead of com.example.my-fancy-task, you now would use system/com.example.my-fancy-task or user/501/com.example.my-fancy-task to identify a task, with 501 being the user ID of a specific user.\nSee documentation of the plist format and of the launchctl command line tool."}
{"instruction": "How to run a cronjob every X minutes?", "output": "In a crontab file, the fields are:\n\nminute of the hour.\nhour of the day.\nday of the month.\nmonth of the year.\nday of the week.\n\nSo:\n\n```\n10 * * * * blah\n\n```\n\nmeans execute blah at 10 minutes past every hour.\nIf you want every five minutes, use either:\n\n```\n*/5 * * * * blah\n\n```\n\nmeaning every minute but only every fifth one, or:\n\n```\n0,5,10,15,20,25,30,35,40,45,50,55 * * * * blah\n\n```\n\nfor older cron executables that don't understand the */x notation.\nIf it still seems to be not working after that, change the command to something like:\n\n```\ndate >>/tmp/debug_cron_pax.txt\n\n```\n\nand monitor that file to ensure something's being written every five minutes. If so, there's something wrong with your PHP scripts. If not, there's something wrong with your cron daemon."}
{"instruction": "How to conditionally enable or disable scheduled jobs in Spring?", "output": "```\n@Component\npublic class ImagesPurgeJob implements Job {\n\n    private Logger logger = Logger.getLogger(this.getClass());\n\n    @Value(\"${jobs.mediafiles.imagesPurgeJob.enable}\")\n    private boolean imagesPurgeJobEnable;\n\n    @Override\n    @Transactional(readOnly=true)\n    @Scheduled(cron = \"${jobs.mediafiles.imagesPurgeJob.schedule}\")\n    public void execute() {\n\n         //Do something\n        //can use DAO or other autowired beans here\n        if(imagesPurgeJobEnable){\n\n            Do your conditional job here...\n\n        }\n    }\n}\n\n```"}
{"instruction": "mysqldump &amp; gzip commands to properly create a compressed file of a MySQL database using crontab", "output": "First the mysqldump command is executed and the output generated is redirected using the pipe. The pipe is sending the standard output into the gzip command as standard input. Following the filename.gz, is the output redirection operator (>) which is going to continue redirecting the data until the last filename, which is where the data will be saved.\nFor example, this command will dump the database and run it through gzip and the data will finally land in three.gz\n\n```\nmysqldump -u user -pupasswd my-database | gzip > one.gz > two.gz > three.gz\n\n$> ls -l\n-rw-r--r--  1 uname  grp     0 Mar  9 00:37 one.gz\n-rw-r--r--  1 uname  grp  1246 Mar  9 00:37 three.gz\n-rw-r--r--  1 uname  grp     0 Mar  9 00:37 two.gz\n\n```\n\nMy original answer is an example of redirecting the database dump to many compressed files (without double compressing). (Since I scanned the question and seriously missed - sorry about that)\nThis is an example of recompressing files:\n\n```\nmysqldump -u user -pupasswd my-database | gzip -c > one.gz; gzip -c one.gz > two.gz; gzip -c two.gz > three.gz\n\n$> ls -l\n-rw-r--r--  1 uname  grp  1246 Mar  9 00:44 one.gz\n-rw-r--r--  1 uname  grp  1306 Mar  9 00:44 three.gz\n-rw-r--r--  1 uname  grp  1276 Mar  9 00:44 two.gz\n\n```\n\nThis is a good resource explaining I/O redirection: http://www.codecoffee.com/tipsforlinux/articles2/042.html"}
{"instruction": "CronJob not running", "output": "The solution for me was to avoid relative paths. Never use relative path in Python scripts to be executed via crontab.\nI did something like this instead:\n\n```\nimport os\nimport sys\nimport time, datetime\n\nCLASS_PATH = '/srv/www/live/mainapp/classes'\nSETTINGS_PATH = '/srv/www/live/foodtrade'\nsys.path.insert(0, CLASS_PATH)\nsys.path.insert(1,SETTINGS_PATH)\n\nimport other_py_files\n\n```\n\nAlso, never suppress the crontab code; instead use mailserver and check the mail for the user. That gives clearer insights of what is going."}
{"instruction": "How to redirect output of systemd service to a file", "output": "I think there's a more elegant way to solve the problem: send the stdout/stderr to syslog with an identifier and instruct your syslog manager to split its output by program name.\nUse the following properties in your systemd service unit file:\n\n```\nStandardOutput=syslog\nStandardError=syslog\nSyslogIdentifier=<your program identifier> # without any quote\n\n```\n\nThen, assuming your distribution is using rsyslog to manage syslogs, create a file in /etc/rsyslog.d/<new_file>.conf with the following content:\n\n```\nif $programname == '<your program identifier>' then /path/to/log/file.log\n& stop\n\n```\n\nNow make the log file writable by syslog:\n\n```\n# ls -alth /var/log/syslog \n-rw-r----- 1 syslog adm 439K Mar  5 19:35 /var/log/syslog\n# chown syslog:adm /path/to/log/file.log\n\n```\n\nRestart rsyslog (sudo systemctl restart rsyslog) and enjoy! Your program stdout/stderr will still be available through journalctl (sudo journalctl -u <your program identifier>) but they will also be available in your file of choice.\nSource via archive.org"}
{"instruction": "How can I use iptables on centos 7?", "output": "With RHEL 7 / CentOS 7, firewalld was introduced to manage iptables. IMHO, firewalld is more suited for workstations than for server environments.\nIt is possible to go back to a more classic iptables setup. First, stop and mask the firewalld service:\n\n```\nsystemctl stop firewalld\nsystemctl mask firewalld\n\n```\n\nThen, install the iptables-services package:\n\n```\nyum install iptables-services\n\n```\n\nEnable the service at boot-time:\n\n```\nsystemctl enable iptables\n\n```\n\nManaging the service\n\n```\nsystemctl [stop|start|restart] iptables\n\n```\n\nSaving your firewall rules can be done as follows:\n\n```\nservice iptables save\n\n```\n\nor\n\n```\n/usr/libexec/iptables/iptables.init save\n\n```"}
{"instruction": "How can I configure a systemd service to restart periodically?", "output": "Yes, you can make your service to restart it periodically by making your service of Type=notify.\nAdd this option in [Service] section of your service file along with Restart=always and give WatchdogSec=xx, where xx is the time period in second you want to restart your service. Here your process will be killed by systemd after xx time period and will be restarted by systemd again.\nfor eg.\n\n```\n[Unit]\n.\n.\n\n[Service]\nType=notify\n.\n.\nWatchdogSec=10\nRestart=always\n.\n.\n\n[Install]\nWantedBy= ....\n\n```"}
{"instruction": "Start systemd service after specific service?", "output": "In the .service file under the [Unit] section:\n\n```\n[Unit]\nDescription=My Website\nAfter=syslog.target network.target mongodb.service\n\n```\n\nThe important part is the mongodb.service\nThe manpage describes it however due to formatting it's not as clear on first sight\nsystemd.unit - well formatted\nsystemd.unit - not so well formatted"}
{"instruction": "Systemd with multiple execStart", "output": "if Type=simple in your unit file, you can only specify one ExecStart, but you can add as many ExecStartPre, ExecStartPost, but none of this is suited for long running commands, because they are executed serially and everything one start is killed before starting the next one.\nIf Type=oneshot you can specify multiple ExecStart, they run serially not in parallel. \nIf what you want is to run multiple units in parallel, there a few things you can do:\nIf they differ on 1 param\nYou can use template units, so you create a /etc/systemd/system/foo@.service. NOTE: (the @ is important).\n\n```\n[Unit]\nDescription=script description %I\n\n[Service]\nType=simple\nExecStart=/script.py %i\nRestart=on-failure\n\n[Install]\nWantedBy=multi-user.target\n\n```\n\nAnd then you exec:\n\n```\n$ systemctl start foo@parameter1.service foo@parameter2.service\n\n```\n\nor...\nTarget dependencies\nYou can create multiple units that links to a single target:\n\n```\n#/etc/systemd/system/bar.target\n[Unit]\nDescription=bar target\nRequires=multi-user.target\nAfter=multi-user.target\nAllowIsolate=yes\n\n```\n\nAnd then you just modify you .service units to be WantedBy=bar.target like:\n\n```\n#/etc/systemd/system/foo@.service\n[Unit]\nDescription=script description %I\n\n[Service]\nType=simple\nExecStart=/script.py %i\nRestart=on-failure\n\n[Install]\nWantedBy=bar.target\n\n```\n\nThen you just enable the foo services you want in parallel, and start the bar target like this:\n\n```\n$ systemctl daemon-reload\n$ systemctl enable foo@param1.service\n$ systemctl enable foo@param2.service\n$ systemctl start bar.target\n\n```\n\nNOTE: that this works with any type of units not only template units."}
{"instruction": "What is the difference between systemd&#39;s &#39;oneshot&#39; and &#39;simple&#39; service types?", "output": "The Type=oneshot service unit:\n\nblocks on a start operation until the first process exits, and its state will be reported as \"activating\";\nonce the first process exits, transitions from \"activating\" straight to \"inactive\", unless RemainAfterExit=true is set (in which case it becomes \"active\" with no processes!);\nmay have any number (0 or more) of ExecStart= directives which will be executed sequentially (waiting for each started process to exit before starting the next one);\nmay leave out ExecStart= but have ExecStop= (useful together with RemainAfterExit=true for arranging things to run on system shutdown).\n\nThe Type=simple service unit:\n\ndoes not block on a start operation (i. e. becomes \"active\" immediately after forking off the first process, even if it is still initializing!);\nonce the first process exits, transitions from \"active\" to \"inactive\" (there is no RemainAfterExit= option);\nis generally discouraged because there is no way to distinguish situations like \"exited on start because of a configuration error\" from \"crashed after 500ms of runtime\" and suchlike.\n\nBoth Type=oneshot and Type=simple units:\n\nignore any children of the first process, so do not use these modes with forking processes (note: you may use Type=oneshot with KillMode=none, but only do this if you know what you are doing)."}
{"instruction": "How can systemd and systemctl be enabled and used in Ubuntu Docker containers?", "output": "This is by design. Docker should be running a process in the foreground in your container and it will be spawned as PID 1 within the container's pid namespace. Docker is designed for process isolation, not for OS virtualization, so there are no other OS processes and daemons running inside the container (like systemd, cron, syslog, etc), only your entrypoint or command you run.\nIf they included systemd commands, you'd find a lot of things not working since your entrypoint replaces init. Systemd also makes use to cgroups which docker restricts inside of containers since the ability to change cgroups could allow a process to escape the container's isolation. Without systemd running as init inside your container, there's no daemon to process your start and stop commands."}
{"instruction": "Fixing a systemd service 203/EXEC failure (no such file or directory)", "output": "I think I found the answer:\nIn the .service file, I needed to add /bin/bash before the path to the script.\nFor example, for backup.service:\nExecStart=/bin/bash /home/user/.scripts/backup.sh\nAs opposed to:\nExecStart=/home/user/.scripts/backup.sh\nI'm not sure why. Perhaps fish. On the other hand, I have another script running for my email, and the service file seems to run fine without /bin/bash. It does use default.target instead multi-user.target, though.\nMost of the tutorials I came across don't prepend /bin/bash, but I then saw this SO answer which had it, and figured it was worth a try.\nThe service file executes the script, and the timer is listed in systemctl --user list-timers, so hopefully this will work.\nUpdate: I can confirm that everything is working now."}
{"instruction": "journalctl - remove logs of a specific unit", "output": "After some research I found that you can't delete logs for a specific systemd unit, because the logs are interlaced and if you delete only one unit you'll corrupt the logs, so journalctl doesn't let you."}
{"instruction": "systemd apparently not finding .service file", "output": "For people from Google:\n\nValidate with sudo systemd-analyze verify NAME.service\nWhen using a symlink, make sure it uses absolute path\nMake sure the name is like /etc/systemd/system/*.service\nDo sudo systemctl daemon-reload after changes"}
{"instruction": "nginx.service: Failed to read PID from file /run/nginx.pid: Invalid argument", "output": "That warning with the nginx.pid file is a know bug (at least for Ubutnu if not for other distros as well). More details here:\nhttps://bugs.launchpad.net/ubuntu/+source/nginx/+bug/1581864\nWorkaround (on a ssh console, as root, use the commands bellow):\n\n```\nmkdir /etc/systemd/system/nginx.service.d\nprintf \"[Service]\\nExecStartPost=/bin/sleep 0.1\\n\" > /etc/systemd/system/nginx.service.d/override.conf\nsystemctl daemon-reload\nsystemctl restart nginx \n\n```\n\nThen check if you still see that nginx.pid error and also if nginx is actually running and if you can connect to port 80 on your server.\nI would also check if this actually exists and the permissions on it:\n\n```\n/run/uwsgi/mysite3.sock\n\n```\n\nIf nginx is running and uWSGI is running as well then I guess it's a configuration problem\nI understand you want to use Django so I would recommend to review your actual configuration and compare it with the one from here:\nhttp://uwsgi-docs.readthedocs.io/en/latest/tutorials/Django_and_nginx.html\nI hope it helps!"}
{"instruction": "tmux open terminal failed: not a terminal", "output": "I think the issue is that the service does not have an associated tty. A workaround I've found is to change your tmux call in your script to\n\n```\ntmux new-session -s username -d\n\n```\n\n(username a user for whom the service was started)"}
{"instruction": "How to restart a service if its dependent service is restarted", "output": "You can use PartOf.\n\n```\n[Unit]\nAfter=foo.service\nRequires=foo.service\nPartOf=foo.service\n\n```\n\nFrom the systemd.unit man page:\n\nPartOf=\nConfigures dependencies similar to Requires=, but limited to stopping and restarting of units. When systemd stops or restarts the units listed here, the action is propagated to this unit. Note that this is a one-way dependency ‚Äî changes to this unit do not affect the listed units."}
{"instruction": "How to log to journald (systemd) via Python?", "output": "python-systemd has a JournalHandler you can use with the logging framework.\nFrom the documentation:\n\n```\nimport logging\nfrom systemd.journal import JournalHandler\n\nlog = logging.getLogger('demo')\nlog.addHandler(JournalHandler())\nlog.setLevel(logging.INFO)\nlog.info(\"sent to journal\")\n\n```"}
{"instruction": "systemd: &quot;Environment&quot; directive to set PATH", "output": "You can't use EnvVars in Environment directives. The whole Environment= will be ignored. If you use EnvironmentFile=, then the specified file will be loaded without substitution. So PATH=/local/bin:$PATH would be exactly that, and this is probably not what you want.\nUnder CentOS7 the following works.\n\n```\n# /etc/systemd/system/nagios.service.d/env.conf\n[Service]\nEnvironment=\"PATH=/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin\"\n\n> sudo systemctl daemon-reload\n> sudo systemctl restart nagios\n> sudo cat /proc/28647/environ\n...\nPATH=/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin\n...\n\n```"}
{"instruction": "systemctl status shows inactive dead", "output": "You have set Type=Forking, but your service doesn't work. Try \nType=oneshot\nYou have a \"&\" your ExecStart line, which is not necessary. \nThe service is disabled, which means it was not enabled to start at boot. You should run systemctl enable hello to set it to start at boot.\n\nYou can check man systemd.directives to find an index of all the directives that you can use in your unit files."}
{"instruction": "In Ansible, what&#39;s the diffence between the service and the systemd modules?", "output": "The module service is a generic one. According to the Ansible documentation :\n\nSupported init systems include BSD init, OpenRC, SysV, Solaris SMF, systemd, upstart.\n\nThe module systemd is available only from Ansible 2.2 and is dedicated to systemd.\nAccording to the developers of Ansible :\n\nwe are moving away from having everything in a monolithic 'service' module and splitting into specific modules, following the same model the 'package' module has."}
{"instruction": "docker change cgroup driver to systemd", "output": "Since I have two configuration file I need to add the entry in the second config file also -- /etc/systemd/system/docker.service.d/docker-thinpool.conf:\n\n```\n--exec-opt native.cgroupdriver=systemd \\\n\n```"}
{"instruction": "&quot;sudo systemctl enable docker&quot; not available: Automatically run Docker at boot on WSL2 (using a &quot;sysvinit&quot; / &quot;init&quot; command or a workaround)", "output": "This answer requires the latest version of Windows and WSL at the time of this posting, and it now works under both Windows 10 and 11.  Run wsl --version and confirm that you are on WSL 1.0.0 (not to be confused with WSL1) or later.\nIf you are on an older release of Windows or WSL, then wsl --version will likely just show the Help text.  See this answer for information on how to upgrade.\nIf you cannot upgrade at this time, then please see my original answer for a workaround for Windows 10.\n\n\nwhat is a workaround here to automatically start docker when opening Ubuntu on WSL2?\n\n\nOption 1: Enable Systemd support in WSL2\nThe latest release of WSL2 includes support for Systemd.  You can read how to enable it in this Community Wiki answer or my original Ask Ubuntu answer.\nHowever, my personal recommendation is to consider whether you really need Systemd.  It will add additional overhead and potentially other complications, and it isn't strictly necessary for Ubuntu to run (well) on WSL, as we've been doing for quite a few years without it.  Option 2 may be a better (and faster) option for many services.\nIf you do have Systemd enabled, then the commands in the original question should work for you:\n\n```\nsudo systemctl enable docker\nsudo systemctl start docker\n\n```\n\nDocker Engine should automatically start for you the next time you restart your WSL2 distribution.  However, please see the bottom of this answer for an important note on keeping the services running.\n\n\n\n\nOption 2:  Add the necessary commands to the [boot] section in /etc/wsl.conf:\n\n```\n[boot]\ncommand= service docker start\n\n```\n\nTo run multiple commands, separate them with a semicolon as in:\n\n```\n[boot]\ncommand= service docker start; service cron start\n\n```\n\n\n\nImportant Note: If you run a service (e.g. cron or docker) using either of these methods, please note that the WSL distribution will still auto-terminate when the last process that was started interactively completes.  You can see more discussion (and a workaround using keychain) for this in my answer to the Ask Ubuntu question Is it possible to run a WSL app in the background?."}
{"instruction": "What benefit do I get from JSVC over just using systemd?", "output": "In general, most of the functionality provided by jsvc is provided by systemd, with the exception of opening of privileged ports (see below). If possible, it is a very good idea to switch to using systemd functionality directly, since things become simpler and more efficient.\nYour unit file looks mostly OK, with the exception of\n\n```\nExecStart=/opt/tomcat/bin/startup.sh\nExecStop=/opt/tomcat/bin/shutdown.sh\n\n```\n\nThis part looks like another wrapper which can be replaced with a direct to\njava -jar ....\nOpening privileged sockets\nUnder Systemd this is usually done through socket activation. Systemd opens the socket and hands it to the daemon as an open file descriptor (like stdin, stdout, stderr). \nThe daemon can then be started as unprivileged user, and does not drop privileges itself. The daemon has to support this, and instead of opening the socket by itself, it should use the one it was given. Under Java this is made very problematic by the lack of support in the Java stdlib. \nAFAIK, tomcat does not support socket activation, so if you want to use an privileged port and run the daemon under an unprivileged user, jsvc might still be necessary."}
{"instruction": "Is it recommended to run systemd inside docker container?", "output": "I'd advise you to avoid systemd in a container if at all possible.\nSystemd mounts filesystems, controls several kernel parameters, has its own internal system for capturing process output, configures system swap space, configures huge pages and POSIX message queues, starts an inter-process message bus, starts per-terminal login prompts, and manages a swath of system services.  Many of these are things Docker does for you; others are system-level controls that Docker by default prevents (for good reason).\nUsually you want a container to do one thing, which occasionally requires multiple coordinating processes, but you usually don't want it to do any of the things systemd does beyond provide the process manager.  Since systemd changes so many host-level parameters you often need to run it as --privileged which breaks the Docker isolation, which is usually a bad idea.\nAs you say in the question, running one \"piece\" per container is usually considered best.  If you can't do this then a light-weight process manager like supervisord that does the very minimum an init process is required to is a better match, both for the Docker and Unix philosophies."}
{"instruction": "DOCKER_OPTS do not work in config file /etc/default/docker", "output": "According to docker documentation, The recommended way  to configure the daemon flags and environment variables for your Docker daemon is to use a systemd drop-in file. \nSo, for this specific case, do the following:\n\nUse the command sudo systemctl edit docker.service to open an override file for docker.service in a text editor.\nAdd or modify the following lines, substituting your own values.\n\n```\n[Service]\nExecStart=\nExecStart=/usr/bin/dockerd -H tcp://127.0.0.1:2375 -H unix:///var/run/docker.sock\n\n```\n\nSave the file.\nReload the systemctl configuration.\n\n```\n $ sudo systemctl daemon-reload\n\n```\n\nRestart Docker:\n\n```\n $ sudo systemctl restart docker.service\n\n```\n\nCheck to see whether the change was honored by reviewing the output of netstat to confirm dockerd is listening on the configured port.\n\n```\n$ sudo netstat -lntp | grep dockerd\ntcp        0      0 127.0.0.1:2375          0.0.0.0:*               LISTEN      3758/dockerd\n\n```"}
{"instruction": "Referencing Other Environment Variables in Systemd", "output": "This is really a question for unix & linux.  But nevertheless: No, systemd will not perform environment variable expansion inside Environment=.  From man systemd.exec:\n\n```\n   Environment=\n       Sets environment variables for executed processes. Takes a space-separated list of variable assignments. This\n       option may be specified more than once, in which case all listed variables will be set. If the same variable is\n       set twice, the later setting will override the earlier setting. If the empty string is assigned to this option,\n       the list of environment variables is reset, all prior assignments have no effect. Variable expansion is not\n       performed inside the strings, however, specifier expansion is possible. The $ character has no special meaning.\n       If you need to assign a value containing spaces to a variable, use double quotes (\") for the assignment.\n\n       Example:\n\n           Environment=\"VAR1=word1 word2\" VAR2=word3 \"VAR3=$word 5 6\"\n\n       gives three variables \"VAR1\", \"VAR2\", \"VAR3\" with the values \"word1 word2\", \"word3\", \"$word 5 6\".\n\n```\n\nAs you see from the example in the documentation $word just means $word no expansion will be performed.  The specifiers that that man talks about are the %i, %n, %u, etc.  They're in man systemd.unit (under their own man section).\n\nOn the other hand ExecStart= and its derivatives will perform environment variable expansion.  Using environment variables on the ExecStart= is the common workaround for extra environment variables in systemd.  I believe, that is also one of the reasons why so many recent programs accept the same parameters from the environment and from command line parameters.\nAn example of expansion in ExecStart=, from man systemd.service:\n\n```\n   Example:\n\n       Environment=\"ONE=one\" 'TWO=two two'\n       ExecStart=/bin/echo $ONE $TWO ${TWO}\n\n   This will execute /bin/echo with four arguments: \"one\", \"two\", \"two\", and \"two two\".\n\n   Example:\n\n       Environment=ONE='one' \"TWO='two two' too\" THREE=\n       ExecStart=/bin/echo ${ONE} ${TWO} ${THREE}\n       ExecStart=/bin/echo $ONE $TWO $THREE\n\n   This results in echo being called twice, the first time with arguments \"'one'\", \"'two two' too\", \"\", and the second\n   time with arguments \"one\", \"two two\", \"too\".\n\n```\n\n\nsystemd has its documentation spread out across several man's, but one gets used to them after a while."}
{"instruction": "Starting a systemd service via python", "output": "You can use systemd's DBus API to call the RestartUnit method of the Manager (need of sufficient privileges, else it won't work)\n\n```\nimport dbus\nsysbus = dbus.SystemBus()\nsystemd1 = sysbus.get_object('org.freedesktop.systemd1', '/org/freedesktop/systemd1')\nmanager = dbus.Interface(systemd1, 'org.freedesktop.systemd1.Manager')\njob = manager.RestartUnit('sshd.service', 'fail')\n\n```"}
{"instruction": "Unable to start postgresql.service?", "output": "Finally, I figured this one out. There was already a file present \n\n```\n/usr/lib/systemd/system/postgresql-9.6.service\n\n```\n\nSo, may be due to the presence of this file, I was not able to start postgresql.service. Then I tried to start postgresql-9.6.service as follows:\n\n```\n[code_master5@BitBox ~]$ sudo systemctl start postgresql-9.6.service\nFailed to start postgresql-9.6.service: Unit postgresql-9.6.service not found.\n\n```\n\nAnd, as you can see the output, again it failed. \nI simply deleted the file using sudo as I thought may be postgresql.service file is not being created by relevant program due to the presence of this file. Then I restarted the system. It's working fine since then, as you can see the output below:\n\n```\n[code_master5@BitBox ~]$ sudo systemctl status postgresql.service\n[sudo] password for code_master5: \n‚óè postgresql.service - PostgreSQL database server\nLoaded: loaded (/usr/lib/systemd/system/postgresql.service; enabled; vendor p\nActive: active (running) since Sat 2017-01-28 09:31:30 IST; 7h ago\nMain PID: 342 (postgres)\nTasks: 6 (limit: 4915)\nCGroup: /system.slice/postgresql.service\n       ‚îú‚îÄ342 /usr/bin/postgres -D /var/lib/postgres/data\n       ‚îú‚îÄ358 postgres: checkpointer process   \n       ‚îú‚îÄ359 postgres: writer process   \n       ‚îú‚îÄ360 postgres: wal writer process   \n       ‚îú‚îÄ361 postgres: autovacuum launcher process   \n       ‚îî‚îÄ362 postgres: stats collector process   \n\nJan 28 09:31:26 BitBox systemd[1]: Starting PostgreSQL database server...\nJan 28 09:31:28 BitBox postgres[340]: FATAL:  the database system is starting up\nJan 28 09:31:28 BitBox postgres[340]: LOG:  database system was shut down at 201\nJan 28 09:31:29 BitBox postgres[340]: FATAL:  the database system is starting up\nJan 28 09:31:29 BitBox postgres[340]: LOG:  MultiXact member wraparound protecti\nJan 28 09:31:29 BitBox postgres[340]: LOG:  database system is ready to accept c\nJan 28 09:31:29 BitBox postgres[340]: LOG:  autovacuum launcher started\nJan 28 09:31:30 BitBox systemd[1]: Started PostgreSQL database server.\n\n```\n\nI would surely like to warn all those having same problem. Please do whatever I did at your own risk. Since these are system files. Messing with these can spoil your weekend!\nI am still a bit confused on this though. Explanations are welcome!"}
{"instruction": "What is the Difference between ConditionPathExists= and ConditionPathExists=| in systemd?", "output": "Sometime you specify multiple files like:\n\n```\nConditionPathExists=!/tmp/abc\nConditionPathExists=!/tmp/abe\n\n```\n\nNow if any of the condition isn't satisfied, it doesn't start service. It's like and operations.\nNow if you use:\n\n```\nConditionPathExists=|!/tmp/abc\nConditionPathExists=|!/tmp/abe\n\n```\n\nIf any of these conditions is satisfied, it will run the service.\n\nCondition checks can be prefixed with a pipe symbol (|) in which case\n  a condition becomes a triggering condition. If at least one triggering\n  condition is defined for a unit, then the unit will be executed if at\n  least one of the triggering conditions apply and all of the\n  non-triggering conditions\n\nIt's like OR operations"}
{"instruction": "How stop systemd service", "output": "You need to put in a ExecStop option in the [Service] section with the command you want to use to stop the service.\nSomething like:\n\n```\n[Service]\nEnvironment=FLUME_CLASSPATH=/opt/flume/current/lib/\nExecStart=/usr/bin/nohup /usr/bin/flume-ng agent -c /etc/flume-ng/conf -f /etc/flume-ng/conf/flume.conf --name a1 &\nExecStop=/usr/bin/flume-ng agent stop\n\n```\n\nor whatever the command is to stop the flume-ng\nThen you can stop the service with systemctl stop flume-ng.\nRead the manual at https://www.freedesktop.org/software/systemd/man/systemd.service.html for the full set of options available to control the service."}
{"instruction": "Can&#39;t detach child process when main process is started from systemd", "output": "Solution is to add\n\n```\nKillMode=process\n\n```\n\nto the service block. Default value is control-group which means systemd cleans up any child processes.\nFrom man systemd.kill\n\nKillMode= Specifies how processes of this unit shall be killed. One of\n  control-group, process, mixed, none.\nIf set to control-group, all remaining processes in the control group\n  of this unit will be killed on unit stop (for services: after the stop\n  command is executed, as configured with ExecStop=). If set to process,\n  only the main process itself is killed. If set to mixed, the SIGTERM\n  signal (see below) is sent to the main process while the subsequent\n  SIGKILL signal (see below) is sent to all remaining processes of the\n  unit's control group. If set to none, no process is killed. In this\n  case, only the stop command will be executed on unit stop, but no\n  process be killed otherwise. Processes remaining alive after stop are\n  left in their control group and the control group continues to exist\n  after stop unless it is empty."}
{"instruction": "set umask for Tomcat via tomcat.service in SystemD", "output": "Try adding UMASK as Environment variable into tomcat's service file:\n\n```\n[Service]\n...\nEnvironment='UMASK=0022'\n...\n\n```\n\nDefault catalina.sh is checking for environment's $UMASK:\n\n```\n# Set UMASK unless it has been overridden\n if [ -z \"$UMASK\" ]; then\n  UMASK=\"0027\"\n fi\n umask $UMASK\n\n```\n\n(It seems to me, that UMask from systemd is not used by Tomcat, but I am not completely sure.)"}
{"instruction": "Systemd http health check", "output": "The Short Answer\nsystemd has a native (socket-based) healthcheck method, but it's not HTTP-based. You can write a shim that polls status over HTTP and forwards it to the native mechanism, however.\n\nThe Long Answer\nThe Right Thing in the systemd world is to use the sd_notify socket mechanism to inform the init system when your application is fully available. Use Type=notify for your service to enable this functionality.\nYou can write to this socket directly using the sd_notify() call, or you can inspect the NOTIFY_SOCKET environment variable to get the name and have your own code write READY=1 to that socket when the application is returning 200s.\nIf you want to put this off to a separate process that polls your process over HTTP and then writes to the socket, you can do that -- ensure that NotifyAccess is set appropriately (by default, only the main process of the service is allowed to write to the socket).\n\nInasmuch as you're interested in detecting cases where the application fails after it was fully initialized, and triggering a restart, the sd_notify socket is appropriate in this scenario as well:\nSend WATCHDOG_USEC=... to set the amount of time which is permissible between successful tests, then WATCHDOG=1 whenever you have a successful self-test; whenever no successful test is seen for the configured period, your service will be restarted."}
{"instruction": "Docker (CentOS 7 with SYSTEMCTL) : Failed to mount tmpfs &amp; cgroup", "output": "try to run your container in privileged mode: \n\n```\ndocker run -ti --privileged=true -v /sys/fs/cgroup:/sys/fs/cgroup:ro -p 80:80 local/centos7-systemd\n\n```\n\nthis should solve your problem"}
{"instruction": "Systemd: Using both After and Requires", "output": "It is perfectly fine to use both After= and Requires=. They have different purposes. Requires= sets up a start dependency. systemd makes sure that if any body is trying to start foo.service, it should start bar.service too. Should bar.service fails at some point, then foo.service is taken down too. \nAfter= is putting a start order between services. If both of the services are scheduled to start, then After= makes sure the start order is set.\nYou can look at systemd's own service file as an example.\n\n```\n/lib/systemd/system/basic.target\n[Unit]\n...\nRequires=sysinit.target\nAfter=sysinit.target\n...\n\n```"}
{"instruction": "Is there any way to list systemd services in linux &quot;in-the-order-of&quot; they were loaded?", "output": "Unfortunately, due to the parallel nature of the boot up process and the potentially complex dependency relationships among services, the service start up order isn't very deterministic.  However, systemd-analyze, if present on your system, can graphically plot the start up order of services:\n\n```\nsystemd-analyze plot > startup_order.svg\n\n```\n\nIt can also be used to graphically plot service dependencies:\n\n```\nsystemd-analyze dot | dot -Tsvg > systemd.svg\n\n```"}
{"instruction": "Cannot disable systemd serial-getty service", "output": "The correct way to stop a service ever being enabled again is to use:\n\n```\nsystemctl mask serial-getty@ttyAMA0.service\n\n```\n\n(using ttyAMA0 as the example in this case). This will add a link to null to the entry for that service."}
{"instruction": "systemd: start service at boot time after network is really up (for WoL purpose)", "output": "I tried all these targets, and they all were reached before DHCP got an IP address. Go figure:\n\nnetwork-online.target\nremote-fs.target\nnfs-client.target\ndbus.service\n\nWhat did work was enabling these two:\n\n```\nsystemctl enable systemd-networkd.service systemd-networkd-wait-online.service\n\n```\n\nAnd then add the following into your .service file:\n\n```\n[Unit]\nAfter=systemd-networkd-wait-online.service\nWants=systemd-networkd-wait-online.service\n\n```\n\nNow it got started after DHCP got an IP address. (A mount point in my case, but could have been your service too)\n(On debian9/stretch)"}
{"instruction": "How do I list all systemd masked units?", "output": "The --state option would do the job\n\n```\nsystemctl list-unit-files --state=masked\n\n```"}
{"instruction": "Supervisor fails to restart half of the time", "output": "This is not necessarily an error from supervisor. I see from your systemctl status output that supervisor is started through the sysv-init compatibility layer, so the failure could be in the /etc/init.d/supervisor script. It would explain the absence of errors in the supervisord logs.\nTo debug the init script, the easiest way is to add a set -x as first non-comment instruction in that file, and look in the journalctl output the trace of the script execution.\nEDIT:\nI've reproduced and debugged it on a test system with Debian Sid. \nThe issue is that the stop target of the supervisor init-script does not check if the daemon has been really terminated but only send a signal if the process exists. If the daemon process takes a while to shutdown, the subsequent start action will fail due to the dying daemon process, which is counted as already running.\nI've opened a bug on Debian Bug Tracker: http://bugs.debian.org/805920\nWORKAROUND:\nYou can workaround the issue with: \n\n```\n/etc/init.d/supervisor force-stop && \\\n/etc/init.d/supervisor stop && \\\n/etc/init.d/supervisor start\n\n```\n\n\nforce-stop will ensure the supervisord has been terminated (outside systemd).\nstop make sure systemd know it's terminated\nstart starts it again\n\nThe stop after the force-stop is required otherwise systemd will ignore any subsequent start request. stop and start can be combined using restart, but here I've put both of them to show how it works."}
{"instruction": "How to define a d-bus activated systemd service?", "output": "Lets take a look at one of the services coming with systemd, hostnamed. \n\n```\n# cat /usr/share/dbus-1/system-services/org.freedesktop.hostname1.service\n\n#  This file is part of systemd.\n#\n#  systemd is free software; you can redistribute it and/or modify it\n#  under the terms of the GNU Lesser General Public License as published by\n#  the Free Software Foundation; either version 2.1 of the License, or\n#  (at your option) any later version.\n\n[D-BUS Service]\nName=org.freedesktop.hostname1\nExec=/bin/false\nUser=root\nSystemdService=dbus-org.freedesktop.hostname1.service\n\n```\n\nThe magic is SystemdService= directive. The service specified with SystemdService= is what dbus-daemon asks systemd to activate. \nWe are expecting a service called dbus-org.freedesktop.hostname1.service in systemd service directory.\n\n```\n# readlink /usr/lib/systemd/system/dbus-org.freedesktop.hostname1.service\nsystemd-hostnamed.service\n\n```\n\nThere you go, this way a dbus service org.freedesktop.hostname1.service tells systemd to activate a systemd service systemd-hostnamed.service.\nAnd the systemd service looks like\n\n```\n# cat /usr/lib/systemd/system/systemd-hostnamed.service\n...\n...\n[Service]\nBusName=org.freedesktop.hostname1\n...\n...\n\n```\n\nThe magic on the systemd service file is BusName= directive. This directive tells systemd to wait until the given bus name to appear on the bus before proceeding.\nNote: A dbus service has completely different syntax than systemd service. You need both to be able to have a dbus activated daemon."}
{"instruction": "Starting Docker-Engine on boot", "output": "Yes. You can use this command to enable automatic start of the docker service after startup:\n\n```\n sudo systemctl enable /usr/lib/systemd/system/docker.service\n\n```"}
{"instruction": "Systemd: Start operation timed out. Terminating", "output": "Your type seems wrong, forking is for programs that detach immediately by themselves. Flask does not, it stays attached to your console. \nYour service type should probably be simple"}
{"instruction": "Passing a JSON file as environment variable in Docker", "output": "Updated answer\nYou mentioned that you use the docker run command in a systemd unit file. A systemd ExecStart options is not started in a shell. Environment variable substitution is supported by name. Also see the documentation on this:\n\nBasic environment variable substitution is supported. Use \"${FOO}\" as part of a word, or as a word of its own, on the command line, in which case it will be replaced by the value of the environment variable including all whitespace it contains, resulting in a single argument.\n\nThe doc also says that StartExec is not executed in a shell:\n\nThis syntax is intended to be very similar to shell syntax, but only the meta-characters and expansions described in the following paragraphs are understood. Specifically, redirection using \"<\", \"<<\", \">\", and \">>\", pipes using \"|\", running programs in the background using \"&\", and other elements of shell syntax are not supported. [...] Note that shell command lines are not directly supported.\n\nHowever, you can use ExecStart to start a shell and then pass a command using the -c flag (you still need to quote the variable as mentioned in my original answer below):\n\n```\nExecStart=/bin/bash -c \"docker run -e \\\"TEMP_CONFIG=$(</etc/config.json)\\\" ...\"\n\n```\n\n\nOriginal answer\nYour JSON string contains spaces, and without quoting your shell will interpret everything after the first space as subsequent arguments. So TEMP_CONFIG=$(cat /etc/config.json) is essentially equivalent to:\n\n```\n--env TEMP_CONFIG={ \"conf\" : { \"...\n\n```\n\nIn this case, the TEMP_CONFIG environmant variable will have the value {, and docker run will assume \"conf\" to be the next argument (in this case, the image name).\nSolution: Quote your bash variables:\n\n```\n--env \"TEMP_CONFIG=$(cat /etc/config.json)\"\n\n```\n\nAlso, don't use cat when you don't have to:\n\n```\n--env \"TEMP_CONFIG=$(</etc/config.json)\"\n\n```"}
{"instruction": "Why use ExecStart= (with no value) before another ExecStart=/new/value in a systemd override?", "output": "When you add entries to an override file, they are by default appended to any existing entries. That is, if your service example.service has:\n\n```\n[Service]\nEnvironmentFile=/etc/foo.env\n\n```\n\nAnd you create /etc/systemd/system/example.service.d/override.conf\nwith:\n\n```\n[Service]\nEnvironmentFile=/etc/bar.env\n\n```\n\nThen the effective configuration is:\n\n```\n[Service]\nEnvironmentFile=/etc/foo.env\nEnvironmentFile=/etc/bar.env\n\n```\n\nThat's fine for many directives, but a service can have only one\nExecStart (unless it's a Type-oneshot service), so if you try to create an override file like this:\n\n```\n[Service]\nExecStart=/new/command/line\n\n```\n\nThat will fail with an error along the lines of:\n\n```\nsystemd: example.service has more than one ExecStart= setting, which is only allowed for Type=oneshot services. Refusing.\n\n```\n\nBy specifying an empty ExecStart, you are \"clearing out\" all\nprevious entries. So if your example.service has:\n\n```\n[Service]\nExecStart=/bin/foo\n\n```\n\nAnd you create an override like:\n\n```\n[Service]\nExecStart=\nExecStart=/bin/bar\n\n```\n\nThe effective configuration is:\n\n```\n[Service]\nExecStart=/bin/bar\n\n```"}
{"instruction": "using sudo with ExecStart (systemd)", "output": "Systemd starts the executable stated in ExecStart= as root by default.\nHowever, if you have specified User= or Group= in your service file overriding that default, and still need to run an executable that requires sudo, prepend the command with the absolute path to your sudo location:\n\n```\n...\nExecStart=/usr/bin/sudo /usr/bin/nodejs /var/www/bin/app.js\n...\n\n```"}
{"instruction": "How to use Systemd to restart a service when down?", "output": "If you are using a systemd service file to start your service, then add the lines below to your service file from where you are starting your service:\n\n```\n[Service]\nType=simple\nExecStart=here will be your service executable name\nRestart=always\nRestartSec=0\n\n```\n\n\nRestart=\nConfigures whether the service shall be restarted when the service process exits, is killed, or a timeout is reached. Takes one of the following values: no, on-success, on-failure, on-abnormal, on-watchdog, on-abort or always. If set to no (the default).\nRestartSec=\nConfigures the time to sleep before restarting a service (as configured with Restart=). Takes a unit-less value in seconds.\n\nThese two options have to be under the [Service] tag in a service file."}
{"instruction": "Launch docker automatically when starting ec2 server", "output": "Using Amazon Linux 2 I tried to replicate the issue. Obviously, I don't have all the dependencies to run your exact docker-compose.yml, thus I used the docker-compose.yml from here for my verification. The file setups wordpress with mysql .\nSteps I took were following (executed as ec2-user in home folder):\n1. Install docker\n\n```\nsudo yum update -y  \nsudo yum install -y docker\nsudo systemctl enable docker\nsudo systemctl start docker\n\n```\n\n2. Install docker-compose\n\n```\nsudo curl -L https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m) -o /usr/bin/docker-compose\n\nsudo chmod +x /usr/bin/docker-compose\n\n```\n\n3. Create docker-compose.yml\n\n```\nmkdir myapp \n\n```\n\nCreate file ./myapp/docker-compose.yml:\n\n```\nversion: '3.3'\n\nservices:\n   db:\n     image: mysql:5.7\n     volumes:\n       - db_data:/var/lib/mysql\n     restart: always\n     environment:\n       MYSQL_ROOT_PASSWORD: somewordpress\n       MYSQL_DATABASE: wordpress\n       MYSQL_USER: wordpress\n       MYSQL_PASSWORD: wordpress\n\n   wordpress:\n     depends_on:\n       - db\n     image: wordpress:latest\n     ports:\n       - \"8000:80\"\n     restart: always\n     environment:\n       WORDPRESS_DB_HOST: db:3306\n       WORDPRESS_DB_USER: wordpress\n       WORDPRESS_DB_PASSWORD: wordpress\n       WORDPRESS_DB_NAME: wordpress\nvolumes:\n    db_data: {}\n\n```\n\n4. Create docker_boot.service\nThe file is different then yours, as there were few potential issues in your file:\n\nnot using absolute paths\nec2-user may have no permissions to run docker\n\nCreate file ./myapp/docker_boot.service:\n\n```\n[Unit]\nDescription=docker boot\nAfter=docker.service\n\n[Service]\nType=oneshot\nRemainAfterExit=yes\nWorkingDirectory=/home/ec2-user/myapp\nExecStart=/usr/bin/docker-compose -f /home/ec2-user/myapp/docker-compose.yml up -d --remove-orphans\n\n[Install]\nWantedBy=multi-user.target\n\n```\n\n5. Copy docker_boot.service to systemd\n\n```\nsudo cp -v ./myapp/docker_boot.service /etc/systemd/system\n\n```\n\n6. Enable and start docker_boot.service\n\n```\nsudo systemctl enable docker_boot.service\nsudo systemctl start docker_boot.service\n\n```\n\nNote: First start may take some time, as it will pull all docker images required. Alternatively start docker-compose manually first to avoid this.\n7. Check status of the docker_boot.service\n\n```\nsudo systemctl status docker_boot.service\n\n```\n\n8. Check if the wordpress is up\n\n```\ncurl -L localhost:8000\n\n```\n\n9. Reboot\nCheck if the docker_boot.service is running after instance reboot by logging in into the instance and using sudo systemctl status docker_boot.service and/or curl -L localhost:8000."}
{"instruction": "Using variable in command path for ExecStart in systemd service", "output": "You can't use variables in the actual command. systemd.service:\n\nThe command to execute must be an absolute path name. It may contain\n  spaces, but control characters are not allowed.\n\nYou might wan't to wrap it in a shell command (which does parameter expansion):\n\n```\nExecStart=/bin/bash -c '/${TESTEXTSERVICESFILES}/syslog/bin/nxlog -c ${TESTCONFDATA}/syslog/nxlog.conf'\n\n```"}
{"instruction": "Configure Prometheus to use non-default port", "output": "The quotes were superfluous. This line will work:\n\n```\nExecStart=/usr/local/bin/prometheus \\\n  --config.file=/etc/prometheus.yaml --web.enable-admin-api \\\n  --web.listen-address=:8080\n\n```"}
{"instruction": "Start Docker container using systemd socket activation?", "output": "In short, you can't.\nBut, if you wanted to approach a solution, you would first need to run a tool like CoreOS or geard that runs each Docker container in a systemd service.\nEven then, Docker's support for inheriting the socket has come and gone. I know geard is working on stable support. CoreOS has published generalized support for socket activation in Go. Red Hat folks have also added in related patches to Fedora's Docker packages that use Go's socket activation library and improve \"foreground mode,\" a key component in making it work.\n(I am the David Strauss from Lennart's early article on socket activation of containers, and this topic interests me a lot. I've emailed the author of the patch at Red Hat and contacted the geard team. I'll try to keep this answer updated.)"}
{"instruction": "How to get actual path to executable when using .netcore 3.0 and using the /p:PublishSingleFile=true flag?", "output": "The following seems to give the path you're after, to the original executable:\n\n```\nSystem.Diagnostics.Process.GetCurrentProcess().MainModule.FileName\n\n```"}
{"instruction": "Nginx log to stderr", "output": "According to the nginx documentation, the error_log directive supports stderr as its argument. The following configuration should therefore log error messages to stderr:\n\n```\nhttp {\n    error_log stderr;\n    ...\n}\n\n```\n\nUnfortunately, access_log doesn't support stdout as its argument. However, it should be possible to set it to syslog (documentation) and get systemd to include syslog calls into its journal."}
{"instruction": "Network usage top/htop on Linux", "output": "jnettop is another candidate.\nedit: it only shows the streams, not the owner processes."}
{"instruction": "How do I interpret the memory usage information from htop", "output": "htop author here.\n\nDoes server #1 have more memory available than server #2?\n\nYes. \nFrom the htop faq:\n\nThe memory meter in htop says a low number, such as 9%, when top shows something like 90%! (Or: the MEM% number is low, but the bar looks almost full. What's going on?)\nThe number showed by the memory meter is the total memory used by processes. The additional available memory is used by the Linux kernel for buffering and disk cache, so in total almost the entire memory is in use by the kernel. I believe the number displayed by htop is a more meaningful metric of resources used: the number corresponds to the green bars; the blue and brown bars correspond to buffers and cache, respectively (as explained in the Help screen accessible through the F1 key). Numeric data about these is also available when configuring the memory meter to display as text (in the Setup screen, F2). \n\nHope that clears things up! Cheers!"}
{"instruction": "Htop showing multiple java processes with different pids", "output": "Actually, it's just that htop by default shows each thread as a separate process.  You can add the line hide_userland_threads=1 to your ~/.config/htop/htoprc file to show only one line.\nSee https://unix.stackexchange.com/questions/10362/why-does-htop-show-more-process-than-ps"}
{"instruction": "Trick to loop/autorefresh docker ps view like top/htop in bash", "output": "Use watch:\n\n```\nwatch docker ps\n\n```\n\nSee man watch"}
{"instruction": "Why does HTOP show exclamation mark next to uptime", "output": "htop adds an exclamation mark if the uptime is greater than 100 days.\nFrom: https://github.com/hishamhm/htop/blob/0e8a02367ec7ca8f52b10de70938dfd07faed3ab/UptimeMeter.c#L41"}
{"instruction": "Python thread name doesn&#39;t show up on ps or htop", "output": "First install the prctl module.  (On debian/ubuntu just type sudo apt-get install python-prctl)\n\n```\nfrom threading import Thread\nimport time\nimport prctl\n\ndef sleeper():\n    prctl.set_name(\"sleeping tiger\")\n    while True:\n        time.sleep(10)\n        print \"sleeping\"\n\nt = Thread(target=sleeper, name=\"Sleeper01\")\nt.start()\nt.join()\n\n```\n\nThis prints\n\n```\n$ ps -T\n  PID  SPID TTY          TIME CMD\n22684 22684 pts/29   00:00:00 bash\n23302 23302 pts/29   00:00:00 python\n23302 23303 pts/29   00:00:00 sleeping tiger\n23304 23304 pts/29   00:00:00 ps\n\n```\n\nNote: python3 users may wish to use pyprctl."}
{"instruction": "How to calculate system memory usage from /proc/meminfo (like htop)", "output": "htop author here. These are the calculations I make to get the numbers for the green, blue and yellow bars in the memory meter:\n\nTotal used memory = MemTotal - MemFree\nNon cache/buffer memory (green) = Total used memory - (Buffers + Cached memory) \nBuffers (blue) = Buffers\nCached memory (yellow) = Cached + SReclaimable - Shmem\nSwap = SwapTotal - SwapFree\n\nIn the htop source code: linux/LinuxProcessList.c and linux/Platform.c."}
{"instruction": "What does a C process status mean in htop?", "output": "htop author here. I am not aware of such status code in the htop codebase. \nKeep in mind that htop is written for Linux only, so there is no support for macOS/OSX. When I hear of people running it on OSX they are often using an outdated, unsupported fork (the latest version of htop is 2.0.1, including macOS support)."}
{"instruction": "When using htop command, do red values in the time+ column mean there&#39;s something wrong?", "output": "I was curious about this too, so I dug into the source code and found this:\n\n```\nif (hours >= 100) {\n   snprintf(buffer, 10, \"%7lluh \", hours);\n   RichString_append(str, CRT_colors[LARGE_NUMBER], buffer);\n} else {\n   if (hours) {\n      snprintf(buffer, 10, \"%2lluh\", hours);\n      RichString_append(str, CRT_colors[LARGE_NUMBER], buffer);\n      snprintf(buffer, 10, \"%02d:%02d \", minutes, seconds);\n   } else {\n      snprintf(buffer, 10, \"%2d:%02d.%02d \", minutes, seconds, hundredths);\n   }\n   RichString_append(str, CRT_colors[DEFAULT_COLOR], buffer);\n}\n\n```\n\nSo, it looks like whenever the CPU time exceeds one hour, the hour portion is just highlighted in red (or whatever CRT_colors[LARGE_NUMBER] happens to be.)\nNotice that the time format changes as the time goes:\n4:33.42 is minutes/seconds/millisconds\n18h26:41 is hours/minutes/seconds\n101h  would be hours > 100"}
{"instruction": "Find tmux session that a PID belongs to", "output": "Given that PID in the below line is the target pid number:\n\n```\n$ tmux list-panes -a -F \"#{pane_pid} #{pane_id}\" | grep ^PID\n\n```\n\nThe above will identify the pane where the PID is running. The output will be two strings. The first number should be the same as PID and the second one (with a percent sign) is \"tmux pane id\". Example output:\n\n```\n2345 %30\n\n```\n\nNow, you can use \"tmux pane id\" to kill the pane without \"manually\" searching for it:\n\n```\n$ tmux kill-pane -t %30\n\n```\n\n\nTo answer your question completely, in order to find *tmux session* that a PID belongs to, this command can be used:\n\n```\n$ tmux list-panes -a -F \"#{pane_pid} #{session_name}\" | grep ^PID\n# example output: 2345 development\n\n```\n\nHere's another possibly useful \"line\":\n\n```\n$ tmux list-panes -a -F \"#{pane_pid} #{session_name}:#{window_index}:#{pane_index}\" | grep ^PID\n# example output: 2345 development:2:0\n\n```\n\n\nThe descriptions for all of the interpolation strings (example #{pane_pid}) can be looked up in tmux man page in the FORMATS section."}
{"instruction": "htop with web interface", "output": "Thanks everything works well!\nIn debian wheezy:\n\n```\n[sudo] service shellinaboxd stop\n\n```\n\nBecomes (without the letter 'd')\n\n```\n[sudo] service shellinabox stop\n\n```\n\nThe same applies to update-rc.d line\n\n```\n[sudo] update-rc.d -f shellinabox remove\n\n```"}
{"instruction": "Building htop from source in Cygwin", "output": "htop author here ‚Äî htop has a lot of platform-specific code, so porting it is not a matter of simply recompiling it. The current version of htop does not support Cygwin. The latest release version, 2.0.1, supports Linux, Mac OS X, FreeBSD and OpenBSD.\nI'm happy to accept patches if someone writes a Cygwin port, though!"}
{"instruction": "Why does Java spawn so many processes?", "output": "A java application is never single threaded (Garbage Collector thread among other). So you always end up with several thread. Moreover, Linux implement thread as Light-weight process therefore, each Java thread will appear as a process with a unique PID in htop."}
{"instruction": "Missing interface elements in htop", "output": "Putty is rendering \"gray\" color (sometimes referred to as \"bright black\") as black. Go to the Putty settings and tweak the colors, it should fix it."}
{"instruction": "How to fetch list of running processes on a system and sort them by various parameters", "output": "Check out sys-proctable:\n\n```\nrequire 'sys/proctable'\n\nSys::ProcTable.ps\n\n```\n\nTo sort by starttime:\n\n```\nSys::ProcTable.ps.sort_by(&:starttime)\n\n```"}
{"instruction": "Way too much resources are used with Pytorch", "output": "Yes, there is. You can use torch.set_num_threads(...) to specify the number of threads. Depending on the PyTorch version you use, maybe this function will not work correctly. See why in this issue. In there, you'll see that if needed you can use environment variables to limit OpenMP or MKL threads usage via OMP_NUM_THREADS=? and MKL_NUM_THREADS=? respectively, where ? is the number of threads.\nKeep in mind that these things are expected to run on GPUs with thousands of cores, so I would limit CPU usage only when extremely necessary."}
{"instruction": "Is it normal for Rails application to keep so many idle Puma and Postgres connections?", "output": "If the number of processes matches your concurrency configuration i would say that's ok, if it keeps growing with every request then you may have an issue with processes hanging. The default for puma i believe is 16. It also looks like you are using clustered mode so it would have multiple processes and multiple threads per process."}
{"instruction": "Why does &quot;htop&quot; show me dozens of PIDs in use by my app, but &quot;ps&quot; only shows me one?", "output": "Those probably are the threads used by your application. To only display the process you can press F2 (Setup) > Display options > Hide userland threads > F10 (Save).\nYou can see here an example with hidden/shown user threads (they are shown here as a tree because the option Tree view from the same menu is enabled):\n\nEDIT: I forgot to mention that the numbers in the PID column are not PID's of the threads, but a kernel thread \"ID\" (not sure if this is the right term). Using the scenario of the image above as source, you would find them in /proc/2321/tasks:\n\n```\n$ ls /proc/2321/task      \n2321  2323  2325  2326  2327\n\n```"}
{"instruction": "Htop process viewer - set column width", "output": "Got here from this question: https://serverfault.com/questions/740496/htop-cgroup-column-widen/740533?noredirect=1#comment926040_740533 which is a duplicate of yours.\nIn short, in current versions the column width is fixed. You can ask for this enhancement to the devs or implement it yourself :-)\nYou can use ps to find the processes in control groups you're interested in."}
{"instruction": "using php to stream data of programs such as `htop`", "output": "htop is an interactive program that runs in a loop until exited, and doesn't seem to offer any flags to do a single iteration then exit which makes capturing the output problematic.\nHave you considered using top? You can limit it to 1 iteration with -n 1:\n\n```\n$resp = system(\"top -n 1\");\nprint $resp; \n\n```"}
{"instruction": "Segfault using htop on AWS Sagemaker pytorch-1.10-cpu-py38 app", "output": "I fixed this with\n\n```\n# Note: add sudo if needed:\nln -fs /lib/x86_64-linux-gnu/libncursesw.so.6 /opt/conda/lib/libncursesw.so.6\n\n```"}
{"instruction": "htop ubuntu wily percentage numbers missing", "output": "htop author here. This is most likely a problem in the way your terminal handles gray text. In terminal ANSI colors (man console_codes) gray text is made by combining black foreground and bold, and this is what htop uses. However, some terminals display this combination as \"bold black\", rendering the text invisible when written over a black background.\nIn the version of htop currently in git, there is a \"Broken Gray\" color scheme, that is identical to the default, but does not use gray at all. You may want to use that, or try to configure your terminal colors so that it properly displays gray."}
{"instruction": "Why is fresh ElasticSearch Install running so Many processes?", "output": "The simple answer is that in order to be efficient ES uses many threadpools for carrying out the many things it needs to do. \nAs you probably know ES provides a very powerful search engine. \nSo in order to enable a potentially massive amount of users to run a potentially massive amount of queries efficiently, ES uses a pool of threads to carry out that work.\nThat's not the end of the story. While all those users might search like mads, other users or processes can also index a potentially massive amount of data at the same time. For that reason ES needs another thread pool for handling the many indexing requests it can get. Those indexing requests can come in two forms: indexing a single document, indexing many documents in bulk. For those two indexing processes, ES uses two different thread pools.\nThat's still not the end of the story. While some users are searching and some others are indexing data, there might be a backup process running (what ES calls snapshotting). For that there's another threadpool.\nAnd so on. The list is not exhaustive, but you can trust that ES has several threadpools in order to handle what it needs to handle and it knows how to do it efficiently as it will only create as many threads as your available processors can handle.\nYou can review the full list of threadpools that ES is managing and you'll probably better understand what it is doing. You can also use the /_cat/thread_pool and the /_nodes/hot_threads endpoints in order to better visualise what those threads are doing."}
{"instruction": "What am I setting when I limit the number of &quot;threads&quot;?", "output": "(This might be better as a comment, feel free to remove this if a better answer comes up, as it's based on my experience using the libraries.)\nI had a similar issue when multiprocessing parts of my code. The numpy/scipy libraries appear to spin up extra threads when you do vectorised operations if you compiled the libraries with BLAS or MKL (or if the conda repo you pulled them from also included a BLAS/MKL library), to accelerate certain calculations.\nThis is fine when running your script in a single process, since it will spawn threads up to the number specified by OPENBLAS_NUM_THREADS or MKL_NUM_THREADS (depending on if you have a BLAS library or MKL library - you can identify which by using numpy.__config__.show()), but if you are explicitly using a multiprocesing.Pool, then you likely want to control the number of processes in multiprocessing - in this case, it makes sense to set n=1 (before importing numpy & scipy), or some small number to make sure you are not oversubscribing:\n\n```\nn = '1'\nos.environ[\"OMP_NUM_THREADS\"] = n\nos.environ[\"MKL_NUM_THREADS\"] = n\n\n```\n\nIf you set multiprocessing.Pool(processes=4), it will use 4*n processes (n threads in each process). In your case, it seems like you have a pool of 4 processes and they fire up 4 threads each, hence the 16 python processes.\nThe htop output gives 100% assuming a single CPU per core. As a Linux machine interprets a thread as a CPU (I might be wrong in the terminology here), if you have 4 threads per CPU, it means that the full load is actually 400%. This might not be maxed out, depending on the operations being performed (and on caching, as your machine looks hyperthreaded).\nSo if you're doing the numpy/scipy operation in parts of the code which are in a single process/single thread, you are better off setting a larger n, but for the multiprocessing sections, it might be better to set a larger pool and single or small n. Unfortunately, you can only set this once, at the beginning of your script if you're passing in flags through the environmental flags. If you want to set it dynamically, I saw in a numpy issues discussion somewhere that you should use threadpoolctl (I'll add a link if I can find it again)."}
{"instruction": "Htop says &quot;530G&quot; in &quot;VIRT&quot; for &quot;vagrant ssh&quot;", "output": "Answer by chrisroberts on github:\n\nHi! I was able to reproduce this behavior, but with any vagrant command executed. The vagrant ssh command is the easiest to see this behavior simply because the process is left running for as long as the ssh session is alive.\nThe tl;dr version of below is simply: Don't worry about it. VIRT isn't allocated memory. If it were, you would either need massive swap space, or nothing would be working.\nSo, what's going on here? The vagrant installer includes a small go executable (vagrant) whose job is to setup the current environment with the proper locations of everything it needs. The installers bin directory, the lib directory for ruby and all the other friends, all the gems, and the vagrant gem itself. Once it has all this configured, it spawns off a new process, the actual Ruby vagrant process.\nBecause your example was referencing vagrant ssh, and as was previously pointed out (#7296 (comment)) a Kernel.exec happens meaning the Ruby process does not persist, I figured it must be the wrapper that was the culprit. After a bit of searching (mostly to find stackoverflow items saying \"don't worry about VIRT\") I stumbled upon:\nkeybase/keybase-issues#1908\nThey refer to the golang FAQ that talks about a bunch of VIRT being claimed up front and it not being a big deal, but never any absolutes about how much was actually being claimed. A link to lwn was dropped in there (keybase/keybase-issues#1908 (comment)) regarding golang's behavior on startup of claiming a huge chunk of VIRT, but still everything referenced a much lower amount than I was seeing locally. So I decided to go dig into the golang runtime code, and within malloc.go we find the answer:\ngolang src/runtime/malloc.go\nThe why it's happening is because of the go wrapper used to start vagrant. Because the VIRT you see is simply a reservation and not actually allocated, it's not a problem and not something that should be worried about.\n(There are some interesting conversations on the golang ML around the pros and cons of this approach, all pretty great reads).\n\nIt's just a copy/paste (and bolded the TLDR), hope it could help someone else."}
{"instruction": "Memory report confusion shown by top and htop commands?", "output": "It looks like htop defaults to showing you the threads view. You can get the same effect in top by pressing shift-H. There is one mysql process using 3.2% memory and 17 threads.\nAlternatively, you can make htop display only a single line per process, like the default behavior in top."}
{"instruction": "htop reports 100% cpu steal time, top reports 0% after virsh restore", "output": "According to this thread, this is caused by a bug in KVM and is fixed in KVM 2.1.3 and 2.2.\nThe latest Debian package (kvm-qemu 1:2.1+dfsg-12+deb8u2) does not seem to include that fix."}
{"instruction": "sails.js multiple child process, what are those? Ubuntu", "output": "The threads you see using htop is userland threads(user's thread) but when you run your application in cluster mode(say 4 instances),it makes kernel thread.\nmore on user threads\nwhen you hide userland threads then whatever you see is kernel thread.while running application in cluster mode you will create multiple kernel threads of an application.And for each application's kernel thread there are several userland threads associated with it.\n(for htop you can hide userland thread by F2 and display options)\nplease see the screen shots for explanation.\nfour instances of my app in cluster mode since number of cores==4\n\nps commands showing kernel threads only\n\nhtop showing kernel threads only\n\nhtop showing kernel threads along with userland threads\n\nIn last screenshot you see a few kernel and userland's threads but there are more.\nI mean for each kernel thread there are some userland's thread associated with it."}
{"instruction": "Why my java long running Threads (5k+ thread) not utilizing all machines cores (12 cores)?", "output": "Thank you guys, I've fixed the problem and now Im having the 12 cores running up to maximum as you see in the picture. :)\n\nI actually tried to run this command jstack <Pid> to see the status of my all running threads in this process ID, and I found that 95% of my threads are actually BLOCKED at the logging line, I did some googling and found that I can use AsynchAppender in log4J so logging will not block the thread"}
{"instruction": "Huge difference between htop and ps aux output", "output": "They are measuring different things.\nFrom the ps man-page:\n\n```\n   CPU usage is currently expressed as the percentage of time spent\n   running during the entire lifetime of a process.  This is not ideal, \n   and it does not conform to the standards that ps otherwise conforms to.\n   CPU usage is unlikely to add up to exactly 100%.   \n\n```\n\nFrom the htop man-page (I am the author of htop):\n\n```\n   PERCENT_CPU (CPU%)\n        The  percentage  of  the  CPU  time  that the process is currently\n        using.\n\n```\n\nSo, in htop this is the percentage of total CPU time used by the program between the last refresh of the screen and now.\nPercentageInHtop = (non-idle CPU time used by process during the last 1.5s) / 1.5s\nIn ps this is the percentage of CPU time used by the program relative to the total time it exists (ie, since it was launched).\nPercentageInPs = (non-idle CPU time used by process since process startup) / (time elapsed since process startup)\nThat is, in your reading it means that htop is saying that the service is taking 12.3% of your CPU now, while ps is saying that your service has spent 99.9% of its total life idle."}
{"instruction": "How does htop display work?", "output": "htop author here. What you're looking for is the NCurses library.\nWith NCurses, you can implement a full-screen textmode application in C.\nIt provides a set of primitives for \"drawing\" on the terminal: ie, move() to go to a X-Y coordinate on the screen, functions to change colors, to erase part of the screen, etc. It also provides some high-level constructs such as \"windows\" through which you can scroll parts of the screen separately, but in htop I don't use that and implement my scrollable panels drawing them \"by hand\" using the lower-level primitives."}
{"instruction": "exit and clean up python fork", "output": "The parent process needs to wait for the child in order for the child process' resources to be released. Until then the process still exists in a \"zombie\" state, and it will still appear in ps and top etc.\nYou can call one of os.wait(), os.waitpid(),  os.wait3(), or os.wait4().\nos.wait3() with the os.WNOHANG option might be most useful to you as it will wait for any child process and the parent will not block until a child terminates (or it's state changes - wait will return child processes that have been stopped or restarted too).\nMore details on the underlying system calls can be found in the Linux man page: man 2 wait."}
{"instruction": "Python multiprocessing Pool.map uses all cores instead of the specified number", "output": "Thank you a lot @CharlesDuffy ! That post: limit number of threads in numpy? (and the relevant comments) has solved my problem.\nAdding those lines:\n\n```\nimport os\nos.environ[\"MKL_NUM_THREADS\"] = \"1\" \nos.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\" \nos.environ[\"OMP_NUM_THREADS\"] = \"1\" \n\n```\n\nBEFORE IMPORTING numpy, made my code work well on both systems !"}
{"instruction": "free the memory from a process in linux?", "output": "That depends on what you mean.  Most likely, the answer is no because this is a list of memory consumptions of different parts of the given process.  You typically have no influence over these parts.  [ anon ] just means that this part has no specific name which could be shown, so it is declared anonymous.  (It could be the main program of the process as opposed to some library used within that process.)\nYour only way of freeing that memory is to end the process, be it by urging it to terminate (maybe via a GUI) or by sending it a signal (e. g. via kill (1)).\nAny other way of freeing the memory from the outside (for which there is no interfaces in existence, but one might be able to do this using a debugger or similar) would mean to meddle in the affairs of the running process.  That is not advisable.  The process will sooner or later access the memory it allocated.  If that isn't allocated anymore, it will be sent a SEGFAULT and consequently typically terminate.\nThat being said, it could be that you are considering changing the program to allocate less memory.  In this case the question is way too broad and would rely heavily on the program you are looking at."}
{"instruction": "C++ Stack and Heap in RAM", "output": "It's probably just the optimizer doing its job (or not, in the second case). It's probably harder for it to optimize the second allocation away since you could theoretically access that memory outside the scope of the pointer. \nI managed to repro this in MSVS 2010, on Release mode, and adding a simple\n\n```\nstd::cout << Array1[42];\n\n```\n\nbrought the memory usage to the same value. (granted, I did use a lower value)\nThere's also no code generated for the first snipped, but there is for the second one."}
{"instruction": "How can I gracefully recover from an attached Docker container terminating?", "output": "You can't avoid such behaviour, because it is the htop duty to setup the terminal state after its termination, but it can't do it when terminated with SIGKILL. However, you can fix this terminal window yourself with the reset command, which is intended to initialize the terminal state.\nAbout the \"attached\" container:\nThe --pid=container:<name> option means that the new container would be run in the PID namespace of first container and as the pid_namespaces(7) man page says:\n\nIf the \"init\" process of a PID namespace terminates, the kernel\n  terminates all of the processes in the namespace via a SIGKILL signal."}
{"instruction": "Web Server extremely slow, while CPU at 0-5% and RAM at 500/1024", "output": "If you aren't maxing out cpu then it's blocking on some other resource.  Some possible candidates:\n\nis there a db query for every request?  If so, how long are they taking?\nhow many threads do you have available to serve requests?  How long does each request take?  If there is only one thread serving requests then an incoming load of 100 requests per second (which it sounds like you were doing) would start backing up once a request took longer than 10ms, at best.  \n\nAs a general strategy, I'd add timers and try to bisect the problem.  Take the time at the beginning and end of the request, if this is small then you know the delay is elsewhere.  \nAlso, for this kind of workload you should consider pushing updates rather than polling.  That way you can push a single message to update listening clients and they don't have to poll frequently to get new messages right away."}
{"instruction": "How do you work out what&#39;s making up the total CPU% on Linux?", "output": "You can sum up CPU usage as shown here.\n\n```\nps -eo pcpu,command --sort=-pcpu | grep gjs | awk '{sum+=$1} END {print sum}'\n\n```\n\nThe solution they linked actually sums memory, not CPU usage (probably a bug they never caught), I've fixed it so it should work for you.\nIf you want to make a shell script to reuse, write this to cpusum for example:\n\n```\nps -eo pcpu,command --sort=-pcpu | grep \"$1\" | awk '{sum+=$1} END {print sum}'\n\n```\n\nthen make it executable: chmod +x cpusum,\nand run it ./cpusum gjs"}
{"instruction": "&quot;used memory&quot; high, but seems inaccurate - ksysguardd compared to htop?", "output": "The yellow bars in htop are for memory used for disk caching, which makes the system run more smoothly. This memory is given to applications whenever they need it, so you're fine!\nYou can read more on this (pretty awesome) website.\nTo know what all of the colours in htop correspond to, read this.\nHope that helps!"}
{"instruction": "Why is the status of top or htop always &quot;R&quot; (running)?", "output": "Well... if you are running top (or htop), and top (or htop) gets its own status from /proc/self/stat... then it must be running, otherwise how would it be possible that it got its status while not running? The sole fact that a process is reading its own status means that the status must be running at the time of reading.\nYou can try this with other programs too, if you want:\n\n```\n$ cat /proc/self/stat\n32482 (cat) R ...\n$ head /proc/self/stat\n32491 (head) R ...\n$ tail /proc/self/stat\n32497 (tail) R ...\n$ less /proc/self/stat\n32503 (less) R ...\n\n```"}
{"instruction": "Prevent excessive CPU usage of Golang background process", "output": "GOMAXPROCS does not what you think it does. From package runtime docu:\n\nThe GOMAXPROCS variable limits the number of operating system threads that can execute user-level Go code simultaneously. There is no limit to the number of threads that can be blocked in system calls on behalf of Go code; those do not count against the GOMAXPROCS limit.\n\nIt limits OS threads. If your code doesn't use goroutines which could be scheduled to OS threads limiting the number of threads does literally nothing.\nJust remove the GOMAXPROCS stuff, it does nothing. (If you set GOMAXPROCS to 12 you will have at most 12 OS threads actually executing goroutine code; If you have just one goroutine limiting  the number of OS threads it can run on is a noop.)\nAll you can do is not busy-looping as you did with time.Sleep(). Depending on your requirements you could call the actual code regularly, e.g. via a time.Ticker. There is no single \"best practice\" (excepet not fiddling with GOMAXPROCS)."}
{"instruction": "htop reporting Tomcat 8.5 as consuming 4GB when I&#39;ve set -Xmx1024M", "output": "Limiting the heap size (-Xmx1024M) only limits the size of the Java heap. The JVM is free to use as much \"native\" memory as it feels is necessary in order to run itself.\nThings that aren't part of the Java heap:\n\nPermGen\nThread stacks\nNative code compiled by the JIT\n\nHave a look at Java native memory usage to see how you can get some insight into what memory is being used outside the Java heap.\nNote that if you are willing to give your JVM a whole gig or RAM, you may as well set -Xms == -Xmx, otherwise you will just waste CPU cycles re-sizing memory at intervals until the heap reaches its maximum size."}
{"instruction": "Several node instances being created for a simple Hello world program", "output": "Nodejs is needs threads to do other tasks that are handled automatically for your by the V8 engine. Some of the things are\n\nInterpreter\nEvent loop\nGarbage collector\nBlocking I/O executor\nand others...\n\nNodejs make programming easy by hiding these complexities from the programmer.\nIf you need more control on these lower level 'stuff' then you can use C,C++ or other low level lanuages where you have to decide what should in which thread."}
{"instruction": "Cassandra: Is it normal to have so many gc.log processes", "output": "It's not gc.log processes, it's the single Cassandra process that has an option that specifies where to store gc.log file.  You see so many because by default htop shows all threads of the process, not single process item (see this answer for explanation).  You can check this by executing ps -aef|grep gc.log - you should see only one item for Cassandra process."}
{"instruction": "OpenCV forEach function parallel access", "output": "Have you taken a look in TBB already? Threading Building Blocks is an apache licensed lib for parallel computing which you can use just compiling OpenCV with the flag -D WITH_TBB=ON\nSee this example of parallel_for: http://www.jayrambhia.com/blog/opencv-with-tbb\nIf you decide to adopt TBB follow these steps:\n1 - Rebuild OpenCV with TBB support. If you are running into a Linux machine just do:\ncmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local -D WITH_TBB=ON BUILD_TBB=ON ..\n2 - Rewrite your program to use TBB\nSee the answers there: Simplest TBB example focusing in the most recent ones."}
{"instruction": "Linux Webserver - htop shows extreme cpu usage?", "output": "In your screenshot the CPU usage bars are colored in green and red. Press '?' in htop for a help screen to show up. From there you will see that green color is for a normal priority userspace applications CPU usage and the red color is for kernel threads.\nBasically, in your screenshot all the CPU cores are 100% busy and most of the time they spend in the kernel.\nYes, this is bad. Further investigation is needed to tell what exactly is going on here."}
{"instruction": "Get thread Id for a process programmatically that matches htop&#39;s corresponding pid", "output": "Is the m_iAudioThreadID supposed to be equal to the PID we see in htop's tree mode for a process? \n\nNo, they are not. htop shows you process-ids, PIDs. PThread-IDs as set by pthread_create() are different: Distinction between processes and threads in Linux\nOne main difference is that PIDs are uniquely indentifying a process within the existing processes of a system, PThread-IDs are uniquely indentifying a thread within the existing threads of a process.\n\nHow do I find the PID of htop's programmatically from my program? \n\nAt least on a recent Linux: To get the PID associated to a certain PThread use the gettid() system call from within the thread in question:\n\n```\n#define _GNU_SOURCE\n\n#include <unistd.h>\n#include <sys/syscall.h>\n#include <sys/types.h>\n\npid_t gettid(void)\n{\n  return syscall(SYS_gettid);\n}\n\n```\n\n(inspired by http://man7.org/linux/man-pages/man2/syscall.2.html)"}
{"instruction": "Extracting certain information from htop command", "output": "On my machine (not as many cores as yours) this works, the first line of process data sits on line number 8, hence the  NR==8 in the awk command. The  little for-loop joins all fields from # 13 (beginning of command) to NF (number of fields in line). Top is instructed to sort by %CPU, output the full command rather than the process name and output one line in batch mode.\n\n```\ntop -o \"%CPU\" -c -b -n 1 | awk 'NR==8{for(i=13;i<=NF;i++){command = command\" \"$i}; printf \"%s\\t%s\\t%s\\n\",$1,$9,command}'\n2555274 29.4     top -o %CPU -c -b -n 1\n\n```\n\nI don't have sample process names that match yours and have no intention to type all that ... so the parsing out of your [Server-mumble]1 is up to you.\nP.S.: I adjusted the awk to cater to your special needs, extracting the numeric substring from your -D parameter ...\n\n```\ntop -o \"%CPU\" -c -b -n 1 |awk 'BEGIN{p=0} $1==\"PID\"{p=1;next} {if(p==1){command=gensub(/^.*-D\\[Server:([0-9-]+).*/,\"\\\\1\",\"1\"); printf \"%s\\t%s\\t%s\\n\",$1,$9,command}p=0;command=\"\"}\n\n```\n\n\nHint: There's a reason that images of text are discouraged"}
{"instruction": "Aggregate Top CPU Using Processes", "output": "top will always capture a first full scan of process info for use as a baseline.  It uses that to initialize the utility's database of values used for later comparative reporting.  That is the basis of the first report presented to the screen.\nThe follow-on reports are the true measures for the specified evaluation intervals.\nYour code snippet will therefore never provide what you are really looking for.\nYou need to skip the results from the first scan and only use the follow on reports, but the only way to do that is to generate them from a single command by specifying the count of scans desired, then parse the resulting combined report.\nTo that end, here is a proposed solution:\n\n```\n#!/bin/bash\n\noutput=\"top_cpu_users\"\nrm -f ${output} ${output}.tmp\n\nsnapshots=5\ninterval=2\nprocess_count=6         ### Number of heavy hitter processes being monitored\ntop_head=7          ### Number of header lines in top report\nlines=$(( ${process_count} + ${top_head} )) ### total lines saved from each report run\n\necho -e \"\\n Collecting process snapshots every ${interval} seconds ...\"\ntop -b -n $(( ${snapshots} + 1 )) -d ${interval} > ${output}.tmp\n\necho -e \"\\n Parsing snapshots ...\"\nawk -v max=\"${lines}\" 'BEGIN{\n    doprint=0 ;\n    first=1 ;\n}\n{\n    if( $1 == \"top\" ){\n        if( first == 1 ){\n            first=0 ;\n        }else{\n            print NR | \"cat >&2\" ;\n            print \"\" ;\n            doprint=1 ;\n            entry=0 ;\n        } ;\n    } ;\n    if( doprint == 1 ){\n        entry++ ;\n        print $0 ;\n        if( entry == max ){\n            doprint=0 ;\n        } ;\n    } ;\n}' ${output}.tmp >${output}\n\nmore ${output}\n\n```\n\nThe session output for that will look like this:\n\n```\n Collecting process snapshots every 2 seconds ...\n\n Parsing snapshots ...\n266\n531\n796\n1061\n1326\n\ntop - 20:14:02 up  8:37,  1 user,  load average: 0.15, 0.13, 0.15\nTasks: 257 total,   1 running, 256 sleeping,   0 stopped,   0 zombie\n%Cpu(s):  0.5 us,  1.0 sy,  0.0 ni, 98.5 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\nMiB Mem :   3678.9 total,    157.6 free,   2753.7 used,    767.6 buff/cache\nMiB Swap:   2048.0 total,   1116.4 free,    931.6 used.    629.2 avail Mem \n\n    PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND\n  31773 root      20   0       0      0      0 I   1.5   0.0   0:09.08 kworker/0:3-events\n  32254 ericthe+  20   0   14500   3876   3092 R   1.0   0.1   0:00.04 top\n   1503 mysql     20   0 2387360  20664   2988 S   0.5   0.5   3:10.11 mysqld\n   2250 ericthe+  20   0 1949412 130004  20272 S   0.5   3.5   0:46.16 caja\n   3104 ericthe+  20   0 4837044 461944 127416 S   0.5  12.3  81:26.50 firefox\n  29998 ericthe+  20   0 2636764 165632  54700 S   0.5   4.4   0:36.97 Isolated Web Co\n\ntop - 20:14:04 up  8:37,  1 user,  load average: 0.14, 0.13, 0.15\nTasks: 257 total,   1 running, 256 sleeping,   0 stopped,   0 zombie\n%Cpu(s):  1.5 us,  0.7 sy,  0.0 ni, 97.4 id,  0.4 wa,  0.0 hi,  0.0 si,  0.0 st\nMiB Mem :   3678.9 total,    157.5 free,   2753.7 used,    767.6 buff/cache\nMiB Swap:   2048.0 total,   1116.4 free,    931.6 used.    629.2 avail Mem \n\n    PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND\n   3104 ericthe+  20   0 4837044 462208 127416 S   3.0  12.3  81:26.56 firefox\n   1503 mysql     20   0 2387360  20664   2988 S   1.0   0.5   3:10.13 mysqld\n  32254 ericthe+  20   0   14500   3876   3092 R   1.0   0.1   0:00.06 top\n   1489 root      20   0  546692  61584  48956 S   0.5   1.6  17:23.78 Xorg\n   2233 ericthe+  20   0  303744  11036   7500 S   0.5   0.3   4:46.84 compton\n   7239 ericthe+  20   0 2617520 127452  44768 S   0.5   3.4   1:41.13 Isolated Web Co\n\ntop - 20:14:06 up  8:37,  1 user,  load average: 0.14, 0.13, 0.15\nTasks: 257 total,   1 running, 256 sleeping,   0 stopped,   0 zombie\n%Cpu(s):  0.6 us,  0.4 sy,  0.0 ni, 99.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\nMiB Mem :   3678.9 total,    157.5 free,   2753.7 used,    767.6 buff/cache\nMiB Swap:   2048.0 total,   1116.4 free,    931.6 used.    629.2 avail Mem \n\n    PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND\n   1489 root      20   0  546700  61584  48956 S   1.5   1.6  17:23.81 Xorg\n   3104 ericthe+  20   0 4837044 462208 127416 S   1.5  12.3  81:26.59 firefox\n   1503 mysql     20   0 2387360  20664   2988 S   0.5   0.5   3:10.14 mysqld\n   2233 ericthe+  20   0  303744  11036   7500 S   0.5   0.3   4:46.85 compton\n   2478 ericthe+  20   0  346156  10368   8792 S   0.5   0.3   0:22.97 mate-cpufreq-ap\n   2481 ericthe+  20   0  346540  11148   9168 S   0.5   0.3   0:41.73 mate-sensors-ap\n\ntop - 20:14:08 up  8:37,  1 user,  load average: 0.14, 0.13, 0.15\nTasks: 257 total,   1 running, 256 sleeping,   0 stopped,   0 zombie\n%Cpu(s):  0.6 us,  0.5 sy,  0.0 ni, 98.9 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\nMiB Mem :   3678.9 total,    157.5 free,   2753.6 used,    767.7 buff/cache\nMiB Swap:   2048.0 total,   1116.4 free,    931.6 used.    629.3 avail Mem \n\n    PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND\n  32254 ericthe+  20   0   14500   3876   3092 R   1.0   0.1   0:00.08 top\n   3104 ericthe+  20   0 4837044 462208 127416 S   0.5  12.3  81:26.60 firefox\n  18370 ericthe+  20   0 2682392  97268  45144 S   0.5   2.6   0:55.36 Isolated Web Co\n  19436 ericthe+  20   0 2618496 123608  52540 S   0.5   3.3   1:55.08 Isolated Web Co\n  26630 ericthe+  20   0 2690464 179020  56060 S   0.5   4.8   1:45.57 Isolated Web Co\n  29998 ericthe+  20   0 2636764 165632  54700 S   0.5   4.4   0:36.98 Isolated Web Co\n\ntop - 20:14:10 up  8:37,  1 user,  load average: 0.13, 0.13, 0.15\nTasks: 257 total,   1 running, 256 sleeping,   0 stopped,   0 zombie\n%Cpu(s):  2.5 us,  0.9 sy,  0.0 ni, 96.6 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\nMiB Mem :   3678.9 total,    157.5 free,   2753.6 used,    767.7 buff/cache\nMiB Swap:   2048.0 total,   1116.4 free,    931.6 used.    629.3 avail Mem \n\n    PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND\n   3104 ericthe+  20   0 4837076 463000 127416 S   7.5  12.3  81:26.75 firefox\n   1489 root      20   0  546716  61584  48956 S   1.5   1.6  17:23.84 Xorg\n   1503 mysql     20   0 2387360  20664   2988 S   1.0   0.5   3:10.16 mysqld\n  32254 ericthe+  20   0   14500   3876   3092 R   1.0   0.1   0:00.10 top\n   2233 ericthe+  20   0  303744  11036   7500 S   0.5   0.3   4:46.86 compton\n   2481 ericthe+  20   0  346540  11148   9168 S   0.5   0.3   0:41.74 mate-sensors-ap\n\n```"}
{"instruction": "How to show processes list in zabbix in ubuntu 16.04", "output": "Zabbix does not have a built-in item to list all the processes. It would be of a limited use, as you cannot really graph such data, or show a compact history.\nYou can easily add one yourself, though - just add a new userparameter, using ps or top. For example, adding the following in the agent daemon configuration file and restarting the agent will give you a new proc.list item:\n\n```\nUserParameter=proc.list,ps -AH\n\n```\n\nAdjust ps or top parameters as necessary."}
{"instruction": "What does the &quot;yield&quot; keyword do in Python?", "output": "To understand what yield does, you must understand what generators are. And before you can understand generators, you must understand iterables.\nIterables\nWhen you create a list, you can read its items one by one. Reading its items one by one is called iteration:\n\n```\n>>> mylist = [1, 2, 3]\n>>> for i in mylist:\n...    print(i)\n1\n2\n3\n\n```\n\nmylist is an iterable. When you use a list comprehension, you create a list, and so an iterable:\n\n```\n>>> mylist = [x*x for x in range(3)]\n>>> for i in mylist:\n...    print(i)\n0\n1\n4\n\n```\n\nEverything you can use \"for... in...\" on is an iterable; lists, strings, files...\nThese iterables are handy because you can read them as much as you wish, but you store all the values in memory and this is not always what you want when you have a lot of values.\nGenerators\nGenerators are iterators, a kind of iterable you can only iterate over once. Generators do not store all the values in memory, they generate the values on the fly:\n\n```\n>>> mygenerator = (x*x for x in range(3))\n>>> for i in mygenerator:\n...    print(i)\n0\n1\n4\n\n```\n\nIt is just the same except you used () instead of []. BUT, you cannot perform for i in mygenerator a second time since generators can only be used once: they calculate 0, then forget about it and calculate 1, and end after calculating 4, one by one.\nYield\nyield is a keyword that is used like return, except the function will return a generator.\n\n```\n>>> def create_generator():\n...    mylist = range(3)\n...    for i in mylist:\n...        yield i*i\n...\n>>> mygenerator = create_generator() # create a generator\n>>> print(mygenerator) # mygenerator is an object!\n<generator object create_generator at 0xb7555c34>\n>>> for i in mygenerator:\n...     print(i)\n0\n1\n4\n\n```\n\nHere it's a useless example, but it's handy when you know your function will return a huge set of values that you will only need to read once.\nTo master yield, you must understand that when you call the function, the code you have written in the function body does not run. The function only returns the generator object, this is a bit tricky.\nThen, your code will continue from where it left off each time for uses the generator.\nNow the hard part:\nThe first time the for calls the generator object created from your function, it will run the code in your function from the beginning until it hits yield, then it'll return the first value of the loop. Then, each subsequent call will run another iteration of the loop you have written in the function and return the next value. This will continue until the generator is considered empty, which happens when the function runs without hitting yield. That can be because the loop has come to an end, or because you no longer satisfy an \"if/else\".\n\nYour code explained\nGenerator:\n\n```\n# Here you create the method of the node object that will return the generator\ndef _get_child_candidates(self, distance, min_dist, max_dist):\n\n    # Here is the code that will be called each time you use the generator object:\n\n    # If there is still a child of the node object on its left\n    # AND if the distance is ok, return the next child\n    if self._leftchild and distance - max_dist < self._median:\n        yield self._leftchild\n\n    # If there is still a child of the node object on its right\n    # AND if the distance is ok, return the next child\n    if self._rightchild and distance + max_dist >= self._median:\n        yield self._rightchild\n\n    # If the function arrives here, the generator will be considered empty\n    # There are no more than two values: the left and the right children\n\n```\n\nCaller:\n\n```\n# Create an empty list and a list with the current object reference\nresult, candidates = list(), [self]\n\n# Loop on candidates (they contain only one element at the beginning)\nwhile candidates:\n\n    # Get the last candidate and remove it from the list\n    node = candidates.pop()\n\n    # Get the distance between obj and the candidate\n    distance = node._get_dist(obj)\n\n    # If the distance is ok, then you can fill in the result\n    if distance <= max_dist and distance >= min_dist:\n        result.extend(node._values)\n\n    # Add the children of the candidate to the candidate's list\n    # so the loop will keep running until it has looked\n    # at all the children of the children of the children, etc. of the candidate\n    candidates.extend(node._get_child_candidates(distance, min_dist, max_dist))\n\nreturn result\n\n```\n\nThis code contains several smart parts:\n\nThe loop iterates on a list, but the list expands while the loop is being iterated. It's a concise way to go through all these nested data even if it's a bit dangerous since you can end up with an infinite loop. In this case, candidates.extend(node._get_child_candidates(distance, min_dist, max_dist)) exhausts all the values of the generator, but while keeps creating new generator objects which will produce different values from the previous ones since it's not applied on the same node.\n\nThe extend() method is a list object method that expects an iterable and adds its values to the list.\n\n\nUsually, we pass a list to it:\n\n```\n>>> a = [1, 2]\n>>> b = [3, 4]\n>>> a.extend(b)\n>>> print(a)\n[1, 2, 3, 4]\n\n```\n\nBut in your code, it gets a generator, which is good because:\n\nYou don't need to read the values twice.\nYou may have a lot of children and you don't want them all stored in memory.\n\nAnd it works because Python does not care if the argument of a method is a list or not. Python expects iterables so it will work with strings, lists, tuples, and generators! This is called duck typing and is one of the reasons why Python is so cool. But this is another story, for another question...\nYou can stop here, or read a little bit to see an advanced use of a generator:\nControlling a generator exhaustion\n\n```\n>>> class Bank(): # Let's create a bank, building ATMs\n...    crisis = False\n...    def create_atm(self):\n...        while not self.crisis:\n...            yield \"$100\"\n>>> hsbc = Bank() # When everything's ok the ATM gives you as much as you want\n>>> corner_street_atm = hsbc.create_atm()\n>>> print(corner_street_atm.next())\n$100\n>>> print(corner_street_atm.next())\n$100\n>>> print([corner_street_atm.next() for cash in range(5)])\n['$100', '$100', '$100', '$100', '$100']\n>>> hsbc.crisis = True # Crisis is coming, no more money!\n>>> print(corner_street_atm.next())\n<type 'exceptions.StopIteration'>\n>>> wall_street_atm = hsbc.create_atm() # It's even true for new ATMs\n>>> print(wall_street_atm.next())\n<type 'exceptions.StopIteration'>\n>>> hsbc.crisis = False # The trouble is, even post-crisis the ATM remains empty\n>>> print(corner_street_atm.next())\n<type 'exceptions.StopIteration'>\n>>> brand_new_atm = hsbc.create_atm() # Build a new one to get back in business\n>>> for cash in brand_new_atm:\n...    print cash\n$100\n$100\n$100\n$100\n$100\n$100\n$100\n$100\n$100\n...\n\n```\n\nNote: For Python 3, useprint(corner_street_atm.__next__()) or print(next(corner_street_atm))\nIt can be useful for various things like controlling access to a resource.\nItertools, your best friend\nThe itertools module contains special functions to manipulate iterables. Ever wish to duplicate a generator?\nChain two generators? Group values in a nested list with a one-liner? Map / Zip without creating another list?\nThen just import itertools.\nAn example? Let's see the possible orders of arrival for a four-horse race:\n\n```\n>>> horses = [1, 2, 3, 4]\n>>> races = itertools.permutations(horses)\n>>> print(races)\n<itertools.permutations object at 0xb754f1dc>\n>>> print(list(itertools.permutations(horses)))\n[(1, 2, 3, 4),\n (1, 2, 4, 3),\n (1, 3, 2, 4),\n (1, 3, 4, 2),\n (1, 4, 2, 3),\n (1, 4, 3, 2),\n (2, 1, 3, 4),\n (2, 1, 4, 3),\n (2, 3, 1, 4),\n (2, 3, 4, 1),\n (2, 4, 1, 3),\n (2, 4, 3, 1),\n (3, 1, 2, 4),\n (3, 1, 4, 2),\n (3, 2, 1, 4),\n (3, 2, 4, 1),\n (3, 4, 1, 2),\n (3, 4, 2, 1),\n (4, 1, 2, 3),\n (4, 1, 3, 2),\n (4, 2, 1, 3),\n (4, 2, 3, 1),\n (4, 3, 1, 2),\n (4, 3, 2, 1)]\n\n```\n\nUnderstanding the inner mechanisms of iteration\nIteration is a process implying iterables (implementing the __iter__() method) and iterators (implementing the __next__() method).\nIterables are any objects you can get an iterator from. Iterators are objects that let you iterate on iterables.\nThere is more about it in this article about how for loops work."}
{"instruction": "What does if __name__ == &quot;__main__&quot;: do?", "output": "Short Answer\nIt's boilerplate code that protects users from accidentally invoking the script when they didn't intend to. Here are some common problems when the guard is omitted from a script:\n\nIf you import the guardless script in another script (e.g. import my_script_without_a_name_eq_main_guard), then the latter script will trigger the former to run at import time and using the second script's command line arguments. This is almost always a mistake.\n\nIf you have a custom class in the guardless script and save it to a pickle file, then unpickling it in another script will trigger an import of the guardless script, with the same problems outlined in the previous bullet.\n\n\nLong Answer\nTo better understand why and how this matters, we need to take a step back to understand how Python initializes scripts and how this interacts with its module import mechanism.\nWhenever the Python interpreter reads a source file, it does two things:\n\nit sets a few special variables like __name__, and then\n\nit executes all of the code found in the file.\n\n\nLet's see how this works and how it relates to your question about the __name__ checks we always see in Python scripts.\nCode Sample\nLet's use a slightly different code sample to explore how imports and scripts work.  Suppose the following is in a file called foo.py.\n\n```\n# Suppose this is foo.py.\n\nprint(\"before import\")\nimport math\n\nprint(\"before function_a\")\ndef function_a():\n    print(\"Function A\")\n\nprint(\"before function_b\")\ndef function_b():\n    print(\"Function B {}\".format(math.sqrt(100)))\n\nprint(\"before __name__ guard\")\nif __name__ == '__main__':\n    function_a()\n    function_b()\nprint(\"after __name__ guard\")\n\n```\n\nSpecial Variables\nWhen the Python interpreter reads a source file, it first defines a few special variables. In this case, we care about the __name__ variable.\nWhen Your Module Is the Main Program\nIf you are running your module (the source file) as the main program, e.g.\n\n```\npython foo.py\n\n```\n\nthe interpreter will assign the hard-coded string \"__main__\" to the __name__ variable, i.e.\n\n```\n# It's as if the interpreter inserts this at the top\n# of your module when run as the main program.\n__name__ = \"__main__\" \n\n```\n\nWhen Your Module Is Imported By Another\nOn the other hand, suppose some other module is the main program and it imports your module. This means there's a statement like this in the main program, or in some other module the main program imports:\n\n```\n# Suppose this is in some other main program.\nimport foo\n\n```\n\nThe interpreter will search for your foo.py file (along with searching for a few other variants), and prior to executing that module, it will assign the name \"foo\" from the import statement to the __name__ variable, i.e.\n\n```\n# It's as if the interpreter inserts this at the top\n# of your module when it's imported from another module.\n__name__ = \"foo\"\n\n```\n\nExecuting the Module's Code\nAfter the special variables are set up, the interpreter executes all the code in the module, one statement at a time. You may want to open another window on the side with the code sample so you can follow along with this explanation.\nAlways\n\nIt prints the string \"before import\" (without quotes).\n\nIt loads the math module and assigns it to a variable called math. This is equivalent to replacing import math with the following (note that __import__ is a low-level function in Python that takes a string and triggers the actual import):\n\n\n\n```\n# Find and load a module given its string name, \"math\",\n# then assign it to a local variable called math.\nmath = __import__(\"math\")\n\n```\n\n\nIt prints the string \"before function_a\".\n\nIt executes the def block, creating a function object, then assigning that function object to a variable called function_a.\n\nIt prints the string \"before function_b\".\n\nIt executes the second def block, creating another function object, then assigning it to a variable called function_b.\n\nIt prints the string \"before __name__ guard\".\n\n\nOnly When Your Module Is the Main Program\n\nIf your module is the main program, then it will see that __name__ was indeed set to \"__main__\" and it calls the two functions, printing the strings \"Function A\" and \"Function B 10.0\".\n\nOnly When Your Module Is Imported by Another\n\n(instead) If your module is not the main program but was imported by another one, then __name__ will be \"foo\", not \"__main__\", and it'll skip the body of the if statement.\n\nAlways\n\nIt will print the string \"after __name__ guard\" in both situations.\n\nSummary\nIn summary, here's what'd be printed in the two cases:\n\n```\n# What gets printed if foo is the main program\nbefore import\nbefore function_a\nbefore function_b\nbefore __name__ guard\nFunction A\nFunction B 10.0\nafter __name__ guard\n\n```\n\n\n```\n# What gets printed if foo is imported as a regular module\nbefore import\nbefore function_a\nbefore function_b\nbefore __name__ guard\nafter __name__ guard\n\n```\n\nWhy Does It Work This Way?\nYou might naturally wonder why anybody would want this.  Well, sometimes you want to write a .py file that can be both used by other programs and/or modules as a module, and can also be run as the main program itself.  Examples:\n\nYour module is a library, but you want to have a script mode where it runs some unit tests or a demo.\n\nYour module is only used as a main program, but it has some unit tests, and the testing framework works by importing .py files like your script and running special test functions. You don't want it to try running the script just because it's importing the module.\n\nYour module is mostly used as a main program, but it also provides a programmer-friendly API for advanced users.\n\n\nBeyond those examples, it's elegant that running a script in Python is just setting up a few magic variables and importing the script. \"Running\" the script is a side effect of importing the script's module.\nFood for Thought\n\nQuestion: Can I have multiple __name__ checking blocks?  Answer: it's strange to do so, but the language won't stop you.\n\nSuppose the following is in foo2.py.  What happens if you say python foo2.py on the command-line? Why?\n\n\n\n```\n# Suppose this is foo2.py.\nimport os, sys; sys.path.insert(0, os.path.dirname(__file__)) # needed for some interpreters\n\ndef function_a():\n    print(\"a1\")\n    from foo2 import function_b\n    print(\"a2\")\n    function_b()\n    print(\"a3\")\n\ndef function_b():\n    print(\"b\")\n\nprint(\"t1\")\nif __name__ == \"__main__\":\n    print(\"m1\")\n    function_a()\n    print(\"m2\")\nprint(\"t2\")\n      \n\n```\n\n\nNow, figure out what will happen in foo3.py (having removed the __name__ check):\n\n\n```\n# Suppose this is foo3.py.\nimport os, sys; sys.path.insert(0, os.path.dirname(__file__)) # needed for some interpreters\n\ndef function_a():\n    print(\"a1\")\n    from foo3 import function_b\n    print(\"a2\")\n    function_b()\n    print(\"a3\")\n\ndef function_b():\n    print(\"b\")\n\nprint(\"t1\")\nprint(\"m1\")\nfunction_a()\nprint(\"m2\")\nprint(\"t2\")\n\n```\n\n\nWhat will this do when used as a script?  When imported as a module?\n\n\n```\n# Suppose this is in foo4.py\n__name__ = \"__main__\"\n\ndef bar():\n    print(\"bar\")\n    \nprint(\"before __name__ guard\")\nif __name__ == \"__main__\":\n    bar()\nprint(\"after __name__ guard\")\n\n```"}
{"instruction": "Does Python have a ternary conditional operator?", "output": "Yes, it was added in version 2.5. The expression syntax is:\n\n```\na if condition else b\n\n```\n\nFirst condition is evaluated, then exactly one of either a or b is evaluated and returned based on the Boolean value of condition. If condition evaluates to True, then a is evaluated and returned but b is ignored, or else when b is evaluated and returned but a is ignored.\nThis allows short-circuiting because when condition is true only a is evaluated and b is not evaluated at all, but when condition is false only b is evaluated and a is not evaluated at all.\nFor example:\n\n```\n>>> 'true' if True else 'false'\n'true'\n>>> 'true' if False else 'false'\n'false'\n\n```\n\nNote that conditionals are an expression, not a statement. This means you can't use statements such as pass, or assignments with = (or \"augmented\" assignments like +=), within a conditional expression:\n\n```\n>>> pass if False else pass\n  File \"<stdin>\", line 1\n    pass if False else pass\n         ^\nSyntaxError: invalid syntax\n\n>>> # Python parses this as `x = (1 if False else y) = 2`\n>>> # The `(1 if False else x)` part is actually valid, but\n>>> # it can't be on the left-hand side of `=`.\n>>> x = 1 if False else y = 2\n  File \"<stdin>\", line 1\nSyntaxError: cannot assign to conditional expression\n\n>>> # If we parenthesize it instead...\n>>> (x = 1) if False else (y = 2)\n  File \"<stdin>\", line 1\n    (x = 1) if False else (y = 2)\n       ^\nSyntaxError: invalid syntax\n\n```\n\n(In 3.8 and above, the := \"walrus\" operator allows simple assignment of values as an expression, which is then compatible with this syntax. But please don't write code like that; it will quickly become very difficult to understand.)\nSimilarly, because it is an expression, the else part is mandatory:\n\n```\n# Invalid syntax: we didn't specify what the value should be if the \n# condition isn't met. It doesn't matter if we can verify that\n# ahead of time.\na if True\n\n```\n\nYou can, however, use conditional expressions to assign a variable like so:\n\n```\nx = a if True else b\n\n```\n\nOr for example to return a value:\n\n```\n# Of course we should just use the standard library `max`;\n# this is just for demonstration purposes.\ndef my_max(a, b):\n    return a if a > b else b\n\n```\n\nThink of the conditional expression as switching between two values. We can use it when we are in a 'one value or another' situation, where we will do the same thing with the result, regardless of whether the condition is met. We use the expression to compute the value, and then do something with it. If you need to do something different depending on the condition, then use a normal if statement instead.\n\nKeep in mind that it's frowned upon by some Pythonistas for several reasons:\n\nThe order of the arguments is different from those of the classic condition ? a : b ternary operator from many other languages (such as C, C++, Go, Perl, Ruby, Java, JavaScript, etc.), which may lead to bugs when people unfamiliar with Python's \"surprising\" behaviour use it (they may reverse the argument order).\nSome find it \"unwieldy\", since it goes contrary to the normal flow of thought (thinking of the condition first and then the effects).\nStylistic reasons. (Although the 'inline if' can be really useful, and make your script more concise, it really does complicate your code)\n\nIf you're having trouble remembering the order, then remember that when read aloud, you (almost) say what you mean. For example, x = 4 if b > 8 else 9 is read aloud as x will be 4 if b is greater than 8 otherwise 9.\nOfficial documentation:\n\nConditional expressions\nIs there an equivalent of C‚Äôs ‚Äù?:‚Äù ternary operator?"}
{"instruction": "What are metaclasses in Python?", "output": "A metaclass is the class of a class. A class defines how an instance of the class (i.e. an object) behaves while a metaclass defines how a class behaves. A class is an instance of a metaclass.\nWhile in Python you can use arbitrary callables for metaclasses (like Jerub shows), the better approach is to make it an actual class itself. type is the usual metaclass in Python. type is itself a class, and it is its own type. You won't be able to recreate something like type purely in Python, but Python cheats a little. To create your own metaclass in Python you really just want to subclass type.\nA metaclass is most commonly used as a class-factory. When you create an object by calling the class, Python creates a new class (when it executes the 'class' statement) by calling the metaclass. Combined with the normal __init__ and __new__ methods, metaclasses therefore allow you to do 'extra things' when creating a class, like registering the new class with some registry or replace the class with something else entirely.\nWhen the class statement is executed, Python first executes the body of the class statement as a normal block of code. The resulting namespace (a dict) holds the attributes of the class-to-be. The metaclass is determined by looking at the baseclasses of the class-to-be (metaclasses are inherited), at the __metaclass__ attribute of the class-to-be (if any) or the __metaclass__ global variable. The metaclass is then called with the name, bases and attributes of the class to instantiate it.\nHowever, metaclasses actually define the type of a class, not just a factory for it, so you can do much more with them. You can, for instance, define normal methods on the metaclass. These metaclass-methods are like classmethods in that they can be called on the class without an instance, but they are also not like classmethods in that they cannot be called on an instance of the class. type.__subclasses__() is an example of a method on the type metaclass. You can also define the normal 'magic' methods, like __add__, __iter__ and __getattr__, to implement or change how the class behaves.\nHere's an aggregated example of the bits and pieces:\n\n```\ndef make_hook(f):\n    \"\"\"Decorator to turn 'foo' method into '__foo__'\"\"\"\n    f.is_hook = 1\n    return f\n\nclass MyType(type):\n    def __new__(mcls, name, bases, attrs):\n\n        if name.startswith('None'):\n            return None\n\n        # Go over attributes and see if they should be renamed.\n        newattrs = {}\n        for attrname, attrvalue in attrs.iteritems():\n            if getattr(attrvalue, 'is_hook', 0):\n                newattrs['__%s__' % attrname] = attrvalue\n            else:\n                newattrs[attrname] = attrvalue\n\n        return super(MyType, mcls).__new__(mcls, name, bases, newattrs)\n\n    def __init__(self, name, bases, attrs):\n        super(MyType, self).__init__(name, bases, attrs)\n\n        # classregistry.register(self, self.interfaces)\n        print \"Would register class %s now.\" % self\n\n    def __add__(self, other):\n        class AutoClass(self, other):\n            pass\n        return AutoClass\n        # Alternatively, to autogenerate the classname as well as the class:\n        # return type(self.__name__ + other.__name__, (self, other), {})\n\n    def unregister(self):\n        # classregistry.unregister(self)\n        print \"Would unregister class %s now.\" % self\n\nclass MyObject:\n    __metaclass__ = MyType\n\n\nclass NoneSample(MyObject):\n    pass\n\n# Will print \"NoneType None\"\nprint type(NoneSample), repr(NoneSample)\n\nclass Example(MyObject):\n    def __init__(self, value):\n        self.value = value\n    @make_hook\n    def add(self, other):\n        return self.__class__(self.value + other.value)\n\n# Will unregister the class\nExample.unregister()\n\ninst = Example(10)\n# Will fail with an AttributeError\n#inst.unregister()\n\nprint inst + inst\nclass Sibling(MyObject):\n    pass\n\nExampleSibling = Example + Sibling\n# ExampleSibling is now a subclass of both Example and Sibling (with no\n# content of its own) although it will believe it's called 'AutoClass'\nprint ExampleSibling\nprint ExampleSibling.__mro__\n\n```"}
{"instruction": "How do I merge two dictionaries in a single expression in Python?", "output": "How can I merge two Python dictionaries in a single expression?\nFor dictionaries x and y, their shallowly-merged dictionary z takes values from y, replacing those from x.\n\nIn Python 3.9.0 or greater (released 17 October 2020, PEP-584, discussed here):\n\n```\nz = x | y\n\n```\n\n\nIn Python 3.5 or greater:\n\n```\nz = {**x, **y}\n\n```\n\n\nIn Python 2, (or 3.4 or lower) write a function:\n\n```\ndef merge_two_dicts(x, y):\n    z = x.copy()   # start with keys and values of x\n    z.update(y)    # modifies z with keys and values of y\n    return z\n\n```\n\nand now:\n\n```\nz = merge_two_dicts(x, y)\n\n```\n\n\n\nExplanation\nSay you have two dictionaries and you want to merge them into a new dictionary without altering the original dictionaries:\n\n```\nx = {'a': 1, 'b': 2}\ny = {'b': 3, 'c': 4}\n\n```\n\nThe desired result is to get a new dictionary (z) with the values merged, and the second dictionary's values overwriting those from the first.\n\n```\n>>> z\n{'a': 1, 'b': 3, 'c': 4}\n\n```\n\nA new syntax for this, proposed in PEP 448 and available as of Python 3.5, is\n\n```\nz = {**x, **y}\n\n```\n\nAnd it is indeed a single expression.\nNote that we can merge in with literal notation as well:\n\n```\nz = {**x, 'foo': 1, 'bar': 2, **y}\n\n```\n\nand now:\n\n```\n>>> z\n{'a': 1, 'b': 3, 'foo': 1, 'bar': 2, 'c': 4}\n\n```\n\nIt is now showing as implemented in the release schedule for 3.5, PEP 478, and it has now made its way into the What's New in Python 3.5 document.\nHowever, since many organizations are still on Python 2, you may wish to do this in a backward-compatible way. The classically Pythonic way, available in Python 2 and Python 3.0-3.4, is to do this as a two-step process:\n\n```\nz = x.copy()\nz.update(y) # which returns None since it mutates z\n\n```\n\nIn both approaches, y will come second and its values will replace x's values, thus b will point to 3 in our final result.\nNot yet on Python 3.5, but want a single expression\nIf you are not yet on Python 3.5 or need to write backward-compatible code, and you want this in a single expression, the most performant while the correct approach is to put it in a function:\n\n```\ndef merge_two_dicts(x, y):\n    \"\"\"Given two dictionaries, merge them into a new dict as a shallow copy.\"\"\"\n    z = x.copy()\n    z.update(y)\n    return z\n\n```\n\nand then you have a single expression:\n\n```\nz = merge_two_dicts(x, y)\n\n```\n\nYou can also make a function to merge an arbitrary number of dictionaries, from zero to a very large number:\n\n```\ndef merge_dicts(*dict_args):\n    \"\"\"\n    Given any number of dictionaries, shallow copy and merge into a new dict,\n    precedence goes to key-value pairs in latter dictionaries.\n    \"\"\"\n    result = {}\n    for dictionary in dict_args:\n        result.update(dictionary)\n    return result\n\n```\n\nThis function will work in Python 2 and 3 for all dictionaries. e.g. given dictionaries a to g:\n\n```\nz = merge_dicts(a, b, c, d, e, f, g) \n\n```\n\nand key-value pairs in g will take precedence over dictionaries a to f, and so on.\nCritiques of Other Answers\nDon't use what you see in the formerly accepted answer:\n\n```\nz = dict(x.items() + y.items())\n\n```\n\nIn Python 2, you create two lists in memory for each dict, create a third list in memory with length equal to the length of the first two put together, and then discard all three lists to create the dict. In Python 3, this will fail because you're adding two dict_items objects together, not two lists -\n\n```\n>>> c = dict(a.items() + b.items())\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: unsupported operand type(s) for +: 'dict_items' and 'dict_items'\n\n```\n\nand you would have to explicitly create them as lists, e.g. z = dict(list(x.items()) + list(y.items())). This is a waste of resources and computation power.\nSimilarly, taking the union of items() in Python 3 (viewitems() in Python 2.7) will also fail when values are unhashable objects (like lists, for example). Even if your values are hashable, since sets are semantically unordered, the behavior is undefined in regards to precedence. So don't do this:\n\n```\n>>> c = dict(a.items() | b.items())\n\n```\n\nThis example demonstrates what happens when values are unhashable:\n\n```\n>>> x = {'a': []}\n>>> y = {'b': []}\n>>> dict(x.items() | y.items())\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: unhashable type: 'list'\n\n```\n\nHere's an example where y should have precedence, but instead the value from x is retained due to the arbitrary order of sets:\n\n```\n>>> x = {'a': 2}\n>>> y = {'a': 1}\n>>> dict(x.items() | y.items())\n{'a': 2}\n\n```\n\nAnother hack you should not use:\n\n```\nz = dict(x, **y)\n\n```\n\nThis uses the dict constructor and is very fast and memory-efficient (even slightly more so than our two-step process) but unless you know precisely what is happening here (that is, the second dict is being passed as keyword arguments to the dict constructor), it's difficult to read, it's not the intended usage, and so it is not Pythonic.\nHere's an example of the usage being remediated in django.\nDictionaries are intended to take hashable keys (e.g. frozensets or tuples), but this method fails in Python 3 when keys are not strings.\n\n```\n>>> c = dict(a, **b)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: keyword arguments must be strings\n\n```\n\nFrom the mailing list, Guido van Rossum, the creator of the language, wrote:\n\nI am fine with\ndeclaring dict({}, **{1:3}) illegal, since after all it is abuse of\nthe ** mechanism.\n\nand\n\nApparently dict(x, **y) is going around as \"cool hack\" for \"call\nx.update(y) and return x\". Personally, I find it more despicable than\ncool.\n\nIt is my understanding (as well as the understanding of the creator of the language) that the intended usage for dict(**y) is for creating dictionaries for readability purposes, e.g.:\n\n```\ndict(a=1, b=10, c=11)\n\n```\n\ninstead of\n\n```\n{'a': 1, 'b': 10, 'c': 11}\n\n```\n\nResponse to comments\n\nDespite what Guido says, dict(x, **y) is in line with the dict specification, which btw. works for both Python 2 and 3. The fact that this only works for string keys is a direct consequence of how keyword parameters work and not a short-coming of dict. Nor is using the ** operator in this place an abuse of the mechanism, in fact, ** was designed precisely to pass dictionaries as keywords.\n\nAgain, it doesn't work for 3 when keys are not strings. The implicit calling contract is that namespaces take ordinary dictionaries, while users must only pass keyword arguments that are strings. All other callables enforced it. dict broke this consistency in Python 2:\n\n```\n>>> foo(**{('a', 'b'): None})\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: foo() keywords must be strings\n>>> dict(**{('a', 'b'): None})\n{('a', 'b'): None}\n\n```\n\nThis inconsistency was bad given other implementations of Python (PyPy, Jython, IronPython). Thus it was fixed in Python 3, as this usage could be a breaking change.\nI submit to you that it is malicious incompetence to intentionally write code that only works in one version of a language or that only works given certain arbitrary constraints.\nMore comments:\n\ndict(x.items() + y.items()) is still the most readable solution for Python 2. Readability counts.\n\nMy response: merge_two_dicts(x, y) actually seems much clearer to me, if we're actually concerned about readability. And it is not forward compatible, as Python 2 is increasingly deprecated.\n\n{**x, **y} does not seem to handle nested dictionaries. the contents of nested keys are simply overwritten, not merged [...] I ended up being burnt by these answers that do not merge recursively and I was surprised no one mentioned it. In my interpretation of the word \"merging\" these answers describe \"updating one dict with another\", and not merging.\n\nYes. I must refer you back to the question, which is asking for a shallow merge of two dictionaries, with the first's values being overwritten by the second's - in a single expression.\nAssuming two dictionaries of dictionaries, one might recursively merge them in a single function, but you should be careful not to modify the dictionaries from either source, and the surest way to avoid that is to make a copy when assigning values. As keys must be hashable and are usually therefore immutable, it is pointless to copy them:\n\n```\nfrom copy import deepcopy\n\ndef dict_of_dicts_merge(x, y):\n    z = {}\n    overlapping_keys = x.keys() & y.keys()\n    for key in overlapping_keys:\n        z[key] = dict_of_dicts_merge(x[key], y[key])\n    for key in x.keys() - overlapping_keys:\n        z[key] = deepcopy(x[key])\n    for key in y.keys() - overlapping_keys:\n        z[key] = deepcopy(y[key])\n    return z\n\n```\n\nUsage:\n\n```\n>>> x = {'a':{1:{}}, 'b': {2:{}}}\n>>> y = {'b':{10:{}}, 'c': {11:{}}}\n>>> dict_of_dicts_merge(x, y)\n{'b': {2: {}, 10: {}}, 'a': {1: {}}, 'c': {11: {}}}\n\n```\n\nComing up with contingencies for other value types is far beyond the scope of this question, so I will point you at my answer to the canonical question on a \"Dictionaries of dictionaries merge\".\nLess Performant But Correct Ad-hocs\nThese approaches are less performant, but they will provide correct behavior.\nThey will be much less performant than copy and update or the new unpacking because they iterate through each key-value pair at a higher level of abstraction, but they do respect the order of precedence (latter dictionaries have precedence)\nYou can also chain the dictionaries manually inside a dict comprehension:\n\n```\n{k: v for d in dicts for k, v in d.items()} # iteritems in Python 2.7\n\n```\n\nor in Python 2.6 (and perhaps as early as 2.4 when generator expressions were introduced):\n\n```\ndict((k, v) for d in dicts for k, v in d.items()) # iteritems in Python 2\n\n```\n\nitertools.chain will chain the iterators over the key-value pairs in the correct order:\n\n```\nfrom itertools import chain\nz = dict(chain(x.items(), y.items())) # iteritems in Python 2\n\n```\n\nPerformance Analysis\nI'm only going to do the performance analysis of the usages known to behave correctly. (Self-contained so you can copy and paste yourself.)\n\n```\nfrom timeit import repeat\nfrom itertools import chain\n\nx = dict.fromkeys('abcdefg')\ny = dict.fromkeys('efghijk')\n\ndef merge_two_dicts(x, y):\n    z = x.copy()\n    z.update(y)\n    return z\n\nmin(repeat(lambda: {**x, **y}))\nmin(repeat(lambda: merge_two_dicts(x, y)))\nmin(repeat(lambda: {k: v for d in (x, y) for k, v in d.items()}))\nmin(repeat(lambda: dict(chain(x.items(), y.items()))))\nmin(repeat(lambda: dict(item for d in (x, y) for item in d.items())))\n\n```\n\nIn Python 3.8.1, NixOS:\n\n```\n>>> min(repeat(lambda: {**x, **y}))\n1.0804965235292912\n>>> min(repeat(lambda: merge_two_dicts(x, y)))\n1.636518670246005\n>>> min(repeat(lambda: {k: v for d in (x, y) for k, v in d.items()}))\n3.1779992282390594\n>>> min(repeat(lambda: dict(chain(x.items(), y.items()))))\n2.740647904574871\n>>> min(repeat(lambda: dict(item for d in (x, y) for item in d.items())))\n4.266070580109954\n\n```\n\n\n```\n$ uname -a\nLinux nixos 4.19.113 #1-NixOS SMP Wed Mar 25 07:06:15 UTC 2020 x86_64 GNU/Linux\n\n```\n\nResources on Dictionaries\n\nMy explanation of Python's dictionary implementation, updated for 3.6.\nAnswer on how to add new keys to a dictionary\nMapping two lists into a dictionary\nThe official Python docs on dictionaries\nThe Dictionary Even Mightier - talk by Brandon Rhodes at Pycon 2017\nModern Python Dictionaries, A Confluence of Great Ideas - talk by Raymond Hettinger at Pycon 2017"}
{"instruction": "How do I execute a program or call a system command?", "output": "Use subprocess.run:\n\n```\nimport subprocess\n\nsubprocess.run([\"ls\", \"-l\"]) \n\n```\n\nAnother common way is os.system but you shouldn't use it because it is unsafe if any parts of the command come from outside your program or can contain spaces or other special characters, also subprocess.run is generally more flexible (you can get the stdout, stderr, the \"real\" status code, better error handling, etc.). Even the documentation for os.system recommends using subprocess instead.\nOn Python 3.4 and earlier, use subprocess.call instead of .run:\n\n```\nsubprocess.call([\"ls\", \"-l\"])\n\n```"}
{"instruction": "How do I create a directory, and any missing parent directories?", "output": "On Python ‚â• 3.5, use pathlib.Path.mkdir:\n\n```\nfrom pathlib import Path\nPath(\"/my/directory\").mkdir(parents=True, exist_ok=True)\n\n```\n\nFor older versions of Python, I see two answers with good qualities, each with a small flaw, so I will give my take on it:\nTry os.path.exists, and consider os.makedirs for the creation.\n\n```\nimport os\nif not os.path.exists(directory):\n    os.makedirs(directory)\n\n```\n\nAs noted in comments and elsewhere, there's a race condition ‚Äì if the directory is created between the os.path.exists and the os.makedirs calls, the os.makedirs will fail with an OSError. Unfortunately, blanket-catching OSError and continuing is not foolproof, as it will ignore a failure to create the directory due to other factors, such as insufficient permissions, full disk, etc.\nOne option would be to trap the OSError and examine the embedded error code (see Is there a cross-platform way of getting information from Python‚Äôs OSError):\n\n```\nimport os, errno\n\ntry:\n    os.makedirs(directory)\nexcept OSError as e:\n    if e.errno != errno.EEXIST:\n        raise\n\n```\n\nAlternatively, there could be a second os.path.exists, but suppose another created the directory after the first check, then removed it before the second one ‚Äì we could still be fooled. \nDepending on the application, the danger of concurrent operations may be more or less than the danger posed by other factors such as file permissions. The developer would have to know more about the particular application being developed and its expected environment before choosing an implementation.\nModern versions of Python improve this code quite a bit, both by exposing FileExistsError (in 3.3+)...\n\n```\ntry:\n    os.makedirs(\"path/to/directory\")\nexcept FileExistsError:\n    # directory already exists\n    pass\n\n```\n\n...and by allowing a keyword argument to os.makedirs called exist_ok (in 3.2+).\n\n```\nos.makedirs(\"path/to/directory\", exist_ok=True)  # succeeds even if directory exists.\n\n```"}
{"instruction": "How can I access the index value in a &#39;for&#39; loop?", "output": "Use the built-in function enumerate():\n\n```\nfor idx, x in enumerate(xs):\n    print(idx, x)\n\n```\n\nIt is non-Pythonic to manually index via for i in range(len(xs)): x = xs[i] or manually manage an additional state variable.\nCheck out PEP 279 for more."}
{"instruction": "How do I make a flat list out of a list of lists?", "output": "A list of lists named xss can be flattened using a nested list comprehension:\n\n```\nflat_list = [\n    x\n    for xs in xss\n    for x in xs\n]\n\n```\n\nThe above is equivalent to:\n\n```\nflat_list = []\n\nfor xs in xss:\n    for x in xs:\n        flat_list.append(x)\n\n```\n\nHere is the corresponding function:\n\n```\ndef flatten(xss):\n    return [x for xs in xss for x in xs]\n\n```\n\nThis is the fastest method.\nAs evidence, using the timeit module in the standard library, we see:\n\n```\n$ python -mtimeit -s'xss=[[1,2,3],[4,5,6],[7],[8,9]]*99' '[x for xs in xss for x in xs]'\n10000 loops, best of 3: 143 usec per loop\n\n$ python -mtimeit -s'xss=[[1,2,3],[4,5,6],[7],[8,9]]*99' 'sum(xss, [])'\n1000 loops, best of 3: 969 usec per loop\n\n$ python -mtimeit -s'xss=[[1,2,3],[4,5,6],[7],[8,9]]*99' 'reduce(lambda xs, ys: xs + ys, xss)'\n1000 loops, best of 3: 1.1 msec per loop\n\n```\n\nExplanation: the methods based on + (including the implied use in sum) are, of necessity, O(L**2) when there are L sublists -- as the intermediate result list keeps getting longer, at each step a new intermediate result list object gets allocated, and all the items in the previous intermediate result must be copied over (as well as a few new ones added at the end). So, for simplicity and without actual loss of generality, say you have L sublists of M items each: the first M items are copied back and forth L-1 times, the second M items L-2 times, and so on; total number of copies is M times the sum of x for x from 1 to L excluded, i.e., M * (L**2)/2.\nThe list comprehension just generates one list, once, and copies each item over (from its original place of residence to the result list) also exactly once."}
{"instruction": "What is the difference between @staticmethod and @classmethod in Python?", "output": "Maybe a bit of example code will help: Notice the difference in the call signatures of foo, class_foo and static_foo:\n\n```\nclass A(object):\n    def foo(self, x):\n        print(f\"executing foo({self}, {x})\")\n\n    @classmethod\n    def class_foo(cls, x):\n        print(f\"executing class_foo({cls}, {x})\")\n\n    @staticmethod\n    def static_foo(x):\n        print(f\"executing static_foo({x})\")\n\na = A()\n\n```\n\nBelow is the usual way an object instance calls a method. The object instance, a, is implicitly passed as the first argument.\n\n```\na.foo(1)\n# executing foo(<__main__.A object at 0xb7dbef0c>, 1)\n\n```\n\n\nWith classmethods, the class of the object instance is implicitly passed as the first argument instead of self.\n\n```\na.class_foo(1)\n# executing class_foo(<class '__main__.A'>, 1)\n\n```\n\nYou can also call class_foo using the class. In fact, if you define something to be\na classmethod, it is probably because you intend to call it from the class rather than from a class instance. A.foo(1) would have raised a TypeError, but A.class_foo(1) works just fine:\n\n```\nA.class_foo(1)\n# executing class_foo(<class '__main__.A'>, 1)\n\n```\n\nOne use people have found for class methods is to create inheritable alternative constructors.\n\nWith staticmethods, neither self (the object instance) nor  cls (the class) is implicitly passed as the first argument. They behave like plain functions except that you can call them from an instance or the class:\n\n```\na.static_foo(1)\n# executing static_foo(1)\n\nA.static_foo('hi')\n# executing static_foo(hi)\n\n```\n\nStaticmethods are used to group functions which have some logical connection with a class to the class.\n\nfoo is just a function, but when you call a.foo you don't just get the function,\nyou get a \"partially applied\" version of the function with the object instance a bound as the first argument to the function. foo expects 2 arguments, while a.foo only expects 1 argument.\na is bound to foo. That is what is meant by the term \"bound\" below:\n\n```\nprint(a.foo)\n# <bound method A.foo of <__main__.A object at 0xb7d52f0c>>\n\n```\n\nWith a.class_foo, a is not bound to class_foo, rather the class A is bound to class_foo.\n\n```\nprint(a.class_foo)\n# <bound method type.class_foo of <class '__main__.A'>>\n\n```\n\nHere, with a staticmethod, even though it is a method, a.static_foo just returns\na good 'ole function with no arguments bound. static_foo expects 1 argument, and\na.static_foo expects 1 argument too.\n\n```\nprint(a.static_foo)\n# <function static_foo at 0xb7d479cc>\n\n```\n\nAnd of course the same thing happens when you call static_foo with the class A instead.\n\n```\nprint(A.static_foo)\n# <function static_foo at 0xb7d479cc>\n\n```"}
{"instruction": "How slicing in Python works", "output": "The syntax is:\n\n```\na[start:stop]  # items start through stop-1\na[start:]      # items start through the rest of the array\na[:stop]       # items from the beginning through stop-1\na[:]           # a copy of the whole array\n\n```\n\nThere is also the step value, which can be used with any of the above:\n\n```\na[start:stop:step] # start through not past stop, by step\n\n```\n\nThe key point to remember is that the :stop value represents the first value that is not in the selected slice. So, the difference between stop and start is the number of elements selected (if step is 1, the default).\nThe other feature is that start or stop may be a negative number, which means it counts from the end of the array instead of the beginning. So:\n\n```\na[-1]    # last item in the array\na[-2:]   # last two items in the array\na[:-2]   # everything except the last two items\n\n```\n\nSimilarly, step may be a negative number:\n\n```\na[::-1]    # all items in the array, reversed\na[1::-1]   # the first two items, reversed\na[:-3:-1]  # the last two items, reversed\na[-3::-1]  # everything except the last two items, reversed\n\n```\n\nPython is kind to the programmer if there are fewer items than you ask for. For example, if you ask for a[:-2] and a only contains one element, you get an empty list instead of an error. Sometimes you would prefer the error, so you have to be aware that this may happen.\nRelationship with the slice object\nA slice object can represent a slicing operation, i.e.:\n\n```\na[start:stop:step]\n\n```\n\nis equivalent to:\n\n```\na[slice(start, stop, step)]\n\n```\n\nSlice objects also behave slightly differently depending on the number of arguments, similar to range(), i.e. both slice(stop) and slice(start, stop[, step]) are supported.\nTo skip specifying a given argument, one might use None, so that e.g. a[start:] is equivalent to a[slice(start, None)] or a[::-1] is equivalent to a[slice(None, None, -1)].\nWhile the :-based notation is very helpful for simple slicing, the explicit use of slice() objects simplifies the programmatic generation of slicing."}
{"instruction": "How can I find the index for a given item in a list?", "output": "```\n>>> [\"foo\", \"bar\", \"baz\"].index(\"bar\")\n1\n\n```\n\nSee the documentation for the built-in .index() method of the list:\n\n\n```\nlist.index(x[, start[, end]])\n\n```\n\nReturn zero-based index in the list of the first item whose value is equal to x. Raises a ValueError if there is no such item.\nThe optional arguments start and end are interpreted as in the slice notation and are used to limit the search to a particular subsequence of the list. The returned index is computed relative to the beginning of the full sequence rather than the start argument.\n\nCaveats\nLinear time-complexity in list length\nAn index call checks every element of the list in order, until it finds a match. If the list is long, and if there is no guarantee that the value will be near the beginning, this can slow down the code.\nThis problem can only be completely avoided by using a different data structure. However, if the element is known to be within a certain part of the list, the start and end parameters can be used to narrow the search.\nFor example:\n\n```\n>>> import timeit\n>>> timeit.timeit('l.index(999_999)', setup='l = list(range(0, 1_000_000))', number=1000)\n9.356267921015387\n>>> timeit.timeit('l.index(999_999, 999_990, 1_000_000)', setup='l = list(range(0, 1_000_000))', number=1000)\n0.0004404920036904514\n\n```\n\nThe second call is orders of magnitude faster, because it only has to search through 10 elements, rather than all 1 million.\nOnly the index of the first match is returned\nA call to index searches through the list in order until it finds a match, and stops there. If there could be more than one occurrence of the value, and all indices are needed, index cannot solve the problem:\n\n```\n>>> [1, 1].index(1) # the `1` index is not found.\n0\n\n```\n\nInstead, use a list comprehension or generator expression to do the search, with enumerate to get indices:\n\n```\n>>> # A list comprehension gives a list of indices directly:\n>>> [i for i, e in enumerate([1, 2, 1]) if e == 1]\n[0, 2]\n>>> # A generator comprehension gives us an iterable object...\n>>> g = (i for i, e in enumerate([1, 2, 1]) if e == 1)\n>>> # which can be used in a `for` loop, or manually iterated with `next`:\n>>> next(g)\n0\n>>> next(g)\n2\n\n```\n\nThe list comprehension and generator expression techniques still work if there is only one match, and are more generalizable.\nRaises an exception if there is no match\nAs noted in the documentation above, using .index will raise an exception if the searched-for value is not in the list:\n\n```\n>>> [1, 1].index(2)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nValueError: 2 is not in list\n\n```\n\nIf this is a concern, either explicitly check first using item in my_list, or handle the exception with try/except as appropriate.\nThe explicit check is simple and readable, but it must iterate the list a second time. See What is the EAFP principle in Python? for more guidance on this choice."}
{"instruction": "Iterating over dictionaries using &#39;for&#39; loops", "output": "key is just a variable name.  \n\n```\nfor key in d:\n\n```\n\nwill simply loop over the keys in the dictionary, rather than the keys and values.  To loop over both key and value you can use the following:\nFor Python 3.x:\n\n```\nfor key, value in d.items():\n\n```\n\nFor Python 2.x:\n\n```\nfor key, value in d.iteritems():\n\n```\n\nTo test for yourself, change the word key to poop.\nIn Python 3.x, iteritems() was replaced with simply items(), which returns a set-like view backed by the dict, like iteritems() but even better. \nThis is also available in 2.7 as viewitems(). \nThe operation items() will work for both 2 and 3, but in 2 it will return a list of the dictionary's (key, value) pairs, which will not reflect changes to the dict that happen after the items() call. If you want the 2.x behavior in 3.x, you can call list(d.items())."}
{"instruction": "How can I iterate over rows in a Pandas DataFrame?", "output": "DataFrame.iterrows is a generator which yields both the index and row (as a Series):\n\n```\nimport pandas as pd\n\ndf = pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 110, 120]})\ndf = df.reset_index()  # make sure indexes pair with number of rows\n\nfor index, row in df.iterrows():\n    print(row['c1'], row['c2'])\n\n```\n\n\n```\n10 100\n11 110\n12 120\n\n```\n\n\nObligatory disclaimer from the documentation\n\nIterating through pandas objects is generally slow. In many cases, iterating manually over the rows is not needed and can be avoided with one of the following approaches:\n\nLook for a vectorized solution: many operations can be performed using built-in methods or NumPy functions, (boolean) indexing, ‚Ä¶\nWhen you have a function that cannot work on the full DataFrame/Series at once, it is better to use apply() instead of iterating over the values. See the docs on function application.\nIf you need to do iterative manipulations on the values but performance is important, consider writing the inner loop with cython or numba. See the enhancing performance section for some examples of this approach.\n\n\nOther answers in this thread delve into greater depth on alternatives to iter* functions if you are interested to learn more."}
{"instruction": "How can I use a global variable in a function?", "output": "You can use a global variable within other functions by declaring it as global within each function that assigns a value to it:\n\n```\nglobvar = 0\n\ndef set_globvar_to_one():\n    global globvar    # Needed to modify global copy of globvar\n    globvar = 1\n\ndef print_globvar():\n    print(globvar)     # No need for global declaration to read value of globvar\n\nset_globvar_to_one()\nprint_globvar()       # Prints 1\n\n```\n\nSince it's unclear whether globvar = 1 is creating a local variable or changing a global variable, Python defaults to creating a local variable, and makes you explicitly choose the other behavior with the global keyword.\nSee other answers if you want to share a global variable across modules."}
{"instruction": "How do I get the current time in Python?", "output": "Use datetime:\n\n```\n>>> import datetime\n>>> now = datetime.datetime.now()\n>>> now\ndatetime.datetime(2009, 1, 6, 15, 8, 24, 78915)\n>>> print(now)\n2009-01-06 15:08:24.789150\n\n```\n\nFor just the clock time without the date:\n\n```\n>>> now.time()\ndatetime.time(15, 8, 24, 78915)\n>>> print(now.time())\n15:08:24.789150\n\n```\n\n\nTo save typing, you can import the datetime object from the datetime module:\n\n```\n>>> from datetime import datetime\n\n```\n\nThen remove the prefix datetime. from all of the above."}
{"instruction": "How can I catch multiple exceptions in one line? (in the &quot;except&quot; block)", "output": "From Python Documentation:\n\nAn except clause may name multiple exceptions as a parenthesized tuple, for example\n\n\n```\nexcept (IDontLikeYouException, YouAreBeingMeanException) as e:\n    pass\n\n```\n\nOr, for Python 2 only:\n\n```\nexcept (IDontLikeYouException, YouAreBeingMeanException), e:\n    pass\n\n```\n\nSeparating the exception from the variable with a comma will still work in Python 2.6 and 2.7, but is now deprecated and does not work in Python 3; now you should be using as."}
{"instruction": "How do I copy a file?", "output": "shutil has many methods you can use. One of which is:\n\n```\nimport shutil\n\nshutil.copyfile(src, dst)\n\n# 2nd option\nshutil.copy(src, dst)  # dst can be a folder; use shutil.copy2() to preserve timestamp\n\n```\n\n\nCopy the contents of the file named src to a file named dst. Both src and dst need to be the entire filename of the files, including path.\nThe destination location must be writable; otherwise, an IOError exception will be raised.\nIf dst already exists, it will be replaced.\nSpecial files such as character or block devices and pipes cannot be copied with this function.\nWith copy, src and dst are path names given as strs.\n\nAnother shutil method to look at is shutil.copy2(). It's similar but preserves more metadata (e.g. time stamps).\nIf you use os.path operations, use copy rather than copyfile. copyfile will only accept strings."}
{"instruction": "What is __init__.py for?", "output": "It used to be a required part of a package (old, pre-3.3 \"regular package\", not newer 3.3+ \"namespace package\").\nHere's the documentation.\n\nPython defines two types of packages, regular packages and namespace packages. Regular packages are traditional packages as they existed in Python 3.2 and earlier. A regular package is typically implemented as a directory containing an __init__.py file. When a regular package is imported, this __init__.py file is implicitly executed, and the objects it defines are bound to names in the package‚Äôs namespace. The __init__.py file can contain the same Python code that any other module can contain, and Python will add some additional attributes to the module when it is imported.\n\nBut just click the link, it contains an example, more information, and an explanation of namespace packages, the kind of packages without __init__.py."}
{"instruction": "Convert bytes to a string in Python 3", "output": "Decode the bytes object to produce a string:\n\n```\n>>> b\"abcde\".decode(\"utf-8\")\n'abcde'\n\n```\n\nThe above example assumes that the bytes object is in UTF-8, because it is a common encoding. However, you should use the encoding your data is actually in!"}
{"instruction": "What is the difference between __str__ and __repr__?", "output": "Alex Martelli summarized well but, surprisingly, was too succinct.\nFirst, let me reiterate the main points in Alex‚Äôs post:\n\nThe default implementation is useless (it‚Äôs hard to think of one which wouldn‚Äôt be, but yeah)\n__repr__ goal is to be unambiguous\n__str__ goal is to be readable\nContainer‚Äôs __str__ uses contained objects‚Äô __repr__\n\nDefault implementation is useless\nThis is mostly a surprise because Python‚Äôs defaults tend to be fairly useful. However, in this case, having a default for __repr__ which would act like:\n\n```\nreturn \"%s(%r)\" % (self.__class__, self.__dict__)\n\n```\n\nOr in new f-string formatting:\n\n```\nreturn f\"{self.__class__!s}({self.__dict__!r})\"\n\n```\n\nwould have been too dangerous (for example, too easy to get into infinite recursion if objects reference each other). So Python cops out. Note that there is one default which is true: if __repr__ is defined, and __str__ is not, the object will behave as though __str__=__repr__.\nThis means, in simple terms: almost every object you implement should have a functional __repr__ that‚Äôs usable for understanding the object. Implementing __str__ is optional: do that if you need a ‚Äúpretty print‚Äù functionality (for example, used by a report generator).\nThe goal of __repr__ is to be unambiguous\nLet me come right out and say it ‚Äî I do not believe in debuggers. I don‚Äôt really know how to use any debugger, and have never used one seriously. Furthermore, I believe that the big fault in debuggers is their basic nature ‚Äî most failures I debug happened a long long time ago, in a galaxy far far away. This means that I do believe, with religious fervor, in logging. Logging is the lifeblood of any decent fire-and-forget server system. Python makes it easy to log: with maybe some project specific wrappers, all you need is a\n\n```\nlog(INFO, \"I am in the weird function and a is\", a, \"and b is\", b, \"but I got a null C ‚Äî using default\", default_c)\n\n```\n\nBut you have to do the last step ‚Äî make sure every object you implement has a useful repr, so code like that can just work. This is why the ‚Äúeval‚Äù thing comes up: if you have enough information so eval(repr(c))==c, that means you know everything there is to know about c. If that‚Äôs easy enough, at least in a fuzzy way, do it. If not, make sure you have enough information about c anyway. I usually use an eval-like format: \"MyClass(this=%r,that=%r)\" % (self.this,self.that). It does not mean that you can actually construct MyClass, or that those are the right constructor arguments ‚Äî but it is a useful form to express ‚Äúthis is everything you need to know about this instance‚Äù.\nNote: I used %r above, not %s. You always want to use repr() [or %r formatting character, equivalently] inside __repr__ implementation, or you‚Äôre defeating the goal of repr. You want to be able to differentiate MyClass(3) and MyClass(\"3\").\nThe goal of __str__ is to be readable\nSpecifically, it is not intended to be unambiguous ‚Äî notice that str(3)==str(\"3\"). Likewise, if you implement an IP abstraction, having the str of it look like 192.168.1.1 is just fine. When implementing a date/time abstraction, the str can be \"2010/4/12 15:35:22\", etc. The goal is to represent it in a way that a user, not a programmer, would want to read it. Chop off useless digits, pretend to be some other class ‚Äî as long is it supports readability, it is an improvement.\nContainer‚Äôs __str__ uses contained objects‚Äô __repr__\nThis seems surprising, doesn‚Äôt it? It is a little, but how readable would it be if it used their __str__?\n\n```\n[moshe is, 3, hello\nworld, this is a list, oh I don't know, containing just 4 elements]\n\n```\n\nNot very. Specifically, the strings in a container would find it way too easy to disturb its string representation. In the face of ambiguity, remember, Python resists the temptation to guess. If you want the above behavior when you‚Äôre printing a list, just\n\n```\nprint(\"[\" + \", \".join(lst) + \"]\")\n\n```\n\n(you can probably also figure out what to do about dictionaries).\nSummary\nImplement __repr__ for any class you implement. This should be second nature. Implement __str__ if you think it would be useful to have a string version which errs on the side of readability."}
{"instruction": "How do I select rows from a DataFrame based on column values?", "output": "To select rows whose column value equals a scalar, some_value, use ==:\n\n```\ndf.loc[df['column_name'] == some_value]\n\n```\n\nTo select rows whose column value is in an iterable, some_values, use isin:\n\n```\ndf.loc[df['column_name'].isin(some_values)]\n\n```\n\nCombine multiple conditions with &:\n\n```\ndf.loc[(df['column_name'] >= A) & (df['column_name'] <= B)]\n\n```\n\nNote the parentheses. Due to Python's operator precedence rules, & binds more tightly than <= and >=. Thus, the parentheses in the last example are necessary. Without the parentheses\n\n```\ndf['column_name'] >= A & df['column_name'] <= B\n\n```\n\nis parsed as\n\n```\ndf['column_name'] >= (A & df['column_name']) <= B\n\n```\n\nwhich results in a Truth value of a Series is ambiguous error.\n\nTo select rows whose column value does not equal some_value, use !=:\n\n```\ndf.loc[df['column_name'] != some_value]\n\n```\n\nThe isin returns a boolean Series, so to select rows whose value is not in some_values, negate the boolean Series using ~:\n\n```\ndf = df.loc[~df['column_name'].isin(some_values)] # .loc is not in-place replacement\n\n```\n\n\nFor example,\n\n```\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'A': 'foo bar foo bar foo bar foo foo'.split(),\n                   'B': 'one one two three two two one three'.split(),\n                   'C': np.arange(8), 'D': np.arange(8) * 2})\nprint(df)\n#      A      B  C   D\n# 0  foo    one  0   0\n# 1  bar    one  1   2\n# 2  foo    two  2   4\n# 3  bar  three  3   6\n# 4  foo    two  4   8\n# 5  bar    two  5  10\n# 6  foo    one  6  12\n# 7  foo  three  7  14\n\nprint(df.loc[df['A'] == 'foo'])\n\n```\n\nyields\n\n```\n     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\n\n```\n\n\nIf you have multiple values you want to include, put them in a\nlist (or more generally, any iterable) and use isin:\n\n```\nprint(df.loc[df['B'].isin(['one','three'])])\n\n```\n\nyields\n\n```\n     A      B  C   D\n0  foo    one  0   0\n1  bar    one  1   2\n3  bar  three  3   6\n6  foo    one  6  12\n7  foo  three  7  14\n\n```\n\n\nNote, however, that if you wish to do this many times, it is more efficient to\nmake an index first, and then use df.loc:\n\n```\ndf = df.set_index(['B'])\nprint(df.loc['one'])\n\n```\n\nyields\n\n```\n       A  C   D\nB              \none  foo  0   0\none  bar  1   2\none  foo  6  12\n\n```\n\nor, to include multiple values from the index use df.index.isin:\n\n```\ndf.loc[df.index.isin(['one','two'])]\n\n```\n\nyields\n\n```\n       A  C   D\nB              \none  foo  0   0\none  bar  1   2\ntwo  foo  2   4\ntwo  foo  4   8\ntwo  bar  5  10\none  foo  6  12\n\n```"}
{"instruction": "How can I add new keys to a dictionary?", "output": "You create a new key/value pair on a dictionary by assigning a value to that key\n\n```\nd = {'key': 'value'}\nprint(d)  # {'key': 'value'}\n\nd['mynewkey'] = 'mynewvalue'\n\nprint(d)  # {'key': 'value', 'mynewkey': 'mynewvalue'}\n\n```\n\nIf the key doesn't exist, it's added and points to that value. If it exists, the current value it points to is overwritten."}
{"instruction": "Does Python have a string &#39;contains&#39; substring method?", "output": "Use the in operator:\n\n```\nif \"blah\" not in somestring: \n    continue\n\n```\n\nNote: This is case-sensitive."}
{"instruction": "How can I delete a file or folder in Python?", "output": "Use one of these methods:\n\npathlib.Path.unlink() removes a file or symbolic link.\n\npathlib.Path.rmdir() removes an empty directory.\n\nshutil.rmtree() deletes a directory and all its contents.\n\n\n\nOn Python 3.3 and below, you can use these methods instead of the pathlib ones:\n\nos.remove() removes a file.\n\nos.unlink() removes a symbolic link.\n\nos.rmdir() removes an empty directory."}
{"instruction": "&quot;Least Astonishment&quot; and the Mutable Default Argument", "output": "Actually, this is not a design flaw, and it is not because of internals or performance. It comes simply from the fact that functions in Python are first-class objects, and not only a piece of code.\nAs soon as you think of it this way, then it completely makes sense: a function is an object being evaluated on its definition; default parameters are kind of \"member data\" and therefore their state may change from one call to the other - exactly as in any other object.\nIn any case, the Effbot (Fredrik Lundh) has a very nice explanation of the reasons for this behavior in Default Parameter Values in Python. I found it very clear, and I really suggest reading it for a better knowledge of how function objects work."}
{"instruction": "What does ** (double star/asterisk) and * (star/asterisk) do for parameters?", "output": "The *args and **kwargs are common idioms to allow an arbitrary number of arguments to functions, as described in the section more on defining functions in the Python tutorial.\nThe *args will give you all positional arguments as a tuple:\n\n```\ndef foo(*args):\n    for a in args:\n        print(a)        \n\nfoo(1)\n# 1\n\nfoo(1, 2, 3)\n# 1\n# 2\n# 3\n\n```\n\nThe **kwargs will give you all\nkeyword arguments as a dictionary:\n\n```\ndef bar(**kwargs):\n    for a in kwargs:\n        print(a, kwargs[a])  \n\nbar(name='one', age=27)\n# name one\n# age 27\n\n```\n\nBoth idioms can be mixed with normal arguments to allow a set of fixed and some variable arguments:\n\n```\ndef foo(kind, *args, bar=None, **kwargs):\n    print(kind, args, bar, kwargs)\n\nfoo(123, 'a', 'b', apple='red')\n# 123 ('a', 'b') None {'apple': 'red'}\n\n```\n\nIt is also possible to use this the other way around:\n\n```\ndef foo(a, b, c):\n    print(a, b, c)\n\nobj = {'b':10, 'c':'lee'}\n\nfoo(100, **obj)\n# 100 10 lee\n\n```\n\nAnother usage of the *l idiom is to unpack argument lists when calling a function.\n\n```\ndef foo(bar, lee):\n    print(bar, lee)\n\nbaz = [1, 2]\n\nfoo(*baz)\n# 1 2\n\n```\n\nIn Python 3 it is possible to use *l on the left side of an assignment (Extended Iterable Unpacking), though it gives a list instead of a tuple in this context:\n\n```\nfirst, *rest = [1, 2, 3, 4]\n# first = 1\n# rest = [2, 3, 4]\n\n```\n\nAlso Python 3 adds a new semantic (refer PEP 3102):\n\n```\ndef func(arg1, arg2, arg3, *, kwarg1, kwarg2):\n    pass\n\n```\n\nSuch function accepts only 3 positional arguments, and everything after * can only be passed as keyword arguments.\nNote:\nA Python dict, semantically used for keyword argument passing, is arbitrarily ordered. However, in Python 3.6+, keyword arguments are guaranteed to remember insertion order.\n\"The order of elements in **kwargs now corresponds to the order in which keyword arguments were passed to the function.\" - What‚Äôs New In Python 3.6.\nIn fact, all dicts in CPython 3.6 will remember insertion order as an implementation detail, and this becomes standard in Python 3.7."}
{"instruction": "How can I access environment variables in Python?", "output": "Environment variables are accessed through os.environ:\n\n```\nimport os\nprint(os.environ['HOME'])\n\n```\n\nTo see a list of all environment variables:\n\n```\nprint(os.environ)\n\n```\n\n\nIf a key is not present, attempting to access it will raise a KeyError. To avoid this:\n\n```\n# Returns `None` if the key doesn't exist\nprint(os.environ.get('KEY_THAT_MIGHT_EXIST'))\n\n# Returns `default_value` if the key doesn't exist\nprint(os.environ.get('KEY_THAT_MIGHT_EXIST', default_value))\n\n# Returns `default_value` if the key doesn't exist\nprint(os.getenv('KEY_THAT_MIGHT_EXIST', default_value))\n\n```"}
{"instruction": "How do I sort a dictionary by value?", "output": "Python 3.7+ or CPython 3.6\nDicts preserve insertion order in Python 3.7+. Same in CPython 3.6, but it's an implementation detail.\n\n```\n>>> x = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\n>>> {k: v for k, v in sorted(x.items(), key=lambda item: item[1])}\n{0: 0, 2: 1, 1: 2, 4: 3, 3: 4}\n\n```\n\nor\n\n```\n>>> dict(sorted(x.items(), key=lambda item: item[1]))\n{0: 0, 2: 1, 1: 2, 4: 3, 3: 4}\n\n```\n\nOlder Python\nIt is not possible to sort a dictionary, only to get a representation of a dictionary that is sorted. Dictionaries are inherently orderless, but other types, such as lists and tuples, are not. So you need an ordered data type to represent sorted values, which will be a list‚Äîprobably a list of tuples.\nFor instance,\n\n```\nimport operator\nx = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\nsorted_x = sorted(x.items(), key=operator.itemgetter(1))\n\n```\n\nsorted_x will be a list of tuples sorted by the second element in each tuple. dict(sorted_x) == x.\nAnd for those wishing to sort on keys instead of values:\n\n```\nimport operator\nx = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\nsorted_x = sorted(x.items(), key=operator.itemgetter(0))\n\n```\n\nIn Python3 since unpacking is not allowed we can use\n\n```\nx = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\nsorted_x = sorted(x.items(), key=lambda kv: kv[1])\n\n```\n\nIf you want the output as a dict, you can use collections.OrderedDict:\n\n```\nimport collections\n\nsorted_dict = collections.OrderedDict(sorted_x)\n\n```"}
{"instruction": "How do I clone a list so that it doesn&#39;t change unexpectedly after assignment?", "output": "new_list = my_list doesn't actually create a second list. The assignment just copies the reference to the list, not the actual list, so both new_list and my_list refer to the same list after the assignment.\nTo actually copy the list, you have several options:\n\nYou can use the built-in list.copy() method (available since Python 3.3):\n\n```\nnew_list = old_list.copy()\n\n```\n\n\nYou can slice it:\n\n```\nnew_list = old_list[:]\n\n```\n\nAlex Martelli's opinion (at least back in 2007) about this is, that it is a weird syntax and it does not make sense to use it ever. ;) (In his opinion, the next one is more readable).\n\nYou can use the built-in list() constructor:\n\n```\nnew_list = list(old_list)\n\n```\n\n\nYou can use generic copy.copy():\n\n```\nimport copy\nnew_list = copy.copy(old_list)\n\n```\n\nThis is a little slower than list() because it has to find out the datatype of old_list first.\n\nIf you need to copy the elements of the list as well, use generic copy.deepcopy():\n\n```\nimport copy\nnew_list = copy.deepcopy(old_list)\n\n```\n\nObviously the slowest and most memory-needing method, but sometimes unavoidable. This operates recursively; it will handle any number of levels of nested lists (or other containers).\n\n\nExample:\n\n```\nimport copy\n\nclass Foo(object):\n    def __init__(self, val):\n         self.val = val\n\n    def __repr__(self):\n        return f'Foo({self.val!r})'\n\nfoo = Foo(1)\n\na = ['foo', foo]\nb = a.copy()\nc = a[:]\nd = list(a)\ne = copy.copy(a)\nf = copy.deepcopy(a)\n\n# edit orignal list and instance \na.append('baz')\nfoo.val = 5\n\nprint(f'original: {a}\\nlist.copy(): {b}\\nslice: {c}\\nlist(): {d}\\ncopy: {e}\\ndeepcopy: {f}')\n\n```\n\nResult:\n\n```\noriginal: ['foo', Foo(5), 'baz']\nlist.copy(): ['foo', Foo(5)]\nslice: ['foo', Foo(5)]\nlist(): ['foo', Foo(5)]\ncopy: ['foo', Foo(5)]\ndeepcopy: ['foo', Foo(1)]\n\n```"}
{"instruction": "How do I pass a variable by reference?", "output": "Arguments are passed by assignment. The rationale behind this is twofold:\n\nthe parameter passed in is actually a reference to an object (but the reference is passed by value)\nsome data types are mutable, but others aren't\n\nSo:\n\nIf you pass a mutable object into a method, the method gets a reference to that same object and you can mutate it to your heart's delight, but if you rebind the reference in the method, the outer scope will know nothing about it, and after you're done, the outer reference will still point at the original object. \nIf you pass an immutable object to a method, you still can't rebind the outer reference, and you can't even mutate the object.\n\nTo make it even more clear, let's have some examples. \nList - a mutable type\nLet's try to modify the list that was passed to a method:\n\n```\ndef try_to_change_list_contents(the_list):\n    print('got', the_list)\n    the_list.append('four')\n    print('changed to', the_list)\n\nouter_list = ['one', 'two', 'three']\n\nprint('before, outer_list =', outer_list)\ntry_to_change_list_contents(outer_list)\nprint('after, outer_list =', outer_list)\n\n```\n\nOutput:\n\n```\nbefore, outer_list = ['one', 'two', 'three']\ngot ['one', 'two', 'three']\nchanged to ['one', 'two', 'three', 'four']\nafter, outer_list = ['one', 'two', 'three', 'four']\n\n```\n\nSince the parameter passed in is a reference to outer_list, not a copy of it, we can use the mutating list methods to change it and have the changes reflected in the outer scope.\nNow let's see what happens when we try to change the reference that was passed in as a parameter:\n\n```\ndef try_to_change_list_reference(the_list):\n    print('got', the_list)\n    the_list = ['and', 'we', 'can', 'not', 'lie']\n    print('set to', the_list)\n\nouter_list = ['we', 'like', 'proper', 'English']\n\nprint('before, outer_list =', outer_list)\ntry_to_change_list_reference(outer_list)\nprint('after, outer_list =', outer_list)\n\n```\n\nOutput:\n\n```\nbefore, outer_list = ['we', 'like', 'proper', 'English']\ngot ['we', 'like', 'proper', 'English']\nset to ['and', 'we', 'can', 'not', 'lie']\nafter, outer_list = ['we', 'like', 'proper', 'English']\n\n```\n\nSince the the_list parameter was passed by value, assigning a new list to it had no effect that the code outside the method could see. The the_list was a copy of the outer_list reference, and we had the_list point to a new list, but there was no way to change where outer_list pointed.\nString - an immutable type\nIt's immutable, so there's nothing we can do to change the contents of the string\nNow, let's try to change the reference\n\n```\ndef try_to_change_string_reference(the_string):\n    print('got', the_string)\n    the_string = 'In a kingdom by the sea'\n    print('set to', the_string)\n\nouter_string = 'It was many and many a year ago'\n\nprint('before, outer_string =', outer_string)\ntry_to_change_string_reference(outer_string)\nprint('after, outer_string =', outer_string)\n\n```\n\nOutput:\n\n```\nbefore, outer_string = It was many and many a year ago\ngot It was many and many a year ago\nset to In a kingdom by the sea\nafter, outer_string = It was many and many a year ago\n\n```\n\nAgain, since the the_string parameter was passed by value, assigning a new string to it had no effect that the code outside the method could see. The the_string was a copy of the outer_string reference, and we had the_string point to a new string, but there was no way to change where outer_string pointed.\nI hope this clears things up a little.\nEDIT: It's been noted that this doesn't answer the question that @David originally asked, \"Is there something I can do to pass the variable by actual reference?\". Let's work on that.\nHow do we get around this?\nAs @Andrea's answer shows, you could return the new value. This doesn't change the way things are passed in, but does let you get the information you want back out:\n\n```\ndef return_a_whole_new_string(the_string):\n    new_string = something_to_do_with_the_old_string(the_string)\n    return new_string\n\n# then you could call it like\nmy_string = return_a_whole_new_string(my_string)\n\n```\n\nIf you really wanted to avoid using a return value, you could create a class to hold your value and pass it into the function or use an existing class, like a list:\n\n```\ndef use_a_wrapper_to_simulate_pass_by_reference(stuff_to_change):\n    new_string = something_to_do_with_the_old_string(stuff_to_change[0])\n    stuff_to_change[0] = new_string\n\n# then you could call it like\nwrapper = [my_string]\nuse_a_wrapper_to_simulate_pass_by_reference(wrapper)\n\ndo_something_with(wrapper[0])\n\n```\n\nAlthough this seems a little cumbersome."}
{"instruction": "Manually raising (throwing) an exception in Python", "output": "How do I manually throw/raise an exception in Python?\n\nUse the most specific Exception constructor that semantically fits your issue.\nBe specific in your message, e.g.:\n\n```\nraise ValueError('A very specific bad thing happened.')\n\n```\n\nDon't raise generic exceptions\nAvoid raising a generic Exception. To catch it, you'll have to catch all other more specific exceptions that subclass it.\nProblem 1: Hiding bugs\n\n```\nraise Exception('I know Python!') # Don't! If you catch, likely to hide bugs.\n\n```\n\nFor example:\n\n```\ndef demo_bad_catch():\n    try:\n        raise ValueError('Represents a hidden bug, do not catch this')\n        raise Exception('This is the exception you expect to handle')\n    except Exception as error:\n        print('Caught this error: ' + repr(error))\n\n>>> demo_bad_catch()\nCaught this error: ValueError('Represents a hidden bug, do not catch this',)\n\n```\n\nProblem 2: Won't catch\nAnd more specific catches won't catch the general exception:\n\n```\ndef demo_no_catch():\n    try:\n        raise Exception('general exceptions not caught by specific handling')\n    except ValueError as e:\n        print('we will not catch exception: Exception')\n \n\n>>> demo_no_catch()\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"<stdin>\", line 3, in demo_no_catch\nException: general exceptions not caught by specific handling\n\n```\n\nBest Practices: raise statement\nInstead, use the most specific Exception constructor that semantically fits your issue.\n\n```\nraise ValueError('A very specific bad thing happened')\n\n```\n\nwhich also handily allows an arbitrary number of arguments to be passed to the constructor:\n\n```\nraise ValueError('A very specific bad thing happened', 'foo', 'bar', 'baz') \n\n```\n\nThese arguments are accessed by the args attribute on the Exception object. For example:\n\n```\ntry:\n    some_code_that_may_raise_our_value_error()\nexcept ValueError as err:\n    print(err.args)\n\n```\n\nprints\n\n```\n('message', 'foo', 'bar', 'baz')    \n\n```\n\nIn Python 2.5, an actual message attribute was added to BaseException in favor of encouraging users to subclass Exceptions and stop using args, but the introduction of message and the original deprecation of args has been retracted.\nBest Practices: except clause\nWhen inside an except clause, you might want to, for example, log that a specific type of error happened, and then re-raise. The best way to do this while preserving the stack trace is to use a bare raise statement. For example:\n\n```\nlogger = logging.getLogger(__name__)\n\ntry:\n    do_something_in_app_that_breaks_easily()\nexcept AppError as error:\n    logger.error(error)\n    raise                 # just this!\n    # raise AppError      # Don't do this, you'll lose the stack trace!\n\n```\n\nDon't modify your errors... but if you insist.\nYou can preserve the stacktrace (and error value) with sys.exc_info(), but this is way more error prone and has compatibility problems between Python 2 and 3, prefer to use a bare raise to re-raise.\nTo explain - the sys.exc_info() returns the type, value, and traceback.\n\n```\ntype, value, traceback = sys.exc_info()\n\n```\n\nThis is the syntax in Python 2 - note this is not compatible with Python 3:\n\n```\nraise AppError, error, sys.exc_info()[2] # avoid this.\n# Equivalently, as error *is* the second object:\nraise sys.exc_info()[0], sys.exc_info()[1], sys.exc_info()[2]\n\n```\n\nIf you want to, you can modify what happens with your new raise - e.g. setting new args for the instance:\n\n```\ndef error():\n    raise ValueError('oops!')\n\ndef catch_error_modify_message():\n    try:\n        error()\n    except ValueError:\n        error_type, error_instance, traceback = sys.exc_info()\n        error_instance.args = (error_instance.args[0] + ' <modification>',)\n        raise error_type, error_instance, traceback\n\n```\n\nAnd we have preserved the whole traceback while modifying the args. Note that this is not a best practice and it is invalid syntax in Python 3 (making keeping compatibility much harder to work around).\n\n```\n>>> catch_error_modify_message()\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"<stdin>\", line 3, in catch_error_modify_message\n  File \"<stdin>\", line 2, in error\nValueError: oops! <modification>\n\n```\n\nIn Python 3:\n\n```\nraise error.with_traceback(sys.exc_info()[2])\n\n```\n\nAgain: avoid manually manipulating tracebacks. It's less efficient and more error prone. And if you're using threading and sys.exc_info you may even get the wrong traceback (especially if you're using exception handling for control flow - which I'd personally tend to avoid.)\nPython 3, Exception chaining\nIn Python 3, you can chain Exceptions, which preserve tracebacks:\n\n```\nraise RuntimeError('specific message') from error\n\n```\n\nBe aware:\n\nthis does allow changing the error type raised, and\nthis is not compatible with Python 2.\n\nDeprecated Methods:\nThese can easily hide and even get into production code. You want to raise an exception, and doing them will raise an exception, but not the one intended!\nValid in Python 2, but not in Python 3 is the following:\n\n```\nraise ValueError, 'message' # Don't do this, it's deprecated!\n\n```\n\nOnly valid in much older versions of Python (2.4 and lower), you may still see people raising strings:\n\n```\nraise 'message' # really really wrong. don't do this.\n\n```\n\nIn all modern versions, this will actually raise a TypeError, because you're not raising a BaseException type. If you're not checking for the right exception and don't have a reviewer that's aware of the issue, it could get into production.\nExample Usage\nI raise Exceptions to warn consumers of my API if they're using it incorrectly:\n\n```\ndef api_func(foo):\n    '''foo should be either 'baz' or 'bar'. returns something very useful.'''\n    if foo not in _ALLOWED_ARGS:\n        raise ValueError('{foo} wrong, use \"baz\" or \"bar\"'.format(foo=repr(foo)))\n\n```\n\nCreate your own error types when apropos\n\n\"I want to make an error on purpose, so that it would go into the except\"\n\nYou can create your own error types, if you want to indicate something specific is wrong with your application, just subclass the appropriate point in the exception hierarchy:\n\n```\nclass MyAppLookupError(LookupError):\n    '''raise this when there's a lookup error for my app'''\n\n```\n\nand usage:\n\n```\nif important_key not in resource_dict and not ok_to_be_missing:\n    raise MyAppLookupError('resource is missing, and that is not ok.')\n\n```"}
{"instruction": "Understanding Python super() with __init__() methods", "output": "super() lets you avoid referring to the base class explicitly, which can be nice. But the main advantage comes with multiple inheritance, where all sorts of fun stuff can happen. See the standard docs on super if you haven't already.\nNote that the syntax changed in Python 3.0: you can just say super().__init__() instead of super(ChildB, self).__init__() which IMO is quite a bit nicer. The standard docs also refer to a guide to using super() which is quite explanatory."}
{"instruction": "How do I concatenate two lists in Python?", "output": "Use the + operator to combine the lists:\n\n```\nlistone = [1, 2, 3]\nlisttwo = [4, 5, 6]\n\njoinedlist = listone + listtwo\n\n```\n\nOutput:\n\n```\n>>> joinedlist\n[1, 2, 3, 4, 5, 6]\n\n```\n\nNOTE: This will create a new list with a shallow copy of the items in the first list, followed by a shallow copy of the items in the second list. Use copy.deepcopy() to get deep copies of lists."}
{"instruction": "How do I check if a list is empty?", "output": "```\nif not a:\n    print(\"List is empty\")\n\n```\n\nUsing the implicit booleanness of the empty list is quite Pythonic."}
{"instruction": "How do I make function decorators and chain them together?", "output": "Check out the documentation to see how decorators work. Here is what you asked for:\n\n```\nfrom functools import wraps\n\ndef makebold(fn):\n    @wraps(fn)\n    def wrapper(*args, **kwargs):\n        return \"<b>\" + fn(*args, **kwargs) + \"</b>\"\n    return wrapper\n\ndef makeitalic(fn):\n    @wraps(fn)\n    def wrapper(*args, **kwargs):\n        return \"<i>\" + fn(*args, **kwargs) + \"</i>\"\n    return wrapper\n\n@makebold\n@makeitalic\ndef hello():\n    return \"hello world\"\n\n@makebold\n@makeitalic\ndef log(s):\n    return s\n\nprint hello()        # returns \"<b><i>hello world</i></b>\"\nprint hello.__name__ # with functools.wraps() this returns \"hello\"\nprint log('hello')   # returns \"<b><i>hello</i></b>\"\n\n```"}
{"instruction": "How do I split a list into equally-sized chunks?", "output": "Here's a generator that yields evenly-sized chunks:\n\n```\ndef chunks(lst, n):\n    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n    for i in range(0, len(lst), n):\n        yield lst[i:i + n]\n\n```\n\n\n```\nimport pprint\npprint.pprint(list(chunks(range(10, 75), 10)))\n[[10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\n [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],\n [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],\n [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],\n [70, 71, 72, 73, 74]]\n\n```\n\nFor Python 2, using xrange instead of range:\n\n```\ndef chunks(lst, n):\n    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n    for i in xrange(0, len(lst), n):\n        yield lst[i:i + n]\n\n```\n\n\nBelow is a list comprehension one-liner. The method above is preferable, though, since using named functions makes code easier to understand. For Python 3:\n\n```\n[lst[i:i + n] for i in range(0, len(lst), n)]\n\n```\n\nFor Python 2:\n\n```\n[lst[i:i + n] for i in xrange(0, len(lst), n)]\n\n```"}
{"instruction": "Find the current directory and file&#39;s directory", "output": "To get the full path to the directory a Python file is contained in, write this in that file:\n\n```\nimport os \ndir_path = os.path.dirname(os.path.realpath(__file__))\n\n```\n\n(Note that the incantation above won't work if you've already used os.chdir() to change your current working directory, since the value of the __file__ constant is relative to the current working directory and is not changed by an os.chdir() call.)\n\nTo get the current working directory use \n\n```\nimport os\ncwd = os.getcwd()\n\n```\n\n\nDocumentation references for the modules, constants and functions used above:\n\nThe os and os.path modules.\nThe __file__ constant\nos.path.realpath(path) (returns \"the canonical path of the specified filename, eliminating any symbolic links encountered in the path\")\nos.path.dirname(path) (returns \"the directory name of pathname path\")\nos.getcwd() (returns \"a string representing the current working directory\")\nos.chdir(path) (\"change the current working directory to path\")"}
{"instruction": "What is the difference between Python&#39;s list methods append and extend?", "output": ".append() appends a single object at the end of the list:\n\n```\n>>> x = [1, 2, 3]\n>>> x.append([4, 5])\n>>> print(x)\n[1, 2, 3, [4, 5]]\n\n```\n\n.extend() appends multiple objects that are taken from inside the specified iterable:\n\n```\n>>> x = [1, 2, 3]\n>>> x.extend([4, 5])\n>>> print(x)\n[1, 2, 3, 4, 5]\n\n```"}
{"instruction": "Why is &quot;1000000000000000 in range(1000000000000001)&quot; so fast in Python 3?", "output": "The Python 3 range() object doesn't produce numbers immediately; it is a smart sequence object that produces numbers on demand. All it contains is your start, stop and step values, then as you iterate over the object the next integer is calculated each iteration.\nThe object also implements the object.__contains__ hook, and calculates if your number is part of its range. Calculating is a (near) constant time operation *. There is never a need to scan through all possible integers in the range.\nFrom the range() object documentation:\n\nThe advantage of the range type over a regular list or tuple is that a range object will always take the same (small) amount of memory, no matter the size of the range it represents (as it only stores the start, stop and step values, calculating individual items and subranges as needed).\n\nSo at a minimum, your range() object would do:\n\n```\nclass my_range:\n    def __init__(self, start, stop=None, step=1, /):\n        if stop is None:\n            start, stop = 0, start\n        self.start, self.stop, self.step = start, stop, step\n        if step < 0:\n            lo, hi, step = stop, start, -step\n        else:\n            lo, hi = start, stop\n        self.length = 0 if lo > hi else ((hi - lo - 1) // step) + 1\n\n    def __iter__(self):\n        current = self.start\n        if self.step < 0:\n            while current > self.stop:\n                yield current\n                current += self.step\n        else:\n            while current < self.stop:\n                yield current\n                current += self.step\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, i):\n        if i < 0:\n            i += self.length\n        if 0 <= i < self.length:\n            return self.start + i * self.step\n        raise IndexError('my_range object index out of range')\n\n    def __contains__(self, num):\n        if self.step < 0:\n            if not (self.stop < num <= self.start):\n                return False\n        else:\n            if not (self.start <= num < self.stop):\n                return False\n        return (num - self.start) % self.step == 0\n\n```\n\nThis is still missing several things that a real range() supports (such as the .index() or .count() methods, hashing, equality testing, or slicing), but should give you an idea.\nI also simplified the __contains__ implementation to only focus on integer tests; if you give a real range() object a non-integer value (including subclasses of int), a slow scan is initiated to see if there is a match, just as if you use a containment test against a list of all the contained values. This was done to continue to support other numeric types that just happen to support equality testing with integers but are not expected to support integer arithmetic as well. See the original Python issue that implemented the containment test.\n\n* Near constant time because Python integers are unbounded and so math operations also grow in time as N grows, making this a O(log N) operation. Since it‚Äôs all executed in optimised C code and Python stores integer values in 30-bit chunks, you‚Äôd run out of memory before you saw any performance impact due to the size of the integers involved here."}
{"instruction": "Renaming column names in Pandas", "output": "Just assign it to the .columns attribute:\n\n```\n>>> df = pd.DataFrame({'$a':[1,2], '$b': [10,20]})\n>>> df\n   $a  $b\n0   1  10\n1   2  20\n\n>>> df.columns = ['a', 'b']\n>>> df\n   a   b\n0  1  10\n1  2  20\n\n```"}
{"instruction": "Convert string &quot;Jun 1 2005 1:33PM&quot; into datetime", "output": "datetime.strptime parses an input string in the user-specified format into a timezone-naive datetime object:\n\n```\n>>> from datetime import datetime\n>>> datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\ndatetime.datetime(2005, 6, 1, 13, 33)\n\n```\n\nTo obtain a date object using an existing datetime object, convert it using .date():\n\n```\n>>> datetime.strptime('Jun 1 2005', '%b %d %Y').date()\ndate(2005, 6, 1)\n\n```\n\n\nLinks:\n\nstrptime docs: Python 2, Python 3\n\nstrptime/strftime format string docs: Python 2, Python 3\n\nstrftime.org format string cheatsheet\n\n\nNotes:\n\nstrptime = \"string parse time\"\nstrftime = \"string format time\""}
{"instruction": "How can I remove a key from a Python dictionary?", "output": "To delete a key regardless of whether it is in the dictionary, use the two-argument form of dict.pop():\n\n```\nmy_dict.pop('key', None)\n\n```\n\nThis will return my_dict[key] if key exists in the dictionary, and None otherwise. If the second parameter is not specified (i.e. my_dict.pop('key')) and key does not exist, a KeyError is raised.\nTo delete a key that is guaranteed to exist, you can also use\n\n```\ndel my_dict['key']\n\n```\n\nThis will raise a KeyError if the key is not in the dictionary."}
{"instruction": "How to upgrade all Python packages with pip", "output": "There isn't a built-in flag yet. Starting with pip version 22.3, the --outdated and --format=freeze have become mutually exclusive. Use Python, to parse the JSON output:\n\n```\npip --disable-pip-version-check list --outdated --format=json | python -c \"import json, sys; print('\\n'.join([x['name'] for x in json.load(sys.stdin)]))\" | xargs -n1 pip install -U\n\n```\n\nIf you are using pip<22.3 you can use:\n\n```\npip list --outdated --format=freeze | grep -v '^\\-e' | cut -d = -f 1  | xargs -n1 pip install -U\n\n```\n\nFor older versions of pip:\n\n```\npip freeze --local | grep -v '^\\-e' | cut -d = -f 1  | xargs -n1 pip install -U\n\n```\n\n\n\nThe grep is to skip editable (\"-e\") package definitions, as suggested by @jawache. (Yes, you could replace grep+cut with sed or awk or perl or...).\n\nThe -n1 flag for xargs prevents stopping everything if updating one package fails (thanks @andsens).\n\n\n\nNote: there are infinite potential variations for this. I'm trying to keep this answer short and simple, but please do suggest variations in the comments!"}
{"instruction": "How to sort a list of dictionaries by a value of the dictionary in Python?", "output": "The sorted() function takes a key= parameter\n\n```\nnewlist = sorted(list_to_be_sorted, key=lambda d: d['name'])\n\n```\n\nAlternatively, you can use operator.itemgetter instead of defining the function yourself\n\n```\nfrom operator import itemgetter\nnewlist = sorted(list_to_be_sorted, key=itemgetter('name'))\n\n```\n\nFor completeness, add reverse=True to sort in descending order\n\n```\nnewlist = sorted(list_to_be_sorted, key=itemgetter('name'), reverse=True)\n\n```"}
{"instruction": "How do I get the last element of a list?", "output": "some_list[-1] is the shortest and most Pythonic.\nIn fact, you can do much more with this syntax. The some_list[-n] syntax gets the nth-to-last element. So some_list[-1] gets the last element, some_list[-2] gets the second to last, etc, all the way down to some_list[-len(some_list)], which gives you the first element.\nYou can also set list elements in this way. For instance:\n\n```\n>>> some_list = [1, 2, 3]\n>>> some_list[-1] = 5 # Set the last element\n>>> some_list[-2] = 3 # Set the second to last element\n>>> some_list\n[1, 3, 5]\n\n```\n\nNote that getting a list item by index will raise an IndexError if the expected item doesn't exist. This means that some_list[-1] will raise an exception if some_list is empty, because an empty list can't have a last element."}
{"instruction": "How to leave/exit/deactivate a Python virtualenv", "output": "Usually, activating a virtualenv gives you a shell function named:\n\n```\n$ deactivate\n\n```\n\nwhich puts things back to normal.\nI have just looked specifically again at the code for virtualenvwrapper, and, yes, it too supports deactivate as the way to escape from all virtualenvs.\nIf you are trying to leave an Anaconda environment, the command depends upon your version of conda. Recent versions (like 4.6) install a conda function directly in your shell, in which case you run:\n\n```\nconda deactivate\n\n```\n\nOlder conda versions instead implement deactivation using a stand-alone script:\n\n```\nsource deactivate\n\n```"}
{"instruction": "How do I install pip on Windows?", "output": "Python 3.4+ and 2.7.9+\nGood news! Python 3.4 (released March 2014) and Python 2.7.9 (released December 2014) ship with Pip. This is the best feature of any Python release. It makes the community's wealth of libraries accessible to everyone. Newbies are no longer excluded from using community libraries by the prohibitive difficulty of setup. In shipping with a package manager, Python joins Ruby, Node.js, Haskell, Perl, Go‚Äîalmost every other contemporary language with a majority open-source community. Thank you, Python.\nIf you do find that pip is not available, simply run ensurepip.\n\nOn Windows:\n\n```\npy -3 -m ensurepip\n\n```\n\n\nOtherwise:\n\n```\npython3 -m ensurepip\n\n```\n\n\n\nOf course, that doesn't mean Python packaging is problem solved. The experience remains frustrating. I discuss this in the Stack Overflow question Does Python have a package/module management system?.\nPython 3 ‚â§ 3.3 and 2 ‚â§ 2.7.8\nFlying in the face of its 'batteries included' motto, Python ships without a package manager. To make matters worse, Pip was‚Äîuntil recently‚Äîironically difficult to install.\nOfficial instructions\nPer https://pip.pypa.io/en/stable/installing/#do-i-need-to-install-pip:\nDownload get-pip.py, being careful to save it as a .py file rather than .txt. Then, run it from the command prompt:\n\n```\npython get-pip.py\n\n```\n\nYou possibly need an administrator command prompt to do this. Follow Start a Command Prompt as an Administrator (Microsoft TechNet).\nThis installs the pip package, which (in Windows) contains ...\\Scripts\\pip.exe that path must be in PATH environment variable to use pip from the command line (see the second part of 'Alternative Instructions' for adding it to your PATH,\nAlternative instructions\nThe official documentation tells users to install Pip and each of its dependencies from source. That's tedious for the experienced and prohibitively difficult for newbies.\nFor our sake, Christoph Gohlke prepares Windows installers (.msi) for popular Python packages. He builds installers for all Python versions, both 32 and 64 bit. You need to:\n\nInstall setuptools\nInstall pip\n\nFor me, this installed Pip at C:\\Python27\\Scripts\\pip.exe. Find pip.exe on your computer, then add its folder (for example, C:\\Python27\\Scripts) to your path (Start / Edit environment variables). Now you should be able to run pip from the command line. Try installing a package:\n\n```\npip install httpie\n\n```\n\nThere you go (hopefully)! Solutions for common problems are given below:\nProxy problems\nIf you work in an office, you might be behind an HTTP proxy. If so, set the environment variables http_proxy and https_proxy. Most Python applications (and other free software) respect these. Example syntax:\n\n```\nhttp://proxy_url:port\nhttp://username:password@proxy_url:port\n\n```\n\nIf you're really unlucky, your proxy might be a Microsoft NTLM proxy. Free software can't cope. The only solution is to install a free software friendly proxy that forwards to the nasty proxy. http://cntlm.sourceforge.net/\nUnable to find vcvarsall.bat\nPython modules can be partly written in C or C++. Pip tries to compile from source. If you don't have a C/C++ compiler installed and configured, you'll see this cryptic error message.\n\nError: Unable to find vcvarsall.bat\n\nYou can fix that by installing a C++ compiler such as MinGW or Visual C++. Microsoft actually ships one specifically for use with Python. Or try Microsoft Visual C++ Compiler for Python 2.7.\nOften though it's easier to check Christoph's site for your package."}
{"instruction": "How do I parse a string to a float or int?", "output": "```\n>>> a = \"545.2222\"\n>>> float(a)\n545.22220000000004\n>>> int(float(a))\n545\n\n```"}
{"instruction": "How do I get a substring of a string in Python?", "output": "```\n>>> x = \"Hello World!\"\n>>> x[2:]\n'llo World!'\n>>> x[:2]\n'He'\n>>> x[:-2]\n'Hello Worl'\n>>> x[-2:]\n'd!'\n>>> x[2:-2]\n'llo Worl'\n\n```\n\nPython calls this concept \"slicing\" and it works on more than just strings. Take a look here for a comprehensive introduction."}
{"instruction": "How to upgrade all Python packages with pip", "output": "There isn't a built-in flag yet. Starting with pip version 22.3, the --outdated and --format=freeze have become mutually exclusive. Use Python, to parse the JSON output:\n\n```\npip --disable-pip-version-check list --outdated --format=json | python -c \"import json, sys; print('\\n'.join([x['name'] for x in json.load(sys.stdin)]))\" | xargs -n1 pip install -U\n\n```\n\nIf you are using pip<22.3 you can use:\n\n```\npip list --outdated --format=freeze | grep -v '^\\-e' | cut -d = -f 1  | xargs -n1 pip install -U\n\n```\n\nFor older versions of pip:\n\n```\npip freeze --local | grep -v '^\\-e' | cut -d = -f 1  | xargs -n1 pip install -U\n\n```\n\n\n\nThe grep is to skip editable (\"-e\") package definitions, as suggested by @jawache. (Yes, you could replace grep+cut with sed or awk or perl or...).\n\nThe -n1 flag for xargs prevents stopping everything if updating one package fails (thanks @andsens).\n\n\n\nNote: there are infinite potential variations for this. I'm trying to keep this answer short and simple, but please do suggest variations in the comments!"}
{"instruction": "How do I install pip on Windows?", "output": "Python 3.4+ and 2.7.9+\nGood news! Python 3.4 (released March 2014) and Python 2.7.9 (released December 2014) ship with Pip. This is the best feature of any Python release. It makes the community's wealth of libraries accessible to everyone. Newbies are no longer excluded from using community libraries by the prohibitive difficulty of setup. In shipping with a package manager, Python joins Ruby, Node.js, Haskell, Perl, Go‚Äîalmost every other contemporary language with a majority open-source community. Thank you, Python.\nIf you do find that pip is not available, simply run ensurepip.\n\nOn Windows:\n\n```\npy -3 -m ensurepip\n\n```\n\n\nOtherwise:\n\n```\npython3 -m ensurepip\n\n```\n\n\n\nOf course, that doesn't mean Python packaging is problem solved. The experience remains frustrating. I discuss this in the Stack Overflow question Does Python have a package/module management system?.\nPython 3 ‚â§ 3.3 and 2 ‚â§ 2.7.8\nFlying in the face of its 'batteries included' motto, Python ships without a package manager. To make matters worse, Pip was‚Äîuntil recently‚Äîironically difficult to install.\nOfficial instructions\nPer https://pip.pypa.io/en/stable/installing/#do-i-need-to-install-pip:\nDownload get-pip.py, being careful to save it as a .py file rather than .txt. Then, run it from the command prompt:\n\n```\npython get-pip.py\n\n```\n\nYou possibly need an administrator command prompt to do this. Follow Start a Command Prompt as an Administrator (Microsoft TechNet).\nThis installs the pip package, which (in Windows) contains ...\\Scripts\\pip.exe that path must be in PATH environment variable to use pip from the command line (see the second part of 'Alternative Instructions' for adding it to your PATH,\nAlternative instructions\nThe official documentation tells users to install Pip and each of its dependencies from source. That's tedious for the experienced and prohibitively difficult for newbies.\nFor our sake, Christoph Gohlke prepares Windows installers (.msi) for popular Python packages. He builds installers for all Python versions, both 32 and 64 bit. You need to:\n\nInstall setuptools\nInstall pip\n\nFor me, this installed Pip at C:\\Python27\\Scripts\\pip.exe. Find pip.exe on your computer, then add its folder (for example, C:\\Python27\\Scripts) to your path (Start / Edit environment variables). Now you should be able to run pip from the command line. Try installing a package:\n\n```\npip install httpie\n\n```\n\nThere you go (hopefully)! Solutions for common problems are given below:\nProxy problems\nIf you work in an office, you might be behind an HTTP proxy. If so, set the environment variables http_proxy and https_proxy. Most Python applications (and other free software) respect these. Example syntax:\n\n```\nhttp://proxy_url:port\nhttp://username:password@proxy_url:port\n\n```\n\nIf you're really unlucky, your proxy might be a Microsoft NTLM proxy. Free software can't cope. The only solution is to install a free software friendly proxy that forwards to the nasty proxy. http://cntlm.sourceforge.net/\nUnable to find vcvarsall.bat\nPython modules can be partly written in C or C++. Pip tries to compile from source. If you don't have a C/C++ compiler installed and configured, you'll see this cryptic error message.\n\nError: Unable to find vcvarsall.bat\n\nYou can fix that by installing a C++ compiler such as MinGW or Visual C++. Microsoft actually ships one specifically for use with Python. Or try Microsoft Visual C++ Compiler for Python 2.7.\nOften though it's easier to check Christoph's site for your package."}
{"instruction": "Installing specific package version with pip", "output": "TL;DR:\nUpdate as of 2022-12-28:\npip install --force-reinstall -v\nFor example: pip install --force-reinstall -v \"MySQL_python==1.2.2\"\nWhat these options mean:\n\n--force-reinstall is an option to reinstall all packages even if they are already up-to-date.\n-v is for verbose. You can combine for even more verbosity (i.e. -vv) up to 3 times (e.g. --force-reinstall -vvv).\n\nThanks to @Peter for highlighting this (and it seems that the context of the question has broadened given the time when the question was first asked!), the documentation for Python discusses a caveat with using -I, in that it can break your installation if it was installed with a different package manager or if if your package is/was a different version.\n\nOriginal answer:\n\npip install -Iv (i.e. pip install -Iv MySQL_python==1.2.2)\n\n\nWhat these options mean:\n\n-I stands for --ignore-installed which will ignore the installed packages, overwriting them.\n-v is for verbose. You can combine for even more verbosity (i.e. -vv) up to 3 times (e.g. -Ivvv).\n\nFor more information, see pip install --help\nFirst, I see two issues with what you're trying to do. Since you already have an installed version, you should either uninstall the current existing driver or use pip install -I MySQL_python==1.2.2\nHowever, you'll soon find out that this doesn't work. If you look at pip's installation log, or if you do a pip install -Iv MySQL_python==1.2.2 you'll find that the PyPI URL link does not work for MySQL_python v1.2.2. You can verify this here: http://pypi.python.org/pypi/MySQL-python/1.2.2\nThe download link 404s and the fallback URL links are re-directing infinitely due to sourceforge.net's recent upgrade and PyPI's stale URL.\nSo to properly install the driver, you can follow these steps:\n\n```\npip uninstall MySQL_python\npip install -Iv http://sourceforge.net/projects/mysql-python/files/mysql-python/1.2.2/MySQL-python-1.2.2.tar.gz/download\n\n```"}
{"instruction": "How can I install packages using pip according to the requirements.txt file from a local directory?", "output": "This works for me:\n\n```\n$ pip install -r requirements.txt --no-index --find-links file:///tmp/packages\n\n```\n\n--no-index - Ignore package index (only look at --find-links URLs instead).\n-f, --find-links <URL> - If <URL> is a URL or a path to an HTML file, then parse for links to archives. If <URL> is a local path or a file:// URL that's a directory, then look for archives in the directory listing."}
{"instruction": "How do I remove all packages installed by pip?", "output": "I've found this snippet as an alternative solution. It's a more graceful removal of libraries than remaking the virtualenv:\n\n```\npip freeze | xargs pip uninstall -y\n\n```\n\n\nIn case you have packages installed via VCS, you need to exclude those lines and remove the packages manually (elevated from the comments below):\n\n```\npip freeze --exclude-editable | xargs pip uninstall -y\n\n```\n\n\nIf you have packages installed directly from github/gitlab, those will have @.\nLike:\ndjango @ git+https://github.com/django.git@<sha>\nYou can add cut -d \"@\" -f1 to get just the package name that is required to uninstall it.\n\n```\npip freeze | cut -d \"@\" -f1 | xargs pip uninstall -y\n\n```"}
{"instruction": "How do I get a list of locally installed Python modules?", "output": "Solution\nDo not use with pip > 10.0!\nMy 50 cents for getting a pip freeze-like list from a Python script:\n\n```\nimport pip\ninstalled_packages = pip.get_installed_distributions()\ninstalled_packages_list = sorted([\"%s==%s\" % (i.key, i.version)\n     for i in installed_packages])\nprint(installed_packages_list)\n\n```\n\nAs a (too long) one liner:\n\n```\nsorted([\"%s==%s\" % (i.key, i.version) for i in pip.get_installed_distributions()])\n\n```\n\nGiving:\n\n```\n['behave==1.2.4', 'enum34==1.0', 'flask==0.10.1', 'itsdangerous==0.24',\n 'jinja2==2.7.2', 'jsonschema==2.3.0', 'markupsafe==0.23', 'nose==1.3.3',\n 'parse-type==0.3.4', 'parse==1.6.4', 'prettytable==0.7.2', 'requests==2.3.0',\n 'six==1.6.1', 'vioozer-metadata==0.1', 'vioozer-users-server==0.1',\n 'werkzeug==0.9.4']\n\n```\n\nScope\nThis solution applies to the system scope or to a virtual environment scope, and covers packages installed by setuptools, pip and (god forbid) easy_install.\nMy use case\nI added the result of this call to my Flask server, so when I call it with http://example.com/exampleServer/environment I get the list of packages installed on the server's virtualenv. It makes debugging a whole lot easier.\nCaveats\nI have noticed a strange behaviour of this technique - when the Python interpreter is invoked in the same directory as a setup.py file, it does not list the package installed by setup.py.\nSteps to reproduce:\nCreate a virtual environment\n\n```\n$ cd /tmp\n$ virtualenv test_env\nNew python executable in test_env/bin/python\nInstalling setuptools, pip...done.\n$ source test_env/bin/activate\n(test_env) $\n\n```\n\nClone a Git repository with setup.py\n\n```\n(test_env) $ git clone https://github.com/behave/behave.git\nCloning into 'behave'...\nremote: Reusing existing pack: 4350, done.\nremote: Total 4350 (delta 0), reused 0 (delta 0)\nReceiving objects: 100% (4350/4350), 1.85 MiB | 418.00 KiB/s, done.\nResolving deltas: 100% (2388/2388), done.\nChecking connectivity... done.\n\n```\n\nWe have behave's setup.py in /tmp/behave:\n\n```\n(test_env) $ ls /tmp/behave/setup.py\n    /tmp/behave/setup.py\n\n```\n\nInstall the Python package from the Git repository\n\n```\n(test_env) $ cd /tmp/behave && pip install .\nrunning install\n...\nInstalled /private/tmp/test_env/lib/python2.7/site-packages/enum34-1.0-py2.7.egg\nFinished processing dependencies for behave==1.2.5a1\n\n```\n\nIf we run the aforementioned solution from /tmp\n\n```\n>>> import pip\n>>> sorted([\"%s==%s\" % (i.key, i.version) for i in pip.get_installed_distributions()])\n['behave==1.2.5a1', 'enum34==1.0', 'parse-type==0.3.4', 'parse==1.6.4', 'six==1.6.1']\n>>> import os\n>>> os.getcwd()\n'/private/tmp'\n\n```\n\nIf we run the aforementioned solution from /tmp/behave\n\n```\n>>> import pip\n>>> sorted([\"%s==%s\" % (i.key, i.version) for i in pip.get_installed_distributions()])\n['enum34==1.0', 'parse-type==0.3.4', 'parse==1.6.4', 'six==1.6.1']\n>>> import os\n>>> os.getcwd()\n'/private/tmp/behave'\n\n```\n\nbehave==1.2.5a1 is missing from the second example, because the working directory contains behave's setup.py file.\nI could not find any reference to this issue in the documentation. Perhaps I shall open a bug for it."}
{"instruction": "How do I install a Python package with a .whl file?", "output": "I just used the following which was quite simple. First open a console then cd to where you've downloaded your file like some-package.whl and use\n\n```\npip install some-package.whl\n\n```\n\nNote: if pip.exe is not recognized, you may find it in the \"Scripts\" directory from where python has been installed. If pip is not installed, this page can help:\nHow do I install pip on Windows?\nNote: for clarification\nIf you copy the *.whl file to your local drive (ex. C:\\some-dir\\some-file.whl) use the following command line parameters --  \n\n```\npip install C:/some-dir/some-file.whl\n\n```"}
{"instruction": "Find which version of package is installed with pip", "output": "As of pip 1.3, there is a pip show command.\n\n```\n$ pip show Jinja2\n---\nName: Jinja2\nVersion: 2.7.3\nLocation: /path/to/virtualenv/lib/python2.7/site-packages\nRequires: markupsafe\n\n```\n\nIn older versions, pip freeze and grep should do the job nicely.\n\n```\n$ pip freeze | grep Jinja2\nJinja2==2.7.3\n\n```"}
{"instruction": "pip install from git repo branch", "output": "Prepend the url prefix git+ (See VCS Support):\n\n```\npip install git+https://github.com/tangentlabs/django-oscar-paypal.git@issue/34/oscar-0.6\n\n```\n\nAnd specify the branch name without the leading /."}
{"instruction": "What is the difference between pip and Conda?", "output": "Quoting from the Conda blog:\n\nHaving been involved in the python world for so long, we are all aware of pip, easy_install, and virtualenv, but these tools did not meet all of our specific requirements. The main problem is that they are focused around Python, neglecting non-Python library dependencies, such as HDF5, MKL, LLVM, etc., which do not have a setup.py in their source code and also do not install files into Python‚Äôs site-packages directory.\n\nSo Conda is a packaging tool and installer that aims to do more than what pip does; handle library dependencies outside of the Python packages as well as the Python packages themselves. Conda also creates a virtual environment, like virtualenv does.\nAs such, Conda should be compared to Buildout perhaps, another tool that lets you handle both Python and non-Python installation tasks.\nBecause Conda introduces a new packaging format, you cannot use pip and Conda interchangeably;  pip cannot install the Conda package format. You can use the two tools side by side (by installing pip with conda install pip) but they do not interoperate either.\nSince writing this answer, Anaconda has published a new page on Understanding Conda and Pip, which echoes this as well:\n\nThis highlights a key difference between conda and pip. Pip installs Python packages whereas conda installs packages which may contain software written in any language. For example, before using pip, a Python interpreter must be installed via a system package manager or by downloading and running an installer. Conda on the other hand can install Python packages as well as the Python interpreter directly.\n\nand further on\n\nOccasionally a package is needed which is not available as a conda package but is available on PyPI and can be installed with pip. In these cases, it makes sense to try to use both conda and pip."}
{"instruction": "How do I update/upgrade pip itself from inside my virtual environment?", "output": "pip is just a PyPI package like any other; you could use it to upgrade itself the same way you would upgrade any package:\n\n```\npip install --upgrade pip\n\n```\n\nOn Windows the recommended command is:\n\n```\npy -m pip install --upgrade pip\n\n```"}
{"instruction": "Why use pip over easy_install?", "output": "Many of the answers here are out of date for 2015 (although the initially accepted one from Daniel Roseman is not). Here's the current state of things:\n\nBinary packages are now distributed as wheels (.whl files)‚Äînot just on PyPI, but in third-party repositories like Christoph Gohlke's Extension Packages for Windows. pip can handle wheels; easy_install cannot.\nVirtual environments (which come built-in with 3.4, or can be added to 2.6+/3.1+ with virtualenv) have become a very important and prominent tool (and recommended in the official docs); they include pip out of the box, but don't even work properly with easy_install.\nThe distribute package that included easy_install is no longer maintained. Its improvements over setuptools got merged back into setuptools. Trying to install distribute will just install setuptools instead.\neasy_install itself is only quasi-maintained.\nAll of the cases where pip used to be inferior to easy_install‚Äîinstalling from an unpacked source tree, from a DVCS repo, etc.‚Äîare long-gone; you can pip install ., pip install git+https://.\npip comes with the official Python 2.7 and 3.4+ packages from python.org, and a pip bootstrap is included by default if you build from source.\nThe various incomplete bits of documentation on installing, using, and building packages have been replaced by the Python Packaging User Guide. Python's own documentation on Installing Python Modules now defers to this user guide, and explicitly calls out pip as \"the preferred installer program\".\nOther new features have been added to pip over the years that will never be in easy_install. For example, pip makes it easy to clone your site-packages by building a requirements file and then installing it with a single command on each side. Or to convert your requirements file to a local repo to use for in-house development. And so on.\n\nThe only good reason that I know of to use easy_install in 2015 is the special case of using Apple's pre-installed Python versions with OS X 10.5-10.8. Since 10.5, Apple has included easy_install, but as of 10.10 they still don't include pip. With 10.9+, you should still just use get-pip.py, but for 10.5-10.8, this has some problems, so it's easier to sudo easy_install pip. (In general, easy_install pip is a bad idea; it's only for OS X 10.5-10.8 that you want to do this.) Also, 10.5-10.8 include readline in a way that easy_install knows how to kludge around but pip doesn't, so you also want to sudo easy_install readline if you want to upgrade that."}
{"instruction": "pip install mysql-python fails with EnvironmentError: mysql_config not found", "output": "It seems mysql_config is missing on your system or the installer could not find it.\nBe sure mysql_config is really installed.\nFor example on Debian/Ubuntu you must install the package:\n\n```\nsudo apt-get install libmysqlclient-dev\n\n```\n\nMaybe the mysql_config is not in your path, it will be the case when you compile by yourself\nthe mysql suite.\nUpdate: For recent versions of debian/ubuntu (as of 2018) it is \n\n```\nsudo apt install default-libmysqlclient-dev\n\n```"}
{"instruction": "Can I force pip to reinstall the current version?", "output": "```\npip install --upgrade --force-reinstall <package>\n\n```\n\nWhen upgrading, reinstall all packages even if they are already up-to-date.\n\n```\npip install -I <package>\npip install --ignore-installed <package>\n\n```\n\nIgnore the installed packages (reinstalling instead)."}
{"instruction": "How to list all available package versions with pip?", "output": "(update: As of March 2020, many people have reported that yolk, installed via pip install yolk3k, only returns latest version.  Chris's answer seems to have the most upvotes and worked for me)\nThe script at pastebin does work. However it's not very convenient if you're working with multiple environments/hosts because you will have to copy/create it every time.\nA better all-around solution would be to use yolk3k, which is available to install with pip. E.g. to see what versions of Django are available:\n\n```\n$ pip install yolk3k\n$ yolk -V django\nDjango 1.3\nDjango 1.2.5\nDjango 1.2.4\nDjango 1.2.3\nDjango 1.2.2\nDjango 1.2.1\nDjango 1.2\nDjango 1.1.4\nDjango 1.1.3\nDjango 1.1.2\nDjango 1.0.4\n\n```\n\nyolk3k is a fork of the original yolk which ceased development in 2012. Though yolk is no longer maintained (as indicated in comments below), yolk3k appears to be and supports Python 3.\nNote: I am not involved in the development of yolk3k. If something doesn't seem to work as it should, leaving a comment here should not make much difference. Use the yolk3k issue tracker instead and consider submitting a fix, if possible."}
{"instruction": "How to update/upgrade a package using pip?", "output": "This is the way\n\n```\npip install <package_name> --upgrade\n\n```\n\nor in short\n\n```\npip install <package_name> -U\n\n```\n\nUsing sudo will ask to enter your root password to confirm the action, but although common, is considered unsafe.\nIf you do not have a root password (if you are not the admin) you should probably work with virtualenv.\nYou can also use the user flag to install it on this user only.\n\n```\npip install <package_name> --upgrade --user\n\n```"}
{"instruction": "How to state in requirements.txt a direct github source", "output": "Normally your requirements.txt file would look something like this:\n\n```\npackage-one==1.9.4\npackage-two==3.7.1\npackage-three==1.0.1\n...\n\n```\n\nTo specify a Github repo, you do not need the package-name== convention.\nThe examples below update package-two using a GitHub repo. The text after @ denotes the specifics of the package.\nSpecify commit hash (41b95ec in the context of updated requirements.txt):\n\n```\npackage-one==1.9.4\npackage-two @ git+https://github.com/owner/repo@41b95ec\npackage-three==1.0.1\n\n```\n\nSpecify branch name (main):\n\n```\npackage-two @ git+https://github.com/owner/repo@main\n\n```\n\nSpecify tag (0.1):\n\n```\npackage-two @ git+https://github.com/owner/repo@0.1\n\n```\n\nSpecify release (3.7.1):\n\n```\npackage-two @ git+https://github.com/owner/repo@releases/tag/v3.7.1\n\n```\n\nNote that in certain versions of pip you will need to update the package version in the package's setup.py, or pip will assume the requirement is already satisfied and not install the new version. For instance, if you have 1.2.1 installed, and want to fork this package with your own version, you could use the above technique in your requirements.txt and then update setup.py to 1.2.1.1.\nSee also the pip documentation on VCS support."}
{"instruction": "Where does pip install its packages?", "output": "pip when used with virtualenv will generally install packages in the path <virtualenv_name>/lib/<python_ver>/site-packages.\nFor example, I created a test virtualenv named venv_test with Python 2.7, and the django folder is in venv_test/lib/python2.7/site-packages/django."}
{"instruction": "Dealing with multiple Python versions and PIP", "output": "The current recommendation is to use python -m pip, where python is the version of Python you would like to use. This is the recommendation because it works across all versions of Python, and in all forms of virtualenv. For example:\n\n```\n# The system default Python installation:\npython -m pip install fish\n\n# A virtualenv's Python installation:\n.env/bin/python -m pip install fish\n\n# A specific version of python:\npython-3.6 -m pip install fish\n\n```\n\nPrevious answer, left for posterity:\nSince version 0.8, Pip supports pip-{version}. You can use it the same as easy_install-{version}:\n\n```\npip-2.5 install myfoopackage\npip-2.6 install otherpackage\npip-2.7 install mybarpackage\n\n```\n\n\npip changed its schema to use pipVERSION instead of pip-VERSION in version 1.5. You should use the following if you have pip >= 1.5:\n\n```\npip2.6 install otherpackage\npip2.7 install mybarpackage\n\n```\n\nCheck Versioned commands consistent with Python. #1053 for more details\n\nReferences:\n\nNeed pip-x.y scripts #200\nv0.8 changelog or News for pip, v0.8"}
{"instruction": "How to install psycopg2 with &quot;pip&quot; on Python?", "output": "Note: Since a while back, there are binary wheels for Windows in PyPI, so this should no longer be an issue for Windows users. Below are solutions for Linux, Mac users, since lots of them find this post through web searches.\n\nOption 1\nInstall the psycopg2-binary PyPI package instead, it has Python wheels for Linux and Mac OS.\n\n```\npip install psycopg2-binary\n\n```\n\n\nOption 2\nInstall the prerequsisites for building the psycopg2 package from source:\nDebian/Ubuntu\n\n\n\n\nPython version\nCommand\nNote\n\n\n\n\nDefault Python 3\nsudo apt install libpq-dev python3-dev\n\n\n\nPython 3.x\nsudo apt install libpq-dev python3.x-dev\nsubstitute x in command\n\n\nPython 2\nsudo apt install libpq-dev python-dev\n\n\n\n\n\nIf that's not enough, you might additionally need to install\n\n```\nsudo apt install build-essential\n\n```\n\nor\n\n```\nsudo apt install postgresql-server-dev-all\n\n```\n\nas well before installing psycopg2 again.\nCentOS 6\nSee Banjer's answer\nmacOS\nSee nichochar's answer"}
{"instruction": "No module named pkg_resources", "output": "July 2018 Update \nMost people should now use pip install setuptools (possibly with sudo).\nSome may need to (re)install the python-setuptools package via their package manager (apt-get install, yum install, etc.).\nThis issue can be highly dependent on your OS and dev environment. See the legacy/other answers below if the above isn't working for you.\nExplanation\nThis error message is caused by a missing/broken Python setuptools package. Per Matt M.'s comment and setuptools issue #581, the bootstrap script referred to below is no longer the recommended installation method.\nThe bootstrap script instructions will remain below, in case it's still helpful to anyone.\nLegacy Answer\nI encountered the same ImportError today while trying to use pip. Somehow the setuptools package had been deleted in my Python environment.\nTo fix the issue, run the setup script for setuptools:\n\n```\nwget https://bootstrap.pypa.io/ez_setup.py -O - | python\n\n```\n\n(or if you don't have wget installed (e.g. OS X), try\n\n```\ncurl https://bootstrap.pypa.io/ez_setup.py | python\n\n```\n\npossibly with sudo prepended.)\nIf you have any version of distribute, or any setuptools below 0.6, you will have to uninstall it first.*\nSee Installation Instructions for further details.\n\n* If you already have a working distribute, upgrading it to the \"compatibility wrapper\" that switches you over to setuptools is easier. But if things are already broken, don't try that."}
{"instruction": "pip uses incorrect cached package version, instead of the user-specified version", "output": "If using pip 6.0 or newer, try adding the --no-cache-dir option (source).\nIf using pip older than pip 6.0, upgrade it with pip install -U pip."}
{"instruction": "How to install pip with Python 3?", "output": "edit: Manual installation and use of setuptools is not the standard process anymore.\nIf you're running Python 2.7.9+ or Python 3.4+\nCongrats, you should already have pip installed. If you do not, read onward.\nIf you're running a Unix-like System\nYou can usually install the package for pip through your package manager if your version of Python is older than 2.7.9 or 3.4, or if your system did not include it for whatever reason.\nInstructions for some of the more common distros follow.\nInstalling on Debian (Wheezy and newer) and Ubuntu (Trusty Tahr and newer) for Python 2.x\nRun the following command from a terminal:\n\n```\nsudo apt-get install python-pip \n\n```\n\nInstalling on Debian (Wheezy and newer) and Ubuntu (Trusty Tahr and newer) for Python 3.x\nRun the following command from a terminal:\n\n```\nsudo apt-get install python3-pip\n\n```\n\nNote:\nOn a fresh Debian/Ubuntu install, the package may not be found until you do:\n\n```\nsudo apt-get update\n\n```\n\nInstalling pip on CentOS 7 for Python 2.x\nOn CentOS 7, you have to install setup tools first, and then use that to install pip, as there is no direct package for it.\n\n```\nsudo yum install python-setuptools\nsudo easy_install pip\n\n```\n\nInstalling pip on CentOS 7 for Python 3.x\nAssuming you installed Python 3.4 from EPEL, you can install Python 3's setup tools and use it to install pip.\n\n```\n# First command requires you to have enabled EPEL for CentOS7\nsudo yum install python34-setuptools\nsudo easy_install pip\n\n```\n\nIf your Unix/Linux distro doesn't have it in package repos\nInstall using the manual way detailed below.\nThe manual way\nIf you want to do it the manual way, the now-recommended method is to install using the get-pip.py script from pip's installation instructions.\n\nInstall pip\nTo install pip, securely download get-pip.py\nThen run the following (which may require administrator access):\n\n```\npython get-pip.py \n\n```\n\nIf setuptools is not already installed, get-pip.py will install setuptools for you."}
{"instruction": "How can I upgrade specific packages using pip and a requirements file?", "output": "First make sure you have checked the most voted answer.\n\nI'm not sure if it's exactly your problem, but in my case, I wasn't able to upgrade Django to 1.2.4 - I was always finishing with 1.2.3 version, so I uninstalled Django with:\n\n```\n<virtualenv>/bin/pip uninstall Django\n\n```\n\nThen I removed <virtualenv>/build/Django directory and finally I installed the proper version with:\n\n```\n<virtualenv>/bin/pip install Django\n\n```"}
{"instruction": "Install a Python package into a different directory using pip?", "output": "Use:\n\n```\npip install --install-option=\"--prefix=$PREFIX_PATH\" package_name\n\n```\n\nYou might also want to use --ignore-installed to force all dependencies to be reinstalled using this new prefix.  You can use --install-option to multiple times to add any of the options you can use with python setup.py install (--prefix is probably what you want, but there are a bunch more options you could use)."}
{"instruction": "&#39;pip&#39; is not recognized as an internal or external command", "output": "You need to add the path of your pip installation to your PATH system variable. By default, pip is installed to C:\\Python34\\Scripts\\pip (pip now comes bundled with new versions of python), so the path \"C:\\Python34\\Scripts\" needs to be added to your PATH variable.\nTo check if it is already in your PATH variable, type echo %PATH% at the CMD prompt\nTo add the path of your pip installation to your PATH variable, you can use the Control Panel or the setx command. For example:\n\n```\nsetx PATH \"%PATH%;C:\\Python34\\Scripts\"\n\n```\n\n\nNote:\nAccording to the official documentation, \"[v]ariables set with setx variables are available in future command windows only, not in the current command window\". In particular, you will need to start a new cmd.exe instance after entering the above command in order to utilize the new environment variable. \nThanks to Scott Bartell for pointing this out."}
{"instruction": "Is it possible to use pip to install a package from a private GitHub repository?", "output": "You can use the git+ssh URI scheme, but you must set a username. Notice the git@ part in the URI:\n\n```\npip install git+ssh://git@github.com/echweb/echweb-utils.git\n\n```\n\nAlso read about deploy keys.\nPS: In my installation, the \"git+ssh\" URI scheme works only with \"editable\" requirements:\n\n```\npip install -e URI#egg=EggName\n\n```\n\nRemember: Change the : character that git remote -v prints to a / character before using the remote's address in the pip command:\n\n```\n$ git remote -v\norigin  git@github.com:echweb/echweb-utils.git (fetch)\n#                     ^ change this to a '/' character\n\n```\n\nIf you forget, you will get this error:\n\n```\nssh: Could not resolve hostname github.com:echweb:\n         nodename nor servname provided, or not known\n\n```"}
{"instruction": "Installing Python packages from local file system folder to virtualenv with pip", "output": "I am pretty sure that what you are looking for is called --find-links option.\nYou can do\n\n```\npip install mypackage --no-index --find-links file:///srv/pkg/mypackage\n\n```"}
{"instruction": "How can I Install a Python module with Pip programmatically (from my code)?", "output": "The officially recommended way to install packages from a script is by calling pip's command-line interface via a subprocess. Most other answers presented here are not supported by pip. Furthermore since pip v10, all code has been moved to pip._internal precisely in order to make it clear to users that programmatic use of pip is not allowed.\nUse sys.executable to ensure that you will call the same pip associated with the current runtime.\n\n```\nimport subprocess\nimport sys\n\ndef install(package):\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n\n```"}
{"instruction": "setup script exited with error: command &#39;x86_64-linux-gnu-gcc&#39; failed with exit status 1", "output": "Try installing these packages.\n\n```\nsudo apt-get install build-essential autoconf libtool pkg-config python-opengl python-pil python-pyrex python-pyside.qtopengl idle-python2.7 qt4-dev-tools qt4-designer libqtgui4 libqtcore4 libqt4-xml libqt4-test libqt4-script libqt4-network libqt4-dbus python-qt4 python-qt4-gl libgle3 python-dev libssl-dev\n\nsudo easy_install greenlet\n\nsudo easy_install gevent\n\n```"}
{"instruction": "How do I install the yaml package for Python?", "output": "You could try the search the feature on https://pypi.org/search (via a browser) and look for packages in PyPI with yaml in the short description.  That reveals various packages, including PyYaml, yamltools, and PySyck, among others (Note that PySyck docs recommend using PyYaml, since syck is out of date).  Now you know a specific package name, you can install it:\n\n```\n$ pip install pyyaml\n\n```\n\nIf you want to install python yaml system-wide in linux, you can also use a package manager, like aptitude or yum:\n\n```\n$ sudo apt-get install python-yaml\n$ sudo yum install python-yaml\n\n```"}
{"instruction": "Error after upgrading pip: cannot import name &#39;main&#39;", "output": "You must have inadvertently upgraded your system pip (probably through something like sudo pip install pip --upgrade)\npip 10.x adjusts where its internals are situated.  The pip3 command you're seeing is one provided by your package maintainer (presumably debian based here?) and is not a file managed by pip.\nYou can read more about this on pip's issue tracker\nYou'll probably want to not upgrade your system pip and instead use a virtualenv.\nTo recover the pip3 binary you'll need to sudo python3 -m pip uninstall pip && sudo apt install python3-pip --reinstall\nIf you want to continue in \"unsupported territory\" (upgrading a system package outside of the system package manager), you can probably get away with python3 -m pip ... instead of pip3."}
{"instruction": "Using Pip to install packages to an Anaconda environment", "output": "For others who run into this situation, I found this to be the most straightforward solution:\n\nRun conda create -n venv_name and conda activate venv_name, where venv_name is the name of your virtual environment.\n\nRun conda install pip. This will install pip to your venv directory.\n\nFind your anaconda directory, and find the actual venv folder. It should be somewhere like /anaconda/envs/venv_name/; or, you could also run conda activate venv_name.\n\nInstall new packages by doing /anaconda/envs/venv_name/bin/pip install package_name; or, simply run pip install package_name.\n\n\nThis should now successfully install packages using that virtual environment's pip!"}
{"instruction": "Error &quot;filename.whl is not a supported wheel on this platform&quot;", "output": "cp33 means CPython 3.3.\nYou need scipy‚Äë0.15.1‚Äëcp27‚Äënone‚Äëwin_amd64.whl instead."}
{"instruction": "Can I add comments to a pip requirements file?", "output": "Sure, you can, just use #\npip docs:\n\nA line that begins with # is treated as a comment and ignored. Whitespace followed by a # causes the # and the remainder of the line to be treated as a comment."}
{"instruction": "How to pip install a package with min and max version range?", "output": "You can do:\n\n```\n$ pip install \"package>=0.2,<0.3\"\n\n```\n\nAnd pip will look for the best match, assuming the version is at least 0.2, and less than 0.3.\nThis also applies to pip requirements files.  See the full details on version specifiers in PEP 440."}
{"instruction": "Reference requirements.txt for the install_requires kwarg in setuptools setup.py file", "output": "Update 12/2024:\nThis does not work in recent versions of Python, primarily because the function has been moved around in the pip module. Either way, as stated below by maintainers of the library, it is not recommended to import and use this function, as it is an internal function which is subject to change and being moved around.\n\nOld answer:\nYou can flip it around and list the dependencies in setup.py and have a single character ‚Äî a dot . ‚Äî in requirements.txt instead.\n\nAlternatively, even if not advised, it is still possible to parse the requirements.txt file (if it doesn't refer any external requirements by URL) with the following hack (tested with pip 9.0.1):\n\n```\ninstall_reqs = parse_requirements('requirements.txt', session='hack')\n\n```\n\nThis doesn't filter environment markers though.\n\nIn old versions of pip, more specifically older than 6.0, there is a public API that can be used to achieve this. A requirement file can contain comments (#) and can include some other files (--requirement or -r). Thus, if you really want to parse a requirements.txt you can use the pip parser:\n\n```\nfrom pip.req import parse_requirements\n\n# parse_requirements() returns generator of pip.req.InstallRequirement objects\ninstall_reqs = parse_requirements(<requirements_path>)\n\n# reqs is a list of requirement\n# e.g. ['django==1.5.1', 'mezzanine==1.4.6']\nreqs = [str(ir.req) for ir in install_reqs]\n\nsetup(\n    ...\n    install_requires=reqs\n)\n\n```"}
{"instruction": "How to install python3 version of package via pip on Ubuntu?", "output": "You may want to build a virtualenv of python3, then install packages of python3 after activating the virtualenv. So your system won't be messed up :)\nThis could be something like:\n\n```\nvirtualenv -p /usr/bin/python3 py3env\nsource py3env/bin/activate\npip install package-name\n\n```"}
{"instruction": "What is the purpose of &quot;pip install --user ...&quot;?", "output": "pip defaults to installing Python packages to a system directory (such as /usr/local/lib/python3.4). This requires root access.\n--user makes pip install packages in your home directory instead, which doesn't require any special privileges."}
{"instruction": "What is pyproject.toml file for?", "output": "What is it for?\nCurrently there are multiple packaging tools being popular in Python community and while setuptools still seems to be prevalent it's not a de¬†facto standard anymore. This situation creates a number of hassles for both end¬†users and developers:\n\nFor setuptools-based packages installation from source / build of a distribution can fail if one doesn't have setuptools installed;\npip doesn't support the installation of packages based on other packaging tools from source, so these tools had to generate a setup.py file to produce a compatible package. To build a distribution package one has to install the packaging tool first and then use tool-specific commands;\nIf package author decides to change the packaging tool, workflows must be changed as¬†well to use different tool-specific commands.\n\npyproject.toml is a new configuration file introduced by PEP¬†517 and PEP¬†518 to solve these problems:\n\n... think of the (rough) steps required to produce a built artifact for a project:\n\nThe source checkout of the project.\nInstallation of the build system.\nExecute the build system.\n\nThis PEP [518] covers step #2. PEP¬†517 covers step #3 ...\n\nAny tool can also extend this file with its own section (table) to accept tool-specific options, but it's up to them and not required.\nPEP¬†621 suggests using pyproject.toml to specify package core¬†metadata in static, tool-agnostic way. Which backends currently support this is shown in the following table:\n\n\n\nenscons\nflit_core\nhatchling\npdm-backend\npoetry-core\nsetuptools\n\n\n\n\n0.26.0+\n3.2+\n0.3+\n0.3.0+\n2.0.0+\n61.0.0+\n\n\n\nDoes it replace setup.py?\nFor setuptools-based packages pyproject.toml is not strictly meant to replace setup.py, but rather to ensure its correct execution if it's still needed. For other packaging tools ‚Äì yes, it is:\n\nWhere the build-backend key exists, this takes precedence and the source tree follows the format and conventions of the specified backend (as such no setup.py is needed unless the backend requires it). Projects may still wish to include a setup.py for compatibility with tools that do not use this spec.\n\nHow to install a package in editable mode?\nOriginally \"editable install\" was a setuptools-specific feature and as such it was not supported by PEP¬†517. Later¬†on PEP¬†660 extended this concept to packages using pyproject.toml.\nThere are two possible conditions for installing a package in editable mode using pip:\n\nModern:\nBoth the frontend (pip) and a backend must support PEP¬†660.\npip supports it since version¬†21.3;\nLegacy:\nPackaging tool must provide a setup.py file which supports the develop command.\nSince version¬†21.1 pip can also install packages using only setup.cfg file in editable mode.\n\nThe following table describes the support of editable installs by various backends:\n\n\n\nenscons\nflit_core\nhatchling\npdm-backend\npoetry-core\nsetuptools\n\n\n\n\n0.28.0+\n3.4+\n0.3+\n0.8.0+\n1.0.8+\n64.0.0+"}
{"instruction": "Could not find a version that satisfies the requirement tensorflow", "output": "The latest requirements for running TensorFlow are documented in the installation documentation.\n\nTensorFlow only supports 64-bit Python\n\nTensorFlow only supports certain versions of Python (for example, Python 3.6 is not supported)\n\n\nSo, if you're using an out-of-range version of Python (older or newer) or a 32-bit version, then you'll need to use a different version."}
{"instruction": "Upgrade Python in a virtual environment", "output": "Did you see this? If I haven't misunderstand that answer, you may try to create a new virtualenv on top of the old one. You just need to know which python is going to use your virtualenv (you will need to see your virtualenv version). \nIf your virtualenv is installed with the same python version of the old one and upgrading your virtualenv package is not an option, you may want to read this in order to install a virtualenv with the python version you want.\nEDIT\nI've tested this approach (the one that create a new virtualenv on top of the old one) and it worked fine for me. I think you may have some problems if you change from python 2.6 to 2.7 or 2.7 to 3.x but if you just upgrade inside the same version (staying at 2.7 as you want) you shouldn't have any problem, as all the packages are held in the same folders for both python versions (2.7.x and 2.7.y packages are inside your_env/lib/python2.7/).\nIf you change your virtualenv python version, you will need to install all your packages again for that version (or just link the packages you need into the new version packages folder, i.e: your_env/lib/python_newversion/site-packages)"}
{"instruction": "What is pip&#39;s `--no-cache-dir` good for?", "output": "Cached is: store away in hiding or for future use\nUsed for\n\n\nstore the installation files(.whl, etc) of the modules that you install through pip\nstore the source files (.tar.gz, etc) to avoid re-download when not expired\n\n\nPossible Reason you might want to disable cache:\n\n\nyou don't have space on your hard drive\npreviously run pip install with unexpected settings\n\neg:\n\npreviously run export PYCURL_SSL_LIBRARY=nss and pip install pycurl\nwant new run export PYCURL_SSL_LIBRARY=openssl and pip install pycurl --compile --no-cache-dir\n\n\n\n\nyou want to keep a Docker image as small as possible\n\nLinks to documentation\nhttps://pip.pypa.io/en/stable/topics/caching"}
{"instruction": "How to install multiple python packages at once using pip", "output": "For installing multiple packages on the command line, just pass them as a space-delimited list, e.g.:\n\n```\npip install wsgiref boto\n\n```\n\nFor installing from a text file, then, from pip install --help:\n\n-r FILENAME, --requirement=FILENAME\nInstall all the packages listed in the given requirements file.  This option can be used multiple times.\n\nTake a look at the pip documentation regarding requirements files for their general layout and syntax - note that you can generate one based on current environment / site-packages with pip freeze if you want a quick example - e.g. (based on having installed wsgiref and boto in a clean virtualenv):\n\n```\n$ pip freeze\nboto==2.3.0\nwsgiref==0.1.2\n\n```"}
{"instruction": "How to install Python package from GitHub?", "output": "You need to use the proper git URL:\n\n```\npip install git+https://github.com/jkbr/httpie.git#egg=httpie\n\n```\n\nAlso see the VCS Support section of the pip documentation.\nDon‚Äôt forget to include the egg=<projectname> part to explicitly name the project; this way pip can track metadata for it without having to have run the setup.py script."}
{"instruction": "Could not find a version that satisfies the requirement &lt;package&gt;", "output": "This approach (having all dependencies in a directory and not downloading from an index) only works when the directory contains all packages. The directory should therefore contain all dependencies but also all packages that those dependencies depend on (e.g., six, pytz etc).\nYou should therefore manually include these in requirements.txt (so that the first step downloads them explicitly) or you should install all packages using PyPI and then pip freeze > requirements.txt to store the list of all packages needed."}
{"instruction": "How to install Python MySQLdb module using pip?", "output": "It's easy to do, but hard to remember the correct spelling:\n\n```\npip install mysqlclient\n\n```\n\nIf you need 1.2.x versions (legacy Python only), use pip install MySQL-python\nNote: Some dependencies might have to be in place when running the above command. Some hints on how to install these on various platforms:\nUbuntu 14, Ubuntu 16, Debian 8.6 (jessie)\n\n```\nsudo apt-get install python-pip python-dev libmysqlclient-dev\n\n```\n\nFedora 24:\n\n```\nsudo dnf install python python-devel mysql-devel redhat-rpm-config gcc\n\n```\n\nMac OS\n\n```\nbrew install mysql-connector-c\n\n```\n\nif that fails, try\n\n```\nbrew install mysql\n\n```"}
{"instruction": "Does uninstalling a package with &quot;pip&quot; also remove the dependent packages?", "output": "No, it doesn't uninstall the dependencies packages. It only removes the specified package:\n\n```\n$ pip install specloud\n$ pip freeze # all the packages here are dependencies of specloud package\n\n```\n\n\nfigleaf==0.6.1\nnose==1.1.2\npinocchio==0.3\nspecloud==0.4.5\n\n\n```\n$ pip uninstall specloud\n$ pip freeze\n\n```\n\n\nfigleaf==0.6.1\nnose==1.1.2\npinocchio==0.3\n\nAs you can see those packages are dependencies from specloud and they're still there, but not the specloud package itself.\nAs mentioned below, you can install and use the pip-autoremove utility to remove a package plus unused dependencies."}
{"instruction": "How to install packages offline?", "output": "If the package is on PYPI, download it and its dependencies to some local directory.\nE.g.\n\n```\n\n$ mkdir /pypi && cd /pypi\n$ ls -la\n  -rw-r--r--   1 pavel  staff   237954 Apr 19 11:31 Flask-WTF-0.6.tar.gz\n  -rw-r--r--   1 pavel  staff   389741 Feb 22 17:10 Jinja2-2.6.tar.gz\n  -rw-r--r--   1 pavel  staff    70305 Apr 11 00:28 MySQL-python-1.2.3.tar.gz\n  -rw-r--r--   1 pavel  staff  2597214 Apr 10 18:26 SQLAlchemy-0.7.6.tar.gz\n  -rw-r--r--   1 pavel  staff  1108056 Feb 22 17:10 Werkzeug-0.8.2.tar.gz\n  -rw-r--r--   1 pavel  staff   488207 Apr 10 18:26 boto-2.3.0.tar.gz\n  -rw-r--r--   1 pavel  staff   490192 Apr 16 12:00 flask-0.9-dev-2a6c80a.tar.gz\n\n```\n\nSome packages may have to be archived into similar looking tarballs by hand. I do it a lot when I want a more recent (less stable) version of something. Some packages aren't on PYPI, so same applies to them.\nSuppose you have a properly formed Python application in ~/src/myapp. ~/src/myapp/setup.py will have install_requires list that mentions one or more things that you have in your /pypi directory. Like so:\n\n```\n  install_requires=[\n    'boto',\n    'Flask',\n    'Werkzeug',\n    # and so on\n\n```\n\nIf you want to be able to run your app with all the necessary dependencies while still hacking on it, you'll do something like this:\n\n```\n\n$ cd ~/src/myapp\n$ python setup.py develop --always-unzip --allow-hosts=None --find-links=/pypi\n\n```\n\nThis way your app will be executed straight from your source directory. You can hack on things, and then rerun the app without rebuilding anything.\nIf you want to install your app and its dependencies into the current python environment, you'll do something like this:\n\n```\n\n$ cd ~/src/myapp\n$ easy_install --always-unzip --allow-hosts=None --find-links=/pypi .\n\n```\n\nIn both cases, the build will fail if one or more dependencies aren't present in /pypi directory. It won't attempt to promiscuously install missing things from Internet.\nI highly recommend to invoke setup.py develop ... and easy_install ... within an active virtual environment to avoid contaminating your global Python environment. It is (virtualenv that is) pretty much the way to go. Never install anything into global Python environment.\nIf the machine that you've built your app has same architecture as the machine on which you want to deploy it, you can simply tarball the entire virtual environment directory into which you easy_install-ed everything. Just before tarballing though, you must make the virtual environment directory relocatable (see --relocatable option). NOTE: the destination machine needs to have the same version of Python installed, and also any C-based dependencies your app may have must be preinstalled there too (e.g. say if you depend on PIL, then libpng, libjpeg, etc must be preinstalled)."}
{"instruction": "After installing with pip, &quot;jupyter: command not found&quot;", "output": "you did not log out and log in ? It should be on your path to execute. \nIf not, pip installed executables in .local, so in a terminal:\n\n```\n ~/.local/bin/jupyter-notebook\n\n```\n\nshould start notebook"}
{"instruction": "What is the difference between pip and Conda?", "output": "Quoting from the Conda blog:\n\nHaving been involved in the python world for so long, we are all aware of pip, easy_install, and virtualenv, but these tools did not meet all of our specific requirements. The main problem is that they are focused around Python, neglecting non-Python library dependencies, such as HDF5, MKL, LLVM, etc., which do not have a setup.py in their source code and also do not install files into Python‚Äôs site-packages directory.\n\nSo Conda is a packaging tool and installer that aims to do more than what pip does; handle library dependencies outside of the Python packages as well as the Python packages themselves. Conda also creates a virtual environment, like virtualenv does.\nAs such, Conda should be compared to Buildout perhaps, another tool that lets you handle both Python and non-Python installation tasks.\nBecause Conda introduces a new packaging format, you cannot use pip and Conda interchangeably;  pip cannot install the Conda package format. You can use the two tools side by side (by installing pip with conda install pip) but they do not interoperate either.\nSince writing this answer, Anaconda has published a new page on Understanding Conda and Pip, which echoes this as well:\n\nThis highlights a key difference between conda and pip. Pip installs Python packages whereas conda installs packages which may contain software written in any language. For example, before using pip, a Python interpreter must be installed via a system package manager or by downloading and running an installer. Conda on the other hand can install Python packages as well as the Python interpreter directly.\n\nand further on\n\nOccasionally a package is needed which is not available as a conda package but is available on PyPI and can be installed with pip. In these cases, it makes sense to try to use both conda and pip."}
{"instruction": "How do I prevent Conda from activating the base environment by default?", "output": "I have conda 4.6 with a similar block of code that was added by conda. In my case, there's a conda configuration setting to disable the automatic base activation:\n\n```\nconda config --set auto_activate_base false\n\n```\n\nThe first time you run it, it'll create a .condarc in your home directory with that setting to override the default.\nThis wouldn't de-clutter your .bash_profile but it's a cleaner solution without manual editing that section that conda manages."}
{"instruction": "How can I rename a conda environment?", "output": "New answer:\nFrom Conda 4.14 you will be able to use just:\n\n```\nconda rename -n old_name  new_name \n\n```\n\nAlthough, under the hood, conda rename still uses [1][2]  undermentioned combination of conda create and conda remove.\nUse the -d flag for dry-run (not destination, as of v22.11.0)\n\n```\nconda rename -n old_name -d new_name \n\n```\n\n\nOld answer:\nYou can't.\nOne workaround is to create clone a new environment and then remove the original one.\nFirst, remember to deactivate your current environment. You can do this with the commands:\n\ndeactivate on Windows or\nsource deactivate on macOS/Linux.\n\nThen:\n\n```\nconda create --name new_name --clone old_name\nconda remove --name old_name --all # or its alias: `conda env remove --name old_name`\n\n```\n\nNotice there are several drawbacks of this method:\n\nIt redownloads packages (you can use --offline flag to disable it)\nTime consumed on copying environment's files\nTemporary double disk usage\n\nThere is an open issue requesting this feature."}
{"instruction": "How to update an existing Conda environment with a .yml file", "output": "Try using conda env update:\n\n```\nconda activate myenv\nconda env update --file local.yml --prune\n\n```\n\n--prune uninstalls dependencies which were removed from local.yml, as pointed out in this answer by @Blink.\nAttention: if there is a name tag with a name other than that of your environment in local.yml, the command above will create a new environment with that name. To avoid this, use (thanks @NumesSanguis):\n\n```\nconda env update --name myenv --file local.yml --prune\n\n```\n\nSee Updating an environment in Conda User Guide."}
{"instruction": "Does Conda replace the need for virtualenv?", "output": "Conda replaces virtualenv. In my opinion it is better. It is not limited to Python but can be used for other languages too. In my experience it provides a much smoother experience, especially for scientific packages. The first time I got MayaVi properly installed on Mac was with conda.   \nYou can still use pip. In fact, conda installs pip in each new environment. It knows about pip-installed packages.\n\nFor example:\n\n```\nconda list\n\n```\n\nlists all installed packages in your current environment.\nConda-installed packages show up like this:\n\n```\nsphinx_rtd_theme          0.1.7                    py35_0    defaults\n\n```\n\nand the ones installed via pip have the <pip> marker:\n\n```\nwxpython-common           3.0.0.0                   <pip>\n\n```"}
{"instruction": "&#39;Conda&#39; is not recognized as internal or external command", "output": "Although you were offered a good solution by others I think it is helpful to point out what is really happening. As per the Anaconda 4.4 changelog, https://docs.anaconda.com/anaconda/reference/release-notes/#what-s-new-in-anaconda-4-4:\n\nOn Windows, the PATH environment variable is no longer changed by default, as this can cause trouble with other software. The recommended approach is to instead use Anaconda Navigator or the Anaconda Command Prompt (located in the Start Menu under ‚ÄúAnaconda‚Äù) when you wish to use Anaconda software.\n\n(Note: recent Win 10 does not assume you have privileges to install or update.  If the command fails, right-click on the Anaconda Command Prompt, choose \"More\", chose \"Run as administrator\")\nThis is a change from previous installations. It is suggested to use Navigator or the Anaconda Prompt although you can always add it to your PATH as well. During the install the box to add Anaconda to the PATH is now unchecked but you can select it."}
{"instruction": "How do I upgrade to Python 3.6 with Conda?", "output": "Anaconda had not updated Python internally to 3.6, but later versions of Anaconda has a Python 3.6 version here.\na) Method 1\n\nIf you wanted to update, you will type conda update python\n\nTo update Anaconda, type conda update conda\n\nIf you want to upgrade between major python versions, like 3.5 to 3.6, you'll have to do\n\n```\nconda install python=$pythonversion$\n\n```\n\n\n\nb) Method 2 - Create a new environment (the better method)\n\n```\nconda create --name py36 python=3.6\n\n```\n\nc) To get the absolute latest Python (3.6.5 at time of writing)\n\n```\nconda create --name py365 python=3.6.5 --channel conda-forge\n\n```\n\nYou can see all this from here.\nAlso, refer to this for force upgrading."}
{"instruction": "Anaconda export Environment file", "output": "I can't find anything in the conda specs which allows you to export an environment file without the prefix: ... line. However, like Alex pointed out in the comments, conda doesn't seem to care about the prefix line when creating an environment from the file.\nWith that in mind, if you want the other user to have no knowledge of your default install path, you can remove the prefix line with grep before writing to environment.yml.\n\n```\nconda env export | grep -v \"^prefix: \" > environment.yml\n\n```\n\nEither way, the other user then runs:\n\n```\nconda env create -f environment.yml\n\n```\n\nand the environment will get installed in their default conda environment path.\nIf you want to specify a different install path than the default for your system (not related to 'prefix' in the environment.yml), just use the -p flag followed by the required path.\n\n```\nconda env create -f environment.yml -p /home/user/anaconda3/envs/env_name\n\n```\n\nNote that Conda recommends creating the environment.yml by hand, which is especially important if you are wanting to share your environment across platforms (Windows/Linux/Mac). In this case, you can just leave out the prefix line."}
{"instruction": "anaconda update all possible packages?", "output": "TL;DR: dependency conflicts: Updating one requires (by its requirements) to downgrade another\nYou are right:\n\n```\nconda update --all\n\n```\n\nis actually the way to go1. Conda always tries to upgrade the packages to the newest version in the series (say Python 2.x or 3.x).\nDependency conflicts\nBut it is possible that there are dependency conflicts (which prevent a further upgrade). Conda usually warns very explicitly if they occur.\ne.g. X requires Y <5.0, so Y will never be >= 5.0\nThat's why you 'cannot' upgrade them all.\nResolving\nUpdate 1: since a while, mamba has proven to be an extremely powerful drop-in replacement for conda in terms of dependency resolution and (IMH experience) finds solutions to problems where conda fails. A way to invoke it without installing mamba is via the --solver=libmamba flag (requires conda-libmamba-solver), as pointed out by matteo in the comments.\nTo add: maybe it could work but a newer version of X working with Y > 5.0 is not available in conda. It is possible to install with pip, since more packages are available in pip. But be aware that pip also installs packages if dependency conflicts exist and that it usually breaks your conda environment in the sense that you cannot reliably install with conda anymore. If you do that, do it as a last resort and after all packages have been installed with conda. It's rather a hack.\nA safe way you can try is to add conda-forge as a channel when upgrading (add -c conda-forge as a flag) or any other channel you find that contains your package if you really need this new version. This way conda does also search in this places for available packages.\nConsidering your update: You can upgrade them each separately, but doing so will not only include an upgrade but also a downgrade of another package as well. Say, to add to the example above:\nX > 2.0 requires Y < 5.0, X < 2.0 requires Y > 5.0\nSo upgrading Y > 5.0 implies downgrading X to < 2.0 and vice versa.\n(this is a pedagogical example, of course, but it's the same in reality, usually just with more complicated dependencies and sub-dependencies)\nSo you still cannot upgrade them all by doing the upgrades separately; the dependencies are just not satisfiable so earlier or later, an upgrade will downgrade an already upgraded package again. Or break the compatibility of the packages (which you usually don't want!), which is only possible by explicitly invoking an ignore-dependencies and force-command. But that is only to hack your way around issues, definitely not the normal-user case!\n\n1 If you actually want to update the packages of your installation, which you usually don't. The command run in the base environment will update the packages in this, but usually you should work with virtual environments (conda create -n myenv and then conda activate myenv). Executing conda update --all inside such an environment will update the packages inside this environment. However, since the base environment is also an environment, the answer applies to both cases in the same way."}
{"instruction": "How can I run Conda?", "output": "It turns out that I had not set the path.\nTo do so, I first had to edit .bash_profile (I downloaded it to my local desktop to do that; I do not know how to text edit a file from Linux)\nThen add this to .bash_profile:\n\n```\nPATH=$PATH:$HOME/anaconda/bin\n\n```"}
{"instruction": "From conda create requirements.txt for pip3", "output": "As the comment at the top indicates, the output of\nconda list -e > requirements.txt\ncan be used to create a conda virtual environment with\nconda create --name <env> --file requirements.txt\nbut this output isn't in the right format for pip.\nIf you want a file which you can use to create a pip virtual environment (i.e. a requirements.txt in the right format)\nyou can install pip within the conda environment, then use pip to create requirements.txt.\n\n```\nconda activate <env>\nconda install pip\npip freeze > requirements.txt\n\n```\n\nThen use the resulting requirements.txt to create a pip virtual environment:\n\n```\npython3 -m venv env\nsource env/bin/activate\npip install -r requirements.txt\n\n```\n\nWhen I tested this, the packages weren't identical across the outputs (pip included fewer packages) but it was sufficient to set up a functional environment.\nFor those getting odd path references in requirements.txt, use:\n\n```\npip list --format=freeze > requirements.txt\n\n```"}
{"instruction": "Combining conda environment.yml with pip requirements.txt", "output": "Pip dependencies can be included in the environment.yml file like this (docs):\n\n```\n# run: conda env create --file environment.yml\nname: test-env\ndependencies:\n- python>=3.5\n- anaconda\n- pip\n- numpy=1.13.3  # pin version for conda\n- pip:\n  # works for regular pip packages\n  - docx\n  - gooey\n  - matplotlib==2.0.0  # pin version for pip\n  # and for wheels\n  - http://www.lfd.uci.edu/~gohlke/pythonlibs/bofhrmxk/opencv_python-3.1.0-cp35-none-win_amd64.whl\n\n```\n\nIt also works for .whl files in the same directory (see Dengar's answer) as well as with common pip packages."}
{"instruction": "The environment is inconsistent, please check the package plan carefully", "output": "I had faced the same problem. Simply running\n\n```\nconda install anaconda\n\n```\n\nsolved the problem for me."}
{"instruction": "How to change default Anaconda python environment", "output": "If you just want to temporarily change to another environment, use\n\n```\nsource activate environment-name\n\n```\n\nETA: This may be deprecated. I believe the current correct command is:\n\n```\nsource conda activate environment-name\n\n```\n\n(you can create environment-name with conda create)\n\nTo change permanently, there is no method except creating a startup script that runs the above code.\n\nTypically it's best to just create new environments. However, if you really want to change the Python version in the default environment, you can do so as follows:\nFirst, make sure you have the latest version of conda by running\n\n```\nconda update conda\n\n```\n\nThen run\n\n```\nconda install python=3.5\n\n```\n\nThis will attempt to update all your packages in your root environment to Python 3 versions. If it is not possible (e.g., because some package is not built for Python 3.5), it will give you an error message indicating which package(s) caused the issue.\nIf you installed packages with pip, you'll have to reinstall them."}
{"instruction": "How to activate an Anaconda environment", "output": "If this happens you would need to set the PATH for your environment (so that it gets the right Python from the environment and Scripts\\ on Windows).\nImagine you have created an environment called py33 by using:\n\n```\nconda create -n py33 python=3.3 anaconda\n\n```\n\nHere the folders are created by default in Anaconda\\envs, so you need to set the PATH as:\n\n```\nset PATH=C:\\Anaconda\\envs\\py33\\Scripts;C:\\Anaconda\\envs\\py33;%PATH%\n\n```\n\nNow it should work in the command window:\n\n```\nactivate py33\n\n```\n\nThe line above is the Windows equivalent to the code that normally appears in the tutorials for Mac and Linux:\n\n```\n$ source activate py33\n\n```\n\nMore info:\nhttps://groups.google.com/a/continuum.io/forum/#!topic/anaconda/8T8i11gO39U\nDoes `anaconda` create a separate PYTHONPATH variable for each new environment?"}
{"instruction": "Should conda, or conda-forge be used for Python environments?", "output": "The short answer is that, in my experience generally, it doesn't matter which you use, with one exception. If you work for a company with more than 200 employees then the default conda channel is not free as of 2020.\nThe long answer:\nSo conda-forge is an additional channel from which packages may be installed. In this sense, it is not any more special than the default channel, or any of the other hundreds (thousands?) of channels that people have posted packages to. You can add your own channel if you sign up at https://anaconda.org and upload your own Conda packages.\nHere we need to make the distinction, which I think you're not clear about from your phrasing in the question, between conda, the cross-platform package manager, and conda-forge, the package channel. Anaconda Inc. (formerly Continuum IO), the main developers of the conda software, also maintain a separate channel of packages, which is the default when you type conda install packagename without changing any options.\nThere are three ways to change the options for channels. The first two are done every time you install a package and the last one is persistent. The first one is to specify a channel every time you install a package:\n\n```\nconda install -c some-channel packagename\n\n```\n\nOf course, the package has to exist on that channel. This way will install packagename and all its dependencies from some-channel. Alternately, you can specify:\n\n```\nconda install some-channel::packagename\n\n```\n\nThe package still has to exist on some-channel, but now, only packagename will be pulled from some-channel. Any other packages that are needed to satisfy dependencies will be searched for from your default list of channels.\nTo see your channel configuration, you can write:\n\n```\nconda config --show channels\n\n```\n\nYou can control the order that channels are searched with conda config. You can write:\n\n```\nconda config --add channels some-channel\n\n```\n\nto add the channel some-channel to the top of the channels configuration list. This gives some-channel the highest priority. Priority determines (in part) which channel is selected when more than one channel has a particular package. To add the channel to the end of the list and give it the lowest priority, type\n\n```\nconda config --append channels some-channel\n\n```\n\nIf you would like to remove the channel that you added, you can do so by writing\n\n```\nconda config --remove channels some-channel\n\n```\n\nSee\n\n```\nconda config -h\n\n```\n\nfor more options.\nWith all of that said, there are five main reasons to use the conda-forge channel instead of the defaults channel maintained by Anaconda:\n\nPackages on conda-forge may be more up-to-date than those on the defaults channel\nThere are packages on the conda-forge channel that aren't available from defaults\nYou would prefer to use a dependency such as openblas (from conda-forge) instead of mkl (from defaults).\nIf you are installing a package that requires a compiled library (e.g., a C extension or a wrapper around a C library), it may reduce the chance of incompatibilities if you install all of the packages in an environment from a single channel due to binary compatibility of the base C library (but this advice may be out of date/change in the future). For reference, see the Conda Forge post on mixing channels.\nconda-forge is free to use even in large companies, while the default conda channel is not. See here."}
{"instruction": "How to create conda environment with specific python version?", "output": "You need to install ipython as well into your given environment\n\n```\nconda create -n \"myenv\" python=3.3.0 ipython\n\n```\n\nThe conda environments are prepended to your PATH variable, so when you are trying to run the executable \"ipython\", Linux will not find \"ipython\" in your activated environment (since it doesn't exist there), but it will continue searching for it, and eventually find it wherever you have it installed."}
{"instruction": "Bulk package updates using Conda", "output": "You want conda update --all.\nconda search --outdated will show outdated packages, and conda update --all will update them (note that the latter will not update you from Python 2 to Python 3, but the former will show Python as being outdated if you do use Python 2)."}
{"instruction": "Conda command is not recognized on Windows 10", "output": "In Windows, you will have to set the path to the location where you installed Anaconda3 to.\nFor me, I installed anaconda3 into C:\\Anaconda3. Therefore you need to add C:\\Anaconda3 as well as C:\\Anaconda3\\Scripts\\ to your path variable, e.g. set PATH=%PATH%;C:\\Anaconda3;C:\\Anaconda3\\Scripts\\.\nYou can do this via powershell (see above, https://msdn.microsoft.com/en-us/library/windows/desktop/bb776899(v=vs.85).aspx ), or hit the windows key ‚Üí enter environment ‚Üí choose from settings ‚Üí edit environment variables for your account ‚Üí select Path variable ‚Üí Edit ‚Üí New.\nTo test it, open a new dos shell, and you should be able to use conda commands now. E.g., try conda --version."}
{"instruction": "Does it make sense to use Conda + Poetry?", "output": "2024-04-05 update:\nIt looks like my tips proved to be useful to many people, but they are not needed anymore. Just use Pixi. It's still alpha, but it works great, and provides the features of the Conda + Poetry setup in a simpler and more unified way. In particular, Pixi supports:\n\ninstalling packages both from Conda channels and from PyPi,\nlockfiles,\ncreating multiple features and environments (prod, dev, etc.),\nvery efficient package version resolution, not just faster than Conda (which is very slow), but in my experience also faster than Mamba, Poetry and pip.\n\nMaking a Pixi env look like a Conda env\nOne non-obvious tip about Pixi is that you can easily make your project's Pixi environment visible as a Conda environment, which may be useful e.g. in VS Code, which allows choosing Python interpreters and Jupyter kernels from detected Conda environments. All you need to do is something like:\n\n```\nln -s /path/to/my/project/.pixi/envs/default /path/to/conda/base/envs/conda-name-of-my-env\n\n```\n\nThe first path is the path to your Pixi environment, which resides in your project directory, under .pixi/envs, and the second path needs to be within one of Conda's environment directories, which can be found with conda config --show envs_dirs.\nOriginal answer:\nI have experience with a Conda + Poetry setup, and it's been working fine. The great majority of my dependencies are specified in pyproject.toml, but when there's something that's unavailable in PyPI, or installing it with Conda is easier, I add it to environment.yml. Moreover, Conda is used as a virtual environment manager, which works well with Poetry: there is no need to use poetry run or poetry shell, it is enough to activate the right Conda environment.\nTips for creating a reproducible environment\n\nAdd Poetry, possibly with a version number (if needed), as a dependency in environment.yml, so that you get Poetry installed when you run conda create, along with Python and other non-PyPI dependencies.\nAdd conda-lock, which gives you lock files for Conda dependencies, just like you have poetry.lock for Poetry dependencies.\nConsider using mamba which is generally compatible with conda, but is better at resolving conflicts, and is also much faster. An additional benefit is that all users of your setup will use the same  package resolver, independent from the locally-installed version of Conda.\nBy default, use Poetry for adding Python dependencies. Install packages via Conda if there's a reason to do so (e.g. in order to get a CUDA-enabled version). In such a case, it is best to specify the package's exact version in environment.yml, and after it's installed, to add an entry with the same version specification to Poetry's pyproject.toml (without ^ or ~ before the version number). This will let Poetry know that the package is there and should not be upgraded.\nIf you use a different channels that provide the same packages, it might be not obvious which channel a particular package will be downloaded from. One solution is to specify the channel for the package using the :: notation (see the pytorch entry below), and another solution is to enable strict channel priority. Unfortunately, in Conda 4.x there is no way to enable this option through environment.yml.\nNote that Python adds user site-packages to sys.path, which may cause lack of reproducibility if the user has installed Python packages outside Conda environments. One possible solution is to make sure that the PYTHONNOUSERSITE environment variable is set to True (or to any other non-empty value).\n\nExample\nenvironment.yml:\n\n```\nname: my_project_env\nchannels:\n  - pytorch\n  - conda-forge\n  # We want to have a reproducible setup, so we don't want default channels,\n  # which may be different for different users. All required channels should\n  # be listed explicitly here.\n  - nodefaults\ndependencies:\n  - python=3.10.*  # or don't specify the version and use the latest stable Python\n  - mamba\n  - pip  # pip must be mentioned explicitly, or conda-lock will fail\n  - poetry=1.*  # or 1.1.*, or no version at all -- as you want\n  - tensorflow=2.8.0\n  - pytorch::pytorch=1.11.0\n  - pytorch::torchaudio=0.11.0\n  - pytorch::torchvision=0.12.0\n\n# Non-standard section listing target platforms for conda-lock:\nplatforms:\n  - linux-64\n\n```\n\nvirtual-packages.yml (may be used e.g. when we want conda-lock to generate CUDA-enabled lock files even on platforms without CUDA):\n\n```\nsubdirs:\n  linux-64:\n    packages:\n      __cuda: 11.5\n\n```\n\nFirst-time setup\nYou can avoid playing with the bootstrap env and simplify the example below if you have conda-lock, mamba and poetry already installed outside your target environment.\n\n```\n# Create a bootstrap env\nconda create -p /tmp/bootstrap -c conda-forge mamba conda-lock poetry='1.*'\nconda activate /tmp/bootstrap\n\n# Create Conda lock file(s) from environment.yml\nconda-lock -k explicit --conda mamba\n# Set up Poetry\npoetry init --python=~3.10  # version spec should match the one from environment.yml\n# Fix package versions installed by Conda to prevent upgrades\npoetry add --lock tensorflow=2.8.0 torch=1.11.0 torchaudio=0.11.0 torchvision=0.12.0\n# Add conda-lock (and other packages, as needed) to pyproject.toml and poetry.lock\npoetry add --lock conda-lock\n\n# Remove the bootstrap env\nconda deactivate\nrm -rf /tmp/bootstrap\n\n# Add Conda spec and lock files\ngit add environment.yml virtual-packages.yml conda-linux-64.lock\n# Add Poetry spec and lock files\ngit add pyproject.toml poetry.lock\ngit commit\n\n```\n\nUsage\nThe above setup may seem complex, but it can be used in a fairly simple way.\nCreating the environment\n\n```\nconda create --name my_project_env --file conda-linux-64.lock\nconda activate my_project_env\npoetry install\n\n```\n\nActivating the environment\n\n```\nconda activate my_project_env\n\n```\n\nUpdating the environment\n\n```\n# Re-generate Conda lock file(s) based on environment.yml\nconda-lock -k explicit --conda mamba\n# Update Conda packages based on re-generated lock file\nmamba update --file conda-linux-64.lock\n# Update Poetry packages and re-generate poetry.lock\npoetry update\n\n```"}
{"instruction": "What are the differences between Conda and Anaconda?", "output": "conda is the package manager. Anaconda is a set of about a hundred packages including conda, numpy, scipy, ipython notebook, and so on. \nYou installed Miniconda, which is a smaller alternative to Anaconda that is just conda and its dependencies, not those listed above. \nOnce you have Miniconda, you can easily install Anaconda into it with conda install anaconda."}
{"instruction": "How to change Python version of existing conda virtual environment?", "output": "Activate the relevant environment, then install your target python version.\n\n```\nconda activate my_env\nconda install python=3.6\n\n```"}
{"instruction": "Conda: Installing / upgrading directly from github", "output": "There's better support for this now through conda-env.  You can, for example, now do:\n\n```\nname: sample_env\nchannels:\ndependencies:\n   - requests\n   - bokeh>=0.10.0\n   - pip:\n     - \"--editable=git+https://github.com/pythonforfacebook/facebook-sdk.git@8c0d34291aaafec00e02eaa71cc2a242790a0fcc#egg=facebook_sdk-master\"\n\n```\n\nIt's still calling pip under the covers, but you can now unify your conda and pip package specifications in a single environment.yml file.\nIf you wanted to update your root environment with this file, you would need to save this to a file (for example, environment.yml), then run the command: conda env update -f environment.yml.\nIt's more likely that you would want to create a new environment:\nconda env create -f environment.yml (changed as supposed in the comments)"}
{"instruction": "How do I revert to a previous package in Anaconda?", "output": "I had to use the install function instead:\n\n```\nconda install pandas=0.13.1\n\n```"}
{"instruction": "How to use Jupyter notebooks in a conda environment?", "output": "Disclaimer: ATM tested only in Ubuntu and Windows (see comments to this answer).\n\nJupyter runs the user's code in a separate process called kernel. The kernel can be a different Python installation (in a different conda environment or virtualenv or Python 2 instead of Python 3) or even an interpreter for a different language (e.g. Julia or R). Kernels are configured by specifying the interpreter and a name and some other parameters (see Jupyter documentation) and configuration can be stored system-wide, for the active environment (or virtualenv) or per user. If nb_conda_kernels is used, additional to statically configured kernels, a separate kernel for each conda environment with ipykernel installed will be available in Jupyter notebooks.\nIn short, there are three options how to use a conda environment and Jupyter:\nOption 1: Run Jupyter server and kernel inside the conda environment\nDo something like:\n\n```\nconda create -n my-conda-env         # creates new virtual env\nconda activate my-conda-env          # activate environment in terminal\nconda install jupyter                # install jupyter + notebook\njupyter notebook                     # start server + kernel inside my-conda-env\n\n```\n\nJupyter will be completely installed in the conda environment. Different versions of Jupyter can be used\nfor different conda environments, but this option might be a bit of overkill. It is enough to\ninclude the kernel in the environment, which is the component wrapping Python which runs the code.\nThe rest of Jupyter notebook can be considered as editor or viewer and it is not necessary to\ninstall this separately for every environment and include it in every env.yml file. Therefore one\nof the next two options might be preferable, but this one is the simplest one and definitely fine.\nOption 2: Create special kernel for the conda environment\nDo something like:\n\n```\nconda create -n my-conda-env                               # creates new virtual env\nconda activate my-conda-env                                # activate environment in terminal\nconda install ipykernel                                    # install Python kernel in new conda env\nipython kernel install --user --name=my-conda-env-kernel   # configure Jupyter to use Python kernel\n\n```\n\nThen run jupyter from the system installation or a different conda environment:\n\n```\nconda deactivate          # this step can be omitted by using a different terminal window than before\nconda install jupyter     # optional, might be installed already in system e.g. by 'apt install jupyter' on debian-based systems\njupyter notebook          # run jupyter from system\n\n```\n\nName of the kernel and the conda environment are independent from each other, but it might make sense to use a similar name.\nOnly the Python kernel will be run inside the conda environment, Jupyter from system or a different conda environment will be used - it is not installed in the conda environment. By calling ipython kernel install the jupyter is configured to use the conda environment as kernel, see Jupyter documentation and IPython documentation for more information. In most Linux installations this configuration is a *.json file in ~/.local/share/jupyter/kernels/my-conda-env-kernel/kernel.json:\n\n```\n{\n \"argv\": [\n  \"/opt/miniconda3/envs/my-conda-env/bin/python\",\n  \"-m\",\n  \"ipykernel_launcher\",\n  \"-f\",\n  \"{connection_file}\"\n ],\n \"display_name\": \"my-conda-env-kernel\",\n \"language\": \"python\"\n}\n\n```\n\nOption 3: Use nb_conda_kernels to use a kernel in the conda environment\nWhen the package nb_conda_kernels is installed, a separate kernel is available automatically for each\nconda environment containing the conda package ipykernel or a different kernel (R, Julia, ...).\n\n```\nconda activate my-conda-env    # this is the environment for your project and code\nconda install ipykernel\nconda deactivate\n\nconda activate base            # could be also some other environment\nconda install nb_conda_kernels\njupyter notebook\n\n```\n\nYou should be able to choose the Kernel Python [conda env:my-conda-env]. Note that nb_conda_kernels seems to be available only via conda and not via pip or other package managers like apt.\nTroubleshooting\nUsing Linux/Mac the command which on the command line will tell you which jupyter is used, if you\nare using option 1 (running Jupyter from inside the conda environment), it should be an executable\nfrom your conda environment:\n\n```\n$ which jupyter\n/opt/miniconda3/envs/my-conda-env/bin/jupyter\n$ which jupyter-notebook   # this might be different than 'which jupyter'! (see below)\n/opt/miniconda3/envs/my-conda-env/bin/jupyter-notebook\n\n```\n\nInside the notebook you should see that Python uses Python paths from the conda environment:\n\n```\n[1] !which python\n/opt/miniconda3/envs/my-conda-env/bin/python\n[2] import sys; sys.executable\n'/opt/miniconda3/envs/my-conda-env/bin/python'\n['/home/my_user',\n '/opt/miniconda3/envs/my-conda-env/lib/python37.zip',\n '/opt/miniconda3/envs/my-conda-env/lib/python3.7',\n '/opt/miniconda3/envs/my-conda-env/lib/python3.7/lib-dynload',\n '',\n '/opt/miniconda3/envs/my-conda-env/lib/python3.7/site-packages',\n '/opt/miniconda3/envs/my-conda-env/lib/python3.7/site-packages/IPython/extensions',\n '/home/my_user/.ipython']\n\n```\n\nJupyter provides the command jupyter-troubleshoot or in a Jupyter notebook:\n\n```\n!jupyter-troubleshoot\n\n```\n\nThis will print a lot of helpful information about including the outputs mentioned above as well as installed libraries and others. When\nasking for help regarding Jupyter installations questions, it might be good idea to provide this information in bug reports or questions.\nTo list all configured Jupyter kernels run:\n\n```\njupyter kernelspec list\n\n```\n\nCommon errors and traps\nJupyter notebook not installed in conda environment\nNote: symptoms are not unique to the issue described here.\nSymptoms: ImportError in Jupyter notebooks for modules installed in the conda environment (but\nnot installed system wide), but no error when importing in a Python terminal\nExplaination: You tried to run jupyter notebook from inside your conda environment\n(option 1, see above), there is no configuration for a kernel for this conda environment (this\nwould be option 2) and nb_conda_kernels is not installed (option 3), but jupyter notebook is not (fully)\ninstalled in the conda environment, even if which jupyter might make you believe it was.\nIn GNU/Linux you can type which jupyter to check which executable of Jupyter is run.\nThis means that system's Jupyter is used, probably because Jupyter is not installed:\n\n```\n(my-conda-env) $ which jupyter-notebook\n/usr/bin/jupyter\n\n```\n\nIf the path points to a file in your conda environment, Jupyter is run from inside Jupyter:\n\n```\n(my-conda-env) $ which jupyter-notebook\n/opt/miniconda3/envs/my-conda-env/bin/jupyter-notebook\n\n```\n\nNote that when the conda package ipykernel is installed, an executable jupyter is shipped, but\nno executable jupyter-notebook. This means that which jupyter will return a path to the conda\nenvironment but jupyter notebook will start system's jupyter-nootebook (see also here):\n\n```\n $ conda create -n my-conda-env\n $ conda activate my-conda-env\n $ conda install ipykernel\n $ which jupyter            # this looks good, but is misleading!\n /opt/miniconda3/envs/my-conda-env/bin/jupyter\n $ which jupyter-notebook   # jupyter simply runs jupyter-notebook from system...\n /usr/bin/jupyter-notebook\n\n```\n\nThis happens because jupyter notebook searches for jupyter-notebook, finds\n/usr/bin/jupyter-notebook and\ncalls it\nstarting a new Python process. The shebang in /usr/bin/jupyter-notebook is #!/usr/bin/python3\nand not a dynamic\n#!/usr/bin/env python.\nTherefore Python manages to break out of the conda environment. I guess jupyter could call\npython /usr/bin/jupyter-notebook instead to overrule the shebang, but mixing\nsystem's bin files and the environment's python path can't work well anyway.\nSolution: Install jupyter notebook inside the conda environment:\n\n```\n conda activate my-conda-env\n conda install jupyter\n jupyter notebook\n\n```\n\nWrong kernel configuration: Kernel is configured to use system Python\nNote: symptoms are not unique to the issue described here.\nSymptoms: ImportError in Jupyter notebooks for modules installed in the conda environment (but\nnot installed system wide), but no error when importing in a Python terminal\nExplanation: Typically the system provides a kernel called python3 (display name \"Python 3\")\nconfigured to use /usr/bin/python3, see e.g. /usr/share/jupyter/kernels/python3/kernel.json.\nThis is usually overridden by a kernel in the conda environment, which points to the environments\npython binary /opt/miniconda3/envs/my-conda-env/bin/python. Both are generated by the package\nipykernel (see here\nand here).\nA user kernel specification in ~/.local/share/jupyter/kernels/python3/kernel.json might override\nthe system-wide and environment kernel. If the environment kernel is missing or the user kernel\npoints to a python installation outside the environment option 1 (installation of jupyter in the\nenvironment) will fail.\nFor occurrences and discussions of this problem and variants see here,\nhere,\nhere\nand also here,\nhere and\nhere.\nSolution: Use jupyter kernelspec list to list the location active kernel locations.\n\n```\n$ conda activate my-conda-env\n$ jupyter kernelspec list\nAvailable kernels:\n  python3 /opt/miniconda3/envs/my-conda-env/share/jupyter/kernels/python3\n\n```\n\nIf the kernel in the environment is missing, you can try creating it manually using\nipython kernel install --sys-prefix in the activated environment, but it is probably better to\ncheck your installation, because conda install ipykernel should have created the environment\n(maybe try re-crate the environment and re-install all packages?).\nIf a user kernel specification is blocking the environment kernel specification, you can either\nremove it or use a relative python path which will use $PATH to figure out which python to use.\nSo something like this, should be totally fine:\n\n```\n$ cat ~/.local/share/jupyter/kernels/python3/kernel.json\n{\n \"argv\": [\n  \"python\",\n  \"-m\",\n  \"ipykernel_launcher\",\n  \"-f\",\n  \"{connection_file}\"\n ],\n \"display_name\": \"Python 3\",\n \"language\": \"python\"\n}\n\n```\n\nCorrect conda environment not activated\nSymptoms: ImportError for modules installed in the conda environment (but not installed system\nwide) in Jupyter notebooks and Python terminals\nExplanation: Each terminal has a set of environment variables, which are lost when the terminal\nis closed. In order to use a conda environment certain environment variables need to be set, which\nis done by activating it using conda activate my-conda-env. If you attempted to run Jupyter\nnotebook from inside the conda environment (option 1), but did not activate the conda environment\nbefore running it, it might run the system's jupyter.\nSolution: Activate conda environment before running Jupyter.\n\n```\n conda activate my-conda-env\n jupyter notebook\n\n```\n\nBroken kernel configuration\nSymptoms: Strange things happening. Maybe similar symptoms as above, e.g. ImportError\nExplanation: If you attempted to use option 2, i.e. running Jupyter from system and the Jupyter\nkernel inside the conda environment by using an explicit configuration for the kernel, but it does\nnot behave as you expect, the configuration might be corrupted in some way.\nSolution: Check configuration in ~/.local/share/jupyter/kernels/my-kernel-name/kernel.json\nand fix mistakes manually or remove the entire directory and re-create it using the command\nprovided above for option 2. If you can't find the kernel configuration there run\njupyter kernelspec list.\nPython 2 vs 3\nSymptoms: ImportError due to wrong Python version of the Jupyter kernel or other problems\nwith Python 2/3\nExplanation: The kernel configuration can have all sorts of confusing and misleading effects.\nFor example the default Python 3 kernel configuration will allow me to launch a Jupyter notebook\nrunning on Python 2:\n\n```\nconda create -n my-conda-env\nconda activate my-conda-env\nconda install python=2\nconda install jupyter\njupyter notebook\n\n```\n\nThe default Python 3 kernel:\n\n```\n$ cat ~/.local/share/jupyter/kernels/python3/kernel.json\n{\n \"argv\": [\n  \"python\",\n  \"-m\",\n  \"ipykernel_launcher\",\n  \"-f\",\n  \"{connection_file}\"\n ],\n \"display_name\": \"Python 3\",\n \"language\": \"python\"\n}\n\n```\n\nAfter creating a new Jupyter Notebook with the Python 3 kernel, Python 2 from the conda\nenvironment will be used even if \"Python 3\" is displayed by Jupyter.\nSolution: Don't use Python 2 ;-)"}
{"instruction": "How to list package versions available with conda", "output": "You can just type \"conda search\" which will give you something like the following.\n\n```\n$ conda search \nFetching package metadata .........\naffine                       2.0.0                    py27_0  defaults\n                             2.0.0                    py35_0  defaults\n                             2.0.0                    py36_0  defaults\nalabaster                    0.7.3                    py27_0  defaults\n                             0.7.3                    py34_0  defaults\n                             0.7.7                    py27_0  defaults\n                             0.7.7                    py34_0  defaults\n                             0.7.7                    py35_0  defaults\n                             0.7.9                    py27_0  defaults\n\n```"}
{"instruction": "How do I keep track of pip-installed packages in an Anaconda (Conda) environment?", "output": "conda-env now does this automatically (if pip was installed with conda).\nYou can see how this works by using the export tool used for migrating an environment:\n\n```\nconda env export -n <env-name> > environment.yml\n\n```\n\nThe file will list both conda packages and pip packages:\n\n```\nname: stats\nchannels:\n  - javascript\ndependencies:\n  - python=3.4\n  - bokeh=0.9.2\n  - numpy=1.9.*\n  - nodejs=0.10.*\n  - flask\n  - pip:\n    - Flask-Testing\n\n```\n\nIf you're looking to follow through with exporting the environment, move environment.yml to the new host machine and run:\n\n```\nconda env create -f path/to/environment.yml\n\n```"}
{"instruction": "How to remove (base) from terminal prompt after updating conda", "output": "Use the base env's activation hook\nFor each env, any scripts in the etc/conda/activate.d directory will be executed post-activation (likewise etc/conda/deactivate.d scripts for deactivation).  If you add a script to remove the (base), similar to @ewindes suggestion, you'll get the behavior you desire.\nI had to create this directory for base, which is just the root of your Anaconda/Miniconda folder. E.g.,\n\n```\nmkdir -p miniconda3/etc/conda/activate.d\n\n```\n\nThen made a simple file in there (e.g., remove_base_ps1.sh) with one line:\n\n```\nPS1=\"$(echo \"$PS1\" | sed 's/(base) //')\"\n\n```\n\nIf you are using zsh, use this instead.\n\n```\nPROMPT=$(echo $PROMPT | sed 's/(base) //')\n\n```\n\nLaunching a new shell then does not show (base), and deactivating out of nested envs also takes care of the PS1 change.\nNote: You must add quotes around $PS1 if you want to preserve ending spaces."}
{"instruction": "Zsh: Conda/Pip installs command not found", "output": "It appears that my PATH is broken in my .zshrc file.\nOpen it and add :\n\n```\nexport PATH=\"$PATH;/Users/Dz/anaconda/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/opt/X11/bin:/Users/Dz/.rvm/bin\"\n\n```\n\nDoh! Well that would explain everything. How did I miss that little semicolon? Changed:\n\n```\nexport PATH=\"$PATH:/Users/Dz/anaconda/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/opt/X11/bin:/Users/Dz/.rvm/bin\"\n\nsource ~/.zshrc\necho $HOME\necho $PATH\n\n```\n\nWe're good now."}
{"instruction": "How to free disk space taken up by (ana)conda?", "output": "You can free some space with:\n\n```\nconda clean --all\n\n```\n\n\nclean        Remove unused packages and caches.\n\nConda already use symlinks when possible for packages. So, not much to improve here, I guess.\n\nOk, thanks, but I would like to know \"not for a specific environment, but in general\" - for all environments. \n\nYou can list all packages in all envs with a few lines of Python:\n\n```\nimport os\nimport subprocess\nfor env in os.listdir('/Users/me/miniconda3/envs'):\n    subprocess.call(['conda', 'list', '-n', env])\n\n```"}
{"instruction": "How to uninstall miniconda?", "output": "In order to uninstall miniconda, simply remove the miniconda folder,\n\n```\nrm -r ~/miniconda/\n\n```\n\nAs for avoiding conflicts between different Python environments, you can use virtual environments. In particular, with Miniconda, the following workflow could be used,\n\n```\n$ wget https://repo.continuum.io/miniconda/Miniconda3-3.7.0-Linux-x86_64.sh -O ~/miniconda.sh\n$ bash miniconda\n$ conda env remove --yes -n new_env    # remove the environement new_env if it exists (optional)\n$ conda create --yes -n new_env pip numpy pandas scipy matplotlib scikit-learn nltk ipython-notebook seaborn python=2\n$ activate new_env\n$ # pip install modules if needed, run python scripts, etc\n  # everything will be installed in the new_env\n  # located in ~/miniconda/envs/new_env\n$ deactivate\n\n```"}
{"instruction": "Where is site-packages located in a Conda environment?", "output": "You can import the module and check the module.__file__ string. It contains the path to the associated source file. \nAlternatively, you can read the File tag in the the module documentation, which can be accessed using help(module), or module? in IPython."}
{"instruction": "Conda - Silently/non-interactively installing a package", "output": "Used $conda install -y pandas and it installed without any prompts (see documentation)."}
{"instruction": "How can you &quot;clone&quot; a conda environment into the base (root) environment?", "output": "There are options to copy dependency names/urls/versions to files.\nRecommendation\nNormally it is safer to work from a new environment rather than changing root.  However, consider backing up your existing environments before attempting changes. Verify the desired outcome by testing these commands in a demo environment.  To backup your root env for example:\n\n```\nŒª conda activate root\nŒª conda env export > environment_root.yml\nŒª conda list --explicit > spec_file_root.txt\n\n```\n\nOptions\nOption 1 - YAML file\nWithin the second environment (e.g. myenv), export names+ to a yaml file:\n\n```\nŒª activate myenv\nŒª conda env export > environment.yml  \n\n```\n\nthen update the first environment+ (e.g. root) with the yaml file:\n\n```\nŒª conda env update --name root --file environment.yml\n\n```\n\nOption 2 - Cloning an environment\nUse the --clone flag to clone environments (see @DevC's answer):\n\n```\nŒª conda create --name myclone --clone root\n\n```\n\nThis basically creates a direct copy of an environment.\nOption 3 - Spec file\nMake a spec-file++ to append dependencies from an env (see @Ormetrom):\n\n```\nŒª activate myenv\nŒª conda list --explicit > spec_file.txt\nŒª conda install --name root --file spec_file.txt\n\n```\n\nAlternatively, replicate a new environment (recommended):\n\n```\nŒª conda create --name myenv2 --file spec_file.txt\n\n```\n\nSee Also\n\nconda env for more details on the env sub-commands.\nAnaconada Navigator desktop program for a more graphical experience.\nDocs on updated commands.  With older conda versions use activate (Windows) and source activate (Linux/Mac OS).  Newer versions of conda can use conda activate (this may require some setup with your shell configuration via conda init).\nDiscussion on keeping conda env\n\nExtras\nThere appears to be an undocumented conda run option to help execute commands in specific environments.\n\n```\n# New command\nŒª conda run --name myenv conda list --explicit > spec_file.txt\n\n```\n\nThe latter command is effective at running commands in environments without the activation/deactivation steps.  See the equivalent command below:\n\n```\n# Equivalent\nŒª activate myenv\nŒª conda list --explicit > spec_file.txt\nŒª deactivate\n\n```\n\nNote, this is likely an experimental feature, so this may not be appropriate in production until official adoption into the public API.\n+ Conda docs have changed since the original post; links updated.\n++ Spec-files only work with environments created on the same OS.  Unlike the first two options, spec-files only capture links to conda dependencies; pip dependencies are not included."}
{"instruction": "anaconda - graphviz - can&#39;t import after installation", "output": "On conda:\nFirst install\n\n```\nconda install graphviz\n\n```\n\nThen the python-library for graphviz python-graphviz\n\ngv_python is a dynamically loaded extension for python that provides\n  access to the graph facilities of graphviz.\n\n\n```\nconda install python-graphviz\n\n```\n\nThere is also pydot package, which can parse and dump into DOT language, used by GraphViz\n\n```\nconda install pydot\n\n```"}
{"instruction": "CondaValueError: The target prefix is the base prefix. Aborting", "output": "You need to use\n\n```\nconda env create -f environment.yml\n\n```\n\nNotice the extra env after conda and before create.\nFor more information check the documentation."}
{"instruction": "Working with Anaconda in Visual Studio Code", "output": "Activating a conda environment does not place conda on your PATH. You need to launch the Anaconda Prompt app from your Start menu to get a command-line with conda on your PATH if you didn't check the box to include conda during installation.\nAlso realize that conda only supports PowerShell as of conda 4.6 which was released in January 2019.\nAnd the Python extension for VS Code works with conda fine. Create a conda environment and the extension will allow you to select it as your environment/interpreter."}
{"instruction": "How do I find the name of the conda environment in which my code is running?", "output": "You want $CONDA_DEFAULT_ENV or $CONDA_PREFIX:\n\n```\n$ source activate my_env\n(my_env) $ echo $CONDA_DEFAULT_ENV\nmy_env\n\n(my_env) $ echo $CONDA_PREFIX\n/Users/nhdaly/miniconda3/envs/my_env\n\n$ source deactivate\n$ echo $CONDA_DEFAULT_ENV  # (not-defined)\n\n$ echo $CONDA_PREFIX  # (not-defined)\n\n```\n\nIn python:\n\n```\nimport os\nprint(os.environ['CONDA_DEFAULT_ENV'])\n\n```\n\nfor the absolute entire path which is usually more useful:\n\n```\nPython 3.9.0 | packaged by conda-forge | (default, Oct 14 2020, 22:56:29) \n[Clang 10.0.1 ] on darwin\nimport os; print(os.environ[\"CONDA_PREFIX\"])\n/Users/miranda9/.conda/envs/synthesis\n\n```\n\n\nThe environment variables are not well documented. You can find CONDA_DEFAULT_ENV mentioned here:\nhttps://www.continuum.io/blog/developer/advanced-features-conda-part-1\nThe only info on CONDA_PREFIX I could find is this Issue:\nhttps://github.com/conda/conda/issues/2764"}
{"instruction": "Difference between pip freeze and conda list", "output": "If the goal only is to list all the installed packages, pip list or conda list are the way to go. \npip freeze, like conda list --export, is more for generating requirements files for your environment. For example, if you have created a package in your customized environment with certain dependencies, you can do conda list --export > requirements.txt. When you are ready to distribute your package to other users, they can easily duplicate your environment and the associated dependencies with conda create --name <envname> --file requirements.txt.\n\nThe differences between conda and pip need a longer discussion. There are plenty of explanations on StackOverflow. This article by Jake VanderPlas is a great read as well.\nYou might also find this table useful. It lists operation equivalences between conda, pip and virtualenv."}
{"instruction": "Conda uninstall one package and one package only", "output": "You can use conda remove --force.\nThe documentation says:\n\n```\n--force               Forces removal of a package without removing packages\n                      that depend on it. Using this option will usually\n                      leave your environment in a broken and inconsistent\n                      state\n\n```"}
{"instruction": "How to install python with conda?", "output": "To create python 3.11 conda environment use the following command\n\n```\nconda create -n py311 python=3.11\n\n```\n\n\n```\npy311 - environment name\n\n```\n\nUpdate 3\nTo create python 3.10 conda environment use the following command\n\n```\nconda create -n py310 python=3.10\n\n```\n\n\n```\npy310 - environment name\n\n```\n\nUpdate 2\nYou can now directly create python 3.9 environment using the following command\n\n```\nconda create -n py39 python=3.9\n\n```\n\n\n```\npy39 - environment name\n\n```\n\nUpdate 1\nPython 3.9 is now available in conda-forge.\nTo download the tar file - https://anaconda.org/conda-forge/python/3.9.0/download/linux-64/python-3.9.0-h852b56e_0_cpython.tar.bz2\nAnaconda Page - https://anaconda.org/conda-forge/python\n\nAs pointed out in the comments, python 3.9 is not yet there on any channels. So, it cannot be install yet via conda.\nInstead, you can download the python 3.9 executable and install it.\nOnce the installation is done, a new executable will be created for python 3.9 and pip 3.9 will be created.\nPython:\n\n```\npython3.7          \npython3.7-config   \npython3.7m         \npython3.7m-config  \npython3.9          \npython3.9-config\n\n```\n\npip\n\n```\npip      \npip3     \npip3.7   \npip3.8   \npip3.9   \npipreqs\n\n```\n\nIn order to install ipython for python 3.9,\n\n```\npip3.9 install ipython\n\n```"}
{"instruction": "Conda install and update do not work also solving environment get errors", "output": "I ran into the same problem and I couldn't find a solution, but I did find a workaround. If you create an env and activate that env and then do the install, it seems to work just fine. If you don't need a lot of libraries I would try that.\nCommands are:\n\nCreate env\n\n\n```\nconda create --name myenv\n\n```\n\n\nActivate the env\n\n\n```\nconda activate myenv\n\n```"}
{"instruction": "Conda update fails with SSL error CERTIFICATE_VERIFY_FAILED", "output": "Please note that the following solution is not secure. See: https://conda.io/projects/conda/en/latest/user-guide/configuration/disable-ssl-verification.html\naccording to @jreback here https://github.com/conda/conda/issues/1166\n\n```\nconda config --set ssl_verify false \n\n```\n\nwill turn off this feature, e.g. here"}
{"instruction": "Use Conda environment in pycharm", "output": "The best PyCharm specific answer is this one by wasabi (below).\nIn general though, if you want to use an interpreter from within a Conda environment then you can change the location of the interpreter to point to the particular environment that you want to use e.g. /home/username/miniconda/envs/bunnies as mentioned in this comment.\nHowever, as mentioned in this answer by Mark Turner, it is possible to have a shell script executed when activating an environment. This method will not run that shell script, but you can follow his workaround if you need that shell script run:\n\n\nopen a conda prompt \nactivate the environment\nrun pycharm from the conda prompt"}
{"instruction": "How to share conda environments across platforms", "output": "This answer is given with the assumption that you would like to make sure that\nthe same versions of the packages that you generally care about are on\ndifferent platforms and that you don't care about the exact same versions of\nall packages in the entire dependency tree. If you are trying to install the\nexact same version of all packages in your entire dependency tree that has a\nhigh likelihood of failure since some conda packages have different\ndependencies for osx/win/linux. For example, the recipe for\notrobopt\nwill install different packages on Win vs. osx/linux, so the environment list\nwould be different.\nRecommendation: manually create an environment.yaml file and specify or pin\nonly the dependencies that you care about. Let the conda solver do the rest.\nProbably worth noting is that conda-env (the tool that you use to manage conda\nenvironments) explicitly recommends that you \"Always create your\nenvironment.yml file by hand.\"\nThen you would just do conda env create --file environment.yml\nHave a look at the readme for\nconda-env.\nThey can be quite simple:\n\n```\nname: basic_analysis\ndependencies:\n  - numpy\n  - pandas\n\n```\n\nOr more complex where you pin dependencies and specify anaconda.org channels to\ninstall from:\n\n```\nname: stats-web\nchannels:\n  - javascript\ndependencies:\n  - python=3.4   # or 2.7 if you are feeling nostalgic\n  - bokeh=0.9.2\n  - numpy=1.9\n  - nodejs=0.10\n  - flask\n  - pip:\n    - Flask-Testing\n\n```\n\nIn either case, you can create an environment with conda env create --file environment.yaml.\nNOTE: You may need to use .* as a version suffix if you're using an older version of conda."}
{"instruction": "Conda: Creating a virtual environment", "output": "I was able to solve my problem. Executing the source activate test_env command wasn't picking up my .bash_profile, and I normally work in tcsh. Simply starting a subprocess in Bash was enough to get activate working. I guess I assumed, incorrectly, that the activate command would start a child process in Bash and use Bash environment variables.\n\n```\n> conda info -e\n> # conda environments:\n> #\n> base                  *  ~/anaconda3\n> test_env                 ~/anaconda3/envs/test_env\n> bash\n~$ source ~/anaconda3/bin/activate test_env\n(test_env) ~$\n(test_env) ~$ conda info -e\n# conda environments:\n#\ntest_env              *  ~/anaconda3/envs/test_env\nroot                     ~/anaconda3\n\n```"}
{"instruction": "How to set specific environment variables when activating conda environment?", "output": "Use the files $CONDA_PREFIX/etc/conda/activate.d and $CONDA_PREFIX/etc/conda/deactivate.d, where $CONDA_PREFIX is the path to the environment. \nSee the section on managing environments in the official documentation for reference."}
{"instruction": "Python - Activate conda env through shell script", "output": "The error message is rather helpful - it's telling you that conda is not properly set up from within the subshell that your script is running in. To be able to use conda within a script, you will need to (as the error message says) run conda init bash (or whatever your shell is) first. The behaviour of conda and how it's set up depends on your conda version, but the reason for the version 4.4+ behaviour is that conda is dependent on certain environment variables that are normally set up by the conda shell itself. Most importantly, this changelog entry explains why your conda activate and deactivate commands no longer behave as you expect in versions 4.4 and above.\nFor more discussion of this, see the official conda issue on GitHub. \n\nEdit: Some more research tells me that the conda init function mentioned in the error message is actually a new v4.6.0 feature that allows a quick environment setup so that you can use conda activate instead of the old source activate. However, the reason why this works is that it adds/changes several environment variables of your current shell and also makes changes to your RC file (e.g.: .bashrc), and RC file changes are never picked up in the current shell - only in newly created shells. (Unless of course you source .bashrc again). In fact, conda init --help says as much:\n\nIMPORTANT: After running conda init, most shells will need to be closed and restarted for changes to take effect\n\nHowever, you've clearly already run conda init, because you are able to use conda activate interactively. In fact, if you open up your .bashrc, you should be able to see a few lines added by conda teaching your shell where to look for conda commands. The problem with your script, though, lies in the fact that the .bashrc is not sourced by the subshell that runs shell scripts (see this answer for more info). This means that even though your non-login interactive shell sees the conda commands, your non-interactive script subshells won't - no matter how many times you call conda init. \nThis leads to a conjecture (I don't have conda on Linux myself, so I can't test it) that by running your script like so:\n\n```\nbash -i shell_script.sh\n\n```\n\nyou should see conda activate work correctly. Why? -i is a bash flag that tells the shell you're starting to run in interactive mode, which means it will automatically source your .bashrc. This should be enough to enable you to use conda within your script as if you were using it normally."}
{"instruction": "No module named &quot;Torch&quot;", "output": "Try to install PyTorch using pip:\nFirst create a Conda environment using:\n\n```\nconda create -n env_pytorch python=3.6\n\n```\n\nActivate the environment using:\n\n```\nconda activate env_pytorch\n\n```\n\nNow install PyTorch using pip:\n\n```\npip install torchvision \n\n```\n\nNote: This will install both torch and torchvision.\nNow go to Python shell and import using the command:\n\n```\nimport torch\nimport torchvision\n\n```"}
{"instruction": "PackageNotInstalledError: Package is not installed in prefix", "output": "Usually this error, \"PackageNotInstalledError: Package is not installed in prefix.\" is because your custom environment doesn't have the conda infrastructure. Instead, it is in your base only. To update the base environment:\n\n```\nconda update --name base conda\n\n```\n\nTo see what version you have installed:\n\n```\nconda list --name base conda\n\n```\n\nexample output of list;\n\n```\n# packages in environment at /Users/me/miniconda2:\n#\n# Name                    Version                   Build  Channel\nconda                     4.6.14                   py27_0  \nconda-env                 2.6.0                h36134e3_0  \n\n```"}
{"instruction": "No module named pip in venv but pip installed", "output": "I ran into the exact same problem after installing Python 3.13 on WSL. Suddenly, all my existing virtual environments (created with Python 3.12) broke in VSCode. I was getting the \"Invalid Python interpreter\" error, Pylance couldn't resolve any imports, and pip appeared to be missing‚Äîeven though I could see it in the venv/bin folder.\nHere‚Äôs what fixed it for me:\nFirst, check what your system python3 now points to:\n\n```\npython3 --version\nwhich python3\n\n```\n\nIn my case, it was now Python 3.13, which explains why stuff started breaking. Your virtual environment still points to the Python 3.12 binary internally, but VSCode (and maybe even pip) is trying to use 3.13 instead.\nYou can confirm that by looking at the pyvenv.cfg file inside your venv:\n\n```\ncat venv/pyvenv.cfg\n\n```\n\nYou should see something like:\n\n```\nhome = /usr/bin/python3.12\n\n```\n\nIf that's the case, then you just need to tell VSCode to use that exact interpreter. Open the command palette (Ctrl+Shift+P) in VSCode, choose ‚ÄúPython: Select Interpreter‚Äù, and manually select the path to your virtualenv‚Äôs Python binary:\n\n```\n/path/to/your/venv/bin/python\n\n```\n\nAlso, double-check the shebang in your pip script:\n\n```\nhead -n 1 venv/bin/pip\n\n```\n\nIf it says #!/usr/bin/python3, that might now point to Python 3.13, which breaks the venv. You can fix this by rebuilding the venv with the correct Python version:\n\n```\npython3.12 -m venv --upgrade-deps venv\n\n```\n\nOr, if that doesn‚Äôt work cleanly:\n\n```\nrm -rf venv\npython3.12 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n\n```\n\nAnd yeah, ensurepip being disabled for system Python is normal on Ubuntu. Just make sure you have the necessary packages installed:\n\n```\nsudo apt install python3.12-venv python3.12-distutils\n\n```\n\nOnce I manually selected the right interpreter in VSCode and fixed the pip shebang, everything worked again‚ÄîIntelliSense, linting, imports, etc. Hope that helps."}
{"instruction": "python venv install skips component file &quot;pointer.png&quot;", "output": "It's because it's not present in tool.setuptools.package-data in pyproject.toml file.\n\n```\n[tool.setuptools.package-data]\n\"*\" = [\"*.fs\", \"*.vs\", \"*.inc\", \"*.gif\"]\n\n```\n\nWith the previous configuration, you add all this extensions in your package as you can see in the next screenshot (content of the package uploaded on pypi).\n\nSo adding the png extension should work:\n\n```\n[tool.setuptools.package-data]\n\"*\" = [\"*.fs\", \"*.vs\", \"*.inc\", \"*.gif\", \"*.png\"]\n\n```"}
{"instruction": "abjad.show() issues &quot;FileNotFoundError: [WinError 2] The system cannot find the file specified&quot; in Python", "output": "Looking at the source of \"...\\abjad\\configuration.py\", line 388 (as pointed to by the error message), we have the following lines:\n\n```\ncommand = [\"lilypond\", \"--version\"]\nproc = subprocess.run(command, stdout=subprocess.PIPE)\n\n```\n\nSo the process is trying to run the command \"lilypond --version\", and failing because Windows cannot find an executable file or command script with that name. You should check where that command is installed and ensure that its full path is added to your PATH environment variable."}
{"instruction": "Check if a python module exists with specific venv path", "output": "```\nimport sys\nimport importlib.util\nimport os\nfrom pathlib import Path\n\ndef is_module_installed_in_venv(module_name, venv_path):\n    venv_python_lib_path = Path(venv_path) / 'lib'\n    for python_dir in venv_python_lib_path.iterdir():\n        if python_dir.name.startswith('python'):\n            site_packages_path = python_dir / 'site-packages'\n            break\n    \n    if not site_packages_path.exists():\n        return False\n    \n    sys.path.insert(0, str(site_packages_path))\n\n    module_spec = importlib.util.find_spec(module_name)\n    \n    sys.path.pop(0)\n    \n    return module_spec is not None\n\nvenv_path = '/home/user/anaconda3/envs/env_name/' \nmodule_name = 'numpy'\n\nif is_module_installed_in_venv(module_name, venv_path):\n    print(\"do something\")\n\n```\n\nthis works , make sure to include the full path"}
{"instruction": "Can 2 VS Code windows for 2 git clones each work with their own venv?", "output": "OK.  It was pretty simple.  I just had to:\n\nOn the command line: activate my venv: source .venv/bin/activate\ncode .\ncommand-shift-p\nClick \"Python: select interpreter\"\nClick \"Enter Interpreter path\"\nPaste and hit Enter\n\nI was mistakenly guessing that just activating the environment (either in the terminal from which I launch vscode or in vscode's terminal) was enough.  I probably did this for the primary clone long ago and just forgot."}
{"instruction": "Cannot import: `from serpapi import GoogleSearch`", "output": "The solution was to remove all system Python installations from my Mac (including the official Python binary and Brew python). Then create a run shell script configuration in PyCharm like this:\nsource venv/bin/activate && streamlit run app/web/app.py"}
