[
  {
    "question": "How do I undo the most recent local commits in Git? I accidentally committed the wrong files to Git but haven't pushed the commit to the server yet.\nHow do I undo those commits from the local repository?",
    "answer": "git commit -m \"Something terribly misguided\" # (0: Your Accident)\ngit reset HEAD~                              # (1)\n\n[ edit files as necessary ]                    # (2)\ngit add .                                    # (3)\ngit commit -c ORIG_HEAD                      # (4)\n\nUndo a commit & redo\n\ngit reset is the command responsible for the undo. It will undo your last commit while leaving your working tree (the state of your files on disk) untouched. You'll need to add them again before you can commit them again.\nMake corrections to working tree files.\ngit add anything that you want to include in your new commit.\nCommit the changes, reusing the old commit message. reset copied the old head to .git/ORIG_HEAD; commit with -c ORIG_HEAD will open an editor, which initially contains the log message from the old commit and allows you to edit it. If you do not need to edit the message, you could use the -C option.\n\nAlternatively, to edit the previous commit (or just its commit message), commit --amend will add changes within the current index to the previous commit.\nTo remove (not revert) a commit that has been pushed to the server, rewriting history with git push origin main --force[-with-lease] is necessary. It's almost always a bad idea to use --force; prefer --force-with-lease instead, and as noted in the git manual:\n\nYou should understand the implications of rewriting history if you amend a commit that has already been published.\n\nFurther Reading\nYou can use git reflog to determine the SHA-1 for the commit to which you wish to revert. Once you have this value, use the sequence of commands as explained above.\n\nHEAD~ is the same as HEAD~1. The article What is the HEAD in git? is helpful if you want to uncommit multiple commits."
  },
  {
    "question": "How do I delete a Git branch locally and remotely? Failed Attempts to Delete a Remote Branch:\n\n```\n$ git branch -d remotes/origin/bugfix\nerror: branch 'remotes/origin/bugfix' not found.\n\n$ git branch -d origin/bugfix\nerror: branch 'origin/bugfix' not found.\n\n$ git branch -rd origin/bugfix\nDeleted remote branch origin/bugfix (was 2a14ef7).\n\n$ git push\nEverything up-to-date\n\n$ git pull\nFrom github.com:gituser/gitproject\n\n* [new branch] bugfix -> origin/bugfix\nAlready up-to-date.\n\n```\n\nHow do I properly delete the remotes/origin/bugfix branch both locally and remotely?",
    "answer": "git push -d <remote_name> <branchname>   # Delete remote\ngit branch -d <branchname>               # Delete local\ngit branch -d <branch_name>\ngit branch -D <branch_name>\ngit push <remote_name> --delete <branch_name>\ngit push <remote_name> :<branch_name>\ngit push origin :serverfix\nTo git@github.com:schacon/simplegit.git\n- [deleted]         serverfix\ngit fetch --all --prune\n\nExecutive Summary\n\nNote: In most cases, <remote_name> will be origin.\nDelete Local Branch\nTo delete the local branch, use one of the following:\n\nThe -d option is an alias for --delete, which only deletes the branch if it has already been fully merged in its upstream branch.\nThe -D option is an alias for --delete --force, which deletes the branch \"irrespective of its merged status.\" [Source: man git-branch]\nAs of Git v2.3, git branch -d (delete) learned to honor the -f (force) flag.\nYou will receive an error if you try to delete the currently selected branch.\n\nDelete Remote Branch\nAs of Git v1.7.0, you can delete a remote branch using\n\nwhich might be easier to remember than\n\nwhich was added in Git v1.5.0 \"to delete a remote branch or a tag.\"\nStarting with Git v2.8.0, you can also use git push with the -d option as an alias for --delete. Therefore, the version of Git you have installed will dictate whether you need to use the easier or harder syntax.\nDelete Remote Branch [Original Answer from 5-Jan-2010]\nFrom Chapter 3 of Pro Git by Scott Chacon:\n\nDeleting Remote Branches\nSuppose you\u2019re done with a remote branch \u2014 say, you and your collaborators are finished with a feature and have merged it into your remote\u2019s main branch (or whatever branch your stable code-line is in). You can delete a remote branch using the rather obtuse syntax git push [remotename] :[branch]. If you want to delete your serverfix branch from the server, you run the following:\n\nBoom. No more branches on your server. You may want to dog-ear this page, because you\u2019ll need that command, and you\u2019ll likely forget the syntax. A way to remember this command is by recalling the git push [remotename] [localbranch]:[remotebranch] syntax that we went over a bit earlier. If you leave off the [localbranch] portion, then you\u2019re basically saying, \u201cTake nothing on my side and make it be [remotebranch].\u201d\n\nI ran git push origin :bugfix, and it worked beautifully. Scott Chacon was right\u2014I will want to dog-ear that page (or virtually dog ear-by answering this on Stack Overflow).\nFetch changes\nFinally, execute the following on other machines to propagate changes:"
  },
  {
    "question": "What is the difference between &#39;git pull&#39; and &#39;git fetch&#39;? What are the differences between git pull and git fetch?",
    "answer": "In the simplest terms, git pull does a git fetch followed by a git merge.\n\ngit fetch updates your remote-tracking branches under refs/remotes/<remote>/. This operation is safe to run at any time since it never changes any of your local branches under refs/heads.\ngit pull brings a local branch up-to-date with its remote version, while also updating your other remote-tracking branches.\nFrom the Git documentation for git pull:\n\ngit pull runs git fetch with the given parameters and then depending on configuration options or command line flags, will call either git rebase or git merge to reconcile diverging branches."
  },
  {
    "question": "How can I rename a local Git branch? How can I rename a local branch which has not yet been pushed to a remote repository?\nRelated:\n\nRename master branch for both local and remote Git repositories\nHow do I rename both a Git local and remote branch name?",
    "answer": "git branch -m <newname>\ngit branch -m <oldname> <newname>\ngit push origin -u <newname>\ngit push origin --delete <oldname>\ngit config --global alias.rename 'branch -m'\ngit branch -M <newname>\n\nTo rename the current branch:\n\nTo rename a branch while pointed to any branch:\n\n-m is short for --move.\n\nTo push the  local branch and reset the upstream branch:\n\nTo delete the  remote branch:\n\nTo create a git rename alias:\n\nOn Windows or another case-insensitive filesystem, use -M if there are only capitalization changes in the name. Otherwise, Git will throw a \"branch already exists\" error."
  },
  {
    "question": "How do I undo &#39;git add&#39; before commit? I mistakenly added files to Git using the command:\n\n```\ngit add myfile.txt\n\n```\n\nI have not yet run git commit. How do I undo this so that these changes will not be included in the commit?",
    "answer": "git reset <file>\ngit reset\n\nTo unstage a specific file\n\nThat will remove the file from the current index (the \"about to be committed\" list) without changing anything else.\nTo unstage all files from the current change set:\n\nIn old versions of Git, the above commands are equivalent to git reset HEAD <file> and git reset HEAD respectively, and will fail if HEAD is undefined (because you haven't yet made any commits in your repository) or ambiguous (because you created a branch called HEAD, which is a stupid thing that you shouldn't do). This was changed in Git 1.8.2, though, so in modern versions of Git you can use the commands above even prior to making your first commit:\n\n\"git reset\" (without options or parameters) used to error out when\nyou do not have any commits in your history, but it now gives you\nan empty index (to match non-existent commit you are not even on).\n\nDocumentation: git reset"
  },
  {
    "question": "How do I get the directory where a Bash script is located from within the script itself? How do I get the path of the directory in which a Bash script is located, inside that script?\nI want to use a Bash script as a launcher for another application. I want to change the working directory to the one where the Bash script is located, so I can operate on the files in that directory, like so:\n\n```\n$ ./application\n\n```",
    "answer": "SCRIPT_DIR=$( cd -- \"$( dirname -- \"${BASH_SOURCE[0]}\" )\" &> /dev/null && pwd )\nget_script_dir()\n{\nlocal SOURCE_PATH=\"${BASH_SOURCE[0]}\"\nlocal SYMLINK_DIR\nlocal SCRIPT_DIR\n\nwhile [ -L \"$SOURCE_PATH\" ]; do\n\nSYMLINK_DIR=\"$( cd -P \"$( dirname \"$SOURCE_PATH\" )\" >/dev/null 2>&1 && pwd )\"\n\nSOURCE_PATH=\"$(readlink \"$SOURCE_PATH\")\"\n\nif [[ $SOURCE_PATH != /* ]]; then\n\nSOURCE_PATH=$SYMLINK_DIR/$SOURCE_PATH\nfi\ndone\n\nSCRIPT_DIR=\"$(cd -P \"$( dirname \"$SOURCE_PATH\" )\" >/dev/null 2>&1 && pwd)\"\necho \"$SCRIPT_DIR\"\n}\n\necho \"get_script_dir: $(get_script_dir)\"\nSOURCE=${BASH_SOURCE[0]}\nwhile [ -L \"$SOURCE\" ]; do # resolve $SOURCE until the file is no longer a symlink\nTARGET=$(readlink \"$SOURCE\")\nif [[ $TARGET == /* ]]; then\necho \"SOURCE '$SOURCE' is an absolute symlink to '$TARGET'\"\nSOURCE=$TARGET\nelse\nDIR=$( dirname \"$SOURCE\" )\necho \"SOURCE '$SOURCE' is a relative symlink to '$TARGET' (relative to '$DIR')\"\nSOURCE=$DIR/$TARGET # if $SOURCE was a relative symlink, we need to resolve it relative to the path where the symlink file was located\nfi\ndone\necho \"SOURCE is '$SOURCE'\"\nRDIR=$( dirname \"$SOURCE\" )\nDIR=$( cd -P \"$( dirname \"$SOURCE\" )\" >/dev/null 2>&1 && pwd )\nif [ \"$DIR\" != \"$RDIR\" ]; then\necho \"DIR '$RDIR' resolves to '$DIR'\"\nfi\necho \"DIR is '$DIR'\"\nSOURCE './scriptdir.sh' is a relative symlink to 'sym2/scriptdir.sh' (relative to '.')\nSOURCE is './sym2/scriptdir.sh'\nDIR './sym2' resolves to '/home/ubuntu/dotfiles/fo fo/real/real1/real2'\nDIR is '/home/ubuntu/dotfiles/fo fo/real/real1/real2'\n\nis a useful one-liner which will give you the full directory name of the script no matter where it is being called from.\nIt will work as long as the last component of the path used to find the script is not a symlink (directory links are OK).  If you also want to resolve any links to the script itself, you need a multi-line solution:\n\nThis last one will work with any combination of aliases, source, bash -c, symlinks, etc.\nBeware: if you cd to a different directory before running this snippet, the result may be incorrect!\nAlso, watch out for $CDPATH gotchas, and stderr output side effects if the user has smartly overridden cd to redirect output to stderr instead (including escape sequences, such as when calling update_terminal_cwd >&2 on Mac). Adding >/dev/null 2>&1 at the end of your cd command will take care of both possibilities.\nTo understand how it works, try running this more verbose form:\n\nAnd it will print something like:"
  },
  {
    "question": "How do I check if a directory exists or not in a Bash shell script? What command checks if a directory exists or not within a Bash shell script?",
    "answer": "if [ -d \"$DIRECTORY\" ]; then\necho \"$DIRECTORY does exist.\"\nfi\nif [ ! -d \"$DIRECTORY\" ]; then\necho \"$DIRECTORY does not exist.\"\nfi\nln -s \"$ACTUAL_DIR\" \"$SYMLINK\"\nif [ -d \"$SYMLINK\" ]; then\nrmdir \"$SYMLINK\"\nfi\nrmdir: failed to remove `symlink': Not a directory\nif [ -d \"$LINK_OR_DIR\" ]; then\nif [ -L \"$LINK_OR_DIR\" ]; then\n\nrm \"$LINK_OR_DIR\"\nelse\n\nrmdir \"$LINK_OR_DIR\"\nfi\nfi\n\nTo check if a directory exists:\n\nTo check if a directory does not exist:\n\nHowever, as Jon Ericson points out, subsequent commands may not work as intended if you do not take into account that a symbolic link to a directory will also pass this check.\nE.g. running this:\n\nWill produce the error message:\n\nSo symbolic links may have to be treated differently, if subsequent commands expect directories:\n\nTake particular note of the double-quotes used to wrap the variables. The reason for this is explained by 8jean in another answer.\nIf the variables contain spaces or other unusual characters it will probably cause the script to fail."
  },
  {
    "question": "How do I tell if a file does not exist in Bash? This checks if a file exists:\n\n```\n#!/bin/bash\n\nFILE=$1     \nif [ -f $FILE ]; then\n   echo \"File $FILE exists.\"\nelse\n   echo \"File $FILE does not exist.\"\nfi\n\n```\n\nHow do I only check if the file does not exist?",
    "answer": "if [ ! -f /tmp/foo.txt ]; then\necho \"File not found!\"\nfi\n\nThe test command (written as [ here) has a \"not\" logical operator, ! (exclamation mark):"
  },
  {
    "question": "Echo newline in Bash prints literal \\n How do I print a newline? This merely prints \\n:\n\n```\necho -e \"Hello,\\nWorld!\"\n\n```\n\nOutput:\n\n```\nHello,\\nWorld!\n\n```",
    "answer": "printf \"hello\\nworld\\n\"\n\nUse printf instead:\n\nprintf behaves more consistently across different environments than echo."
  },
  {
    "question": "How to check if a string contains a substring in Bash I have a string in Bash:\n\n```\nstring=\"My string\"\n\n```\n\nHow can I test if it contains another string?\n\n```\nif [ $string ?? 'foo' ]; then\n  echo \"It's there!\"\nfi\n\n```\n\nWhere ?? is my unknown operator. Do I use echo and grep?\n\n```\nif echo \"$string\" | grep 'foo'; then\n  echo \"It's there!\"\nfi\n\n```\n\nThat looks a bit clumsy.",
    "answer": "string='My long string'\nif [[ $string == *\"My long\"* ]]; then\necho \"It's there!\"\nfi\n\nYou can use Marcus's answer (* wildcards) outside a case statement, too, if you use double brackets:\n\nNote that spaces in the needle string need to be placed between double quotes, and the * wildcards should be outside. Also note that a simple comparison operator is used (i.e. ==), not the regex operator =~."
  },
  {
    "question": "Shell command to tar directory excluding certain files/folders Is there a simple shell command/script that supports excluding certain files/folders from being archived?\nI have a directory that need to be archived with a sub directory that has a number of very large files I do not need to backup.\nNot quite solutions:\nThe tar --exclude=PATTERN command matches the given pattern and excludes those files, but I need specific files & folders to be ignored (full file path), otherwise valid files might be excluded.\nI could also use the find command to create a list of files and exclude the ones I don't want to archive and pass the list to tar, but that only works with for a small amount of files. I have tens of thousands.\nI'm beginning to think the only solution is to create a file with a list of files/folders to be excluded, then use rsync with --exclude-from=file to copy all the files to a tmp directory, and then use tar to archive that directory.\nCan anybody think of a better/more efficient solution?\nEDIT: Charles Ma's solution works well. The big gotcha is that the --exclude='./folder' MUST be at the beginning of the tar command. Full command (cd first, so backup is relative to that directory):\n\n```\ncd /folder_to_backup\ntar --exclude='./folder' --exclude='./upload/folder2' -zcvf /backup/filename.tgz .\n\n```",
    "answer": "tar --exclude='./folder' --exclude='./upload/folder2' -zcvf /backup/filename.tgz .\n\nYou can have multiple exclude options for tar so\n\netc will work. Make sure to put --exclude before the source and destination items."
  },
  {
    "question": "How do I tar a directory of files and folders without including the directory itself? I typically do:\n\n```\ntar -czvf my_directory.tar.gz my_directory\n\n```\n\nWhat if I just want to include everything (including any hidden system files) in my_directory, but not the directory itself? I don't want:\n\n```\nmy_directory\n   --- my_file\n   --- my_file\n   --- my_file\n\n```\n\nI want:\n\n```\nmy_file\nmy_file\nmy_file\n\n```",
    "answer": "cd my_directory/ && tar -zcvf ../my_dir.tgz . && cd -\nmkdir my_directory\ntouch my_directory/file1\ntouch my_directory/file2\ntouch my_directory/.hiddenfile1\ntouch my_directory/.hiddenfile2\ncd my_directory/ && tar -zcvf ../my_dir.tgz . && cd ..\n./\n./file1\n./file2\n./.hiddenfile1\n./.hiddenfile2\ntar ztf my_dir.tgz\n./\n./file1\n./file2\n./.hiddenfile1\n./.hiddenfile2\n\nshould do the job in one line. It works well for hidden files as well. \"*\" doesn't expand hidden files by path name expansion at least in bash. Below is my experiment:"
  },
  {
    "question": "Tar a directory, but don&#39;t store full absolute paths in the archive I have the following command in the part of a backup shell script:\n\n```\ntar -cjf site1.bz2 /var/www/site1/\n\n```\n\nWhen I list the contents of the archive, I get:\n\n```\ntar -tf site1.bz2\nvar/www/site1/style.css\nvar/www/site1/index.html\nvar/www/site1/page2.html\nvar/www/site1/page3.html\nvar/www/site1/images/img1.png\nvar/www/site1/images/img2.png\nvar/www/site1/subdir/index.html\n\n```\n\nBut I would like to remove the part /var/www/site1 from directory and file names within the archive, in order to simplify extraction and avoid useless constant directory structure. Never know, in case I would extract backuped websites in a place where web data weren't stored under /var/www.\nFor the example above, I would like to have :\n\n```\ntar -tf site1.bz2\nstyle.css\nindex.html\npage2.html\npage3.html\nimages/img1.png\nimages/img2.png\nsubdir/index.html\n\n```\n\nSo, that when I extract, files are extracted in the current directory and I don't need to move extracted files afterwards, and so that sub-directory structures is preserved.\nThere are already many questions about tar and backuping in stackoverflow and at other places on the web, but most of them ask for dropping the entire sub-directory structure (flattening), or just add or remove the initial / in the names (I don't know what it changes exactly when extracting), but no more.\nAfter having read some of the solutions found here and there as well as the manual, I tried :\n\n```\ntar -cjf site1.bz2 -C . /var/www/site1/\ntar -cjf site1.bz2 -C / /var/www/site1/\ntar -cjf site1.bz2 -C /var/www/site1/ /var/www/site1/\ntar -cjf site1.bz2 --strip-components=3 /var/www/site1/\n\n```\n\nBut none of them worked the way I want. Some do nothing, some others don't archive sub-directories anymore.\nIt's inside a backup shell script launched by a Cron, so I don't know well, which user runs it, what is the path and the current directory, so always writing absolute path is required for everything, and would prefer not changing current directory to avoid breaking something further in the script (because it doesn't only backup websites, but also databases, then send all that to FTP etc.)\nHow to achieve this?\nHave I just misunderstood how the option -C works?",
    "answer": "tar -cjf site1.tar.bz2 -C /var/www/site1 .\nOTHER OPTIONS\n\n-C, --directory DIR\nchange to directory DIR\n\nIn the above example, tar will change to directory /var/www/site1 before doing its thing because the option -C /var/www/site1 was given.\nFrom man tar:"
  },
  {
    "question": "Utilizing multi core for tar+gzip/bzip compression/decompression I normally compress using tar zcvf and decompress using tar zxvf (using gzip due to habit). \nI've recently gotten a quad core CPU with hyperthreading, so I have 8 logical cores, and I notice that many of the cores are unused during compression/decompression. \nIs there any way I can utilize the unused cores to make it faster?",
    "answer": "tar cf - paths-to-archive | pigz > archive.tar.gz\ntar cf - paths-to-archive | pigz -9 -p 32 > archive.tar.gz\n\nYou can use pigz instead of gzip, which does gzip compression on multiple cores.  Instead of using the -z option, you would pipe it through pigz:\n\nBy default, pigz uses the number of available cores, or eight if it could not query that.  You can ask for more with -p n, e.g. -p 32.  pigz has the same options as gzip, so you can request better compression with -9.  E.g."
  },
  {
    "question": "How to uncompress a tar.gz in another directory I have an archive \n\n*.tar.gz\n\nHow can I uncompress this in a destination directory?",
    "answer": "gzip -dc archive.tar.gz | tar -xf - -C /destination\ntar xzf archive.tar.gz -C /destination\n\nor, with GNU tar"
  },
  {
    "question": "How are zlib, gzip and zip related? What do they have in common and how are they different? The compression algorithm used in zlib is essentially the same as that in gzip and zip. What are gzip and zip? How are they different and how are they same?\n\nMod note: This post is off-topic, but the top-voted answer has historical significance as a primary source.  This post is locked with a Community Wiki lock (rather than a historical lock, as would otherwise be more appropriate) to allow its author to continue to maintain it, should they wish to do so.",
    "answer": "- The file is stored (no compression)\n- The file is Shrunk\n- The file is Reduced with compression factor 1\n- The file is Reduced with compression factor 2\n- The file is Reduced with compression factor 3\n- The file is Reduced with compression factor 4\n- The file is Imploded\n- Reserved for Tokenizing compression algorithm\n- The file is Deflated\n- Enhanced Deflating using Deflate64(tm)\n- PKWARE Data Compression Library Imploding (old IBM TERSE)\n- Reserved by PKWARE\n- File is compressed using BZIP2 algorithm\n- Reserved by PKWARE\n- LZMA\n- Reserved by PKWARE\n- IBM z/OS CMPSC Compression\n- Reserved by PKWARE\n- File is compressed using IBM TERSE (new)\n- IBM LZ77 z Architecture\n- deprecated (use method 93 for zstd)\n- Zstandard (zstd) Compression\n- MP3 Compression\n- XZ Compression\n- JPEG variant\n- WavPack compressed data\n- PPMd version I, Rev 1\n- AE-x encryption marker (see APPENDIX E)\n\nShort form:\n.zip is an archive format using, usually, the Deflate compression method.  The .gz gzip format is for single files, also using the Deflate compression method.  Often gzip is used in combination with tar to make a compressed archive format, .tar.gz.  The zlib library provides Deflate compression and decompression code for use by zip, gzip, png (which uses the zlib wrapper on deflate data), and many other applications.\nLong form:\nThe ZIP format was developed by Phil Katz as an open format with an open specification, where his implementation, PKZIP, was shareware.  It is an archive format that stores files and their directory structure, where each file is individually compressed.  The file type is .zip.  The files, as well as the directory structure, can optionally be encrypted.\nThe ZIP format supports several compression methods:\n\nMethods 1 to 7 are historical and are not in use.  Methods 9 through 98 are relatively recent additions and are in varying, small amounts of use.  The only method in truly widespread use in the ZIP format is method 8, Deflate, and to some smaller extent method 0, which is no compression at all.  Virtually every .zip file that you will come across in the wild will use exclusively methods 8 and 0, likely just method 8.  (Method 8 also has a means to effectively store the data with no compression and relatively little expansion, and Method 0 cannot be streamed whereas Method 8 can be.)\nThe ISO/IEC 21320-1:2015 standard for file containers is a restricted zip format, such as used in Java archive files (.jar), Office Open XML files (Microsoft Office .docx, .xlsx, .pptx), Office Document Format files (.odt, .ods, .odp), and EPUB files (.epub). That standard limits the compression methods to 0 and 8, as well as other constraints such as no encryption or signatures.\nAround 1990, the Info-ZIP group wrote portable, free, open-source implementations of zip and unzip utilities, supporting compression with the Deflate format, and decompression of that and the earlier formats.  This greatly expanded the use of the .zip format.\nIn the early '90s, the gzip format was developed as a replacement for the Unix compress utility, derived from the Deflate code in the Info-ZIP utilities.  Unix compress was designed to compress a single file or stream, appending a .Z to the file name.  compress uses the LZW compression algorithm, which at the time was under patent and its free use was in dispute by the patent holders.  Though some specific implementations of Deflate were patented by Phil Katz, the format was not, and so it was possible to write a Deflate implementation that did not infringe on any patents.  That implementation has not been so challenged in the last 20+ years.  The Unix gzip utility was intended as a drop-in replacement for compress, and in fact is able to decompress compress-compressed data (assuming that you were able to parse that sentence).  gzip appends a .gz to the file name.  gzip uses the Deflate compressed data format, which compresses quite a bit better than Unix compress, has very fast decompression, and adds a CRC-32 as an integrity check for the data.  The header format also permits the storage of more information than the compress format allowed, such as the original file name and the file modification time.\nThough compress only compresses a single file, it was common to use the tar utility to create an archive of files, their attributes, and their directory structure into a single .tar file, and then compress it with compress to make a .tar.Z file.  In fact, the tar utility had and still has the option to do the compression at the same time, instead of having to pipe the output of tar to compress.  This all carried forward to the gzip format, and tar has an option to compress directly to the .tar.gz format.  The tar.gz format compresses better than the .zip approach, since the compression of a .tar can take advantage of redundancy across files, especially many small files.  .tar.gz is the most common archive format in use on Unix due to its very high portability, but there are more effective compression methods in use as well, so you will often see .tar.bz2 and .tar.xz archives.\nUnlike .tar, .zip has a central directory at the end, which provides a list of the contents. That and the separate compression provides random access to the individual entries in a .zip file. A .tar file would have to be decompressed and scanned from start to end in order to build a directory, which is how a .tar file is listed.\nShortly after the introduction of gzip, around the mid-1990s, the same patent dispute called into question the free use of the .gif image format, very widely used on bulletin boards and the World Wide Web (a new thing at the time).  So a small group created the PNG losslessly compressed image format, with file type .png, to replace .gif.  That format also uses the Deflate format for compression, which is applied after filters on the image data expose more of the redundancy.  In order to promote widespread usage of the PNG format, two free code libraries were created.  libpng and zlib.  libpng handled all of the features of the PNG format, and zlib provided the compression and decompression code for use by libpng, as well as for other applications.  zlib was adapted from the gzip code.\nAll of the mentioned patents have since expired.\nThe zlib library supports Deflate compression and decompression, and three kinds of wrapping around the deflate streams.  Those are no wrapping at all (\"raw\" deflate), zlib wrapping, which is used in the PNG format data blocks, and gzip wrapping, to provide gzip routines for the programmer.  The main difference between zlib and gzip wrapping is that the zlib wrapping is more compact, six bytes vs. a minimum of 18 bytes for gzip, and the integrity check, Adler-32, runs faster than the CRC-32 that gzip uses.  Raw deflate is used by programs that read and write the .zip format, which is another format that wraps around deflate compressed data.\nzlib is now in wide use for data transmission and storage.  For example, most HTTP transactions by servers and browsers compress and decompress the data using zlib, specifically HTTP header Content-Encoding: deflate means deflate compression method wrapped inside the zlib data format.\nDifferent implementations of deflate can result in different compressed output for the same input data, as evidenced by the existence of selectable compression levels that allow trading off compression effectiveness for CPU time. zlib and PKZIP are not the only implementations of deflate compression and decompression. Both the 7-Zip archiving utility and Google's zopfli library have the ability to use much more CPU time than zlib in order to squeeze out the last few bits possible when using the deflate format, reducing compressed sizes by a few percent as compared to zlib's highest compression level. The pigz utility, a parallel implementation of gzip, includes the option to use zlib (compression levels 1-9) or zopfli (compression level 11), and somewhat mitigates the time impact of using zopfli by splitting the compression of large files over multiple processors and cores."
  },
  {
    "question": "Command Line Tool - Error - xcrun: error: unable to find utility &quot;xcodebuild&quot;, not a developer tool or in PATH I am getting this error while building the SwiftJSON framework to the Some Xcode project through Carthage Dependency Manager.\n\nSivaramaiahs-Mac-mini:GZipDemoApp vsoftMacmini5$ carthage update\n  --platform iOS\n*** Fetching GzipSwift\n*** Fetching SwiftyJSON\n*** Checking out GzipSwift at \"3.1.1\"\n*** Downloading SwiftyJSON.framework binary at \"3.1.3\"\n*** xcodebuild output can be found in /var/folders/7m/y0r2mdhn0f16zz1nlt34ypzr0000gn/T/carthage-xcodebuild.apLXCc.log\nA shell task (/usr/bin/xcrun xcodebuild -project\n  /Users/vsoftMacmini5/Desktop/GZipDemoApp/Carthage/Checkouts/GzipSwift/Gzip.xcodeproj\n  CODE_SIGNING_REQUIRED=NO CODE_SIGN_IDENTITY= CARTHAGE=YES -list)\n  failed with exit code 72:\nxcrun: error: unable to find utility \"xcodebuild\", not a developer\n  tool or in PATH",
    "answer": "I solved that problem by setting the Command Line Tools in Xcode. Go to:\n\nXcode > Preferences > Locations\n\nAnd select the command line tool from the dropdown. If you have only one version of Xcode installed, there should be only one option. If you have several versions of Xcode, then you must choose the one you need.\nUpdate (added image for reference)"
  },
  {
    "question": "Find all files containing a specific text (string) on Linux? How do I find all files containing a specific string of text within their file contents?\nThe following doesn't work. It seems to display every single file in the system.\n\n```\nfind / -type f -exec grep -H 'text-to-find-here' {} \\;\n\n```",
    "answer": "grep -rnw '/path/to/somewhere/' -e 'pattern'\ngrep --include=\\*.{c,h} -rnw '/path/to/somewhere/' -e \"pattern\"\ngrep --exclude=\\*.o -rnw '/path/to/somewhere/' -e \"pattern\"\ngrep --exclude-dir={dir1,dir2,*.dst} -rnw '/path/to/search/' -e \"pattern\"\n\nDo the following:\n\n-r or -R is recursive,\n-n is line number, and\n-w stands for match the whole word.\n-l (lower-case L) can be added to just give the file name of matching files.\n-e is the pattern used during the search\n\nAlong with these, --exclude, --include, --exclude-dir flags could be used for efficient searching:\n\nThis will only search through those files which have .c or .h extensions:\n\nThis will exclude searching all the files ending with .o extension:\n\nFor directories it's possible to exclude one or more directories using the --exclude-dir parameter. For example, this will exclude the dirs dir1/, dir2/ and all of them matching *.dst/:\n\nThis works very well for me, to achieve almost the same purpose like yours.\nFor more options, see man grep."
  },
  {
    "question": "grep: show lines surrounding each match How do I grep and show the preceding and following 5 lines surrounding each matched line?",
    "answer": "grep -B 3 -A 2 foo README.txt\ngrep -C 3 foo README.txt\n\nFor BSD or GNU grep you can use -B num to set how many lines before the match and -A num for the number of lines after the match.\n\nIf you want the same number of lines before and after you can use -C num.\n\nThis will show 3 lines before and 3 lines after."
  },
  {
    "question": "How do I recursively grep all directories and subdirectories? How do I recursively grep all directories and subdirectories?\n\n```\nfind . | xargs grep \"texthere\" *\n\n```",
    "answer": "grep -r \"texthere\" .\n\nThe first parameter represents the regular expression to search for, while the second one represents the directory that should be searched. In this case, . means the current directory.\nNote: This works for GNU grep, and on some platforms like Solaris you must specifically use GNU grep as opposed to legacy implementation.  For Solaris this is the ggrep command."
  },
  {
    "question": "How to grep (search through) committed code in the Git history I have deleted a file or some code in a file sometime in the past. Can I search through the content (not just the commit messages)?\nA very poor solution is to grep the log:\n\n```\ngit log -p | grep <pattern>\n\n```\n\nHowever, this doesn't return the commit hash straight away. I played around with git grep to no avail.",
    "answer": "git log -SFoo -- path_containing_change\ngit log -SFoo --since=2009.1.1 --until=2010.1.1 -- path_containing_change\n\nYou should use the pickaxe (-S) option of git log.\nTo search for Foo:\n\nSee Git history - find lost line by keyword for more.\n-S (named pickaxe) comes originally from a git diff option (Git v0.99, May 2005).\nThen -S (pickaxe) was ported to git log in May 2006 with Git 1.4.0-rc1.\n\nAs Jakub Nar\u0119bski commented:\n\nthis looks for differences that introduce or remove an instance of <string>.\nIt usually means \"revisions where you added or removed line with 'Foo'\".\n\nthe --pickaxe-regex option allows you to use extended POSIX regex instead of searching for a string.\nExample (from git log): git log -S\"frotz\\(nitfol\" --pickaxe-regex\n\nAs Rob commented, this search is case-sensitive - he opened a follow-up question on how to search case-insensitive.\n\nHi Angel notes in the comments:\n\nExecuting a git log -G<regexp> --branches --all (the -G is same as -S but for regexes) does same thing as the accepted one (git grep <regexp> $(git rev-list --all)), but it soooo much faster!\nThe accepted answer was still searching for text after \u224810 minutes of me running it, whereas this one gives results after \u22484 seconds \ud83e\udd37\u200d\u2642\ufe0f.\nThe output here is more useful as well"
  },
  {
    "question": "How can I use grep to show just filenames on Linux? How can I use grep to show just file-names (no in-line matches) on Linux?\nI am usually using something like:\n\n```\nfind . -iname \"*php\" -exec grep -H myString {} \\;\n\n```\n\nHow can I just get the file-names (with paths), but without the matches? Do I have to use xargs? I didn't see a way to do this on my grep man page.",
    "answer": "-l\n(The letter ell.) Write only the names of files containing selected\nlines to standard output. Pathnames are written once per file searched.\nIf the standard input is searched, a pathname of (standard input) will\nbe written, in the POSIX locale. In other locales, standard input may be\nreplaced by something more appropriate in those locales.\n\nThe standard option grep -l (that is a lowercase L) could do this.\nFrom the Unix standard:\n\nYou also do not need -H in this case."
  },
  {
    "question": "No module named pip in venv but pip installed I work in WSL Ubuntu. After instalation python3.13 dependencies from my previous projects stopped working. Venv with python 3.12 stopped activate in vscode interface. ErrorMessage:\n\nAn Invalid Python interpreter is selected, please try changing it to enable features such as IntelliSense, linting, and debugging. See output for more details regarding why the interpreter is invalid.\n\ncommand \"source venv/bin/activate\" works but all libraries \"could not be resolvedPylance\". When I try reinstal I see error:\n\nModuleNotFoundError: No module named 'pip'\n\nBut pip installed in this venv. I can see it folders. init and main files etc.\nI have pip for python3.12 and I have -m venv for python 3.12. I can recreate this venv but why I can`t turn it on properly?\nTried reinstall venv and pip for python3.12.\npython3 -m ensurepip is ok:\n\nRequirement already satisfied: pip in /usr/lib/python3/dist-packages (24.0)\n\nBut python3.12 -m ensurepip\n\nensurepip is disabled in Debian/Ubuntu for the system python.\n\npython3.12 -m pip --version is working also as python3.12 -m pip install and python3.12 -m pip --upgrade pip",
    "answer": "python3 --version\nwhich python3\ncat venv/pyvenv.cfg\nhome = /usr/bin/python3.12\n/path/to/your/venv/bin/python\nhead -n 1 venv/bin/pip\npython3.12 -m venv --upgrade-deps venv\nrm -rf venv\npython3.12 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\nsudo apt install python3.12-venv python3.12-distutils\n\nI ran into the exact same problem after installing Python 3.13 on WSL. Suddenly, all my existing virtual environments (created with Python 3.12) broke in VSCode. I was getting the \"Invalid Python interpreter\" error, Pylance couldn't resolve any imports, and pip appeared to be missing\u2014even though I could see it in the venv/bin folder.\nHere\u2019s what fixed it for me:\nFirst, check what your system python3 now points to:\n\nIn my case, it was now Python 3.13, which explains why stuff started breaking. Your virtual environment still points to the Python 3.12 binary internally, but VSCode (and maybe even pip) is trying to use 3.13 instead.\nYou can confirm that by looking at the pyvenv.cfg file inside your venv:\n\nYou should see something like:\n\nIf that's the case, then you just need to tell VSCode to use that exact interpreter. Open the command palette (Ctrl+Shift+P) in VSCode, choose \u201cPython: Select Interpreter\u201d, and manually select the path to your virtualenv\u2019s Python binary:\n\nAlso, double-check the shebang in your pip script:\n\nIf it says #!/usr/bin/python3, that might now point to Python 3.13, which breaks the venv. You can fix this by rebuilding the venv with the correct Python version:\n\nOr, if that doesn\u2019t work cleanly:\n\nAnd yeah, ensurepip being disabled for system Python is normal on Ubuntu. Just make sure you have the necessary packages installed:\n\nOnce I manually selected the right interpreter in VSCode and fixed the pip shebang, everything worked again\u2014IntelliSense, linting, imports, etc. Hope that helps."
  },
  {
    "question": "python venv install skips component file &quot;pointer.png&quot; This is a strange issue. I maintain the pi3d python module and it contains this file\ngithub.com/tipam/pi3d/blob/master/src/pi3d/util/icons/pointer.png\nWhen I clone the repo locally it has the .png file but when the package is installed using pip it seems to be missing. This didn't used to be a problem. Is it something to do with the fact that pip insists on installing to a venv now, i.e. if I made pip install with --no-warn-script-location would it include the missing file?",
    "answer": "[tool.setuptools.package-data]\n\"*\" = [\"*.fs\", \"*.vs\", \"*.inc\", \"*.gif\"]\n[tool.setuptools.package-data]\n\"*\" = [\"*.fs\", \"*.vs\", \"*.inc\", \"*.gif\", \"*.png\"]\n\nIt's because it's not present in tool.setuptools.package-data in pyproject.toml file.\n\nWith the previous configuration, you add all this extensions in your package as you can see in the next screenshot (content of the package uploaded on pypi).\n\nSo adding the png extension should work:"
  },
  {
    "question": "How is Docker different from a virtual machine? I keep rereading the Docker documentation to try to understand the difference between Docker and a full VM. How does it manage to provide a full filesystem, isolated networking environment, etc. without being as heavy?\nWhy is deploying software to a Docker image (if that's the right term) easier than simply deploying to a consistent production environment?",
    "answer": "Docker originally used LinuX Containers (LXC), but later switched to runC (formerly known as libcontainer), which runs in the same operating system as its host. This allows it to share a lot of the host operating system resources. Also, it uses a layered filesystem (AuFS) and manages networking.\nAuFS is a layered file system, so you can have a read only part and a write part which are merged together. One could have the common parts of the operating system as read only (and shared amongst all of your containers) and then give each container its own mount for writing.\nSo, let's say you have a 1\u00a0GB container image; if you wanted to use a full VM, you would need to have 1\u00a0GB x number of VMs you want. With Docker and AuFS you can share the bulk of the 1\u00a0GB between all the containers and if you have 1000 containers you still might only have a little over 1\u00a0GB of space for the containers OS (assuming they are all running the same OS image).\nA full virtualized system gets its own set of resources allocated to it, and does minimal sharing. You get more isolation, but it is much heavier (requires more resources). With Docker you get less isolation, but the containers are lightweight (require fewer resources). So you could easily run thousands of containers on a host, and it won't even blink. Try doing that with Xen, and unless you have a really big host, I don't think it is possible.\nA full virtualized system usually takes minutes to start, whereas Docker/LXC/runC containers take seconds, and often even less than a second.\nThere are pros and cons for each type of virtualized system. If you want full isolation with guaranteed resources, a full VM is the way to go. If you just want to isolate processes from each other and want to run a ton of them on a reasonably sized host, then Docker/LXC/runC seems to be the way to go.\nFor more information, check out this set of blog posts which do a good job of explaining how LXC works.\n\nWhy is deploying software to a docker image (if that's the right term) easier than simply deploying to a consistent production environment?\n\nDeploying a consistent production environment is easier said than done. Even if you use tools like Chef and Puppet, there are always OS updates and other things that change between hosts and environments.\nDocker gives you the ability to snapshot the OS into a shared image, and makes it easy to deploy on other Docker hosts. Locally, dev, qa, prod, etc.: all the same image. Sure you can do this with other tools, but not nearly as easily or fast.\nThis is great for testing; let's say you have thousands of tests that need to connect to a database, and each test needs a pristine copy of the database and will make changes to the data. The classic approach to this is to reset the database after every test either with custom code or with tools like Flyway - this can be very time-consuming and means that tests must be run serially. However, with Docker you could create an image of your database and run up one instance per test, and then run all the tests in parallel since you know they will all be running against the same snapshot of the database. Since the tests are running in parallel and in Docker containers they could run all on the same box at the same time and should finish much faster. Try doing that with a full VM.\nFrom comments...\n\nInteresting! I suppose I'm still confused by the notion of \"snapshot[ting] the OS\". How does one do that without, well, making an image of the OS?\n\nWell, let's see if I can explain. You start with a base image, and then make your changes, and commit those changes using docker, and it creates an image. This image contains only the differences from the base. When you want to run your image, you also need the base, and it layers your image on top of the base using a layered file system: as mentioned above, Docker uses AuFS. AuFS merges the different layers together and you get what you want; you just need to run it. You can keep adding more and more images (layers) and it will continue to only save the diffs. Since Docker typically builds on top of ready-made images from a registry, you rarely have to \"snapshot\" the whole OS yourself."
  },
  {
    "question": "From inside of a Docker container, how do I connect to the localhost of the machine? I have an Nginx instance running inside a Docker container. I have a MySQL running on the host system. I want to connect to the MySQL from within my container. MySQL is only binding to the localhost device.\nIs there a way to connect to this MySQL or any other program on localhost from within this docker container?\nThis question is different from \"How to get the IP address of the Docker host from inside a Docker container\" due to the fact that the IP address of the Docker host could be the public IP address or the private IP address in the network which may or may not be reachable from within the Docker container (I mean public IP address if hosted at AWS or something).\nEven if you have the IP address of the Docker host, it does not mean you can connect to the Docker host from within the container, given that IP address as your Docker network may be overlay, host, bridge, macvlan, none. etc., which restricts the reachability of that IP address.",
    "answer": "extra_hosts:\n- \"host.docker.internal:host-gateway\"\n[vagrant@docker:~] $ sudo ip addr show docker0\n4: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default\nlink/ether 56:84:7a:fe:97:99 brd ff:ff:ff:ff:ff:ff\ninet 172.17.42.1/16 scope global docker0\nvalid_lft forever preferred_lft forever\ninet6 fe80::5484:7aff:fefe:9799/64 scope link\nvalid_lft forever preferred_lft forever\nroot@e77f6a1b3740:/# ip addr show eth0\n863: eth0: <BROADCAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\nlink/ether 66:32:13:f0:f1:e3 brd ff:ff:ff:ff:ff:ff\ninet 172.17.1.192/16 scope global eth0\nvalid_lft forever preferred_lft forever\ninet6 fe80::6432:13ff:fef0:f1e3/64 scope link\nvalid_lft forever preferred_lft forever\nroot@e77f6a1b3740:/# route\nKernel IP routing table\nDestination     Gateway         Genmask         Flags Metric Ref    Use Iface\ndefault         172.17.42.1     0.0.0.0         UG    0      0        0 eth0\n172.17.0.0      *               255.255.0.0     U     0      0        0 eth0\nroot@e77f6a1b3740:/# ping 172.17.42.1\nPING 172.17.42.1 (172.17.42.1) 56(84) bytes of data.\nbytes from 172.17.42.1: icmp_seq=1 ttl=64 time=0.070 ms\nbytes from 172.17.42.1: icmp_seq=2 ttl=64 time=0.201 ms\nbytes from 172.17.42.1: icmp_seq=3 ttl=64 time=0.116 ms\n[vagrant@docker:~] $ ip addr show eth0\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\nlink/ether 08:00:27:98:dc:aa brd ff:ff:ff:ff:ff:ff\ninet 10.0.2.15/24 brd 10.0.2.255 scope global eth0\nvalid_lft forever preferred_lft forever\ninet6 fe80::a00:27ff:fe98:dcaa/64 scope link\nvalid_lft forever preferred_lft forever\n[vagrant@docker:~] $ docker run --rm -it --network=host ubuntu:trusty ip addr show eth0\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\nlink/ether 08:00:27:98:dc:aa brd ff:ff:ff:ff:ff:ff\ninet 10.0.2.15/24 brd 10.0.2.255 scope global eth0\nvalid_lft forever preferred_lft forever\ninet6 fe80::a00:27ff:fe98:dcaa/64 scope link\nvalid_lft forever preferred_lft forever\nexport DOCKER_HOST_IP=$(route -n | awk '/UG[ \\t]/{print $2}')\n[vagrant@docker:~] $ docker run --rm -it --network=host mysql mysql -h 127.0.0.1 -uroot -p\nEnter password:\nWelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 36\nServer version: 5.5.41-0ubuntu0.14.04.1 (Ubuntu)\n\nCopyright (c) 2000, 2014, Oracle and/or its affiliates. All rights reserved.\n\nOracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners.\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n\nmysql>\n\nIf you are using Docker-for-mac or Docker-for-Windows 18.03+, connect to your MySQL service using the host host.docker.internal (instead of the 127.0.0.1 in your connection string).\nIf you are using Docker-for-Linux 20.10.0+, you can also use the host host.docker.internal if you started your Docker container with the --add-host host.docker.internal:host-gateway option, or added the following snippet in your docker-compose.yml file:\n\nOtherwise, read below\n\nTLDR\nUse --network=\"host\" in your docker run command, then 127.0.0.1 in your Docker container will point to your Docker host.\nNote: This mode only works on Docker for Linux, per the documentation.\n\nNote on Docker container networking modes\nDocker offers different networking modes when running containers. Depending on the mode you choose you would connect to your MySQL database running on the Docker host differently.\ndocker run --network=\"bridge\" (default)\nDocker creates a bridge named docker0 by default. Both the Docker host and the Docker containers have an IP address on that bridge.\nOn the Docker host, type sudo ip addr show docker0 you will have an output looking like:\n\nSo here my Docker host has the IP address 172.17.42.1 on the docker0 network interface.\nNow start a new container and get a shell on it: docker run --rm -it ubuntu:trusty bash and within the container type ip addr show eth0 to discover how its main network interface is set up:\n\nHere my container has the IP address 172.17.1.192. Now look at the routing table:\n\nSo the IP address of the Docker host 172.17.42.1 is set as the default route and is accessible from your container.\n\ndocker run --network=\"host\"\nAlternatively you can run a Docker container with network settings set to host. Such a container will share the network stack with the Docker host and from the container point of view, localhost (or 127.0.0.1) will refer to the Docker host.\nBe aware that any port opened in your Docker container would be opened on the Docker host. And this without requiring the -p or -P docker run option.\nIP configuration on my Docker host:\n\nAnd from a Docker container in host mode:\n\nAs you can see, both the Docker host and Docker container share the exact same network interface and as such have the same IP address.\n\nConnecting to MySQL from containers\nBridge mode\nTo access MySQL running on the Docker host from containers in bridge mode, you need to make sure the MySQL service is listening for connections on the 172.17.42.1 IP address.\nTo do so, make sure you have either bind-address = 172.17.42.1 or bind-address = 0.0.0.0 in your MySQL configuration file (my.cnf).\nIf you need to set an environment variable with the IP address of the gateway, you can run the following code in a container:\n\nThen in your application, use the DOCKER_HOST_IP environment variable to open the connection to MySQL.\nNote: if you use bind-address = 0.0.0.0, your MySQL server will listen for connections on all network interfaces. That means your MySQL server could be reached from the Internet; make sure to set up firewall rules accordingly.\nNote 2: if you use bind-address = 172.17.42.1 your MySQL server won't listen for connections made to 127.0.0.1. Processes running on the Docker host that would want to connect to MySQL would have to use the 172.17.42.1 IP address.\nHost mode\nTo access MySQL running on the docker host from containers in host mode, you can keep bind-address = 127.0.0.1 in your MySQL configuration and connect to 127.0.0.1 from your containers:\n\nNote: Do use mysql -h 127.0.0.1 and not mysql -h localhost; otherwise the MySQL client would try to connect using a Unix socket."
  },
  {
    "question": "What is the difference between the &#39;COPY&#39; and &#39;ADD&#39; commands in a Dockerfile? What is the difference between the COPY and ADD commands in a Dockerfile, and when would I use one over the other?\n\n```\nCOPY <src> <dest>\n\n```\n\n\nThe COPY instruction will copy new files from <src> and add them to the container's filesystem at path <dest>\n\n\n```\nADD <src> <dest>\n\n```\n\n\nThe ADD instruction will copy new files from <src> and add them to the container's filesystem at path <dest>.",
    "answer": "You should check the ADD and COPY documentation for a more detailed description of their behaviors, but in a nutshell, the major difference is that ADD can do more than COPY:\n\nADD allows <src> to be a URL\nReferring to comments below, the ADD documentation states that:\n\nIf  is a local tar archive in a recognized compression format (identity, gzip, bzip2 or xz) then it is unpacked as a directory. Resources from remote URLs are not decompressed.\n\nNote that the Best practices for writing Dockerfiles suggests using COPY where the magic of ADD is not required. Otherwise, you (since you had to look up this answer) are likely to get surprised someday when you mean to copy keep_this_archive_intact.tar.gz into your container, but instead, you spray the contents onto your filesystem."
  },
  {
    "question": "Copying files from Docker container to host I'm thinking of using Docker to build my dependencies on a Continuous Integration (CI) server, so that I don't have to install all the runtimes and libraries on the agents themselves. \nTo achieve this I would need to copy the build artifacts that are built inside the container back into the host. Is that possible?",
    "answer": "docker cp <containerId>:/file/path/within/container /host/path/target\nsudo docker cp goofy_roentgen:/out_read.jpg .\nsudo docker ps\n\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS                                            NAMES\n1b4ad9311e93        bamos/openface      \"/bin/bash\"         33 minutes ago      Up 33 minutes       0.0.0.0:8000->8000/tcp, 0.0.0.0:9000->9000/tcp   goofy_roentgen\nsudo docker cp 1b4a:/out_read.jpg .\n\nIn order to copy a file from a container to the host, you can use the command\n\nHere's an example:\n\nHere goofy_roentgen is the container name I got from the following command:\n\nYou can also use (part of) the Container ID. The following command is equivalent to the first"
  },
  {
    "question": "What is the difference between CMD and ENTRYPOINT in a Dockerfile? In Dockerfiles there are two commands that look similar to me: CMD and ENTRYPOINT. But I guess that there is a (subtle?) difference between them - otherwise it would not make any sense to have two commands for the very same thing.\nThe documentation states for CMD-\n\nThe main purpose of a CMD is to provide defaults for an executing container.\n\nand for ENTRYPOINT:\n\nAn ENTRYPOINT helps you to configure a container that you can run as an executable.\n\nSo, what's the difference between those two commands?",
    "answer": "Docker has a default entrypoint which is /bin/sh -c but does not have a default command.\nWhen you run docker like this:\ndocker run -i -t ubuntu bash\nthe entrypoint is the default /bin/sh -c, the image is ubuntu and the command is bash.\nThe command is run via the entrypoint. i.e., the actual thing that gets executed is /bin/sh -c bash. This allowed Docker to implement RUN quickly by relying on the shell's parser.\nLater on, people asked to be able to customize this, so ENTRYPOINT and --entrypoint were introduced.\nEverything after the image name, ubuntu in the example above, is the command and is passed to the entrypoint. When using the CMD instruction, it is exactly as if you were executing\ndocker run -i -t ubuntu <cmd>\nThe parameter of the entrypoint is <cmd>.\nYou will also get the same result if you instead type this command docker run -i -t ubuntu: a bash shell will start in the container because in the ubuntu Dockerfile a default CMD is specified:\nCMD [\"bash\"].\nAs everything is passed to the entrypoint, you can have a very nice behavior from your images. @Jiri example is good, it shows how to use an image as a \"binary\". When using [\"/bin/cat\"] as entrypoint and then doing docker run img /etc/passwd, you get it, /etc/passwd is the command and is passed to the entrypoint so the end result execution is simply /bin/cat /etc/passwd.\nAnother example would be to have any cli as entrypoint. For instance, if you have a redis image, instead of running docker run redisimg redis -H something -u toto get key, you can simply have ENTRYPOINT [\"redis\", \"-H\", \"something\", \"-u\", \"toto\"] and then run like this for the same result: docker run redisimg get key."
  },
  {
    "question": "How do I copy a folder from remote to local using scp? How do I copy a folder from remote to local host using scp?\nI use ssh to log in to my server.\nThen, I would like to copy the remote folder foo to local /home/user/Desktop.\nHow do I achieve this?",
    "answer": "scp -r user@your.server.example.com:/path/to/foo /home/user/Desktop/\n\nBy not including the trailing '/' at the end of foo, you will copy the directory itself (including contents), rather than only the contents of the directory.\nFrom man scp (See online manual)\n\n-r Recursively copy entire directories"
  },
  {
    "question": "ssh &quot;permissions are too open&quot; I get the following error from ssh:\n\n```\nPermissions 0777 for '/Users/username/.ssh/id_rsa' are too open.\nIt is recommended that your private key files are NOT accessible by others.\nThis private key will be ignored.\n\n```\n\nWhat permissions should I give to the id_rsa file?",
    "answer": "chmod 600 ~/.ssh/id_rsa\nchmod 400 ~/.ssh/id_rsa\n~/.ssh/id_rsa\nContains the private key for authentication.  These files contain sensitive\ndata and should be readable by the user but not\naccessible by others (read/write/execute).  ssh will simply ignore a private\nkey file if it is\naccessible by others.  It is possible to specify a\npassphrase when generating the key which will be used to encrypt the sensitive\npart of this file using 3DES.\n\n~/.ssh/identity.pub\n~/.ssh/id_dsa.pub\n~/.ssh/id_ecdsa.pub\n~/.ssh/id_rsa.pub\nContains the public key for authentication.  These files are not sensitive and\ncan (but need not) be readable by anyone.\n\nThe keys need to be read-writable only by you:\n\nAlternatively, the keys can be only readable by you (this also blocks your write access):\n\n600 appears to be better in most cases, because you don't need to change file permissions later to edit it. (See the comments for more nuances)\nThe relevant portion from the manpage (man ssh)"
  },
  {
    "question": "How to specify the private SSH-key to use when executing shell command on Git? A rather unusual situation perhaps, but I want to specify a private SSH-key to use when executing a shell (git) command from the local computer.\nBasically like this:\n\n```\ngit clone git@github.com:TheUser/TheProject.git -key \"/home/christoffer/ssh_keys/theuser\"\n\n```\n\nOr even better (in Ruby):\n\n```\nwith_key(\"/home/christoffer/ssh_keys/theuser\") do\n  sh(\"git clone git@github.com:TheUser/TheProject.git\")\nend\n\n```\n\nI have seen examples of connecting to a remote server with Net::SSH that uses a specified private key, but this is a local command. Is it possible?",
    "answer": "ssh-agent bash -c 'ssh-add /somewhere/yourkey; git clone git@github.com:user/project.git'\nssh-agent $(ssh-add /somewhere/yourkey; git clone git@github.com:user/project.git)\n\nSomething like this should work (suggested by orip):\n\nif you prefer subshells, you could try the following (though it is more fragile):\n\nGit will invoke SSH which will find its agent by environment variable; this will, in turn, have the key loaded.\nAlternatively, setting HOME may also do the trick, provided you are willing to setup a directory that contains only a .ssh directory as HOME; this may either contain an identity.pub, or a config file setting IdentityFile."
  },
  {
    "question": "How do I remove the passphrase for the SSH key without having to create a new key? I set a passphrase when creating a new SSH key on my laptop. But, as I realise now, this is quite painful when you are trying to commit (Git and SVN) to a remote location over SSH many times in an hour.\nOne way I can think of is, delete my SSH keys and create new. Is there a way to remove the passphrase, while still keeping the same keys?",
    "answer": "ssh-keygen -p\nssh-keygen -p [-P old_passphrase] [-N new_passphrase] [-f keyfile]\n\nShort answer:\n\nThis will then prompt you to enter the keyfile location, the old passphrase, and the new passphrase (which can be left blank to have no passphrase).\n\nIf you would like to do it all on one line without prompts do:\n\nImportant: Beware that when executing commands they will typically be logged in your ~/.bash_history file (or similar) in plain text including all arguments provided (i.e. the passphrases in this case). It is, therefore, is recommended that you use the first option unless you have a specific reason to do otherwise.\nNotice though that you can still use -f keyfile without having to specify -P nor -N, and that the keyfile defaults to ~/.ssh/id_rsa, so in many cases, it's not even needed.\nYou might want to consider using ssh-agent, which can cache the passphrase for a time. The latest versions of gpg-agent also support the protocol that is used by ssh-agent."
  },
  {
    "question": "How do I POST JSON data with cURL? I use Ubuntu and installed cURL on it. I want to test my Spring REST application with cURL. I wrote my POST code at the Java side. However, I want to test it with cURL. I am trying to post a JSON data. Example data is like this:\n\n```\n{\"value\":\"30\",\"type\":\"Tip 3\",\"targetModule\":\"Target 3\",\"configurationGroup\":null,\"name\":\"Configuration Deneme 3\",\"description\":null,\"identity\":\"Configuration Deneme 3\",\"version\":0,\"systemId\":3,\"active\":true}\n\n```\n\nI use this command:\n\n```\ncurl -i \\\n    -H \"Accept: application/json\" \\\n    -H \"X-HTTP-Method-Override: PUT\" \\\n    -X POST -d \"value\":\"30\",\"type\":\"Tip 3\",\"targetModule\":\"Target 3\",\"configurationGroup\":null,\"name\":\"Configuration Deneme 3\",\"description\":null,\"identity\":\"Configuration Deneme 3\",\"version\":0,\"systemId\":3,\"active\":true \\\n    http://localhost:8080/xx/xxx/xxxx\n\n```\n\nIt returns this error:\n\n```\nHTTP/1.1 415 Unsupported Media Type\nServer: Apache-Coyote/1.1\nContent-Type: text/html;charset=utf-8\nContent-Length: 1051\nDate: Wed, 24 Aug 2011 08:50:17 GMT\n\n```\n\nThe error description is this:\n\nThe server refused this request because the request entity is in a format not supported by the requested resource for the requested method ().\n\nTomcat log:\n\"POST /ui/webapp/conf/clear HTTP/1.1\" 415 1051\nWhat is the right format of the cURL command?\nThis is my Java side PUT code (I have tested GET and DELETE and they work):\n\n```\n@RequestMapping(method = RequestMethod.PUT)\npublic Configuration updateConfiguration(HttpServletResponse response, @RequestBody Configuration configuration) { //consider @Valid tag\n    configuration.setName(\"PUT worked\");\n    //todo If error occurs response.sendError(HttpServletResponse.SC_NOT_FOUND);\n    return configuration;\n}\n\n```",
    "answer": "-H \"Content-Type: application/json\"\ncurl --header \"Content-Type: application/json\" \\\n--request POST \\\n--data '{\"username\":\"xyz\",\"password\":\"xyz\"}' \\\nhttp://localhost:3000/api/login\n\nYou need to set your content-type to application/json. But -d (or --data) sends the Content-Type application/x-www-form-urlencoded by default, which is not accepted on Spring's side.\nLooking at the curl man page, I think you can use -H (or --header):\n\nFull example:\n\n(-H is short for --header, -d for --data)\nNote that -request POST is optional if you use -d, as the -d flag implies a POST request.\n\nOn Windows, things are slightly different. See the comment thread."
  },
  {
    "question": "How do I get a YouTube video thumbnail from the YouTube API? If I have a YouTube video URL, is there any way to use PHP and cURL to get the associated thumbnail from the YouTube API?",
    "answer": "https://img.youtube.com/vi/<insert-youtube-video-id-here>/0.jpg\nhttps://img.youtube.com/vi/<insert-youtube-video-id-here>/1.jpg\nhttps://img.youtube.com/vi/<insert-youtube-video-id-here>/2.jpg\nhttps://img.youtube.com/vi/<insert-youtube-video-id-here>/3.jpg\nhttps://img.youtube.com/vi/<insert-youtube-video-id-here>/default.jpg\nhttps://img.youtube.com/vi/<insert-youtube-video-id-here>/hqdefault.jpg\nhttps://img.youtube.com/vi/<insert-youtube-video-id-here>/mqdefault.jpg\nhttps://img.youtube.com/vi/<insert-youtube-video-id-here>/sddefault.jpg\nhttps://img.youtube.com/vi/<insert-youtube-video-id-here>/maxresdefault.jpg\n\nEach YouTube video has four generated images. They are predictably formatted as follows:\n\nThe first one in the list is a full size image and others are thumbnail images. The default thumbnail image (i.e., one of 1.jpg, 2.jpg, 3.jpg) is:\n\nFor the high quality version of the thumbnail use a URL similar to this:\n\nThere is also a medium quality version of the thumbnail, using a URL similar to the HQ:\n\nFor the standard definition version of the thumbnail, use a URL similar to this:\n\nFor the maximum resolution version of the thumbnail use a URL similar to this:\n\nAll of the above URLs are available over HTTP too. Additionally, the slightly shorter hostname i3.ytimg.com works in place of img.youtube.com in the example URLs above.\nAlternatively, you can use the YouTube Data API (v3) to get thumbnail images."
  },
  {
    "question": "How to send a header using a HTTP request through a cURL call? I wish to send a header to my Apache server on a Linux box.  How can I achieve this via a cURL call?",
    "answer": "curl -i -H \"Accept: application/json\" -H \"Content-Type: application/json\" http://hostname/resource\ncurl -H \"Accept: application/xml\" -H \"Content-Type: application/xml\" -X GET http://hostname/resource\ncurl --data \"param1=value1&param2=value2\" http://hostname/resource\ncurl --form \"fileupload=@filename.txt\" http://hostname/resource\ncurl -X POST -d @filename http://hostname/resource\ncurl -d \"username=admin&password=admin&submit=Login\" --dump-header headers http://localhost/Login\ncurl -L -b headers http://localhost/\n\nGET:\nwith JSON:\n\nwith XML:\n\nPOST:\nFor posting data:\n\nFor file upload:\n\nRESTful HTTP Post:\n\nFor logging into a site (auth):\n\nSource"
  },
  {
    "question": "How do I measure request and response times at once using cURL? I have a web service that receives data in JSON format, processes the data, and then returns the result to the requester.\nI want to measure the request, response, and total time using cURL.\nMy example request looks like:\n\n```\ncurl -X POST -d @file server:port\n\n```\n\nand I currently measure this using the time command in Linux:\n\n```\ntime curl -X POST -d @file server:port\n\n```\n\nThe time command only measures total time, though - which isn't quite what I am looking for.\nIs there any way to measure request and response times using cURL?",
    "answer": "time_namelookup:  %{time_namelookup}s\\n\ntime_connect:  %{time_connect}s\\n\ntime_appconnect:  %{time_appconnect}s\\n\ntime_pretransfer:  %{time_pretransfer}s\\n\ntime_redirect:  %{time_redirect}s\\n\ntime_starttransfer:  %{time_starttransfer}s\\n\n----------\\n\ntime_total:  %{time_total}s\\n\ncurl -w \"@curl-format.txt\" -o /dev/null -s \"http://wordpress.com/\"\ncurl -w \"@curl-format.txt\" -o NUL -s \"http://wordpress.com/\"\ntime_namelookup:  0.001s\ntime_connect:  0.037s\ntime_appconnect:  0.000s\ntime_pretransfer:  0.037s\ntime_redirect:  0.000s\ntime_starttransfer:  0.092s\n----------\ntime_total:  0.164s\nalias curltime=\"curl -w \\\"@$HOME/.curl-format.txt\\\" -o /dev/null -s \"\ncurltime wordpress.org\ncurl -w @- -o /dev/null -s \"$@\" <<'EOF'\ntime_namelookup:  %{time_namelookup}\\n\ntime_connect:  %{time_connect}\\n\ntime_appconnect:  %{time_appconnect}\\n\ntime_pretransfer:  %{time_pretransfer}\\n\ntime_redirect:  %{time_redirect}\\n\ntime_starttransfer:  %{time_starttransfer}\\n\n----------\\n\ntime_total:  %{time_total}\\n\nEOF\ncurltime wordpress.org\ncurl -w \"@%~dp0curl-format.txt\" -o NUL -s %*\ncurltime wordpress.org\n\nFrom this brilliant blog post...  https://blog.josephscott.org/2011/10/14/timing-details-with-curl/\ncURL supports formatted output for the details of the request (see the cURL manpage for details, under -w, \u2013write-out <format>). For our purposes we\u2019ll focus just on the timing details that are provided. Times below are in seconds.\n\nCreate a new file, curl-format.txt, and paste in:\n\nMake a request:\n\nOr on Windows, it's...\n\nWhat this does:\n-w \"@curl-format.txt\" tells cURL to use our format file\n-o /dev/null redirects the output of the request to /dev/null\n-s\ntells cURL not to show a progress meter\n\"http://wordpress.com/\" is\nthe URL we are requesting. Use quotes particularly if your URL has \"&\" query string parameters\n\nAnd here is what you get back:\n\nI have not yet seen an option to output the results in microseconds, but if you're aware of one, post in the comments below.\n\nMake a Linux/Mac shortcut (alias)\n\nThen you can simply call...\n\nThanks to commenter Pete Doyle!\n\nMake a Linux/Mac stand-alone script\nThis script does not require a separate .txt file to contain the formatting.\nCreate a new file, curltime, somewhere in your executable path, and paste in:\n\nThen call it the same way as the alias:\n\nMake a Windows shortcut (aka BAT file)\nCreate a new text file called curltime.bat in the same folder as curl.exe and curl-format.txt, and paste in the following line:\n\nThen from the command line you can simply call:\n\n(Make sure the folder is listed in your Windows PATH variable to be able to use the command from any folder.)"
  },
  {
    "question": "How do I get cURL to not show the progress bar? I'm trying to use cURL in a script and get it to not show the progress bar. \nI've tried the -s, -silent, -S, and -quiet options, but none of them work.\nHere's a typical command I've tried:\n\n```\ncurl -s http://google.com > temp.html\n\n```\n\nI only get the progress bar when pushing it to a file, so curl -s http://google.com doesn't have a progress bar, but curl -s http://google.com > temp.html does.",
    "answer": "curl -s http://google.com > temp.html\ncurl  http://google.com 2>/dev/null > temp.html\n\nworks for curl version 7.19.5 on Ubuntu 9.10 (no progress bar). But if for some reason that does not work on your platform, you could always redirect stderr to /dev/null:"
  },
  {
    "question": "Bash tool to get nth line from a file Is there a \"canonical\" way of doing that? I've been using head -n | tail -1 which does the trick, but I've been wondering if there's a Bash tool that specifically extracts a line (or a range of lines) from a file.\nBy \"canonical\" I mean a program whose main function is doing that.",
    "answer": "sed 'NUMq;d' file\nsed \"${NUM}q;d\" file\n\nhead and pipe with tail will be slow for a huge file. I would suggest sed like this:\n\nWhere NUM is the number of the line you want to print; so, for example, sed '10q;d' file to print the 10th line of file.\nExplanation:\nNUMq will quit immediately when the line number is NUM.\nd will delete the line instead of printing it; this is inhibited on the last line because the q causes the rest of the script to be skipped when quitting.\nIf you have NUM in a variable, you will want to use double quotes instead of single:"
  },
  {
    "question": "Find and kill a process in one line using bash and regex I often need to kill a process during programming.\nThe way I do it now is:\n\n```\n[~]$ ps aux | grep 'python csp_build.py'\nuser    5124  1.0  0.3 214588 13852 pts/4    Sl+  11:19   0:00 python csp_build.py\nuser    5373  0.0  0.0   8096   960 pts/6    S+   11:20   0:00 grep python csp_build.py\n[~]$ kill 5124\n\n```\n\nHow can I extract the process id automatically and kill it in the same line?\nLike this:\n\n```\n[~]$ ps aux | grep 'python csp_build.py' | kill <regex that returns the pid>\n\n```",
    "answer": "kill $(ps aux | grep '[p]ython csp_build.py' | awk '{print $2}')\npax> sleep 3600 &\n[1] 2225\npax> sleep 3600 &\n[2] 2226\npax> sleep 3600 &\n[3] 2227\npax> sleep 3600 &\n[4] 2228\npax> sleep 3600 &\n[5] 2229\npax> kill $(ps aux | grep '[s]leep' | awk '{print $2}')\n[5]+  Terminated              sleep 3600\n[1]   Terminated              sleep 3600\n[2]   Terminated              sleep 3600\n[3]-  Terminated              sleep 3600\n[4]+  Terminated              sleep 3600\n\nIn bash, using only the basic tools listed in your question(1), you should be able to do:\n\nDetails on its workings are as follows:\n\nThe ps gives you the list of all the processes.\nThe grep filters that based on your search string, [p] is a trick to stop you picking up the actual grep process itself.\nThe awk just gives you the second field of each line, which is the PID.\nThe $(x) construct means to execute x then take its output and put it on the command line. The output of that ps pipeline inside that construct above is the list of process IDs so you end up with a command like kill 1234 1122 7654.\n\nHere's a transcript showing it in action:\n\nand you can see it terminating all the sleepers.\nExplaining the grep '[p]ython csp_build.py' bit in a bit more detail: when you do sleep 3600 & followed by ps -ef | grep sleep, you tend to get two processes with sleep in it, the sleep 3600 and the grep sleep (because they both have sleep in them, that's not rocket science).\nHowever, ps -ef | grep '[s]leep' won't create a grep process with sleep in it, it instead creates one with the command grep '[s]leep' and here's the tricky bit: the grep doesn't find that one, because it's looking for the regular expression \"any character from the character class [s] (which is basically just s) followed by leep.\nIn other words, it's looking for sleep but the grep process is grep '[s]leep' which doesn't have the text sleep in it.\nWhen I was shown this (by someone here on SO), I immediately started using it because\n\nit's one less process than adding | grep -v grep; and\nit's elegant and sneaky, a rare combination :-)\n\n(1) If you're not limited to using those basic tools, there's a nifty pgrep command which will find processes based on certain criteria (assuming you have it available on your system, of course).\nFor example, you can use pgrep sleep to output the process IDs for all sleep commands (by default, it matches the process name). If you want to match the entire command line as shown in ps, you can do something like pgrep -f 'sleep 9999'.\nAs an aside, it doesn't list itself if you do pgrep pgrep, so the tricky filter method shown above is not necessary in this case.\nYou can check that the processes are the ones you're interested in by using -a to show the full process names. You can also limit the scope to your own processes (or a specific set of users) with -u or -U. See the man page for pgrep/pkill for more options.\nOnce you're satisfied it will only show the processes you're interested in, you can then use pkill with the same parameters to send a signal to all those processes."
  },
  {
    "question": "How to remove double-quotes in jq output for parsing json files in bash? I'm using jq to parse a JSON file as shown here. However, the results for string values contain the \"double-quotes\" as expected, as shown below:\n\n```\n$ cat json.txt | jq '.name'\n\"Google\"\n\n```\n\nHow can I pipe this into another command to remove the \"\"? so I get\n\n```\n$ cat json.txt | jq '.name' | some_other_command\nGoogle\n\n```\n\nWhat some_other_command can I use?",
    "answer": "jq -r '.name' <json.txt\n\nUse the -r (or --raw-output) option to emit raw strings as output:"
  },
  {
    "question": "What is the difference between sed and awk? What is the difference between awk\nand sed ?\nWhat kind of application are best use\ncases for sed and awk tools ?",
    "answer": "sed 's/-\\([0-9.]\\+\\)/(\\1)/g' inputfile\nawk -F, 'BEGIN {OFS = \",\"} {gsub(\"-([0-9.]+)\", \"(\" substr($3, 2) \")\", $3); print}' inputfile\n\nsed is a stream editor. It works with streams of characters on a per-line basis. It has a primitive programming language that includes goto-style loops and simple conditionals (in addition to pattern matching and address matching). There are essentially only two \"variables\": pattern space and hold space. Readability of scripts can be difficult. Mathematical operations are extraordinarily awkward at best.\nThere are various versions of sed with different levels of support for command line options and language features.\nawk is oriented toward delimited fields on a per-line basis. It has much more robust programming constructs including if/else, while, do/while and for (C-style and array iteration). There is complete support for variables and single-dimension associative arrays plus (IMO) kludgey multi-dimension arrays. Mathematical operations resemble those in C. It has printf and functions. The \"K\" in \"AWK\" stands for \"Kernighan\" as in \"Kernighan and Ritchie\" of the book \"C Programming Language\" fame (not to forget Aho and Weinberger). One could conceivably write a detector of academic plagiarism using awk.\nGNU awk (gawk) has numerous extensions, including true multidimensional arrays in the latest version. There are other variations of awk including mawk and nawk.\nBoth programs use regular expressions for selecting and processing text.\nI would tend to use sed where there are patterns in the text. For example, you could replace all the negative numbers in some text that are in the form \"minus-sign followed by a sequence of digits\" (e.g. \"-231.45\") with the \"accountant's brackets\" form (e.g. \"(231.45)\") using this (which has room for improvement):\n\nI would use awk when the text looks more like rows and columns or, as awk refers to them \"records\" and \"fields\". If I was going to do a similar operation as above, but only on the third field in a simple comma delimited file I might do something like:\n\nOf course those are just very simple examples that don't illustrate the full range of capabilities that each has to offer."
  },
  {
    "question": "How do I force &quot;git pull&quot; to overwrite local files? How do I force an overwrite of local files on a git pull? My local repository contains a file of the same filename as on the server.\n\nerror: Untracked working tree file 'example.txt' would be overwritten by merge",
    "answer": "git fetch --all\ngit branch backup-main\ngit reset --hard origin/main\ngit checkout main\ngit branch new-branch-to-save-current-commits\ngit fetch --all\ngit reset --hard origin/main\ngit stash\ngit stash pop\n\n\u26a0 Warning:\nAny uncommitted local change to tracked files will be lost, even if staged.\nBut any local file that's not tracked by Git will not be affected.\n\nFirst, update all origin/<branch> refs to latest:\n\nBackup your current branch (e.g. main):\n\nJump to the latest commit on origin/main and checkout those files:\n\nExplanation:\ngit fetch downloads the latest from remote without trying to merge or rebase anything.\ngit reset resets the master branch to what you just fetched. The --hard option changes all the files in your working tree to match the files in origin/main.\n\nMaintain current local commits\n[*]: It's worth noting that it is possible to maintain current local commits by creating a branch from main before resetting:\n\nAfter this, all of the old commits will be kept in new-branch-to-save-current-commits.\nUncommitted changes\nUncommitted changes, even if staged (with git add), will be lost. Make sure to stash or commit anything you need. For example, run the following:\n\nAnd later (after git reset), reapply these uncommitted changes:\n\nWhich may create merge conflicts."
  },
  {
    "question": "How to check out a remote Git branch? How do I check out the remote test branch? I can see it with git branch -r. I tried:\n\ngit checkout test, which does nothing\ngit checkout origin/test gives * (no branch)",
    "answer": "git fetch\ngit branch -v -a\n\n...\nremotes/origin/test\ngit switch test\ngit fetch origin\ngit branch -v -a\ngit switch -c test origin/test\nman git-switch\ngit checkout test\ngit checkout -b test <name of remote>/test\n\nThe answer has been split depending on whether there is one remote repository configured or multiple. The reason for this is that for the single remote case, some of the commands can be simplified as there is less ambiguity.\nUpdated for Git 2.23: For older versions, see the section at the end.\nWith One Remote\nIn both cases, start by fetching from the remote repository to make sure you have all the latest changes downloaded.\n\nThis will fetch all of the remote branches for you. You can see the branches available for checkout with:\n\nThe branches that start with remotes/* can be thought of as read only copies of the remote branches. To work on a branch you need to create a local branch from it. This is done with the Git command switch (since Git 2.23) by giving it the name of the remote branch (minus the remote name):\n\nIn this case Git is guessing (can be disabled with --no-guess) that you are trying to checkout and track the remote branch with the same name.\nWith Multiple Remotes\nIn the case where multiple remote repositories exist, the remote repository needs to be explicitly named.\nAs before, start by fetching the latest remote changes:\n\nThis will fetch all of the remote branches for you. You can see the branches available for checkout with:\n\nWith the remote branches in hand, you now need to check out the branch you are interested in with -c to create a new local branch:\n\nFor more information about using git switch:\n\nPrior to Git 2.23\ngit switch was added in Git 2.23, prior to this git checkout was used to switch branches.\nTo checkout out with only a single remote repository:\n\nif there are multiple remote repositories configured then it becomes a bit longer"
  },
  {
    "question": "How do I make Git forget about a file that was tracked, but is now in .gitignore? I put a file that was previously being tracked by Git onto the .gitignore list. However, the file still shows up in git status after it is edited. How do I force Git to completely forget the file?",
    "answer": "git rm --cached <file>\ngit rm -r --cached <folder>\n\n.gitignore will prevent untracked files from being added (without an add -f) to the set of files tracked by Git. However, Git will continue to track any files that are already being tracked.\nUpdated Answer in 2024\nDo NOT use git rm --cached <file> if you ever want to see that file again. It will remove it from git, and also your local machine.\nIf you want to keep the file locally, but remove it from git tracking, use the answer by Konstantin. In short, use the following instead of git rm:\ngit update-index --skip-worktree <file>\nHowever, according to the official git documentation:\n\nUsers often try to use the assume-unchanged and skip-worktree bits to tell Git to ignore changes to files that are tracked. This does not work as expected, since Git may still check working tree files against the index when performing certain operations. In general, Git does not provide a way to ignore changes to tracked files, so alternate solutions are recommended.\n\nTherefore, you should still consider using the original answer below.\nOriginal Answer\nWARNING: This will remove the physical file from your local machine and other developers' machines on your or their next git pull.\nTo stop tracking a file, we must remove it from the index:\n\nTo remove a folder and all files in the folder recursively:\n\nThe removal of the file from the head revision will happen on the next commit."
  },
  {
    "question": "How do I remove local (untracked) files from the current Git working tree? How do I delete untracked local files from the current working tree?",
    "answer": "git clean [-d] [-f] [-i] [-n] [-q] [-e <pattern>] [-x | -X] [--] <path>\u2026\u200b\ngit clean -n -d\ngit clean -f\n\ngit-clean - Remove untracked files from the working tree\nSynopsis\n\nDescription\nCleans the working tree by recursively removing files that are not under version control, starting from the current directory.\nNormally, only files unknown to Git are removed, but if the -x option is specified, ignored files are also removed. This can, for example, be useful to remove all build products.\nIf any optional <path>... arguments are given, only those paths are affected.\n\nStep 1 is to show what will be deleted by using the -n option:\n\nClean Step - beware: this will delete files:\n\nTo remove directories, run git clean -f -d or git clean -fd\nTo remove ignored files, run git clean -f -X or git clean -fX\nTo remove ignored and non-ignored files, run git clean -f -x or git clean -fx\n\nNote the case difference on the X for the two latter commands.\nIf clean.requireForce is set to \"true\" (the default) in your configuration, one needs to specify -f otherwise nothing will actually happen.\nAgain see the git-clean docs for more information.\n\nOptions\n-f, --force\nIf the Git configuration variable clean.requireForce is not set to\nfalse, git clean will refuse to run unless given -f, -n or -i.\n-x\nDon\u2019t use the standard ignore rules read from .gitignore (per\ndirectory) and $GIT_DIR/info/exclude, but do still use the ignore\nrules given with -e options. This allows removing all untracked files,\nincluding build products. This can be used (possibly in conjunction\nwith git reset) to create a pristine working directory to test a clean\nbuild.\n-X\nRemove only files ignored by Git. This may be useful to rebuild\neverything from scratch, but keep manually created files.\n-n, --dry-run\nDon\u2019t actually remove anything, just show what would be done.\n-d\nRemove untracked directories in addition to untracked files. If an\nuntracked directory is managed by a different Git repository, it is\nnot removed by default. Use -f option twice if you really want to\nremove such a directory."
  },
  {
    "question": "How to modify existing, unpushed commit messages? I wrote the wrong thing in a commit message.\nHow can I change the message? The commit has not been pushed yet.",
    "answer": "git commit --amend\ngit commit --amend -m \"New commit message\"\ngit push <remote> <branch> --force\n\ngit push <remote> <branch> -f\n// n is the number of commits up to the last commit you want to be able to edit\ngit rebase -i HEAD~n\n\nAmending the most recent commit message\n\nwill open your editor, allowing you to change the commit message of the most recent commit. Additionally, you can set the commit message directly in the command line with:\n\n\u2026however, this can make multi-line commit messages or small corrections more cumbersome to enter.\nMake sure you don't have any working copy changes staged before doing this or they will get committed too. (Unstaged changes will not get committed.)\nChanging the message of a commit that you've already pushed to your remote branch\nIf you've already pushed your commit up to your remote branch, then - after amending your commit locally (as described above) - you'll also need to force push the commit with:\n\nWarning: force-pushing will overwrite the remote branch with the state of your local one. If there are commits on the remote branch that you don't have in your local branch, you will lose those commits.\nWarning: be cautious about amending commits that you have already shared with other people. Amending commits essentially rewrites them to have different SHA IDs, which poses a problem if other people have copies of the old commit that you've rewritten. Anyone who has a copy of the old commit will need to synchronize their work with your newly re-written commit, which can sometimes be difficult, so make sure you coordinate with others when attempting to rewrite shared commit history, or just avoid rewriting shared commits altogether.\n\nPerform an interactive rebase\nAnother option is to use interactive rebase.\nThis allows you to edit any message you want to update even if it's not the latest message.\nIn order to do a Git squash, follow these steps:\n\nOnce you squash your commits - choose the e/r for editing the message:\n\nImportant note about interactive rebase\nWhen you use git rebase -i HEAD~n there can be more than n commits. Git will \"collect\" all the commits in the last n commits, and if there was a merge somewhere in between that range you will see all the commits as well, so the outcome will be n + .\nGood tip:\nIf you have to do it for more than a single branch and you might face conflicts when amending the content, set up git rerere and let Git resolve those conflicts automatically for you.\n\nDocumentation\n\ngit-commit(1) Manual Page\n\ngit-rebase(1) Manual Page\n\ngit-push(1) Manual Page"
  },
  {
    "question": "How do I revert a Git repository to a previous commit? How do I revert from my current state to a snapshot made on a certain commit?\nIf I do git log, then I get the following output:\n\n```\n$ git log\ncommit a867b4af366350be2e7c21b8de9cc6504678a61b`\nAuthor: Me <me@me.com>\nDate:   Thu Nov 4 18:59:41 2010 -0400\n\nblah blah blah...\n\ncommit 25eee4caef46ae64aa08e8ab3f988bc917ee1ce4\nAuthor: Me <me@me.com>\nDate:   Thu Nov 4 05:13:39 2010 -0400\n\nmore blah blah blah...\n\ncommit 0766c053c0ea2035e90f504928f8df3c9363b8bd\nAuthor: Me <me@me.com>\nDate:   Thu Nov 4 00:55:06 2010 -0400\n\nAnd yet more blah blah...\n\ncommit 0d1d7fc32e5a947fbd92ee598033d85bfc445a50\nAuthor: Me <me@me.com>\nDate:   Wed Nov 3 23:56:08 2010 -0400\n\nYep, more blah blah.\n\n```\n\nHow do I revert to the commit from November 3, i.e. commit 0d1d7fc?",
    "answer": "git checkout 0d1d7fc32\ngit checkout -b old-state 0d1d7fc32\ngit reset --hard 0d1d7fc32\n\ngit stash\ngit reset --hard 0d1d7fc32\ngit stash pop\ngit log --merges --pretty=format:\"%h\" 0d1d7fc..HEAD | tr '\\n' ' '\n\ngit log --no-merges --pretty=format:\"%h\" 0d1d7fc..HEAD | tr '\\n' ' '\ngit revert a867b4af 25eee4ca 0766c053\n\ngit revert HEAD~2..HEAD\n\ngit revert 0d1d7fc..a867b4a\n\ngit revert -m 1 <merge_commit_sha>\n\ngit checkout 0d1d7fc32 .\n\ngit commit\n\nThis depends a lot on what you mean by \"revert\".\nTemporarily switch to a different commit\nIf you want to temporarily go back to it, fool around, then come back to where you are, all you have to do is check out the desired commit:\n\nOr if you want to make commits while you're there, go ahead and make a new branch while you're at it:\n\nTo go back to where you were, just check out the branch you were on again. (If you've made changes, as always when switching branches, you'll have to deal with them as appropriate. You could reset to throw them away; you could stash, checkout, stash pop to take them with you; you could commit them to a branch there if you want a branch there.)\nHard delete unpublished commits\nIf, on the other hand, you want to really get rid of everything you've done since then, there are two possibilities. One, if you haven't published any of these commits, simply reset:\n\nIf you mess up, you've already thrown away your local changes, but you can at least get back to where you were before by resetting again.\nUndo published commits with new commits\nOn the other hand, if you've published the work, you probably don't want to reset the branch, since that's effectively rewriting history. In that case, you could indeed revert the commits. In many enterprise organisations, the concept of \"protected\" branches will even prevent history from being rewritten on some major branches. In this case, reverting is your only option.\nWith Git, revert has a very specific meaning: create a commit with the reverse patch to cancel it out. This way you don't rewrite any history.\nFirst figure out what commits to revert. Depending on the technique chosen below, you want to either revert only the merge commits, or only the non-merge commits.\n\nNote: if you revert multiple commits, the order matters. Start with the most recent commit.\n\nThe git-revert manpage actually covers a lot of this in its description. Another useful link is this git-scm.com section discussing git-revert.\nIf you decide you didn't want to revert after all, you can revert the revert (as described here) or reset back to before the revert (see the previous section).\nYou may also find this answer helpful in this case:\nHow can I move HEAD back to a previous location? (Detached head) & Undo commits"
  },
  {
    "question": "How do I change the URI (URL) for a remote Git repository? I have a repo (origin) on a USB key that I cloned on my hard drive (local). I moved \"origin\" to a NAS and successfully tested cloning it from here.\nI would like to know if I can change the URI of \"origin\" in the settings of \"local\" so it will now pull from the NAS, and not from the USB key.\nFor now, I can see two solutions:\n\npush everything to the USB origin, and copy it to the NAS again (implies a lot of work due to new commits to NAS origin);\n\nadd a new remote to \"local\" and delete the old one (I fear I'll break my history).",
    "answer": "git remote -v\ngit remote set-url origin <NEW_GIT_URL_HERE>\n\nFirst, view the existing remotes to verify which URL is currently set:\n\nThen, you can set it with:\n\nSee git help remote. You also can edit .git/config and change the URLs there.\nYou're not in any danger of losing history unless you do something very silly (and if you're worried, just make a copy of your repo, since your repo is your history.)"
  },
  {
    "question": "Move the most recent commit(s) to a new branch with Git How do I move my recent commits on master to a new branch, and reset master to before those commits were made? e.g. From this:\n\n```\nmaster A - B - C - D - E\n\n```\n\nTo this:\n\n```\nnewbranch     C - D - E\n             /\nmaster A - B \n\n```",
    "answer": "git checkout existingbranch\ngit merge branchToMoveCommitFrom\ngit checkout branchToMoveCommitFrom\ngit reset --hard HEAD~3 # Go back 3 commits. You *will* lose uncommitted work.\ngit checkout existingbranch\ngit branch newbranch      # Create a new branch, saving the desired commits\ngit checkout master       # checkout master, this is the place you want to go back\ngit reset --hard HEAD~3   # Move master back by 3 commits (Make sure you know how many commits you need to go back)\ngit checkout newbranch    # Go to the new branch that still has the desired commits\ngit reset --hard a1b2c3d4\ngit push origin master --force\n\nWARNING: You need to store uncommitted edits to your stash before doing this, using git stash. Once complete, you can retrieve the stashed uncommitted edits with git stash pop. git reset hard command will remove all changes!\nMoving to an existing branch\nIf you want to move your commits to an existing branch, it will look like this:\n\nMoving to a new branch\nWARNING: This method works because you are creating a new branch with the first command: git branch newbranch. If you want to move commits to an existing branch you need to merge your changes into the existing branch before executing git reset --hard HEAD~3 (see Moving to an existing branch above). If you don't merge your changes first, they will be lost.\nUnless there are other circumstances involved, this can be easily done by branching and rolling back.\n\nBut do make sure how many commits to go back. Alternatively, you can instead of HEAD~3, simply provide the hash of the commit (or the reference like origin/master) you want to \"revert back to\" on the master (/current) branch, e.g:\n\nNote: You will only be \"losing\" commits from the master branch, but don't worry, you'll have those commits in newbranch! An easy way to check that, after completing the 4 step sequence of commands above, is by looking at git log -n4 which will show the history of newbranch actually retained the 3 commits (and the reason is that newbranch was created at the time those changes were already commited on master!). They have only been removed from master, as git reset only affected the branch that was checked out at the time of its execution, i.e. master (see git reset description: Reset current HEAD to the specified state). git status however will not show any checkouts on the newbranch, which might be surprising at first but that is actually expected.\nLastly, you may need to force push your latest changes to main repo:\n\nWARNING: With Git version 2.0 and later, if you later git rebase the new branch upon the original (master) branch, you may need an explicit --no-fork-point option during the rebase to avoid losing the carried-over commits.  Having branch.autosetuprebase always set makes this more likely.  See John Mellor's answer for details."
  },
  {
    "question": "How do I discard unstaged changes in Git? How do I discard changes in my working copy that are not in the index?",
    "answer": "git stash save --keep-index --include-untracked\n\nAnother quicker way is:\n\nYou don't need to include --include-untracked if you don't want to be thorough about it.\nAfter that, you can drop that stash with a git stash drop command if you like."
  },
  {
    "question": "Reset local repository branch to be just like remote repository HEAD How do I reset my local branch to be just like the branch on the remote repository?\nI tried:\n\n```\ngit reset --hard HEAD\n\n```\n\nBut git status claims I have modified files:\n\n```\nOn branch master\nChanges to be committed:\n  (use \"git reset HEAD <file>...\" to unstage)\n      modified:   java/com/mycompany/TestContacts.java\n      modified:   java/com/mycompany/TestParser.java\n\n```",
    "answer": "git fetch origin\ngit reset --hard origin/master\ngit commit -a -m \"Saving my work, just in case\"\ngit branch my-saved-work\n\nSetting your branch to exactly match the remote branch can be done in two steps:\n\nIf you want to save your current branch's state before doing this (just in case), you can do:\n\nNow your work is saved on the branch \"my-saved-work\" in case you decide you want it back (or want to look at it later or diff it against your updated branch).\nNote: the first example assumes that the remote repo's name is origin and that the branch named master in the remote repo matches the currently checked-out branch in your local repo, since that is in line with the example given in the question. If you are trying to reset to the default branch in a more recent repository, it is likely that it will be main.\nBTW, this situation that you're in looks an awful lot like a common case where a push has been done into the currently checked out branch of a non-bare repository. Did you recently push into your local repo? If not, then no worries -- something else must have caused these files to unexpectedly end up modified. Otherwise, you should be aware that it's not recommended to push into a non-bare repository (and not into the currently checked-out branch, in particular)."
  },
  {
    "question": "How can I reset or revert a file to a specific revision? How can I revert a modified file to its previous revision at a specific commit hash (which I determined via git log and git diff)?",
    "answer": "git checkout c5f567 -- file1/to/restore file2/to/restore\ngit checkout c5f567~1 -- file1/to/restore file2/to/restore\n\nAssuming the hash of the commit you want is c5f567:\n\nThe git checkout man page gives more information.\nIf you want to revert to the commit before c5f567, append ~1 (where 1 is the number of commits you want to go back, it can be anything):\n\nAs a side note, I've always been uncomfortable with this command because it's used for both ordinary things (changing between branches) and unusual, destructive things (discarding changes in the working directory).\nFor the meaning of -- in the command, refer to In Git, what does -- (dash dash) mean?\n\nThere is also a new git restore command that is specifically designed for restoring working copy files that have been modified. If your git is new enough you can use this command, but the documentation comes with a warning:\n\nTHIS COMMAND IS EXPERIMENTAL. THE BEHAVIOR MAY CHANGE.\n\nBecause git restore is experimental, it should not yet be promoted as the primary answer to this question. When the command is no longer marked as \"experimental\", then this answer can be amended to promote the use of git restore. [At the time of writing, the git restore command has been marked as \"experimental\" for at least four years.]"
  },
  {
    "question": "How do I push a new local branch to a remote Git repository and track it too? How do I:\n\nCreate a local branch from another branch (via git branch or git checkout -b).\n\nPush the local branch\nto the remote repository (i.e. publish), but make it\ntrackable so that git pull and git push will work.",
    "answer": "git checkout -b <branch>\ngit push -u origin <branch>\n\nIn Git 1.7.0 and later, you can checkout a new branch:\n\nEdit files, add and commit. Then push with the -u (short for --set-upstream) option:\n\nGit will set up the tracking information during the push."
  },
  {
    "question": "How do I squash my last N commits together? How do I squash my last N commits together into one commit?",
    "answer": "Use git rebase -i <after-this-commit> and replace \"pick\" on the second and subsequent commits with \"squash\" or \"fixup\", as described in the manual.\nIn this example, <after-this-commit> is either the SHA1 hash or the relative location from the HEAD of the current branch from which commits are analyzed for the rebase command. For example, if the user wishes to view 5 commits from the current HEAD in the past, the command is git rebase -i HEAD~5."
  },
  {
    "question": "How to determine the URL that a local Git repository was originally cloned from I pulled a project with several forks on GitHub, but forgot which fork it was. How do I determine which fork I pulled?",
    "answer": "git config --get remote.origin.url\ngit remote show origin\nC:\\Users\\jaredpar\\VsVim> git remote show origin\n* remote origin\nFetch URL: git@github.com:jaredpar/VsVim.git\nPush  URL: git@github.com:jaredpar/VsVim.git\nHEAD branch: master\nRemote branches:\n\nTo obtain only the remote URL:\n\nIf you require full output, and you are on a network that can reach the remote repo where the origin resides:\n\nWhen using git clone (from GitHub, or any source repository for that matter) the default name for the source of the clone is \"origin\". Using git remote show will display the information about this remote name. The first few lines should show:\n\nIf you want to use the value in a script, you would use the first command listed in this answer."
  },
  {
    "question": "How do I add an empty directory to a Git repository? How do I add an empty directory (that contains no files) to a Git repository?",
    "answer": "*\n\n!.gitignore\n\nAnother way to make a directory stay (almost) empty (in the repository) is to create a .gitignore file inside that directory that contains these four lines:\n\nThen you don't have to get the order right the way that you have to do in m104's solution.\nThis also gives the benefit that files in that directory won't show up as \"untracked\" when you do a git status.\nMaking @GreenAsJade's comment persistent:\n\nI think it's worth noting that this solution does precisely what the question asked for, but is not perhaps what many people looking at this question will have been looking for. This solution guarantees that the directory remains empty. It says \"I truly never want files checked in here\". As opposed to \"I don't have any files to check in here, yet, but I need the directory here, files may be coming later\"."
  },
  {
    "question": "How do I resolve merge conflicts in a Git repository? How do I resolve merge conflicts in my Git repository?",
    "answer": "git mergetool\ngit config merge.tool vimdiff\ngit config merge.conflictstyle diff3\ngit config mergetool.prompt false\ngit mergetool\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551       \u2551      \u2551        \u2551\n\u2551 LOCAL \u2551 BASE \u2551 REMOTE \u2551\n\u2551       \u2551      \u2551        \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551                       \u2551\n\u2551        MERGED         \u2551\n\u2551                       \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n:diffg RE\n:diffg BA\n:diffg LO\n\nTry:\n\nIt opens a GUI that steps you through each conflict, and you get to choose how to merge.  Sometimes it requires a bit of hand editing afterwards, but usually it's enough by itself.  It is much better than doing the whole thing by hand certainly.\n\nAs per Josh Glover's comment:\n\n[This command]\ndoesn't necessarily open a GUI unless you install one. Running git mergetool for me resulted in vimdiff being used. You can install\none of the following tools to use it instead: meld, opendiff,\nkdiff3, tkdiff, xxdiff, tortoisemerge, gvimdiff, diffuse,\necmerge, p4merge, araxis, vimdiff, emerge.\n\nBelow is a sample procedure using vimdiff to resolve merge conflicts, based on this link.\n\nRun the following commands in your terminal\n\nThis will set vimdiff as the default merge tool.\n\nRun the following command in your terminal\n\nYou will see a vimdiff display in the following format:\n\nThese 4 views are\n\nLOCAL: this is the file from the current branch\nBASE: the common ancestor, how this file looked before both changes\nREMOTE: the file you are merging into your branch\nMERGED: the merge result; this is what gets saved in the merge commit and used in the future\n\nYou can navigate among these views using ctrl+w. You can directly reach the MERGED view using ctrl+w followed by j.\nMore information about vimdiff navigation is here and here.\n\nYou can edit the MERGED view like this:\n\nIf you want to get changes from REMOTE\n\nIf you want to get changes from BASE\n\nIf you want to get changes from LOCAL\n\nSave, Exit, Commit, and Clean up\n:wqa save and exit from vi\ngit commit -m \"message\"\ngit clean Remove extra files (e.g. *.orig). Warning: It will remove all untracked files, if you won't pass any arguments."
  },
  {
    "question": "How can I delete a remote tag? How can I delete a Git tag that has already been pushed?",
    "answer": "git push origin :tagname\ngit push --delete origin tagname\ngit push origin :refs/tags/tagname\ngit tag --delete tagname\ngit tag -d tagname\ngit push remote-repo source-ref:destination-ref\ngit push origin refs/heads/master:refs/heads/master\ngit push origin master:master\ngit push origin refs/tags/release-1.0:refs/tags/release-1.0\ngit push origin release-1.0:release-1.0\n\nYou can push an 'empty' reference to the remote tag name:\n\nOr, more expressively, use the --delete option (or -d if your git version is older than 1.8.0):\n\nNote that git has tag namespace and branch namespace so you may use the same name for a branch and for a tag. If you want to make sure that you cannot accidentally remove the branch instead of the tag, you can specify full ref which will never delete a branch:\n\nIf you also need to delete the local tag, use:\n\nor\n\nBackground\nPushing a branch, tag, or other ref to a remote repository involves specifying \"which repo, what source, what destination?\"\n\nA real world example where you push your master branch to the origin's master branch is:\n\nWhich because of default paths, can be shortened to:\n\nTags work the same way:\n\nWhich can also be shortened to:\n\nBy omitting the source ref (the part before the colon), you push 'nothing' to the destination, deleting the ref on the remote end."
  },
  {
    "question": "Undo a Git merge that hasn&#39;t been pushed yet I accidentally ran git merge some_other_branch on my local master branch. I haven't pushed the changes to origin master. How do I undo the merge?\n\nAfter merging, git status says:\n\n```\n# On branch master\n# Your branch is ahead of 'origin/master' by 5 commits.\n\n```\n\nHow do I undo all these commits?",
    "answer": "git reset --hard commit_sha\ngit reset --hard HEAD~1\ngit reset --hard ORIG_HEAD\ngit reset --merge ORIG_HEAD\n\nWith git reflog check which commit is one prior the merge (git reflog will be a better option than git log). Then you can reset it using:\n\nThere's also another way:\n\nIt will get you back 1 commit.\nBe aware that any modified and uncommitted/unstashed files will be reset to their unmodified state. To keep them either stash changes away or see --merge option below.\n\nAs @Velmont suggested below in his answer, in this direct case using:\n\nmight yield better results, as it should preserve your changes. ORIG_HEAD will point to a commit directly before merge has occurred, so you don't have to hunt for it yourself.\n\nA further tip is to use the --merge switch instead of --hard since it doesn't reset files unnecessarily:\n\n--merge\nResets the index and updates the files in the working tree that are different between <commit> and HEAD, but keeps those which are different between the index and working tree (i.e. which have changes which have not been added)."
  },
  {
    "question": "How do I clone all remote branches? My master and development branches are tracked remotely on GitHub. How do I clone both these branches?",
    "answer": "git clone git://example.com/myproject\ncd myproject\ngit branch\n* master\ngit branch -a\n* master\nremotes/origin/HEAD\nremotes/origin/master\nremotes/origin/v1.0-stable\nremotes/origin/experimental\ngit checkout origin/experimental\ngit checkout experimental\n\nBranch experimental set up to track remote branch experimental from origin.\nSwitched to a new branch 'experimental'\ngit branch\n* experimental\nmaster\ngit remote add win32 git://example.com/users/joe/myproject-win32-port\ngit branch -a\n* master\nremotes/origin/HEAD\nremotes/origin/master\nremotes/origin/v1.0-stable\nremotes/origin/experimental\nremotes/win32/master\nremotes/win32/new-widgets\ngitk --all &\n\nFirst, clone a remote Git repository and cd into it:\n\nNext, look at the local branches in your repository:\n\nBut there are other branches hiding in your repository! See these using the -a flag:\n\nTo take a quick peek at an upstream branch, check it out directly:\n\nTo work on that branch, create a local tracking branch, which is done automatically by:\n\nHere, \"new branch\" simply means that the branch is taken from the index and created locally for you.  As the previous line tells you, the branch is being set up to track the remote branch, which usually means the origin/branch_name branch.\nYour local branches should now show:\n\nYou can track more than one remote repository using git remote:\n\nAt this point, things are getting pretty crazy, so run gitk to see what's going on:"
  },
  {
    "question": "How do I update or sync a forked repository on GitHub? I forked a project, made changes, and created a pull request which was accepted. New commits were later added to the repository. How do I get those commits into my fork?",
    "answer": "git remote add upstream https://github.com/whoever/whatever.git\n\ngit fetch upstream\n\ngit checkout main\n\ngit rebase upstream/main\ngit push -f origin main\n\nIn your local clone of your forked repository, you can add the original GitHub repository as a \"remote\".  (\"Remotes\" are like nicknames for the URLs of repositories - origin is one, for example.)  Then you can fetch all the branches from that upstream repository, and rebase your work to continue working on the upstream version.  In terms of commands that might look like:\n\nIf you don't want to rewrite the history of your main branch, (for example because other people may have cloned it) then you should replace the last command with git merge upstream/main.  However, for making further pull requests that are as clean as possible, it's probably better to rebase.\n\nIf you've rebased your branch onto upstream/main you may need to force the push in order to push it to your own forked repository on GitHub.  You'd do that with:\n\nYou only need to use the -f the first time after you've rebased."
  },
  {
    "question": "How do I remove a submodule? How do I remove a Git submodule?\nWhy can't I do\ngit submodule rm module_name?",
    "answer": "0. mv a/submodule a/submodule_tmp\n\n1. git submodule deinit -f -- a/submodule\n2. rm -rf .git/modules/a/submodule\n3. git rm -f a/submodule\n\n3.   git rm --cached a/submodule\n3bis mv a/submodule_tmp a/submodule\ngit add mysubmodule/file.txt\nPath 'mysubmodule/file.txt' is in submodule 'mysubmodule'\ngit ${wt_prefix:+-C \"$wt_prefix\"} submodule--helper deinit \\\n{GIT_QUIET:+--quiet} \\\n{prefix:+--prefix \"$prefix\"} \\\n{force:+--force} \\\n{deinit_all:+--all} \"$@\"\n\nSince git1.8.3 (April 22d, 2013):\n\nThere was no Porcelain way to say \"I no longer am interested in this submodule\", once you express your interest in a submodule with \"git submodule init\".\n\"git submodule deinit\" is the way to do so.\n\nThe deletion process also uses git rm (since git1.8.5 October 2013).\nSummary\nThe 3-steps removal process would then be:\n\nExplanation\nrm -rf: This is mentioned in Daniel Schroeder's answer, and summarized by Eonil in the comments:\n\nThis leaves .git/modules/<path-to-submodule>/ unchanged.\nSo if you once delete a submodule with this method and re-add them again, it will not be possible because repository already been corrupted.\n\ngit rm: See commit 95c16418:\n\nCurrently using \"git rm\" on a submodule removes the submodule's work tree from that of the superproject and the gitlink from the index.\nBut the submodule's section in .gitmodules is left untouched, which is a leftover of the now removed submodule and might irritate users (as opposed to the setting in .git/config, this must stay as a reminder that the user showed interest in this submodule so it will be repopulated later when an older commit is checked out).\n\nLet \"git rm\" help the user by not only removing the submodule from the work tree but by also removing the \"submodule.<submodule name>\" section from the .gitmodules file and stage both.\n\ngit submodule deinit: It stems from this patch:\n\nWith \"git submodule init\" the user is able to tell git they care about one or more submodules and wants to have it populated on the next call to \"git submodule update\".\nBut currently there is no easy way they can tell git they do not care about a submodule anymore and wants to get rid of the local work tree (unless the user knows a lot about submodule internals and removes the \"submodule.$name.url\" setting from .git/config together with the work tree himself).\n\nHelp those users by providing a 'deinit' command.\nThis removes the whole submodule.<name> section from .git/config either for the given\nsubmodule(s) (or for all those which have been initialized if '.' is given).\nFail if the current work tree contains modifications unless forced.\nComplain when for a submodule given on the command line the url setting can't be found in .git/config, but nonetheless don't fail.\n\nThis takes care if the (de)initialization steps (.git/config and .git/modules/xxx)\nSince git1.8.5, the git rm takes also care of the:\n\n'add' step which records the url of a submodule in the .gitmodules file: it is need to removed for you.\nthe submodule special entry (as illustrated by this question): the git rm removes it from the index:\ngit rm --cached path_to_submodule (no trailing slash)\nThat will remove that directory stored in the index with a special mode \"160000\", marking it as a submodule root directory.\n\nIf you forget that last step, and try to add what was a submodule as a regular directory, you would get error message like:\n\nNote: since Git 2.17 (Q2 2018), git submodule deinit is no longer a shell script.\nIt is a call to a C function.\nSee commit 2e61273, commit 1342476 (14 Jan 2018) by Prathamesh Chavan (pratham-pc).\n(Merged by Junio C Hamano -- gitster -- in commit ead8dbe, 13 Feb 2018)"
  },
  {
    "question": "How do I delete a commit from a branch? How do I delete a commit from my branch history? Should I use git reset --hard HEAD?",
    "answer": "git reset --hard HEAD~1\ngit reset --hard <sha1-commit-id>\ngit push origin HEAD --force\n\nCareful: git reset --hard WILL DELETE YOUR WORKING DIRECTORY CHANGES.\nBe sure to stash any local changes you want to keep before running this command.\nAssuming you are sitting on that commit, then this command will wack it...\n\nThe HEAD~1 means the commit before head.\nOr, you could look at the output of git log, find the commit id of the commit you want to back up to, and then do this:\n\nIf you already pushed it, you will need to do a force push to get rid of it...\n\nHowever, if others may have pulled it, then you would be better off starting a new branch.  Because when they pull, it will just merge it into their work, and you will get it pushed back up again.\nIf you already pushed, it may be better to use git revert, to create a \"mirror image\" commit that will undo the changes.  However, both commits will be in the log.\n\nFYI: git reset --hard HEAD is great if you want to get rid of WORK IN PROGRESS.It will reset you back to the most recent commit, and erase all the changes in your working tree and index.\ngit stash does the same except you can restore it later if you need, versus permanently delete with reset hard mode. Check your stashes by using git stash list and git stash show 'stash@123'\n\nLastly, if you need to find a commit that you \"deleted\", it is typically present in git reflog unless you have garbage collected your repository."
  },
  {
    "question": "Undoing a git rebase How do I easily undo a git rebase? A lengthy manual method is:\n\ncheckout the commit parent to both of the branches\ncreate and checkout a temporary branch\ncherry-pick all commits by hand\nreset the faulty rebased branch to point to the temporary branch\n\nIn my current situation, this works because I can easily spot commits from both branches (one was my stuff, the other was my colleague's stuff). However, my approach strikes me as suboptimal and error-prone (let's say I had just rebased with two of my own branches).\nClarification: I am talking about a rebase during which multiple commits were replayed, not only one.",
    "answer": "git reflog\ngit reset --soft \"HEAD@{2}\"\n[core]\nlogAllRefUpdates = true\n\nThe easiest way would be to find the head commit of the branch as it was immediately before the rebase started in the reflog...\n\nand to reset the current branch to it.\nSuppose the old commit was HEAD@{2} in the ref log:\n\n(If you do not want to retain the working copy changes, you can use --hard instead of --soft)\nYou can check the history of the candidate old head by just doing a git log \"HEAD@{2}\".\nIf you've not disabled per branch reflogs you should be able to simply do git reflog \"branchname@{1}\" as a rebase detaches the branch head before reattaching to the final head. I would double-check this behavior, though, as I haven't verified it recently.\nPer default, all reflogs are activated for non-bare repositories:"
  },
  {
    "question": "How do I get the current branch name in Git? How do I get the name of the current branch in Git?",
    "answer": "git branch\ngit rev-parse --abbrev-ref HEAD\ngit branch --show-current\n\nshould show all the local branches of your repo. The starred branch is your current branch.\n\nTo retrieve only the name of the branch you are on:\n\nVersion 2.22 adds the --show-current option to \u201dprint the name of the current branch\u201d. The combination also works for freshly initialized repositories before the first commit:"
  },
  {
    "question": "How do I clone a specific Git branch? Git clone will clone remote branch into local.\nIs there any way to clone a specific branch by myself without switching branches on the remote repository?",
    "answer": "git clone --single-branch --branch <branchname> <remote-repo>\n\nThe --single-branch option is valid from version 1.7.10 and later.\nPlease see also the other answer which many people prefer.\nYou may also want to make sure you understand the difference. And the difference is: by invoking git clone --branch <branchname> url you're fetching all the branches and checking out one. That may, for instance, mean that your repository has a 5kB documentation or wiki branch and 5GB data branch. And whenever you want to edit your frontpage, you may end up cloning 5GB of data.\nAgain, that is not to say git clone --branch is not the way to accomplish that, it's just that it's not always what you want to accomplish, when you're asking about cloning a specific branch."
  },
  {
    "question": "Make an existing Git branch track a remote branch? I know how to make a new branch that tracks remote branches, but how do I make an existing branch track a remote branch?\nI know I can just edit the .git/config file, but it seems there should be an easier way.",
    "answer": "git branch -u upstream/foo\ngit branch -u upstream/foo foo\ngit branch --set-upstream-to=upstream/foo\n\ngit branch --set-upstream-to=upstream/foo foo\ngit branch --set-upstream foo upstream/foo\n\nGiven a branch foo and a remote upstream:\nAs of Git 1.8.0:\n\nOr, if local branch foo is not the current branch:\n\nOr, if you like to type longer commands, these are equivalent to the above two:\n\nAs of Git 1.7.0 (before 1.8.0):\n\nNotes:\n\nAll of the above commands will cause local branch foo to track remote branch foo from remote upstream.\nThe old (1.7.x) syntax is deprecated in favor of the new (1.8+) syntax.  The new syntax is intended to be more intuitive and easier to remember.\nDefining an upstream branch will fail when run against newly-created remotes that have not already been fetched. In that case, run git fetch upstream beforehand.\n\nSee also: Why do I need to do `--set-upstream` all the time?"
  },
  {
    "question": "Remove a file from a Git repository without deleting it from the local filesystem I want to remove a file from my repository.\n\n```\ngit rm file_to_remove.txt\n\n```\n\nwill remove the file from the repository, but it will also remove the file from the local file system. How do I remove this file from the repo without deleting my local copy of the file?",
    "answer": "git rm --cached file_to_remove.txt\ngit rm --cached -r directory_to_remove\n\nThe git rm documentation states:\n\nWhen --cached is given, the staged content has to match either the tip of the branch or the file on disk, allowing the file to be removed from just the index.\n\nSo, for a single file:\n\nand for a single directory:"
  },
  {
    "question": "Move existing, uncommitted work to a new branch in Git I started some work on a new feature and after coding for a bit, I decided this feature should be on its own branch. \nHow do I move the existing uncommitted changes to a new branch and reset my current one?\nI want to reset my current branch while preserving existing work on the new feature.",
    "answer": "git switch -c <new-branch>\ngit checkout -b <new-branch>\ngit add <files>\ngit commit -m \"<Brief description of this commit>\"\n\nUpdate 2020 / Git 2.23\nGit 2.23 adds the new switch subcommand in an attempt to clear some of the confusion that comes from the overloaded usage of checkout (switching branches, restoring files, detaching HEAD, etc.)\nStarting with this version of Git, replace the checkout command with:\n\nThe behavior is identical and remains unchanged.\n\nBefore Update 2020 / Git 2.23\nUse the following:\n\nThis will leave your current branch as it is, create and checkout a new branch and keep all your changes. You can then stage changes in files to commit with:\n\nand commit to your new branch with:\n\nThe changes in the working directory and changes staged in index do not belong to any branch yet. This changes the branch where those modifications would end in.\nYou don't reset your original branch, it stays as it is. The last commit on <old-branch> will still be the same. Therefore you checkout -b and then commit."
  },
  {
    "question": "View the change history of a file using Git versioning How do I view the history of an individual file with complete details of what has changed?\ngit log -- [filename] shows me the commit history of a file, but how do I see the file content that changed?",
    "answer": "gitk [filename]\ngitk --follow [filename]\n\nFor a graphical view, use gitk:\n\nTo follow the file across file renames:"
  },
  {
    "question": "How do I stash only one file out of multiple files that have changed? How do I stash only one of the multiple changed files on my branch?",
    "answer": "git stash push -p -m \"my commit message\"\ny - stash this hunk\nn - do not stash this hunk\nq - quit; do not stash this hunk or any of the remaining ones\na - stash this hunk and all later hunks in the file\nd - do not stash this hunk or any of the later hunks in the file\ng - select a hunk to go to\n/ - search for a hunk matching the given regex\nj - leave this hunk undecided, see next undecided hunk\nJ - leave this hunk undecided, see next hunk\nk - leave this hunk undecided, see previous undecided hunk\nK - leave this hunk undecided, see previous hunk\ns - split the current hunk into smaller hunks\ne - manually edit the current hunk\n? - print help\n\n-p let's you select the hunks that should be stashed; whole files can be selected as well.\nYou'll be prompted with a few actions for each hunk:"
  },
  {
    "question": "How do I create a remote Git branch? I created a local branch. How do I push it to the remote server?\nUPDATE: I have written a simpler answer for Git 2.0 here.",
    "answer": "git push -u\ngit config --global push.default current\n\nSimple Git 2.0+ solution:\nAs of Git 2.0, the behavior has become simpler:\nYou can configure git with push.default = current to make life easier:\nI added this so now I can just push a new branch upstream with\n\n-u will track remote branch of the same name. Now with this configuration, you will auto-guess the remote reference to git push. From git.config documentation:\n\npush.default\nDefines the action git push should take if no refspec is explicitly given.\npush.default = current - push the current branch to update a branch with the\nsame name on the receiving end. Works in both central and non-central workflows.\n\nFor me, this is a good simplification of my day-to-day Git workflow. The configuration setting takes care of the 'usual' use case where you add a branch locally and want to create it remotely. Also, I can just as easily create local branches from remotes by just doing git co remote_branch_name (as opposed to using --set-upstream-to flag).\nI know this question and the accepted answers are rather old, but the behavior has changed so that now configuration options exist to make your workflow simpler.\nTo add to your global Git configuration, run this on the command line:"
  },
  {
    "question": "Commit only part of a file&#39;s changes in Git When I make changes to a file in Git, how can I commit only some of the changes?\nFor example, how could I commit only 15 lines out of 30 lines that have been changed in a file?",
    "answer": "git add --patch <filename>\ngit add -p <filename>\nStage this hunk [y,n,q,a,d,/,j,J,g,s,e,?]?\n\nYou can use:\n\nor for short:\n\nGit will break down your file into what it thinks are sensible \"hunks\" (portions of the file). It will then prompt you with this question:\n\nHere is a description of each option:\n\ny stage this hunk for the next commit\nn do not stage this hunk for the next commit\nq quit; do not stage this hunk or any of the remaining hunks\na stage this hunk and all later hunks in the file\nd do not stage this hunk or any of the later hunks in the file\ng select a hunk to go to\n/ search for a hunk matching the given regex\nj leave this hunk undecided, see next undecided hunk\nJ leave this hunk undecided, see next hunk\nk leave this hunk undecided, see previous undecided hunk\nK leave this hunk undecided, see previous hunk\ns split the current hunk into smaller hunks\ne manually edit the current hunk\n\nYou can then edit the hunk manually by replacing +/- by # (thanks veksen)\n\n? print hunk help\n\nIf the file is not in the repository yet, you can first do git add -N <filename>. Afterwards you can go on with git add -p <filename>.\nAfterwards, you can use:\n\ngit diff --staged to check that you staged the correct changes\ngit reset -p to unstage mistakenly added hunks\ngit commit -v to view your commit while you edit the commit message.\n\nNote this is far different than the git format-patch command, whose purpose is to parse commit data into a .patch files.\nReference for future: Git Tools - Interactive Staging"
  },
  {
    "question": "How do I list all the files in a commit? How can I print a plain list of all files that were part of a given commit?\nAlthough the following lists the files, it also includes unwanted diff information for each:\n\n```\ngit show a303aa90779efdd2f6b9d90693e2cbbbe4613c1d\n\n```",
    "answer": "git diff-tree --no-commit-id --name-only bd61ad98 -r\nindex.html\njavascript/application.js\njavascript/ie6.js\ngit show --pretty=\"\" --name-only bd61ad98\nindex.html\njavascript/application.js\njavascript/ie6.js\n\nPreferred Way (because it's a plumbing command; meant to be programmatic):\n\nAnother Way (less preferred for scripts, because it's a porcelain command; meant to be user-facing)\n\nThe --no-commit-id suppresses the commit ID output.\nThe --pretty argument specifies an empty format string to avoid the cruft at the beginning.\nThe --name-only argument shows only the file names that were affected (Thanks Hank). Use --name-status instead, if you want to see what happened to each file (Deleted, Modified, Added)\nThe -r argument is to recurse into sub-trees"
  },
  {
    "question": "How do you push a tag to a remote repository using Git? I added a tag to the master branch on my machine:\n\n```\ngit tag mytag master\n\n```\n\nHow do I push this to the remote repository? Running git push gives the message:\n\nEverything up-to-date\n\nHowever, the remote repository does not contain my tag.",
    "answer": "To push specific, one tag do following\ngit push origin tag_name"
  },
  {
    "question": "Git is not working after macOS update (&quot;xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools&quot;) I updated to the latest OS, and/or restarted my computer (this happens on every major update, but this time all I did was restart my computer on 2022-09-13).\nThis morning I navigated to my work's codebase in the command line on my MacBook Pro, typed in \"git status\" in the repository and received an error:\n(In 9/2022, this error was much different, but I didn't capture it)\n\nxcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun\n\nGit will not work!\nHow do I fix Git, and the command-line tools?",
    "answer": "xcode-select --install\nxcode-select: note: install requested for command line developer tools\n\nThe problem is that Xcode Command-line Tools needs to be updated due to a MacOs update.\n\nDid not run into this on Sonoma.\n\nMaybe Apple fixed the process?\n\nUpdated for Ventura\n\nAfter opening the terminal after restarting, I tried to go to my code, and do a git status, and I got an error and prompt for command line software agreement:\nSo press space until you get to the [agree, print, cancel] option, so careful hit space to scroll down to the end, if you blow past It you have to run a command to get it back. Use sudo xcodebuild -license to get to it again.\nJust be careful on scrolling down and enter agree and press return and it will launch into an update.\n\nThen I tried to use git after the install, and it prompted me to install Xcode tools again.\nI followed my own advice from previous years (see below), and went to https://developer.apple.com/download/all and downloaded\n\"Command Line Tools for Xcode 14\" (You have to log in with your Apple ID and enter MFA code, so have all the devices you need for that handy. Then select \"Command Line Tools for Xcode 14\", or if you want to get into the alphas or betas, that's up to you. But stable releases are probably the best choice for software developers.\n\nYou have to either download the tools from CLI or the developer page and before you can use git, you need to reboot!!! Or you will get stuck in a loop of prompt & downloading\nRebooting will break the loop and complete the installation of your CLI tools including git so that you can get back to work\nSolutions for previous years, these may or may not be valid these days as the downloads page has changed significantly:\nPREVIOUS YEARS SOLUTIONS, probably #2 is most helpful.\n*** Solution #1:\nGo back to your terminal and enter:\n\nYou'll then receive the following output:\n\nYou will then be prompted in a window to update Xcode Command Line tools. (which could take a while)\nOpen a new terminal window and your development tools should be returned.\nAddition: With any major or semi-major update you'll need to update the command line tools in order to get them functioning properly again. Check Xcode with any update. This goes beyond Mojave...\nAfter that restart your terminal\nAlternatively, IF that fails, and it might.... you'll get a pop-up box saying \"Software not found on server\", proceed to solution 2.\n*** Solution #2: (Preferred method)\nIf you hit xcode-select --install and it doesn't find the software, log into Apple Developer, and install it via webpage.\nLog in or sign up here:\nhttps://developer.apple.com/download/more/\nLook for: \"Command Line Tools for Xcode 14.x\" in the list of downloads\nThen click the dmg and download. (See previous image above) either way, you will probably wind up at an apple downloads webpage."
  },
  {
    "question": "Difference between &quot;git add -A&quot; and &quot;git add .&quot; What is the difference between git add [--all | -A] and git add .?",
    "answer": "git init\necho Change me > change-me\necho Delete me > delete-me\ngit add change-me delete-me\ngit commit -m initial\n\necho OK >> change-me\nrm delete-me\necho Add me > add-me\n\ngit status\n\ngit add .\ngit status\n\ngit reset\n\ngit add -u\ngit status\n\ngit reset\n\ngit add -A\ngit status\n\nThis answer only applies to Git version 1.x. For Git version 2.x, see other answers.\n\nSummary:\n\ngit add -A stages all changes\n\ngit add . stages new files and modifications, without deletions (on the current directory and its subdirectories).\n\ngit add -u stages modifications and deletions, without new files\n\nDetail:\ngit add -A is equivalent to  git add .; git add -u.\nThe important point about git add . is that it looks at the working tree and adds all those paths to the staged changes if they are either changed or are new and not ignored, it does not stage any 'rm' actions.\ngit add -u looks at all the already tracked files and stages the changes to those files if they are different or if they have been removed. It does not add any new files, it only stages changes to already tracked files.\ngit add -A is a handy shortcut for doing both of those.\nYou can test the differences out with something like this (note that for Git version 2.x your output for git add . git status will be different):"
  },
  {
    "question": "How do I make git use the editor of my choice for editing commit messages? How do I globally configure git to use a particular editor (e.g. vim) for commit messages?",
    "answer": "git config --global core.editor \"vim\"\nexport GIT_EDITOR=vim\nexport VISUAL=vim\nexport EDITOR=\"$VISUAL\"\nexport VISUAL=\"subl --wait\"\nexport VISUAL=\"code --wait\"\n\nSetting the default editor for Git\nPick one:\n\nSet core.editor in your Git config:\n\nSet the GIT_EDITOR environment variable:\n\nSetting the default editor for all programs\nSet the standardized VISUAL and EDITOR environment variables*:\n\nNOTE: Setting both is not necessarily needed, but some programs may not use the more-correct VISUAL. See VISUAL vs. EDITOR.\n\nFixing compatibility issues\nSome editors require a --wait flag, or they will open a blank page. For example:\n\nSublime Text (if correctly set up; or use the full path to the executable in place of subl):\n\nVS Code (after adding the shell command):"
  },
  {
    "question": "What does cherry-picking a commit with Git mean? What does git cherry-pick <commit> do?",
    "answer": "git switch master\ngit cherry-pick <commit-hash>\ngit cherry-pick -x <commit-hash>\ngit notes copy <from> <to>\n\nCherry-picking in Git means choosing a commit from one branch and applying it to another.\nThis contrasts with other ways such as merge and rebase which normally apply many commits to another branch.\nIt's also possible to cherry-pick multiple commits but merge is the preferred way over cherry-picking.\n\nMake sure you are on the branch you want to apply the commit to.\n\nExecute the following:\n\nN.B.:\n\nIf you cherry-pick from a public branch, you should consider using\n\nThis will generate a standardized commit message. This way, you (and your co-workers) can still keep track of the origin of the commit and may avoid merge conflicts in the future.\n\nIf you have notes attached to the commit they do not follow the cherry-pick. To bring them over as well, You have to use:\n\nAdditional links:\n\ngit official guide page\ngit cherry-pick guide"
  },
  {
    "question": "How do I modify a specific commit? I have the following commit history:\n\nHEAD\nHEAD~\nHEAD~2\nHEAD~3\n\ngit commit --amend modifies the current HEAD commit. But how do I modify HEAD~3?",
    "answer": "git rebase --interactive bbc643cd~\ngit commit --all --amend --no-edit\ngit rebase --continue\n\nUse git rebase. For example, to modify commit bbc643cd, run:\n\nPlease note the tilde ~ at the end of the command, because you need to reapply commits on top of the previous commit of bbc643cd (i.e. bbc643cd~).\nIn the default editor, modify pick to edit in the line mentioning bbc643cd.\nSave the file and exit. git will interpret and automatically execute the commands in the file. You will find yourself in the previous situation in which you just had created commit bbc643cd.\nAt this point, bbc643cd is your last commit and you can easily amend it. Make your changes and then commit them with the command:\n\nAfter that, return back to the previous HEAD commit using:\n\nWARNING: Note that this will change the SHA-1 of that commit as well as all children -- in other words, this rewrites the history from that point forward. You can break repos doing this if you push using the command git push --force."
  },
  {
    "question": "How can I change the commit author for a single commit? I want to change the author of one specific commit in the history. It's not the latest commit.\nRelated: How do I change the author and committer name/email for multiple commits?",
    "answer": "git commit --amend --author=\"Author Name <email@address.com>\" --no-edit\n\nInteractive rebase off of a point earlier in the history than the commit you need to modify (git rebase -i <earliercommit>). In the list of commits being rebased, change the text from pick to edit next to the hash of the one you want to modify. Then when git prompts you to change the commit, use this:\n\nFor example, if your commit history is A-B-C-D-E-F with F as HEAD, and you want to change the author of C and D, then you would...\n\nSpecify git rebase -i B (here is an example of what you will see after executing the git rebase -i B command)\n\nif you need to edit A, use git rebase -i --root\n\nChange the lines for both C and D from pick to edit\nExit the editor (for vim, this would be pressing Esc and then typing :wq).\nOnce the rebase started, it would first pause at C\nYou would git commit --amend --author=\"Author Name <email@address.com>\"\nThen git rebase --continue\nIt would pause again at D\nThen you would git commit --amend --author=\"Author Name <email@address.com>\" again\ngit rebase --continue\nThe rebase would complete.\nUse git push -f to update your origin with the updated commits."
  },
  {
    "question": "How do I find and restore a deleted file in a Git repository? Say I'm in a Git repository. I delete a file and commit that change. I continue working and make some more commits. Then, I discover that I need to restore that file after deleting it.\nI know I can checkout a file using git checkout <commit> -- filename.txt, but I don't know when that file was deleted.\n\nHow do I find the commit that deleted a given filename?\nHow do I restore that file back into my working copy?",
    "answer": "git rev-list -n 1 HEAD -- <file_path>\ngit checkout <deleting_commit>^ -- <file_path>\ngit checkout $(git rev-list -n 1 HEAD -- \"$file\")^ -- \"$file\"\ngit checkout $(git rev-list -n 1 HEAD -- \"$file\")~1 -- \"$file\"\n\nFind the last commit that affected the given path. As the file isn't in the HEAD commit, that previous commit must have deleted it.\n\nThen checkout the version at the commit before, using the caret (^) symbol:\n\nOr in one command, if $file is the file in question.\n\nIf you are using zsh and have the EXTENDED_GLOB option enabled, the caret symbol won't work. You can use ~1 instead."
  },
  {
    "question": "`git fetch` a remote branch The remote repository contains various branches such as origin/daves_branch:\n\n```\n$ git branch -r\norigin/HEAD -> origin/master\norigin/daves_branch\norigin/master\n\n```\n\nHow do I switch to daves_branch in the local repository so that it tracks origin/daves_branch?\nI tried:\n\n```\n$ git fetch origin daves_branch\n$ git checkout daves_branch\n\n```",
    "answer": "git switch daves_branch\ngit checkout --track origin/daves_branch\ngit checkout --track -b daves_branch origin/daves_branch\ngit checkout daves_branch\n\nUpdate: Using Git Switch\nAll of the information written below was accurate, but a new command, git switch has been added that simplifies the effort.\nIf daves_branch exists on the remote repository, but not on your local branch, you can simply type:\n\nSince you do not have the branch locally, this will automatically make switch look on the remote repo.  It will then also automatically set up remote branch tracking.\nNote that if daves_branch doesn't exist locally you'll need to git fetch first before using switch.\n\nOriginal Post\nYou need to create a local branch that tracks a remote branch. The following command will create a local branch named daves_branch, tracking the remote branch origin/daves_branch. When you push your changes the remote branch will be updated.\nFor most recent versions of Git:\n\n--track is shorthand for git checkout -b [branch] [remotename]/[branch] where [remotename] is origin in this case and [branch] is twice the same, daves_branch in this case.\nFor Git 1.5.6.5 you needed this:\n\nFor Git 1.7.2.3 and higher, this is enough (it might have started earlier, but this is the earliest confirmation I could find quickly):\n\nNote that with recent Git versions, this command will not create a local branch and will put you in a 'detached HEAD' state. If you want a local branch, use the --track option.\nFull details are here: 3.5 Git Branching - Remote Branches, Tracking Branches"
  },
  {
    "question": "How do I change the author and committer name/email for multiple commits? How do I change the author for a range of commits?",
    "answer": "git filter-branch --env-filter '\nOLD_EMAIL=\"your-old-email@example.com\"\nCORRECT_NAME=\"Your Correct Name\"\nCORRECT_EMAIL=\"your-correct-email@example.com\"\nif [ \"$GIT_COMMITTER_EMAIL\" = \"$OLD_EMAIL\" ]\nthen\nexport GIT_COMMITTER_NAME=\"$CORRECT_NAME\"\nexport GIT_COMMITTER_EMAIL=\"$CORRECT_EMAIL\"\nfi\nif [ \"$GIT_AUTHOR_EMAIL\" = \"$OLD_EMAIL\" ]\nthen\nexport GIT_AUTHOR_NAME=\"$CORRECT_NAME\"\nexport GIT_AUTHOR_EMAIL=\"$CORRECT_EMAIL\"\nfi\n' --tag-name-filter cat -- --branches --tags\nProper Name <proper@email.xx> Commit Name <commit@email.xx>\ngit filter-repo --mailmap git-mailmap\n\nThis answer uses git-filter-branch, for which the docs now give this warning:\ngit filter-branch has a plethora of pitfalls that can produce non-obvious manglings of the intended history rewrite (and can leave you with little time to investigate such problems since it has such abysmal performance). These safety and performance issues cannot be backward compatibly fixed and as such, its use is not recommended. Please use an alternative history filtering tool such as git filter-repo. If you still need to use git filter-branch, please carefully read SAFETY (and PERFORMANCE) to learn about the land mines of filter-branch, and then vigilantly avoid as many of the hazards listed there as reasonably possible.\n\nChanging the author (or committer) would require rewriting all of the history.  If you're okay with that and think it's worth it then you should check out git filter-branch.  The manual page includes several examples to get you started.  Also note that you can use environment variables to change the name of the author, committer, dates, etc. -- see the \"Environment Variables\" section of the git manual page.\nSpecifically, you can fix all the wrong author names and emails for all branches and tags with this command (source: GitHub help):\n\nFor using alternative history filtering tool git filter-repo, you can first install it and construct a git-mailmap according to the format of gitmailmap.\n\nAnd then run filter-repo with the created mailmap:"
  },
  {
    "question": "How do I clone a Git repository into a specific folder? The command git clone git@github.com:whatever creates a directory named whatever containing a Git repository:\n\n```\n./\n    whatever/\n        .git\n\n```\n\nI want the contents of the Git repository cloned into my current directory ./ instead:\n\n```\n./\n    .git\n\n```",
    "answer": "git clone git@github.com:whatever folder-name\ngit clone git@github.com:whatever .\nmv /where/it/is/right/now/* /where/I/want/it/\nmv /where/it/is/right/now/.* /where/I/want/it/\nln -s /where/it/is/right/now /the/path/I/want/to/use\nln -sfn /opt/projectA/prod/public /httpdocs/public\nln -sfn /opt/projectA/test/public /httpdocs/public\n\nOption A:\n\nErgo, for right here use:\n\nOption B:\nMove the .git folder, too. Note that the .git folder is hidden in most graphical file explorers, so be sure to show hidden files.\n\nThe first line grabs all normal files, the second line grabs dot-files. It is also possibe to do it in one line by enabling dotglob (i.e. shopt -s dotglob) but that is probably a bad solution if you are asking the question this answer answers.\nBetter yet:\nKeep your working copy somewhere else, and create a symbolic link. Like this:\n\nFor your case this would be something like:\n\nWhich easily could be changed to test if you wanted it, i.e.:\n\nwithout moving files around. Added -fn in case someone is copying these lines (-f is force,  -n avoid some often unwanted interactions with already and non-existing links).\nIf you just want it to work, use Option A, if someone else is going to look at what you have done, use Option C."
  },
  {
    "question": "How do I make Git ignore file mode (chmod) changes? I have a project in which I have to change the mode of files with chmod to 777 while developing, but which should not change in the main repo. \nGit picks up on chmod -R 777 . and marks all files as changed. Is there a way to make Git ignore mode changes that have been made to files?",
    "answer": "git config core.fileMode false\ncore.fileMode\nTells Git if the executable bit of files in the working tree\nis to be honored.\n\nSome filesystems lose the executable bit when a file that is\nmarked as executable is checked out, or checks out a\nnon-executable file with executable bit on. git-clone(1)\nor git-init(1) probe the filesystem to see if it handles the\nexecutable bit correctly and this variable is automatically\nset as necessary.\n\nA repository, however, may be on a filesystem that handles\nthe filemode correctly, and this variable is set to true when\ncreated, but later may be made accessible from another\nenvironment that loses the filemode (e.g. exporting ext4\nvia CIFS mount, visiting a Cygwin created repository with Git\nfor Windows or Eclipse). In such a case it may be necessary\nto set this variable to false. See git-update-index(1).\n\nThe default is true (when core.filemode is not specified\nin the config file).\ngit -c core.fileMode=false diff\ngit config --global core.fileMode false\n\ngit config core.fileMode false\nfind . -type d -exec chmod a+rwx {} \\; # Make folders traversable and read/write\nfind . -type f -exec chmod a+rw {} \\;  # Make files read/write\n\nTry:\n\nFrom git-config(1):\n\nThe -c flag can be used to set this option for one-off commands:\n\nTyping the -c core.fileMode=false can be bothersome and so you can set this flag for all git repos or just for one git repo:\n\nAdditionally, git clone and git init explicitly set core.fileMode to true in the repo config as discussed in Git global core.fileMode false overridden locally on clone\nWarning\ncore.fileMode is not the best practice and should be used carefully. This setting only covers the executable bit of mode and never the read/write bits. In many cases you think you need this setting because you did something like chmod -R 777, making all your files executable. But in most projects most files don't need and should not be executable for security reasons.\nThe proper way to solve this kind of situation is to handle folder and file permission separately, with something like:\n\nIf you do that, you'll never need to use core.fileMode, except in very rare environment."
  },
  {
    "question": "How to concatenate string variables in Bash In PHP, strings are concatenated together as follows:\n\n```\n$foo = \"Hello\";\n$foo .= \" World\";\n\n```\n\nHere, $foo becomes \"Hello World\".\nHow is this accomplished in Bash?",
    "answer": "foo=\"Hello\"\nfoo=\"${foo} World\"\necho \"${foo}\"\n> Hello World\na='Hello'\nb='World'\nc=\"${a} ${b}\"\necho \"${c}\"\n> Hello World\n\nIn general to concatenate two variables you can just write them one after another:"
  },
  {
    "question": "What does &quot; 2&gt;&amp;1 &quot; mean? To combine stderr and stdout into the stdout stream, we append this to a command:\n\n```\n2>&1\n\n```\n\nFor example, the following command shows the first few errors from compiling main.cpp:\n\n```\ng++ main.cpp 2>&1 | head\n\n```\n\nBut what does 2>&1 mean?",
    "answer": "File descriptor 1 is the standard output (stdout).\nFile descriptor 2 is the standard error (stderr).\nAt first, 2>1 may look like a good way to redirect stderr to stdout. However, it will actually be interpreted as \"redirect stderr to a file named 1\".\n& indicates that what follows and precedes is a file descriptor, and not a filename. Thus, we use 2>&1. Consider >& to be a redirect merger operator."
  },
  {
    "question": "How can I check if a program exists from a Bash script? How would I validate that a program exists, in a way that will either return an error and exit, or continue with the script?\nIt seems like it should be easy, but it's been stumping me.",
    "answer": "command -v <the_command>\nif ! command -v <the_command> >/dev/null 2>&1\nthen\necho \"<the_command> could not be found\"\nexit 1\nfi\nhash <the_command> # For regular commands. Or...\ntype <the_command> # To check built-ins and keywords\ncommand -v foo >/dev/null 2>&1 || { echo >&2 \"I require foo but it's not installed.  Aborting.\"; exit 1; }\ntype foo >/dev/null 2>&1 || { echo >&2 \"I require foo but it's not installed.  Aborting.\"; exit 1; }\nhash foo 2>/dev/null || { echo >&2 \"I require foo but it's not installed.  Aborting.\"; exit 1; }\ngnudate() {\nif hash gdate 2>/dev/null; then\ngdate \"$@\"\nelse\ndate \"$@\"\nfi\n}\n\nAnswer\nPOSIX compatible:\n\nExample use:\n\nFor Bash specific environments:\n\nExplanation\nAvoid which. Not only is it an external process you're launching for doing very little (meaning builtins like hash, type or command are way cheaper), you can also rely on the builtins to actually do what you want, while the effects of external commands can easily vary from system to system.\nWhy care?\n\nMany operating systems have a which that doesn't even set an exit status, meaning the if which foo won't even work there and will always report that foo exists, even if it doesn't (note that some POSIX shells appear to do this for hash too).\nMany operating systems make which do custom and evil stuff like change the output or even hook into the package manager.\n\nSo, don't use which. Instead use one of these:\n\n(Minor side-note: some will suggest 2>&- is the same 2>/dev/null but shorter \u2013 this is untrue.  2>&- closes FD 2 which causes an error in the program when it tries to write to stderr, which is very different from successfully writing to it and discarding the output (and dangerous!))\n(Additional minor side-note: some will suggest &>/dev/null, but this is not POSIX compliant)\nIf your hash bang is /bin/sh then you should care about what POSIX says. type and hash's exit codes aren't terribly well defined by POSIX, and hash is seen to exit successfully when the command doesn't exist (haven't seen this with type yet).  command's exit status is well defined by POSIX, so that one is probably the safest to use.\nIf your script uses bash though, POSIX rules don't really matter anymore and both type and hash become perfectly safe to use. type now has a -P to search just the PATH and hash has the side-effect that the command's location will be hashed (for faster lookup next time you use it), which is usually a good thing since you probably check for its existence in order to actually use it.\nAs a simple example, here's a function that runs gdate if it exists, otherwise date:"
  },
  {
    "question": "How do I split a string on a delimiter in Bash? I have this string stored in a variable:\n\n```\nIN=\"bla@some.com;john@home.com\"\n\n```\n\nNow I would like to split the strings by ; delimiter so that I have:\n\n```\nADDR1=\"bla@some.com\"\nADDR2=\"john@home.com\"\n\n```\n\nI don't necessarily need the ADDR1 and ADDR2 variables. If they are elements of an array that's even better.\n\nAfter suggestions from the answers below, I ended up with the following which is what I was after:\n\n```\n#!/usr/bin/env bash\n\nIN=\"bla@some.com;john@home.com\"\n\nmails=$(echo $IN | tr \";\" \"\\n\")\n\nfor addr in $mails\ndo\n    echo \"> [$addr]\"\ndone\n\n```\n\nOutput:\n\n```\n> [bla@some.com]\n> [john@home.com]\n\n```\n\nThere was a solution involving setting Internal_field_separator (IFS) to ;. I am not sure what happened with that answer, how do you reset IFS back to default?\nRE: IFS solution, I tried this and it works, I keep the old IFS and then restore it:\n\n```\nIN=\"bla@some.com;john@home.com\"\n\nOIFS=$IFS\nIFS=';'\nmails2=$IN\nfor x in $mails2\ndo\n    echo \"> [$x]\"\ndone\n\nIFS=$OIFS\n\n```\n\nBTW, when I tried \n\n```\nmails2=($IN)\n\n```\n\nI only got the first string when printing it in loop, without brackets around $IN it works.",
    "answer": "IFS=';' read -ra ADDR <<< \"$IN\"\nfor i in \"${ADDR[@]}\"; do\n\ndone\nwhile IFS=';' read -ra ADDR; do\nfor i in \"${ADDR[@]}\"; do\n\ndone\ndone <<< \"$IN\"\n\nYou can set the internal field separator (IFS) variable, and then let it parse into an array. When this happens in a command, then the assignment to IFS only takes place to that single command's environment (to read ). It then parses the input according to the IFS variable value into an array, which we can then iterate over.\nThis example will parse one line of items separated by ;, pushing it into an array:\n\nThis other example is for processing the whole content of $IN, each time one line of input separated by ;:"
  },
  {
    "question": "Extract filename and extension in Bash I want to get the filename (without extension) and the extension separately.\nThe best solution I found so far is:\n\n```\nNAME=`echo \"$FILE\" | cut -d'.' -f1`\nEXTENSION=`echo \"$FILE\" | cut -d'.' -f2`\n\n```\n\nThis is wrong because it doesn't work if the file name contains multiple . characters. If, let's say, I have a.b.js, it will consider a and b.js, instead of a.b and js.\nIt can be easily done in Python with\n\n```\nfile, ext = os.path.splitext(path)\n\n```\n\nbut I'd prefer not to fire up a Python interpreter just for this, if possible.\nAny better ideas?",
    "answer": "filename=$(basename -- \"$fullfile\")\nextension=\"${filename##*.}\"\nfilename=\"${filename%.*}\"\nfilename=\"${fullfile##*/}\"\n\nFirst, get file name without the path:\n\nAlternatively, you can focus on the last '/' of the path instead of the '.' which should work even if you have unpredictable file extensions:\n\nYou may want to check the documentation :\n\nOn the web at section \"3.5.3 Shell Parameter Expansion\"\nIn the bash manpage at section called \"Parameter Expansion\""
  },
  {
    "question": "How do I parse command line arguments in Bash? Say, I have a script that gets called with this line:\n\n```\n./myscript -vfd ./foo/bar/someFile -o /fizz/someOtherFile\n\n```\n\nor this one:\n\n```\n./myscript -v -f -d -o /fizz/someOtherFile ./foo/bar/someFile \n\n```\n\nWhat's the accepted way of parsing this such that in each case (or some combination of the two) $v, $f, and  $d will all be set to true and $outFile will be equal to /fizz/someOtherFile?",
    "answer": "cat >/tmp/demo-space-separated.sh <<'EOF'\n\nPOSITIONAL_ARGS=()\n\nwhile [[ $# -gt 0 ]]; do\ncase $1 in\n-e|--extension)\nEXTENSION=\"$2\"\nshift # past argument\nshift # past value\n;;\n-s|--searchpath)\nSEARCHPATH=\"$2\"\nshift # past argument\nshift # past value\n;;\n--default)\nDEFAULT=YES\nshift # past argument\n;;\n-*|--*)\necho \"Unknown option $1\"\nexit 1\n;;\n*)\nPOSITIONAL_ARGS+=(\"$1\") # save positional arg\nshift # past argument\n;;\nesac\ndone\n\nset -- \"${POSITIONAL_ARGS[@]}\" # restore positional parameters\n\necho \"FILE EXTENSION  = ${EXTENSION}\"\necho \"SEARCH PATH     = ${SEARCHPATH}\"\necho \"DEFAULT         = ${DEFAULT}\"\necho \"Number files in SEARCH PATH with EXTENSION:\" $(ls -1 \"${SEARCHPATH}\"/*.\"${EXTENSION}\" | wc -l)\n\nif [[ -n $1 ]]; then\necho \"Last line of file specified as non-opt/last argument:\"\ntail -1 \"$1\"\nfi\nEOF\n\nchmod +x /tmp/demo-space-separated.sh\n\n/tmp/demo-space-separated.sh -e conf -s /etc /etc/hosts\nFILE EXTENSION  = conf\nSEARCH PATH     = /etc\nDEFAULT         =\nNumber files in SEARCH PATH with EXTENSION: 14\nLast line of file specified as non-opt/last argument:\ndemo-space-separated.sh -e conf -s /etc /etc/hosts\ncat >/tmp/demo-equals-separated.sh <<'EOF'\n\nfor i in \"$@\"; do\ncase $i in\n-e=*|--extension=*)\nEXTENSION=\"${i#*=}\"\nshift # past argument=value\n;;\n-s=*|--searchpath=*)\nSEARCHPATH=\"${i#*=}\"\nshift # past argument=value\n;;\n--default)\nDEFAULT=YES\nshift # past argument with no value\n;;\n-*|--*)\necho \"Unknown option $i\"\nexit 1\n;;\n*)\n;;\nesac\ndone\n\necho \"FILE EXTENSION  = ${EXTENSION}\"\necho \"SEARCH PATH     = ${SEARCHPATH}\"\necho \"DEFAULT         = ${DEFAULT}\"\necho \"Number files in SEARCH PATH with EXTENSION:\" $(ls -1 \"${SEARCHPATH}\"/*.\"${EXTENSION}\" | wc -l)\n\nif [[ -n $1 ]]; then\necho \"Last line of file specified as non-opt/last argument:\"\ntail -1 $1\nfi\nEOF\n\nchmod +x /tmp/demo-equals-separated.sh\n\n/tmp/demo-equals-separated.sh -e=conf -s=/etc /etc/hosts\nFILE EXTENSION  = conf\nSEARCH PATH     = /etc\nDEFAULT         =\nNumber files in SEARCH PATH with EXTENSION: 14\nLast line of file specified as non-opt/last argument:\ndemo-equals-separated.sh -e=conf -s=/etc /etc/hosts\ncat >/tmp/demo-getopts.sh <<'EOF'\n\nOPTIND=1         # Reset in case getopts has been used previously in the shell.\n\noutput_file=\"\"\nverbose=0\n\nwhile getopts \"h?vf:\" opt; do\ncase \"$opt\" in\nh|\\?)\nshow_help\nexit 0\n;;\nv)  verbose=1\n;;\nf)  output_file=$OPTARG\n;;\nesac\ndone\n\nshift $((OPTIND-1))\n\n[ \"${1:-}\" = \"--\" ] && shift\n\necho \"verbose=$verbose, output_file='$output_file', Leftovers: $@\"\nEOF\n\nchmod +x /tmp/demo-getopts.sh\n\n/tmp/demo-getopts.sh -vf /etc/hosts foo bar\nverbose=1, output_file='/etc/hosts', Leftovers: foo bar\ndemo-getopts.sh -vf /etc/hosts foo bar\n\nBash Space-Separated (e.g., --option argument)\n\nOutput from copy-pasting the block above\n\nUsage\n\nBash Equals-Separated (e.g., --option=argument)\n\nOutput from copy-pasting the block above\n\nUsage\n\nTo better understand ${i#*=} search for \"Substring Removal\" in this guide. It is functionally equivalent to `sed 's/[^=]*=//' <<< \"$i\"` which calls a needless subprocess or `echo \"$i\" | sed 's/[^=]*=//'` which calls two needless subprocesses.\n\nUsing bash with getopt[s]\ngetopt(1) limitations (older, relatively-recent getopt versions):\n\ncan't handle arguments that are empty strings\ncan't handle arguments with embedded whitespace\n\nMore recent getopt versions don't have these limitations. For more information, see these docs.\n\nPOSIX getopts\nAdditionally, the POSIX shell and others offer getopts which doen't have these limitations. I've included a simplistic getopts example.\n\nOutput from copy-pasting the block above\n\nUsage\n\nThe advantages of getopts are:\n\nIt's more portable, and will work in other shells like dash.\nIt can handle multiple single options like -vf filename in the typical Unix way, automatically.\n\nThe disadvantage of getopts is that it can only handle short options (-h, not --help) without additional code.\nThere is a getopts tutorial which explains what all of the syntax and variables mean.  In bash, there is also help getopts, which might be informative."
  },
  {
    "question": "How do I set a variable to the output of a command in Bash? I have a pretty simple script that is something like the following:\n\n```\n#!/bin/bash\n\nVAR1=\"$1\"\nMOREF='sudo run command against $VAR1 | grep name | cut -c7-'\n\necho $MOREF\n\n```\n\nWhen I run this script from the command line and pass it the arguments, I am not getting any output.  However, when I run the commands contained within the $MOREF variable, I am able to get output.\nHow can one take the results of a command that needs to be run within a script, save it to a variable, and then output that variable on the screen?",
    "answer": "OUTPUT=\"$(ls -1)\"\necho \"${OUTPUT}\"\n\nMULTILINE=\"$(ls \\\n-1)\"\necho \"${MULTILINE}\"\n\nIn addition to backticks `command`, command substitution can be done with $(command) or \"$(command)\", which I find easier to read, and allows for nesting.\n\nQuoting (\") does matter to preserve multi-line variable values and it is safer to use with whitespace and special characters such as (*) and therefore advised; it is, however, optional on the right-hand side of an assignment when word splitting is not performed, so OUTPUT=$(ls -1) would work fine."
  },
  {
    "question": "How to check if a variable is set in Bash How do I know if a variable is set in Bash?\nFor example, how do I check if the user gave the first parameter to a function?\n\n```\nfunction a {\n    # if $1 is set ?\n}\n\n```",
    "answer": "if [ -z ${var+x} ]; then echo \"var is unset\"; else echo \"var is set to '$var'\"; fi\nif [ -z \"$var\" ]; then echo \"var is blank\"; else echo \"var is set to '$var'\"; fi\n\n(Usually) The right way\n\nwhere ${var+x} is a parameter expansion which evaluates to nothing if var is unset, and substitutes the string x otherwise.\nQuotes Digression\nQuotes can be omitted (so we can say ${var+x} instead of \"${var+x}\") because this syntax & usage guarantees this will only expand to something that does not require quotes (since it either expands to x (which contains no word breaks so it needs no quotes), or to nothing (which results in [ -z  ], which conveniently evaluates to the same value (true) that [ -z \"\" ] does as well)).\nHowever, while quotes can be safely omitted, and it was not immediately obvious to all (it wasn't even apparent to the first author of this quotes explanation who is also a major Bash coder), it would sometimes be better to write the solution with quotes as [ -z \"${var+x}\" ], at the very small possible cost of an O(1) speed penalty.  The first author also added this as a comment next to the code using this solution giving the URL to this answer, which now also includes the explanation for why the quotes can be safely omitted.\n(Often) The wrong way\n\nThis is often wrong because it doesn't distinguish between a variable that is unset and a variable that is set to the empty string. That is to say, if var='', then the above solution will output \"var is blank\".\nThe distinction between unset and \"set to the empty string\" is essential in situations where the user has to specify an extension, or additional list of properties, and that not specifying them defaults to a non-empty value, whereas specifying the empty string should make the script use an empty extension or list of additional properties.\nThe distinction may not be essential in every scenario though. In those cases  [ -z \"$var\" ] will be just fine."
  },
  {
    "question": "Loop through an array of strings in Bash? I want to write a script that loops through 15 strings (array possibly?) Is that possible?\nSomething like:\n\n```\nfor databaseName in listOfNames\nthen\n  # Do something\nend\n\n```",
    "answer": "declare -a arr=(\"element1\" \"element2\" \"element3\")\n\nfor i in \"${arr[@]}\"\ndo\necho \"$i\"\n\ndone\ndeclare -a arr=(\"element1\"\n\"element2\" \"element3\"\n\"element4\"\n)\n\nYou can use it like this:\n\nAlso works for multi-line array declaration"
  },
  {
    "question": "How to reload .bashrc settings without logging out and back in again? If I make changes to .bashrc, how do I reload it without logging out and back in?",
    "answer": "source ~/.bashrc\n. ~/.bashrc\n\nYou can enter the long form command:\n\nor you can use the shorter version of the command:"
  },
  {
    "question": "How do I iterate over a range of numbers defined by variables in Bash? How do I iterate over a range of numbers in Bash when the range is given by a variable?\nI know I can do this (called \"sequence expression\" in the Bash documentation):\n\n```\nfor i in {1..5}; do echo $i; done\n\n```\n\nWhich gives:\n\n1 \n2 \n3 \n4 \n5\n\nYet, how can I replace either of the range endpoints with a variable? This doesn't work:\n\n```\nEND=5\nfor i in {1..$END}; do echo $i; done\n\n```\n\nWhich prints:\n\n{1..5}",
    "answer": "for i in $(seq 1 $END); do echo $i; done\n\nedit: I prefer seq over the other methods because I can actually remember it ;)"
  },
  {
    "question": "Looping through the content of a file in Bash How do I iterate through each line of a text file with Bash?\nWith this script:\n\n```\necho \"Start!\"\nfor p in (peptides.txt)\ndo\n    echo \"${p}\"\ndone\n\n```\n\nI get this output on the screen:\n\n```\nStart!\n./runPep.sh: line 3: syntax error near unexpected token `('\n./runPep.sh: line 3: `for p in (peptides.txt)'\n\n```\n\n(Later I want to do something more complicated with $p than just output to the screen.)\n\nThe environment variable SHELL is (from env):\n\n```\nSHELL=/bin/bash\n\n```\n\n/bin/bash --version output:\n\n```\nGNU bash, version 3.1.17(1)-release (x86_64-suse-linux-gnu)\nCopyright (C) 2005 Free Software Foundation, Inc.\n\n```\n\ncat /proc/version output:\n\n```\nLinux version 2.6.18.2-34-default (geeko@buildhost) (gcc version 4.1.2 20061115 (prerelease) (SUSE Linux)) #1 SMP Mon Nov 27 11:46:27 UTC 2006\n\n```\n\nThe file peptides.txt contains:\n\n```\nRKEKNVQ\nIPKKLLQK\nQYFHQLEKMNVK\nIPKKLLQK\nGDLSTALEVAIDCYEK\nQYFHQLEKMNVKIPENIYR\nRKEKNVQ\nVLAKHGKLQDAIN\nILGFMK\nLEDVALQILL\n\n```",
    "answer": "while read p; do\necho \"$p\"\ndone <peptides.txt\nwhile IFS=\"\" read -r p || [ -n \"$p\" ]\ndo\nprintf '%s\\n' \"$p\"\ndone < peptides.txt\nwhile read -u 10 p; do\n...\ndone 10<peptides.txt\n\nOne way to do it is:\n\nAs pointed out in the comments, this has the side effects of trimming leading whitespace, interpreting backslash sequences, and skipping the last line if it's missing a terminating linefeed. If these are concerns, you can do:\n\nExceptionally, if the loop body may read from standard input, you can open the file using a different file descriptor:\n\nHere, 10 is just an arbitrary number (different from 0, 1, 2)."
  },
  {
    "question": "How can I count all the lines of code in a directory recursively? We've got a PHP application and want to count all the lines of code under a specific directory and its subdirectories.\nWe don't need to ignore comments, as we're just trying to get a rough idea.\n\n```\nwc -l *.php \n\n```\n\nThat command works great for a given directory, but it ignores subdirectories. I was thinking the following comment might work, but it is returning 74, which is definitely not the case...\n\n```\nfind . -name '*.php' | wc -l\n\n```\n\nWhat's the correct syntax to feed in all the files from a directory resursively?",
    "answer": "find . -name '*.php' | xargs wc -l\nfind . -name '*.php' | sed 's/.*/\"&\"/' | xargs  wc -l\n\nTry:\n\nor (when file names include special characters such as spaces)\n\nThe SLOCCount tool may help as well.\nIt will give an accurate source lines of code count for whatever\nhierarchy you point it at, as well as some additional stats.\nSorted output:\nfind . -name '*.php' | xargs wc -l | sort -nr"
  },
  {
    "question": "How to redirect and append both standard output and standard error to a file with Bash To redirect standard output to a truncated file in Bash, I know to use:\n\n```\ncmd > file.txt\n\n```\n\nTo redirect standard output in Bash, appending to a file, I know to use:\n\n```\ncmd >> file.txt\n\n```\n\nTo redirect both standard output and standard error to a truncated file, I know to use:\n\n```\ncmd &> file.txt\n\n```\n\nHow do I redirect both standard output and standard error appending to a file? cmd &>> file.txt did not work for me.",
    "answer": "cmd >>file.txt 2>&1\n\nBash executes the redirects from left to right as follows:\n\n>>file.txt: Open file.txt in append mode and redirect stdout there.\n2>&1: Redirect stderr to \"where stdout is currently going\". In this case, that is a file opened in append mode. In other words, the &1 reuses the file descriptor which stdout currently uses."
  },
  {
    "question": "Check existence of input argument in a Bash shell script I need to check the existence of an input argument. I have the following script\n\n```\nif [ \"$1\" -gt \"-1\" ]\n  then echo hi\nfi\n\n```\n\nI get\n\n```\n[: : integer expression expected\n\n```\n\nHow do I check the input argument1 first to see if it exists?",
    "answer": "if [ $# -eq 0 ]\nthen\necho \"No arguments supplied\"\nfi\nif [ -z \"$1\" ]\nthen\necho \"No argument supplied\"\nfi\n\nIt is:\n\nThe $# variable will tell you the number of input arguments the script was passed.\nOr you can check if an argument is an empty string or not like:\n\nThe -z switch will test if the expansion of \"$1\" is a null string or not. If it is a null string then the body is executed."
  },
  {
    "question": "How do I prompt for Yes/No/Cancel input in a Linux shell script? I want to pause input in a shell script, and prompt the user for choices.\nThe standard Yes, No, or Cancel type question.\nHow do I accomplish this in a typical bash prompt?",
    "answer": "while true; do\nread -p \"Do you wish to install this program? \" yn\ncase $yn in\n[Yy]* ) make install; break;;\n[Nn]* ) exit;;\n* ) echo \"Please answer yes or no.\";;\nesac\ndone\necho \"Do you wish to install this program?\"\nselect yn in \"Yes\" \"No\"; do\ncase $yn in\nYes ) make install; break;;\nNo ) exit;;\nesac\ndone\necho \"Do you wish to install this program?\"\nselect strictreply in \"Yes\" \"No\"; do\nrelaxedreply=${strictreply:-$REPLY}\ncase $relaxedreply in\nYes | yes | y ) make install; break;;\nNo  | no  | n ) exit;;\nesac\ndone\nset -- $(locale LC_MESSAGES)\nyesexpr=\"$1\"; noexpr=\"$2\"; yesword=\"$3\"; noword=\"$4\"\n\nwhile true; do\nread -p \"Install (${yesword} / ${noword})? \" yn\nif [[ \"$yn\" =~ $yesexpr ]]; then make install; exit; fi\nif [[ \"$yn\" =~ $noexpr ]]; then exit; fi\necho \"Answer ${yesword} / ${noword}.\"\ndone\n\nA widely available method to get user input at a shell prompt is the read command. Here is a demonstration:\n\nAnother method, pointed out by Steven Huwig, is Bash's select command. Here is the same example using select:\n\nWith select you don't need to sanitize the input \u2013 it displays the available choices, and you type a number corresponding to your choice. It also loops automatically, so there's no need for a while true loop to retry if they give invalid input. If you want to allow more flexible input (accepting the words of the options, rather than just their number), you can alter it like this:\n\nAlso, L\u00e9a Gris demonstrated a way to make the request language agnostic in her answer. Adapting my first example to better serve multiple languages might look like this:\n\nObviously other communication strings remain untranslated here (Install, Answer) which would need to be addressed in a more fully completed translation, but even a partial translation would be helpful in many cases.\n\nFinally, please check out the excellent answer by F. Hauri."
  },
  {
    "question": "Make a Bash alias that takes a parameter? I used to use CShell (csh), which lets you make an alias that takes a parameter. The notation was something like\n\n```\nalias junk=\"mv \\\\!* ~/.Trash\"\n\n```\n\nIn Bash, this does not seem to work. Given that Bash has a multitude of useful features, I would assume that this one has been implemented but I am wondering how.",
    "answer": "myfunction() {\n\nmv \"$1\" \"$1.bak\"\ncp \"$2\" \"$1\"\n}\n\nmyfunction old.conf new.conf #calls `myfunction`\nmyfunction original.conf my.conf\n\nBash alias does not directly accept parameters. You will have to create a function.\nalias does not accept parameters but a function can be called just like an alias. For example:\n\nBy the way, Bash functions defined in your .bashrc and other files are available as commands within your shell. So for instance you can call the earlier function like this"
  },
  {
    "question": "How to convert a string to lower case in Bash Is there a way in bash to convert a string into a lower case string?\nFor example, if I have:\n\n```\na=\"Hi all\"\n\n```\n\nI want to convert it to:\n\n```\n\"hi all\"\n\n```",
    "answer": "echo \"$a\" | tr '[:upper:]' '[:lower:]'\nhi all\necho \"$a\" | awk '{print tolower($0)}'\nhi all\necho \"${a,,}\"\nhi all\necho \"$a\" | sed -e 's/\\(.*\\)/\\L\\1/'\nhi all\n\nsed -e 's/\\(.*\\)/\\L\\1/' <<< \"$a\"\nhi all\necho \"$a\" | perl -ne 'print lc'\nhi all\nlc(){\ncase \"$1\" in\n[A-Z])\nn=$(printf \"%d\" \"'$1\")\nn=$((n+32))\nprintf \\\\$(printf \"%o\" \"$n\")\n;;\n*)\nprintf \"%s\" \"$1\"\n;;\nesac\n}\nword=\"I Love Bash\"\nfor((i=0;i<${#word};i++))\ndo\nch=\"${word:$i:1}\"\nlc \"$ch\"\ndone\n\nThere are various ways:\nPOSIX standard\ntr\n\nAWK\n\nNon-POSIX\nYou may run into portability issues with the following examples:\nBash 4.0\n\nsed\n\nPerl\n\nBash\n\nNote: YMMV on this one. Doesn't work for me (GNU bash version 4.2.46 and 4.0.33 (and same behaviour 2.05b.0 but nocasematch  is not implemented)) even with using shopt -u nocasematch;. Unsetting that nocasematch causes [[ \"fooBaR\" == \"FOObar\" ]] to match OK BUT inside case weirdly [b-z] are incorrectly matched by [A-Z]. Bash is confused by the double-negative (\"unsetting nocasematch\")! :-)"
  },
  {
    "question": "echo that outputs to stderr Is there a standard Bash command that acts like echo but outputs to stderr rather than stdout?\nI know I can do echo foo 1>&2 but it's kinda ugly and, I suspect, error-prone (e.g. more likely to get edited wrong when things change).",
    "answer": ">&2 echo \"error\"\n\nYou could do this, which facilitates reading:\n\n>&2 copies file descriptor #2 to file descriptor #1. Therefore, after this redirection is performed, both file descriptors will refer to the same file: the one file descriptor #2 was originally referring to. For more information see the Bash Hackers Illustrated Redirection Tutorial."
  },
  {
    "question": "How to redirect output to a file and stdout In bash, calling foo would display any output from that command on the stdout.\nCalling foo > output would redirect any output from that command to the file specified (in this case 'output').\nIs there a way to redirect output to a file and have it display on stdout?",
    "answer": "foo | tee output.file\nls -a | tee output.file\nprogram [arguments...] 2>&1 | tee outfile\nprogram [arguments...] 2>&1 | tee -a outfile\n\nThe command you want is named tee:\n\nFor example, if you only care about stdout:\n\nIf you want to include stderr, do:\n\n2>&1 redirects channel 2 (stderr/standard error) into channel 1 (stdout/standard output), such that both is written as stdout. It is also directed to the given output file as of the tee command.\nFurthermore, if you want to append to the log file, use tee -a as:"
  },
  {
    "question": "YYYY-MM-DD format date in shell script I tried using $(date) in my bash shell script, however, I want the date in YYYY-MM-DD format.\nHow do I get this?",
    "answer": "printf -v date '%(%Y-%m-%d)T\\n' -1\n\nprintf -v date '%(%Y-%m-%d %H:%M:%S)T\\n' -1\n\nprintf '%(%Y-%m-%d)T\\n' -1\ndate=$(date '+%Y-%m-%d')\n\ndate=$(date '+%Y-%m-%d %H:%M:%S')\n\necho $(date '+%Y-%m-%d')\nman date\n\nIn bash (>=4.2) it is preferable to use printf's built-in date formatter (part of bash) rather than the external date (usually GNU date). Note that invoking a subshell has performance problems in Cygwin due to a slow fork() call on Windows.\nAs such:\n\nIn bash (<4.2):\n\nOther available date formats can be viewed from the date man pages (for external non-bash specific command):"
  },
  {
    "question": "Passing parameters to a Bash function I am trying to search how to pass parameters in a Bash function, but what comes up is always how to pass parameter from the command line.\nI would like to pass parameters within my script. I tried:\n\n```\nmyBackupFunction(\"..\", \"...\", \"xx\")\n\nfunction myBackupFunction($directory, $options, $rootPassword) {\n     ...\n}\n\n```\n\nBut the syntax is not correct. How can I pass a parameter to my function?",
    "answer": "function function_name {\ncommand...\n}\nfunction_name () {\ncommand...\n}\nfunction_name \"$arg1\" \"$arg2\"\nfunction_name () {\necho \"Parameter #1 is $1\"\n}\nfoo 1  # this will fail because foo has not been declared yet.\n\nfoo() {\necho \"Parameter #1 is $1\"\n}\n\nfoo 2 # this will work.\n./myScript.sh: line 2: foo: command not found\nParameter #1 is 2\n\nThere are two typical ways of declaring a function. I prefer the second approach.\n\nor\n\nTo call a function with arguments:\n\nThe function refers to passed arguments by their position (not by name), that is $1, $2, and so forth. $0 is the name of the script itself.\nExample:\n\nAlso, you need to call your function after it is declared.\n\nOutput:\n\nReference: Advanced Bash-Scripting Guide."
  },
  {
    "question": "How can I declare and use Boolean variables in a shell script? I tried to declare a Boolean variable in a shell script using the following syntax:\n\n```\nvariable=$false\n\nvariable=$true\n\n```\n\nIs this correct? Also, if I wanted to update that variable would I use the same syntax? Finally, is the following syntax for using Boolean variables as expressions correct?\n\n```\nif [ $variable ]\n\nif [ !$variable ]\n\n```",
    "answer": "the_world_is_flat=true\n\nif [ \"$the_world_is_flat\" = true ] ; then\necho 'Be careful not to fall off!'\nfi\nthe_world_is_flat=true\n\nif $the_world_is_flat ; then\necho 'Be careful not to fall off!'\nfi\n\nRevised Answer (Feb 12, 2014)\n\nOriginal Answer\nCaveats: https://stackoverflow.com/a/21210966/89391\n\nFrom: Using boolean variables in Bash\nThe reason the original answer is included here is because the comments before the revision on Feb 12, 2014 pertain only to the original answer, and many of the comments are wrong when associated with the revised answer. For example, Dennis Williamson's comment about Bash's builtin true on Jun 2, 2010 only applies to the original answer, not the revised."
  },
  {
    "question": "How to escape single quotes within single quoted strings Let's say, you have a Bash alias like:\n\n```\nalias rxvt='urxvt'\n\n```\n\nwhich works fine.\nHowever:\n\n```\nalias rxvt='urxvt -fg '#111111' -bg '#111111''\n\n```\n\nwon't work, and neither will:\n\n```\nalias rxvt='urxvt -fg \\'#111111\\' -bg \\'#111111\\''\n\n```\n\nSo how do you end up matching up opening and closing quotes inside a string once you have escaped quotes?\n\n```\nalias rxvt='urxvt -fg'\\''#111111'\\'' -bg '\\''#111111'\\''\n\n```\n\nseems ungainly although it would represent the same string if you're allowed to concatenate them like that.",
    "answer": "alias rxvt='urxvt -fg '\"'\"'#111111'\"'\"' -bg '\"'\"'#111111'\"'\"\necho 'abc''123'\nabc123\necho 'abc'\\''123'\nabc'123\necho 'abc'\"'\"'123'\nabc'123\nalias test='echo '\"'\"'hi'\"'\"\nalias test\nalias test='echo '\\''hi'\\'''\ntest\nhi\n\nIf you really want to use single quotes in the outermost layer, remember that you can glue both kinds of quotation. Example:\n\nExplanation of how '\"'\"' is interpreted as just ':\n\n' End first quotation which uses single quotes.\n\" Start second quotation, using double-quotes.\n' Quoted character.\n\" End second quotation, using double-quotes.\n' Start third quotation, using single quotes.\n\nIf you do not place any whitespaces between (1) and (2), or between (4) and (5), the shell will interpret that string as a one long word:\n\nIt will also keep the internal representation with 'to be joined' strings, and will also prefer the shorter escape syntax when possible:"
  },
  {
    "question": "Assigning default values to shell variables with a single command in bash I have a whole bunch of tests on variables in a bash (3.00) shell script where if the variable is not set, then it assigns a default, e.g.:\n\n```\nif [ -z \"${VARIABLE}\" ]; then \n    FOO='default'\nelse \n    FOO=${VARIABLE}\nfi\n\n```\n\nI seem to recall there's some syntax to doing this in one line, something resembling a ternary operator, e.g.:\n\n```\nFOO=${ ${VARIABLE} : 'default' }\n\n```\n\n(though I know that won't work...)\nAm I crazy, or does something like that exist?",
    "answer": "FOO=\"${VARIABLE:-default}\"  # FOO will be assigned 'default' value if VARIABLE not set or null.\nFOO=\"${VARIABLE:=default}\"  # If VARIABLE not set or null, set its value to 'default'.\n\nVery close to what you posted, actually. You can use something called Bash parameter expansion to accomplish this.\nTo get the assigned value, or default if it's missing:\n\nTo do the same, as well as assign default to VARIABLE:"
  },
  {
    "question": "Count number of lines in a non binary file (Like a CSV or a TXT) file in terminal I have a text file, and I like to know the total number of line withou opening it. My document is like these, and I want to know how many lines actually...\n\n```\n09:16:39 AM  all    2.00    0.00    4.00    0.00    0.00    0.00    0.00    0.00   94.00\n09:16:40 AM  all    5.00    0.00    0.00    4.00    0.00    0.00    0.00    0.00   91.00\n09:16:41 AM  all    0.00    0.00    4.00    0.00    0.00    0.00    0.00    0.00   96.00\n09:16:42 AM  all    3.00    0.00    1.00    0.00    0.00    0.00    0.00    0.00   96.00\n09:16:43 AM  all    0.00    0.00    1.00    0.00    1.00    0.00    0.00    0.00   98.00\n09:16:44 AM  all    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00  100.00\n09:16:45 AM  all    2.00    0.00    6.00    0.00    0.00    0.00    0.00    0.00   92.00\n\n```\n\nIs there a way to count in Linux Terminal?",
    "answer": "wc -l <filename>\nwc -l /dir/file.txt\n/dir/file.txt\nwc -l < /dir/file.txt\ncat /dir/file.txt | wc -l\ncurl yahoo.com --silent | wc -l\n\nUse wc:\n\nThis will output the number of lines in <filename>:\n\nOr, to omit the <filename> from the result use wc -l < <filename>:\n\nYou can also pipe data to wc as well:"
  },
  {
    "question": "Replace one substring for another string in shell script I have \"I love Suzi and Marry\" and I want to change \"Suzi\" to \"Sara\".\n\n```\nfirstString=\"I love Suzi and Marry\"\nsecondString=\"Sara\"\n\n```\n\nDesired result:\n\n```\nfirstString=\"I love Sara and Marry\"\n\n```",
    "answer": "firstString=\"I love Suzi and Marry\"\nsecondString=\"Sara\"\necho \"${firstString/Suzi/\"$secondString\"}\"\nmessage='The secret code is 12345'\necho \"${message//[0-9]/X}\"\n\nTo replace the first occurrence of a pattern with a given string, use ${parameter/pattern/string}:\n\nTo replace all occurrences, use ${parameter//pattern/string}:\n\n(This is documented in the Bash Reference Manual, \u00a73.5.3 \"Shell Parameter Expansion\".)\nNote that this feature is not specified by POSIX \u2014 it's a Bash extension \u2014 so not all Unix shells implement it. For the relevant POSIX documentation, see The Open Group Technical Standard Base Specifications, Issue 7, the Shell & Utilities volume, \u00a72.6.2 \"Parameter Expansion\"."
  },
  {
    "question": "Listing only directories using ls in Bash? This command lists directories in the current path:\n\n```\nls -d */\n\n```\n\nWhat exactly does the pattern */ do?\nAnd how can we give the absolute path in the above command (e.g. ls -d /home/alice/Documents) for listing only directories in that path?",
    "answer": "*/ is a pattern that matches all of the subdirectories in the current directory (* would match all files and subdirectories; the / restricts it to directories). Similarly, to list all subdirectories under /home/alice/Documents, use ls -d /home/alice/Documents/*/"
  },
  {
    "question": "How to echo shell commands as they are executed In a shell script, how do I echo all shell commands called and expand any variable names?\nFor example, given the following line:\n\n```\nls $DIRNAME\n\n```\n\nI would like the script to run the command and display the following\n\n```\nls /full/path/to/some/dir\n\n```\n\nThe purpose is to save a log of all shell commands called and their arguments. Is there perhaps a better way of generating such a log?",
    "answer": "cat shl\n\nDIR=/tmp/so\nls $DIR\nbash -x shl\n+ DIR=/tmp/so\n+ ls /tmp/so\n\nset -x or set -o xtrace expands variables and prints a little + sign before the line.\nset -v or set -o verbose does not expand the variables before printing.\nUse set +x and set +v to turn off the above settings.\nOn the first line of the script, one can put #!/bin/sh -x (or -v) to have the same effect as set -x (or -v) later in the script.\nThe above also works with /bin/sh.\nSee the bash-hackers' wiki on set attributes, and on debugging."
  },
  {
    "question": "How to compare strings in Bash How do I compare a variable to a string (and do something if they match)?",
    "answer": "if [ \"$x\" = \"valid\" ]; then\necho \"x has the value 'valid'\"\nfi\nif [ = \"valid\" ]; then\nif [[ \"$x\" == \"valid\" ]]; then\nif [ \"$x\" = \"valid\" ]; then\n\nUsing variables in if statements\n\nIf you want to do something when they don't match, replace = with !=. You can read more about string operations and arithmetic operations in their respective documentation.\nWhy do we use quotes around $x?\nYou want the quotes around $x, because if it is empty, your Bash script encounters a syntax error as seen below:\n\nNon-standard use of == operator\nNote that Bash allows == to be used for equality with [, but this is not standard.\nUse either the first case wherein the quotes around $x are optional:\n\nor use the second case:"
  },
  {
    "question": "Pipe to/from the clipboard in a Bash script Is it possible to pipe to/from the clipboard in Bash?\nWhether it is piping to/from a device handle or using an auxiliary application, I can't find anything.\nFor example, if /dev/clip was a device linking to the clipboard we could do:\n\n```\ncat /dev/clip        # Dump the contents of the clipboard\ncat foo > /dev/clip  # Dump the contents of \"foo\" into the clipboard\n\n```",
    "answer": "npm install -g clipboard-cli\necho foo | clipboard\nalias cb=clipboard\n\n2018 answer\nUse clipboard-cli. It works with macOS, Windows, Linux, OpenBSD, FreeBSD, and Android without any real issues.\nInstall it with:\n\nThen you can do:\n\nIf you want, you can alias to cb by putting the following in your .bashrc, .bash_profile, or .zshrc:"
  },
  {
    "question": "How can I pipe stderr, and not stdout? I have a program that writes information to stdout and stderr, and I need to process the stderr with grep, leaving stdout aside.\nUsing a temporary file, one could do it in two steps:\n\n```\ncommand > /dev/null 2> temp.file\ngrep 'something' temp.file\n\n```\n\nBut how can this be achieved without temp files, using one command and pipes?",
    "answer": "command 2>&1 >/dev/null | grep 'something'\n\nFirst redirect stderr to stdout \u2014 the pipe; then redirect stdout to /dev/null (without changing where stderr is going):\n\nFor the details of I/O redirection in all its variety, see the chapter on Redirections in the Bash reference manual.\nNote that the sequence of I/O redirections is interpreted left-to-right, but pipes are set up before the I/O redirections are interpreted.  File descriptors such as 1 and 2 are references to open file descriptions.  The operation 2>&1 makes file descriptor 2 aka stderr refer to the same open file description as file descriptor 1 aka stdout is currently referring to (see dup2() and open()).  The operation >/dev/null then changes file descriptor 1 so that it refers to an open file description for /dev/null, but that doesn't change the fact that file descriptor 2 refers to the open file description which file descriptor 1 was originally pointing to \u2014 namely, the pipe."
  },
  {
    "question": "Parsing JSON with Unix tools I'm trying to parse JSON returned from a curl request, like so:\n\n```\ncurl 'http://twitter.com/users/username.json' |\n    sed -e 's/[{}]/''/g' | \n    awk -v k=\"text\" '{n=split($0,a,\",\"); for (i=1; i<=n; i++) print a[i]}'\n\n```\n\nThe above splits the JSON into fields, for example:\n\n```\n% ...\n\"geo_enabled\":false\n\"friends_count\":245\n\"profile_text_color\":\"000000\"\n\"status\":\"in_reply_to_screen_name\":null\n\"source\":\"web\"\n\"truncated\":false\n\"text\":\"My status\"\n\"favorited\":false\n% ...\n\n```\n\nHow do I print a specific field (denoted by the -v k=text)?",
    "answer": "curl -s 'https://api.github.com/users/lambda' | jq -r '.name'\ncurl -s 'https://api.github.com/users/lambda' | \\\npython3 -c \"import sys, json; print(json.load(sys.stdin)['name'])\"\nexport PYTHONIOENCODING=utf8\ncurl -s 'https://api.github.com/users/lambda' | \\\npython2 -c \"import sys, json; print json.load(sys.stdin)['name']\"\ncurl -s 'https://api.github.com/users/lambda' | jsawk -a 'return this.name'\ncurl 'http://twitter.com/users/username.json' | jq -r '.text'\n\nThere are a number of tools specifically designed for the purpose of manipulating JSON from the command line, and will be a lot easier and more reliable than doing it with Awk, such as jq:\n\nYou can also do this with tools that are likely already installed on your system, like Python using the json module, and so avoid any extra dependencies, while still having the benefit of a proper JSON parser. The following assume you want to use UTF-8, which the original JSON should be encoded in and is what most modern terminals use as well:\nPython 3:\n\nPython 2:\n\nFrequently Asked Questions\nWhy not a pure shell solution?\nThe standard POSIX/Single Unix Specification shell is a very limited language which doesn't contain facilities for representing sequences (list or arrays) or associative arrays (also known as hash tables, maps, dicts, or objects in some other languages). This makes representing the result of parsing JSON somewhat tricky in portable shell scripts. There are somewhat hacky ways to do it, but many of them can break if keys or values contain certain special characters.\nBash 4 and later, zsh, and ksh have support for arrays and associative arrays, but these shells are not universally available (macOS stopped updating Bash at Bash 3, due to a change from GPLv2 to GPLv3, while many Linux systems don't have zsh installed out of the box). It's possible that you could write a script that would work in either Bash 4 or zsh, one of which is available on most macOS, Linux, and BSD systems these days, but it would be tough to write a shebang line that worked for such a polyglot script.\nFinally, writing a full fledged JSON parser in shell would be a significant enough dependency that you might as well just use an existing dependency like jq or Python instead. It's not going to be a one-liner, or even small five-line snippet, to do a good implementation.\nWhy not use awk, sed, or grep?\nIt is possible to use these tools to do some quick extraction from JSON with a known shape and formatted in a known way, such as one key per line. There are several examples of suggestions for this in other answers.\nHowever, these tools are designed for line based or record based formats; they are not designed for recursive parsing of matched delimiters with possible escape characters.\nSo these quick and dirty solutions using awk/sed/grep are likely to be fragile, and break if some aspect of the input format changes, such as collapsing whitespace, or adding additional levels of nesting to the JSON objects, or an escaped quote within a string. A solution that is robust enough to handle all JSON input without breaking will also be fairly large and complex, and so not too much different than adding another dependency on jq or Python.\nI have had to deal with large amounts of customer data being deleted due to poor input parsing in a shell script before, so I never recommend quick and dirty methods that may be fragile in this way. If you're doing some one-off processing, see the other answers for suggestions, but I still highly recommend just using an existing tested JSON parser.\nHistorical notes\nThis answer originally recommended jsawk, which should still work, but is a little more cumbersome to use than jq, and depends on a standalone JavaScript interpreter being installed which is less common than a Python interpreter, so the above answers are probably preferable:\n\nThis answer also originally used the Twitter API from the question, but that API no longer works, making it hard to copy the examples to test out, and the new Twitter API requires API keys, so I've switched to using the GitHub API which can be used easily without API keys.  The first answer for the original question would be:"
  },
  {
    "question": "Propagate all arguments in a Bash shell script I am writing a very simple script that calls another script, and I need to propagate the parameters from my current script to the script I am executing.\nFor instance, my script name is foo.sh and calls bar.sh.\nfoo.sh:\n\n```\nbar $1 $2 $3 $4\n\n```\n\nHow can I do this without explicitly specifying each parameter?",
    "answer": "cat no_quotes.sh\n\n./echo_args.sh $@\ncat quotes.sh\n\n./echo_args.sh \"$@\"\ncat echo_args.sh\n\necho Received: $1\necho Received: $2\necho Received: $3\necho Received: $4\n./no_quotes.sh first second\nReceived: first\nReceived: second\nReceived:\nReceived:\n./no_quotes.sh \"one quoted arg\"\nReceived: one\nReceived: quoted\nReceived: arg\nReceived:\n./quotes.sh first second\nReceived: first\nReceived: second\nReceived:\nReceived:\n./quotes.sh \"one quoted arg\"\nReceived: one quoted arg\nReceived:\nReceived:\nReceived:\n\nUse \"$@\" instead of plain $@ if you actually wish your parameters to be passed the same.\nObserve:"
  },
  {
    "question": "Defining a variable with or without export What is export for?\nWhat is the difference between:\n\n```\nexport name=value\n\n```\n\nand\n\n```\nname=value\n\n```",
    "answer": "export name=value\nname=value\nMY_VAR=yay node my-script.js\n\nexport makes the variable available to sub-processes.\nThat is,\n\nmeans that the variable name is available to any process you run from that shell process. If you want a process to make use of this variable, use export, and run the process from that shell.\n\nmeans the variable scope is restricted to the shell, and is not available to any other process. You would use this for (say) loop variables, temporary variables etc. An important exception to this rule is that if you define the variable while running a command, that variable will be available to child processes. For example\n\nIn this case MY_VAR will be available to the node process running my-script.\nIt's important to note that exporting a variable doesn't make it available to parent processes. That is, specifying and exporting a variable in a spawned process doesn't make it available in the process that launched it."
  },
  {
    "question": "How to reload .bash_profile from the command line How can I reload file .bash_profile from the command line?\nI can get the shell to recognize changes to .bash_profile by exiting and logging back in, but I would like to be able to do it on demand.",
    "answer": "Simply type source ~/.bash_profile\nAlternatively, if you like saving keystrokes, you can type . ~/.bash_profile"
  },
  {
    "question": "How do I clear/delete the current line in terminal? If I'm using terminal and typing in a line of text for a command, is there a hotkey or any way to clear/delete that line?\nFor example, if my current line/command is something really long like:\n\n```\n> git log --graph --all --blah..uh oh i want to cancel and clear this line <cursor is here now>\n\n```\n\nIs there a hotkey or command to go from the above to:\n\n```\n>\n\n```\n\n?\nUsually I will press the \u2193 key, and if my current line is a brand new one on the history, that will clear it.  But if I'm going through my command history via the \u2191 key and start editing or using those commands, \u2193 will only change the prompt to the next newest command in history, so it doesn't work here unless I press \u2193 multiple times.",
    "answer": "You can use Ctrl+U to clear up to the beginning.\nYou can use Ctrl+W to delete just a word.\nYou can also use Ctrl+C to cancel.\nIf you want to keep the history, you can use Alt+Shift+# to make it a comment.\n\nBash Emacs Editing Mode Cheat Sheet"
  },
  {
    "question": "Set environment variables from file of key/value pairs TL;DR: How do I export a set of key/value pairs from a text file into the shell environment?\n\nFor the record, below is the original version of the question, with examples.\nI'm writing a script in bash which parses files with 3 variables in a certain folder, this is one of them:\n\n```\nMINIENTREGA_FECHALIMITE=\"2011-03-31\"\nMINIENTREGA_FICHEROS=\"informe.txt programa.c\"\nMINIENTREGA_DESTINO=\"./destino/entrega-prac1\"\n\n```\n\nThis file is stored in ./conf/prac1\nMy script minientrega.sh then parses the file using this code:\n\n```\ncat ./conf/$1 | while read line; do\n    export $line\ndone\n\n```\n\nBut when I execute minientrega.sh prac1 in the command line it doesn't set the environment variables\nI also tried using source ./conf/$1 but the same problem still applies\nMaybe there is some other way to do this, I just need to use the environment variables of the file I pass as the argument of my script.",
    "answer": "export MINIENTREGA_FECHALIMITE=\"2011-03-31\"\nexport MINIENTREGA_FICHEROS=\"informe.txt programa.c\"\nexport MINIENTREGA_DESTINO=\"./destino/entrega-prac1\"\n. ./conf/prac1\nsource ./conf/prac1\n\nProblem with your approach is the export in the while loop is happening in a sub shell, and those variable will not be available in current shell (parent shell of while loop).\nAdd export command in the file itself:\n\nThen you need to source in the file in current shell using:\n\nOR"
  },
  {
    "question": "In Bash, how can I check if a string begins with some value? I would like to check if a string begins with \"node\" e.g. \"node001\". Something like\n\n```\nif [ $HOST == node* ]\n  then\n  echo yes\nfi\n\n```\n\nHow can I do it correctly?\n\nI further need to combine expressions to check if HOST is either \"user1\" or begins with \"node\":\n\n```\nif [ [[ $HOST == user1 ]] -o [[ $HOST == node* ]] ];\nthen\necho yes\nfi\n\n> > > -bash: [: too many arguments\n\n```\n\nHow can I do it correctly?",
    "answer": "[[ $a == z* ]]   # True if $a starts with a \"z\" (wildcard matching).\n[[ $a == \"z*\" ]] # True if $a is equal to z* (literal matching).\nHOST=user1\nif  [[ $HOST == user1 ]] || [[ $HOST == node* ]] ;\nthen\necho yes1\nfi\n\nHOST=node001\nif [[ $HOST == user1 ]] || [[ $HOST == node* ]] ;\nthen\necho yes2\nfi\nyes1\nyes2\n\nThis snippet on the Advanced Bash Scripting Guide says:\n\nSo you had it nearly correct; you needed double brackets, not single brackets.\n\nWith regards to your second question, you can write it this way:\n\nWhich will echo\n\nBash's if syntax is hard to get used to (IMO)."
  },
  {
    "question": "How to iterate over arguments in a Bash script I have a complex command that I'd like to make a shell/bash script of.  I can write it in terms of $1 easily:\n\n```\nfoo $1 args -o $1.ext\n\n```\n\nI want to be able to pass multiple input names to the script. What's the right way to do it?  \nAnd, of course, I want to handle filenames with spaces in them.",
    "answer": "for var in \"$@\"\ndo\necho \"$var\"\ndone\nsh test.sh 1 2 '3 4'\n4\n\nUse \"$@\" to represent all the arguments:\n\nThis will iterate over each argument and print it out on a separate line.  $@ behaves like $* except that, when quoted, the arguments are broken up properly if there are spaces in them:"
  },
  {
    "question": "Extract substring in Bash Given a filename in the form someletters_12345_moreleters.ext, I want to extract the 5 digits and put them into a variable.\nSo to emphasize the point, I have a filename with x number of characters then a five digit sequence surrounded by a single underscore on either side then another set of x number of characters.  I want to take the 5 digit number and put that into a variable.\nI am very interested in the number of different ways that this can be accomplished.",
    "answer": "echo 'someletters_12345_moreleters.ext' | cut -d'_' -f 2\nINPUT='someletters_12345_moreleters.ext'\nSUBSTRING=$(echo $INPUT| cut -d'_' -f 2)\necho $SUBSTRING\n\nUse cut:\n\nMore generic:"
  },
  {
    "question": "What does &#39;set -e&#39; mean in a Bash script? I'm studying the content of this preinst file that the script executes before that package is unpacked from its Debian archive (.deb) file.\nThe script has the following code:\n\n```\n#!/bin/bash\nset -e\n# Automatically added by dh_installinit\nif [ \"$1\" = install ]; then\n   if [ -d /usr/share/MyApplicationName ]; then\n     echo \"MyApplicationName is just installed\"\n     return 1\n   fi\n   rm -Rf $HOME/.config/nautilus-actions/nautilus-actions.conf\n   rm -Rf $HOME/.local/share/file-manager/actions/*\nfi\n# End automatically added section\n\n```\n\nMy first query is about the line:\n\n```\nset -e\n\n```\n\nI think that the rest of the script is pretty simple: It checks whether the Debian/Ubuntu package manager is executing an install operation. If it is, it checks whether my application has just been installed on the system. If it has, the script prints the message \"MyApplicationName is just installed\" and ends (return 1 means that the script exits with an error, doesn\u2019t it?).\nIf the user is asking the Debian/Ubuntu package system to install my package, the script also deletes two directories.\nIs this right or am I missing something?",
    "answer": "-e  Exit immediately if a command exits with a non-zero status.\ntrap 'do_something' ERR\n\nFrom help set and Bash Reference Documentation: The Set Builtin:\n\nBut it's considered bad practice by some (Bash FAQ and IRC Freenode #bash FAQ authors). It's recommended to use:\n\nto run do_something function when errors occur.\nSee Why doesn't set -e (or set -o errexit, or trap ERR) do what I expected?"
  },
  {
    "question": "Get current directory or folder name (without the full path) How could I retrieve the current working directory/folder name in a bash script, or even better, just a terminal command.\npwd gives the full path of the current working directory, e.g. /opt/local/bin but I only want bin.",
    "answer": "result=${PWD##*/}          # to assign to a variable\nresult=${result:-/}        # to correct for the case where PWD is / (root)\n\nprintf '%s\\n' \"${PWD##*/}\" # to print to stdout\n\nprintf '%q\\n' \"${PWD##*/}\" # to print to stdout, quoted for use as shell input\ndirname=/path/to/somewhere//\nshopt -s extglob           # enable +(...) glob syntax\nresult=${dirname%%+(/)}    # trim however many trailing slashes exist\nresult=${result##*/}       # remove everything before the last / that still remains\nresult=${result:-/}        # correct for dirname=/ case\nprintf '%s\\n' \"$result\"\ndirname=\"/path/to/somewhere//\"\nresult=\"${dirname%\"${dirname##*[!/]}\"}\" # extglob-free multi-trailing-/ trim\nresult=\"${result##*/}\"                  # remove everything before the last /\nresult=${result:-/}                     # correct for dirname=/ case\n\nNo need for basename, and especially no need for a subshell running pwd (which adds an extra, and expensive, fork operation); the shell can do this internally using parameter expansion:\n\nNote that if you're applying this technique in other circumstances (not PWD, but some other variable holding a directory name), you might need to trim any trailing slashes. The below uses bash's extglob support to work even with multiple trailing slashes:\n\nAlternatively, without extglob:"
  },
  {
    "question": "How do I use sudo to redirect output to a location I don&#39;t have permission to write to? I've been given sudo access on one of our development RedHat linux boxes, and I seem to find myself quite often needing to redirect output to a location I don't normally have write access to.\nThe trouble is, this contrived example doesn't work:\n\n```\nsudo ls -hal /root/ > /root/test.out\n\n```\n\nI just receive the response:\n\n```\n-bash: /root/test.out: Permission denied\n\n```\n\nHow can I get this to work?",
    "answer": "sudo sh -c 'ls -hal /root/ > /root/test.out'\nls -hal /root/ > /root/test.out\n[nobody@so]$ sudo -s\n[root@so]# ls -hal /root/ > /root/test.out\n[root@so]# ^D\n[nobody@so]$\nsudo ls -hal /root/ | sudo tee /root/test.out > /dev/null\n\nYour command does not work because the redirection is performed by your shell which does not have the permission to write to /root/test.out. The redirection of the output is not performed by sudo.\nThere are multiple solutions:\n\nRun a shell with sudo and give the command to it by using the -c option:\n\nCreate a script with your commands and run that script with sudo:\n\nRun sudo ls.sh. See Steve Bennett's answer if you don't want to create a temporary file.\nLaunch a shell with sudo -s then run your commands:\n\nUse sudo tee (if you have to escape a lot when using the -c option):\n\nThe redirect to /dev/null is needed to stop tee from outputting to the screen. To append instead of overwriting the output file\n(>>), use tee -a or tee --append (the last one is specific to GNU coreutils).\n\nThanks go to Jd, Adam J. Forster and Johnathan for the second, third and fourth solutions."
  },
  {
    "question": "How do I execute a program or call a system command? How do I call an external command within Python as if I had typed it in a shell or command prompt?",
    "answer": "import subprocess\n\nsubprocess.run([\"ls\", \"-l\"])\nsubprocess.call([\"ls\", \"-l\"])\n\nUse subprocess.run:\n\nAnother common way is os.system but you shouldn't use it because it is unsafe if any parts of the command come from outside your program or can contain spaces or other special characters, also subprocess.run is generally more flexible (you can get the stdout, stderr, the \"real\" status code, better error handling, etc.). Even the documentation for os.system recommends using subprocess instead.\nOn Python 3.4 and earlier, use subprocess.call instead of .run:"
  },
  {
    "question": "How can I get the current date and time in the terminal and set a custom command in the terminal for it? I have to check the time in a Linux terminal.\nWhat is the command for getting date and time in a Linux terminal?\nIs there a way in which we can set a custom function?",
    "answer": "The command is date\nTo customise the output there are a myriad of options available, see date --help for a list.\nFor example, date '+%A %W %Y %X' gives Tuesday 34 2013 08:04:22 which is the name of the day of the week, the week number, the year and the time."
  },
  {
    "question": "How to move the cursor word by word in the OS X Terminal I know the combination Ctrl+A to jump to the beginning of the current command, and Ctrl+E to jump to the end. \nBut is there any way to jump word by word, like Alt+\u2190/\u2192 in Cocoa applications does?",
    "answer": "Out of the box you can use the quite bizarre Esc+F to move to the beginning of the next word and Esc+B to move to the beginning of the current word."
  },
  {
    "question": "How can I copy the output of a command directly into my clipboard? How can I pipe the output of a command into my clipboard and paste it back when using a terminal? For instance:\n\n```\ncat file | clipboard\n\n```",
    "answer": "cat file | xclip -selection clipboard\nalias \"c=xclip\"\nalias \"v=xclip -o\"\nTerminal 1:\npwd | c\n\nTerminal 2:\ncd `v`\ncat file | xclip\n\nOne way of doing it follows:\n\nInstall xclip, such as:\nsudo apt-get install xclip\n\nPipe the output into xclip to be copied into the clipboard:\ncat file | xclip\n\nPaste the text you just copied into a X application:\nxclip -o\n\nTo paste somewhere else other than an X application, such as a text area of a web page in a browser window, use:\n\nConsider creating an alias:\n\nTo see how useful this is, imagine I want to open my current path in a new terminal window (there may be other ways of doing it like Ctrl+T on some systems, but this is just for illustration purposes):\n\nNotice the ` `  around v. This executes v as a command first and then substitutes it in-place for cd to use.\nOnly copy the content to the X clipboard"
  },
  {
    "question": "Run / Open VSCode from Mac Terminal I'd like to run / open Visual Studio Code from the Mac OSX Terminal by running this command code ..  I found instructions here:\nhttps://code.visualstudio.com/Docs/setup\nApparently I need to include this in my .bashrc file, so I did, but to no avail.\n\n```\ncode () {\n    if [[ $# = 0 ]]\n    then\n        open -a \"Visual Studio Code\"\n    else\n        [[ $1 = /* ]] && F=\"$1\" || F=\"$PWD/${1#./}\"\n        open -a \"Visual Studio Code\" --args \"$F\"\n    fi\n}\n\n```\n\nI edited the .bashrc file here:\n~/.bashrc which points to /Users/username/.bashrc\nWhich .bashrc should I be editing?",
    "answer": "code .\nxattr \"/Applications/Visual Studio Code.app\"\nsudo xattr -r -d com.apple.quarantine \"/Applications/Visual Studio Code.app\"\n\nAccording to the docs on Launching from the command line:\n\nOpen Visual Studio Code\nOpen the command pallette with Command + Shift + P (or F1)\nType Shell in command palette\nSelect Shell Command: Install code in PATH from suggested list\n\nThat's it.\nNow open your terminal type.\n\nTo make this change persist after restart on MacOS\nMany Mac users find this is forgotten and needs to be re-applied after any restart. This may happen if MacOS has applied the quarantine attribute to VS Code, which the OS uses for the \"Are you sure?\" notice applied on first using apps downloaded from the internet.\nTo check if this attribute is applied, look for com.apple.quarantine in the list returned by this command (changing the path if that's not where you installed it):\n\nIf that does return com.apple.quarantine, you can remove the attribute using the same command with the -d flag (alongside -r to recursively remove it from all contained files and sudo to allow the change):\n\n...then do Shell Command : Install code in PATH as above after the attribute has been removed, and it should persist after restart.\nCredit: derflounder.wordpress.com article linked to by RicardoVallejo in this comment."
  },
  {
    "question": "Git branch command behaves like &#39;less&#39; When I use the git branch command to list all branches, I see the output of git branch | less.\nThe command git branch is supposed to show a list of branches, like ls does for files.\nThis is the output I get:\n\nHow do I get the default behaviour of git branch? What causes the paged output?\nMy .gitconfig looks like this:\n\n```\n[user]\n  email = myemail@mail.com\n  name = Dennis H.\n[push]\n  default = simple\n[merge]\n   tool = vimdiff\n[core]\n  editor = nvim\n  excludesfile = /Users/dennish/.gitignore_global\n[color]\n  ui = true\n[alias]\n  br = branch\n  ci = commit -v\n  cam = commit -am\n  co = checkout\n  df = diff\n  st = status\n  sa = stash\n  mt = mergetool\n  cp = cherry-pick\n  pl = pull --rebase\n[difftool \"sourcetree\"]\n  cmd = opendiff \\\"$LOCAL\\\" \\\"$REMOTE\\\"\n[mergetool \"sourcetree\"]\n  cmd = /Applications/SourceTree.app/Contents/Resources/opendiff-w.sh \n  \\\"$LOCAL\\\" \\\"$REMOTE\\\" -ancestor \\\"$BASE\\\" -merge \\\"$MERGED\\\"\n  trustExitCode = true\n\n```",
    "answer": "git config --global pager.branch false\n\nAs mentioned in comments to Mark Adelsberger's answer, this was a default behavior change introduced in Git 2.16.\nYou can turn paged output for git branch back off by default with the pager.branch config setting:"
  },
  {
    "question": "Node Version Manager install - nvm command not found I am trying to install NVM as per these instructions\nI typed in this command in terminal:\n\n```\n$ curl https://raw.github.com/creationix/nvm/master/install.sh | sh\n\n```\n\nAfter running the install, I restart the terminal and attempt to install Node.js with this command:\n\n```\n$ nvm install 0.8\n\n```\n\nbut I get the response:\n\n```\n-bash: nvm: command not found\n\n```\n\nI'm not sure what I am doing wrong here.\nAdditional Info:\nI've been looking around for solutions from other posts and forums. I found another solution using\n\n```\n$ git clone git://github.com/creationix/nvm.git ~/.nvm\n\n```\n\nbut this times out every time I attempt that.",
    "answer": "[[ -s $HOME/.nvm/nvm.sh ]] && . $HOME/.nvm/nvm.sh  # This loads NVM\nls -a | grep .nvm\ngit clone http://github.com/creationix/nvm.git .nvm\n\nCheck your .bash_profile,  .zshrc, or .profile file. You most likely had a problem during the installation.\nYou should have the following at the end of one of those files.\n\nThe . $HOME/.nvm/nvm.sh is the same as source $HOME/.nvm/nvm.sh\nSee: Sourcing a File\nYou can also check to see if you have a .nvm folder.\n\nIf you're missing that folder then the installation failed to run the git command. This could be due to being behind a proxy. Try running the following instead."
  },
  {
    "question": "psql: FATAL: role &quot;postgres&quot; does not exist I'm a postgres novice.\nI installed the postgres.app for mac. I was playing around with the psql commands and I accidentally dropped the postgres database. I don't know what was in it.\nI'm currently working on a tutorial: http://www.rosslaird.com/blog/building-a-project-with-mezzanine/\nAnd I'm stuck at sudo -u postgres psql postgres\nERROR MESSAGE: psql: FATAL:  role \"postgres\" does not exist\n$ which psql\n\n```\n/Applications/Postgres.app/Contents/MacOS/bin/psql\n\n```\n\nThis is what prints out of psql -l\n\n```\n                                List of databases\n    Name    |   Owner    | Encoding | Collate | Ctype |     Access privileges     \n------------+------------+----------+---------+-------+---------------------------\n user       | user       | UTF8     | en_US   | en_US | \n template0  | user       | UTF8     | en_US   | en_US | =c/user                  +\n            |            |          |         |       | user      =CTc/user      \n template1  | user       | UTF8     | en_US   | en_US | =c/user                  +\n            |            |          |         |       | user      =CTc/user      \n(3 rows)\n\n```\n\nSo what are the steps I should take? Delete an everything related to psql and reinstall everything?",
    "answer": "List of roles\nRole name |            Attributes             | Member of\n-----------+-----------------------------------+-----------\npostgres  | Superuser, Create role, Create DB | {}\nsudo -u user psql user\nCREATE USER postgres SUPERUSER;\nCREATE DATABASE postgres WITH OWNER postgres;\n\nNOTE: If you installed postgres using homebrew, see the comments from @user3402754 and @originalhat below.\nNote that the error message does NOT talk about a missing database, it talks about a missing role. Later in the login process it might also stumble over the missing database.\nBut the first step is to check the missing role: What is the output within psql of the command \\du ? On my Ubuntu system the relevant line looks like this:\n\nIf there is not at least one role with superuser, then you have a problem :-)\nIf there is one, you can use that to login. And looking at the output of your \\l command: The permissions for user on the template0 and template1 databases are the same as on my Ubuntu system for the superuser postgres. So I think your setup simple uses user as the superuser. So you could try this command to login:\n\nIf user is really the DB superuser you can create another DB superuser and a private, empty database for him:\n\nBut since your postgres.app setup does not seem to do this, you also should not. Simple adapt the tutorial."
  },
  {
    "question": "How do I see which version of Swift I&#39;m using? I just created a new Swift project within Xcode. I am wondering which version of Swift it's using. \nHow can I see, in Xcode or the terminal, what version of Swift I am using inside my project?",
    "answer": "print(\"Hello, Swift 5.10\")\n\nprint(\"Hello, Swift 5.9\")\n\nprint(\"Hello, Swift 5.8\")\n\nprint(\"Hello, Swift 5.7\")\n\nprint(\"Hello, Swift 5.6\")\n\nprint(\"Hello, Swift 5.5\")\n\nprint(\"Hello, Swift 5.4\")\n\nprint(\"Hello, Swift 5.3\")\n\nprint(\"Hello, Swift 5.2\")\n\nprint(\"Hello, Swift 5.1\")\n\nprint(\"Hello, Swift 5.0\")\n\nprint(\"Hello, Swift 4.2\")\n\nprint(\"Hello, Swift 4.1\")\n\nprint(\"Hello, Swift 4.0\")\n\nprint(\"Hello, Swift 3.2\")\n\nprint(\"Hello, Swift 3.0\")\n\nprint(\"Hello, Swift 2.2\")\n\nprint(\"Hello, Swift 2.1\")\n\nprint(\"Hello, Swift 2.0\")\n\nprint(\"Hello, Swift 1.2\")\n\nprint(\"Hello, Swift 1.1\")\n\nprint(\"Hello, Swift 1.0\")\n\nProject build settings have a block 'Swift Compiler - Languages', which stores information about Swift Language Version in key-value format. It will show you all available (supported) Swift Language Version for your Xcode and active version also by a tick mark.\n\nProject \u25ba (Select Your Project Target) \u25ba Build Settings \u25ba (Type\n'swift_version' in the Search bar) Swift Compiler Language \u25ba Swift Language\nVersion \u25ba Click on Language list to open it (and there will be a tick mark on any one of  list-item, that will be current swift version).\n\nLook at this snapshot, for easy understanding:\n\nWith help of following code, programmatically you can find Swift version supported by your project.\n\nHere is result using Playground (with Xcode 11.x)\nPLEASE NOTE:\nBuilding your code as Swift 4 in Xcode doesn't actually switch to a Swift 4 compiler, it just tells the compiler to try to apply the same language rules that the Swift 4 compiler would have used."
  },
  {
    "question": "How to use &#39;cp&#39; command to exclude a specific directory? I want to copy all files in a directory except some files in a specific sub-directory.\nI have noticed that cp command didn't have the --exclude option.\nSo, how can I achieve this?",
    "answer": "rsync -av --progress sourcefolder /destinationfolder --exclude thefoldertoexclude\nrsync -av --progress sourcefolder /destinationfolder --exclude thefoldertoexclude --exclude anotherfoldertoexclude\n\nrsync is fast and easy:\n\nYou can use --exclude multiples times.\n\nNote that the dir thefoldertoexclude after --exclude option is relative to the sourcefolder, i.e., sourcefolder/thefoldertoexclude.\nAlso you can add -n for dry run to see what will be copied before performing  real operation, and if everything is ok, remove -n from command line."
  },
  {
    "question": "Open terminal here in Mac OS finder Is there something similar to the \"Open Command Window Here\" Windows Powertoy for Mac OS? I've found a couple plugins through a google search but wanted to see what works best for developers out there.",
    "answer": "open -a Terminal /path/to/folder\n\nAs of Mac OS X Lion 10.7, Terminal includes exactly this functionality as a Service. As with most Services, these are disabled by default, so you'll need to enable this to make it appear in the Services menu.\n\nSystem Preferences > Keyboard > Shortcuts > Services\n\nEnable New Terminal at Folder. There's also New Terminal Tab at Folder, which will create a tab in the frontmost Terminal window (if any, else it will create a new window). These Services work in all applications, not just Finder, and they operate on folders as well as absolute pathnames selected in text.\nYou can even assign command keys to them.\nServices appear in the Services submenu of each application menu, and within the contextual menu (Control-Click or Right-Click on a folder or pathname).\nThe New Terminal at Folder service will become active when you select a folder in Finder. You cannot simply have the folder open and run the service \"in place\". Go back to the parent folder, select the relevant folder, then activate the service via the Services menu or context menu.\nIn addition, Lion Terminal will open a new terminal window if you drag a folder (or pathname) onto the Terminal application icon, and you can also drag to the tab bar of an existing window to create a new tab.\nFinally, if you drag a folder or pathname onto a tab (in the tab bar) and the foreground process is the shell, it will automatically execute a \"cd\" command. (Dragging into the terminal view within the tab merely inserts the pathname on its own, as in older versions of Terminal.)\nYou can also do this from the command line or a shell script:\n\nThis is the command-line equivalent of dragging a folder/pathname onto the Terminal application icon.\nOn a related note, Lion Terminal also has new Services for looking up man pages: Open man page in Terminal displays the selected man page topic in a new terminal window, and Search man Pages in Terminal performs \"apropos\" on the selected text. The former also understands man page references (\"open(2)\"), man page command line arguments (\"2 open\") and man page URLs (\"x-man-page://2/open\")."
  },
  {
    "question": "List Git aliases How do I print a list of my git aliases, i.e., something analogous to the bash alias command?",
    "answer": "git help -a\nCommand aliases\nrestore-deleted      !git restore $(git ls-files -d)\n\nJust adding this because it's so simple and I didn't see it in previous answers (sorry if I missed it).\n\nYou'll have to scroll to the bottom (use > as ma11hew28 pointed out)  to see the list, e.g.:\n\nIf you forget even this switch, a simple git help will help you remember:\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help ' or 'git help ' to\nread about a specific subcommand or concept."
  },
  {
    "question": "List of ANSI color escape sequences On most terminals it is possible to colorize output using the \\033 ANSI escape sequence.\nI'm looking for a list of all supported colors and options (like bright and blinking).\nAs there are probably differences between the terminals supporting them, I'm mainly interested in sequences supported by xterm-compatible terminals.",
    "answer": "\\033[XXXm\nprintf(\"\\033[31;1;4mHello\\033[0m\");\nstd::cout<<\"\\033[31;1;4mHello\\033[0m\";\nprint(\"\\033[31;1;4mHello\\033[0m\")\necho -e \"\\033[31;1;4mHello\\033[0m\"\n\\033[31;42m\n\\033[38;5;206m     #That is, \\033[38;5;<FG COLOR>m\n\\033[48;5;57m      #That is, \\033[48;5;<BG COLOR>m\n\\033[38;5;206;48;5;57m\n\\033[38;2;<r>;<g>;<b>m     #Select RGB foreground color\n\\033[48;2;<r>;<g>;<b>m     #Select RGB background color\n\\033[38;2;255;82;197;48;2;155;106;0mHello\nfor i in range(30, 37 + 1):\nprint(\"\\033[%dm%d\\t\\t\\033[%dm%d\" % (i, i, i + 60, i + 60))\n\nprint(\"\\033[39m\\\\033[49m                 - Reset color\")\nprint(\"\\\\033[2K                          - Clear Line\")\nprint(\"\\\\033[<L>;<C>H or \\\\033[<L>;<C>f  - Put the cursor at line L and column C.\")\nprint(\"\\\\033[<N>A                        - Move the cursor up N lines\")\nprint(\"\\\\033[<N>B                        - Move the cursor down N lines\")\nprint(\"\\\\033[<N>C                        - Move the cursor forward N columns\")\nprint(\"\\\\033[<N>D                        - Move the cursor backward N columns\\n\")\nprint(\"\\\\033[2J                          - Clear the screen, move to (0,0)\")\nprint(\"\\\\033[K                           - Erase to end of line\")\nprint(\"\\\\033[s                           - Save cursor position\")\nprint(\"\\\\033[u                           - Restore cursor position\\n\")\nprint(\"\\\\033[4m                          - Underline on\")\nprint(\"\\\\033[24m                         - Underline off\\n\")\nprint(\"\\\\033[1m                          - Bold on\")\nprint(\"\\\\033[21m                         - Bold off\")\n\nThe ANSI escape sequences you're looking for are the Select Graphic Rendition subset. All of these have the form\n\nwhere XXX is a series of semicolon-separated parameters.\nTo say, make text red, bold, and underlined (we'll discuss many other options below) in C you might write:\n\nIn C++ you'd use\n\nIn Python3 you'd use\n\nand in Bash you'd use\n\nwhere the first part makes the text red (31), bold (1), underlined (4) and the last part clears all this (0).\nAs described in the table below, there are a large number of text properties you can set, such as boldness, font, underlining, &c.\nFont Effects\n\nCode\nEffect\nNote\n\n0\nReset / Normal\nall attributes off\n\n1\nBold or increased intensity\n\n2\nFaint (decreased intensity)\nNot widely supported.\n\n3\nItalic\nNot widely supported. Sometimes treated as inverse.\n\n4\nUnderline\n\n5\nSlow Blink\nless than 150 per minute\n\n6\nRapid Blink\nMS-DOS ANSI.SYS; 150+ per minute; not widely supported\n\n7\n[[reverse video]]\nswap foreground and background colors\n\n8\nConceal\nNot widely supported.\n\n9\nCrossed-out\nCharacters legible, but marked for deletion.  Not widely supported.\n\n10\nPrimary(default) font\n\n11\u201319\nAlternate font\nSelect alternate font n-10\n\n20\nFraktur\nhardly ever supported\n\n21\nBold off or Double Underline\nBold off not widely supported; double underline hardly ever supported.\n\n22\nNormal color or intensity\nNeither bold nor faint\n\n23\nNot italic, not Fraktur\n\n24\nUnderline off\nNot singly or doubly underlined\n\n25\nBlink off\n\n27\nInverse off\n\n28\nReveal\nconceal off\n\n29\nNot crossed out\n\n30\u201337\nSet foreground color\nSee color table below\n\n38\nSet foreground color\nNext arguments are 5;<n> or 2;<r>;<g>;<b>, see below\n\n39\nDefault foreground color\nimplementation defined (according to standard)\n\n40\u201347\nSet background color\nSee color table below\n\n48\nSet background color\nNext arguments are 5;<n> or 2;<r>;<g>;<b>, see below\n\n49\nDefault background color\nimplementation defined (according to standard)\n\n51\nFramed\n\n52\nEncircled\n\n53\nOverlined\n\n54\nNot framed or encircled\n\n55\nNot overlined\n\n60\nideogram underline\nhardly ever supported\n\n61\nideogram double underline\nhardly ever supported\n\n62\nideogram overline\nhardly ever supported\n\n63\nideogram double overline\nhardly ever supported\n\n64\nideogram stress marking\nhardly ever supported\n\n65\nideogram attributes off\nreset the effects of all of 60-64\n\n90\u201397\nSet bright foreground color\naixterm (not in standard)\n\n100\u2013107\nSet bright background color\naixterm (not in standard)\n\n2-bit Colours\nYou've got this already!\n4-bit Colours\nThe standards implementing terminal colours began with limited (4-bit) options. The table below lists the RGB values of the background and foreground colours used for these by a variety of terminal emulators:\n\nUsing the above, you can make red text on a green background (but why?) using:\n\n11 Colours (An Interlude)\nIn their book \"Basic Color Terms: Their Universality and Evolution\", Brent Berlin and Paul Kay used data collected from twenty different languages from a range of language families to identify eleven possible basic color categories: white, black, red, green, yellow, blue, brown, purple, pink, orange, and gray.\nBerlin and Kay found that, in languages with fewer than the maximum eleven color categories, the colors followed a specific evolutionary pattern. This pattern is as follows:\n\nAll languages contain terms for black (cool colours) and white (bright colours).\nIf a language contains three terms, then it contains a term for red.\nIf a language contains four terms, then it contains a term for either green or yellow (but not both).\nIf a language contains five terms, then it contains terms for both green and yellow.\nIf a language contains six terms, then it contains a term for blue.\nIf a language contains seven terms, then it contains a term for brown.\nIf a language contains eight or more terms, then it contains terms for purple, pink, orange or gray.\n\nThis may be why story Beowulf only contains the colours black, white, and red. Homer's Odyssey contains black almost 200 times and white about 100 times. Red appears 15 times, while yellow and green appear only 10 times. (More information here)\nDifferences between languages are also interesting: note the profusion of distinct colour words used by English vs. Chinese. However, digging deeper into these languages shows that each uses colour in distinct ways. (More information)\n\nGenerally speaking, the naming, use, and grouping of colours in human languages is fascinating. Now, back to the show.\n8-bit (256) colours\nTechnology advanced, and tables of 256 pre-selected colours became available, as shown below.\n\nUsing these above, you can make pink text like so:\n\nAnd make an early-morning blue background using\n\nAnd, of course, you can combine these:\n\nThe 8-bit colours are arranged like so:\n\nRange\nDescription\n\n0x00-0x07\nstandard colors (same as the 4-bit colours)\n\n0x08-0x0F\nhigh intensity colors\n\n0x10-0xE7\n6 \u00d7 6 \u00d7 6 cube (216 colors): 16 + 36 \u00d7 r + 6 \u00d7 g + b (0 \u2264 r, g, b \u2264 5)\n\n0xE8-0xFF\ngrayscale from black to white in 24 steps\n\nALL THE COLOURS\nNow we are living in the future, and the full RGB spectrum is available using:\n\nSo you can put pinkish text on a brownish background using\n\nSupport for \"true color\" terminals is listed here.\nMuch of the above is drawn from the Wikipedia page \"ANSI escape code\".\nA Handy Script to Remind Yourself\nSince I'm often in the position of trying to remember what colours are what, I have a handy script called: ~/bin/ansi_colours:\n\nThis prints"
  },
  {
    "question": "Unable to show a Git tree in terminal Killswitchcollective.com's old article, 30 June 2009, has the following inputs and outputs\n\n```\ngit co master\ngit merge [your_branch]\ngit push\n\nupstream    A-B-C-D-E            A-B-C-D-E-F-G\n                 \\        ---->               \\\nyour branch       C-D-E                        G\n\n```\n\nI am interested how you get the tree like-view of commits in your terminal without using Gitk or Gitx in OS/X.\nHow can you get the tree-like view of commits in terminal?",
    "answer": "git log --graph --oneline --all\nexport LESS=\"-R\"\ngit log --graph --pretty=oneline --abbrev-commit | tig   // Masi needed this\ngit log --graph --pretty=oneline --abbrev-commit\n\nHow can you get the tree-like view of commits in terminal?\n\nis a good start.\nYou may get some strange letters. They are ASCII codes for colors and structure. To solve this problem add the following to your .bashrc:\n\nsuch that you do not need use Tig's ASCII filter by\n\nThe article text-based graph from Git-ready contains other options:\n\nRegarding the article you mention, I would go with Pod's answer: ad-hoc hand-made output.\n\nJakub Nar\u0119bski mentions in the comments tig, a ncurses-based text-mode interface for git. See their releases.\nIt added a --graph option back in 2007."
  },
  {
    "question": "OS X Terminal Colors I just got a Mac after working with Ubuntu Linux for some time. Among the many things I'm trying to figure out is the absence of colors in my terminal window- like the ones that are shown (on Linux) when running ls -la or git status.\nHow can one activate colors in their shell?",
    "answer": "export CLICOLOR=1\nexport LSCOLORS=GxFxCxDxBxegedabagaced\n\nHere is a solution I've found to enable the global\nterminal colors.\nEdit your .bash_profile (since OS X 10.8) \u2014 or (for 10.7 and earlier): .profile or .bashrc or /etc/profile (depending on availability) \u2014 in your home directory and add following code:\n\nCLICOLOR=1 simply enables coloring of your terminal.\nLSCOLORS=... specifies how to color specific items.\nAfter editing .bash_profile, start a Terminal and force the changes to take place by executing:\nsource ~/.bash_profile\nThen go to Terminal > Preferences, click on the Profiles tab and then the Text subtab and check Display ANSI Colors.\nVerified on Sierra (May 2017)."
  },
  {
    "question": "Keep SSH session alive I use ssh -p8520 username@remote_host to login remote server.\nIssue:\nIt is always connected and works properly when I am in the work place. Unfortunately, terminal freezes in 10 - 15 minutes after I connected with the remote server from home. \nThere's no error/timeout report on the console but the cursor cannot move any more.\nWhen enter w to check the login users, some zombies login users are there, and I have to kill them manually.\nThis is quite annoying. Can anyone help me?",
    "answer": "Host remotehost\nHostName remotehost.com\nServerAliveInterval 240\nHost *\nServerAliveInterval 240\n\nThe ssh daemon (sshd), which runs server-side, closes the connection from the server-side if the client goes silent (i.e., does not send information). To prevent connection loss, instruct the ssh client to send a sign-of-life signal to the server once in a while.\nThe configuration for this is in the file $HOME/.ssh/config, create the file if it does not exist (the config file must not be world-readable, so run chmod 600 ~/.ssh/config after creating the file). To send the signal every e.g. four minutes (240 seconds) to the remote host, put the following in that configuration file:\n\nTo enable sending a keep-alive signal for all hosts, place the following contents in the configuration file:"
  },
  {
    "question": "Change the default terminal in Visual Studio Code I am using Visual Studio Code on my Windows 10 PC. I want to change my default terminal from Windows PowerShell to Bash on Ubuntu (on Windows).\nHow can I do that?",
    "answer": "You can also select your default terminal by pressing F1 in Visual Studio Code and typing/selecting Terminal: Select Default Profile (or Terminal: Select Default Shell in older Visual Studio Code versions).\n\nOlder:"
  },
  {
    "question": "How can I turn off &quot;scrolling the history&quot; in iTerm2 I have installed the new iTerm 2.\nIt asked me in a yellow bar at the top if I'd like to enable a mouse feature. Unfortunately, I don't remember the exact sentence anymore.\nBy accident I approved. Now when I use the scroll wheel on the mouse in iTerm, it doesn't scroll the up anymore, but instead it goes triggers the command history as if I had pressed the up-cursor.\nWhere I can toggle this option to turn it off again?",
    "answer": "A few terminals, including iTerm2, have a feature where they change the behavior of the wheel mouse when a full-screen program such as vi, or screen or tmux is running.  This happens when those programs use the alternate screen, to provide a useful function.  Normally, when using the alternate screen in iTerm2, the wheel mouse acts like the scrollbar, scrolling the entire screen up/down.  But when this feature is enabled, iTerm2 sends cursor up/down keys, making your command-history change.\nAs suggested in another comment, select the Preferences menu:\n\nand in that, select the Advanced tab.  Scroll down to the Mouse section,\n\nand toggle the entry for\nScroll wheel sends arrow keys when in alternate screen mode\nfrom Yes to No.  You will have to restart iTerm2 for the change to take effect.  (With iTerm2 v3.1.5 changes take effect without restarting.)"
  },
  {
    "question": "Clear a terminal screen for real Using the clear command on the terminal only fools the user into thinking the screen has been cleared...you can still see output from the previous commands when you scroll using the mouse. This makes life difficult when you are drowning in a tsunami of text. \nVarious solutions (escape code etc.) which can be found on the Internet are only variations of what the clear command already does.\nSo how do you clear the contents of a terminal in Linux for real?",
    "answer": "printf \"\\033c\"\nalias cls='printf \"\\033c\"'\n\\033 == \\x1B == 27 == ESC\nprintf \"\\ec\" #\\e is ESC in bash\necho -en \"\\ec\" #thanks @Jonathon Reinhart.\nclear && echo -en \"\\e[3J\"\nalias cls='clear && echo -en \"\\e[3J\"'\n\nUse the following command to do a clear screen instead of merely adding new lines ...\n\nyes that's a 'printf' on the bash prompt.\nYou will probably want to define an alias though...\n\nExplanation\n\nSo this becomes <ESC>c which is the VT100 escape code for resetting the terminal. Here is some more information on terminal escape codes.\nEdit\nHere are a few other ways of doing it...\n\nKDE\nThe above does not work on the KDE console (called Konsole) but there is hope! Use the following sequence of commands to clear the screen and the scroll-back buffer...\n\nOr perhaps use the following alias on KDE...\n\nI got the scroll-back clearing command from here."
  },
  {
    "question": "How to remove files and directories quickly via terminal (bash shell) From a terminal window:\nWhen I use the rm command it can only remove files. \nWhen I use the rmdir command it only removes empty folders.\nIf I have a directory nested with files and folders within folders with files and so on, is there a way to delete all the files and folders without all the strenuous command typing?\nIf it makes a difference, I am using the Mac Bash shell from a terminal, not Microsoft DOS or Linux.",
    "answer": "rm -rf some_dir\n\n-r \"recursive\"\n-f \"force\" (suppress confirmation messages)\nBe careful!"
  },
  {
    "question": "How do I find the width &amp; height of a terminal window? As a simple example, I want to write a CLI script which can print = across the entire width of the terminal window.\n\n```\n#!/usr/bin/env php\n<?php\necho str_repeat('=', ???);\n\n```\n\nor\n\n```\n#!/usr/bin/env python\nprint '=' * ???\n\n```\n\nor\n\n```\n#!/usr/bin/env bash\nx=0\nwhile [ $x -lt ??? ]; do echo -n '='; let x=$x+1 done; echo\n\n```",
    "answer": "tput cols tells you the number of columns.\ntput lines tells you the number of rows."
  },
  {
    "question": "How do I output coloured text to a Linux terminal? How do I print coloured characters to a Linux terminal that supports it?\nHow do I tell whether the terminal supports colour codes?",
    "answer": "cout << \"\\033[1;31mbold red text\\033[0m\\n\";\nforeground background\nblack        30         40\nred          31         41\ngreen        32         42\nyellow       33         43\nblue         34         44\nmagenta      35         45\ncyan         36         46\nwhite        37         47\nreset             0  (everything back to normal)\nbold/bright       1  (often a brighter shade of the same colour)\nunderline         4\ninverse           7  (swap foreground and background colours)\nbold/bright off  21\nunderline off    24\ninverse off      27\n\nYou need to output ANSI colour codes. Note that not all terminals support this; if colour sequences are not supported, garbage will show up.\nExample:\n\nHere, \\033 is the ESC character, ASCII 27. It is followed by [, then zero or more numbers separated by ;, and finally the letter m. The numbers describe the colour and format to switch to from that point onwards.\nThe codes for foreground and background colours are:\n\nAdditionally, you can use these:\n\nSee the table on Wikipedia for other, less widely supported codes.\n\nTo determine whether your terminal supports colour sequences, read the value of the TERM environment variable. It should specify the particular terminal type used (e.g. vt100, gnome-terminal, xterm, screen, ...). Then look that up in the terminfo database; check the colors capability."
  },
  {
    "question": "`npm install` fails on node-gyp rebuild with `gyp: No Xcode or CLT version detected!` Every time I try npm install. I get the following error. How do I fix it?\n\n```\ngyp: No Xcode or CLT version detected!\n\n```\n\nI am on node -v \u2192 v8.8.0 & npm -v \u2192 v6.11.3\nI tried to run it on VSCode terminal and iTerm, but both ended up getting the same error. (both are updated to the latest version). The only new thing I did is updating my macOS to the latest version (Catalina 10.15.3 today).\n\n```\n$ npm install          Fri Mar  6 17:22:40 2020\n\n> fsevents@1.2.11 install /Users/synapse/Documents/synapsefi-dev-ui/node_modules/watchpack/node_modules/fsevents\n> node-gyp rebuild\n\nNo receipt for 'com.apple.pkg.CLTools_Executables' found at '/'.\n\nNo receipt for 'com.apple.pkg.DeveloperToolsCLILeo' found at '/'.\n\nNo receipt for 'com.apple.pkg.DeveloperToolsCLI' found at '/'.\n\ngyp: No Xcode or CLT version detected!\ngyp ERR! configure error\ngyp ERR! stack Error: `gyp` failed with exit code: 1\ngyp ERR! stack     at ChildProcess.onCpExit (/Users/synapse/.nvm/versions/node/v8.8.0/lib/node_modules/npm/node_modules/node-gyp/lib/configure.js:344:16)\ngyp ERR! stack     at emitTwo (events.js:125:13)\ngyp ERR! stack     at ChildProcess.emit (events.js:213:7)\ngyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:200:12)\ngyp ERR! System Darwin 19.3.0\ngyp ERR! command \"/Users/synapse/.nvm/versions/node/v8.8.0/bin/node\" \"/Users/synapse/.nvm/versions/node/v8.8.0/lib/node_modules/npm/node_modules/node-gyp/bin/node-gyp.js\" \"rebuild\"\ngyp ERR! cwd /Users/synapse/Documents/synapsefi-dev-ui/node_modules/watchpack/node_modules/fsevents\ngyp ERR! node -v v8.8.0\ngyp ERR! node-gyp -v v5.0.3\ngyp ERR! not ok\nnpm WARN synapsefi-dev-ui@2.0.20 No repository field.\nnpm WARN The package country-data is included as both a dev and production dependency.\nnpm WARN The package react-dropzone is included as both a dev and production dependency.\nnpm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.11 (node_modules/watchpack/node_modules/fsevents):\nnpm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.11 install: `node-gyp rebuild`\nnpm WARN optional SKIPPING OPTIONAL DEPENDENCY: Exit status 1\n\n```",
    "answer": "xcode-select --print-path\n\nsudo xcode-select --switch /Library/Developer/CommandLineTools\n\nsudo xcode-select --reset\n\nsudo rm -rf $(xcode-select --print-path)\n\nxcode-select --install\n\nEven though you have them installed (my case), but by upgrading to Catalina (10.15.*) you can get this error (my case :) ).\nTherefore, simply installing wouldn't help as you will get an error that they are already installed.\nTherefore you need to hopefully just (I) reset the tool or in worse case (II) uninstall and install (requires large redownload) it again (this is based on the @Dane_duPlessis's answer).\n\nNOTE:\n\nYou do not need to do it per project, but only once\nThere was also discussion that you have to do it on every OSX update, in my case, later updating the OSX system didn't trigger this issue again\n\nCredits to: gyp: No Xcode or CLT version detected macOS Catalina"
  },
  {
    "question": "count number of lines in terminal output couldn't find this on SO. I ran the following command in the terminal: \n\n```\n>> grep -Rl \"curl\" ./\n\n```\n\nand this displays the list of files where the keyword curl occurs. I want to count the number of files. First way I can think of, is to count the number of lines in the output that came in the terminal. How can I do that?",
    "answer": "grep -Rl \"curl\" ./ | wc -l\n\nPipe the result to wc using the -l (line count) switch:"
  },
  {
    "question": "Setting the correct encoding when piping stdout in Python When piping the output of a Python program, the Python interpreter gets confused about encoding and sets it to None. This means a program like this:\n\n```\n# -*- coding: utf-8 -*-\nprint u\"\u00e5\u00e4\u00f6\"\n\n```\n\nwill work fine when run normally, but fail with:\n\nUnicodeEncodeError: 'ascii' codec can't encode character u'\\xa0' in position 0: ordinal not in range(128)\n\nwhen used in a pipe sequence.\nWhat is the best way to make this work when piping? Can I just tell it to use whatever encoding the shell/filesystem/whatever is using? \nThe suggestions I have seen thus far is to modify your site.py directly, or hardcoding the defaultencoding using this hack:\n\n```\n# -*- coding: utf-8 -*-\nimport sys\nreload(sys)\nsys.setdefaultencoding('utf-8')\nprint u\"\u00e5\u00e4\u00f6\"\n\n```\n\nIs there a better way to make piping work?",
    "answer": "print u\"\u00e5\u00e4\u00f6\".encode('utf-8')\nimport sys\nfor line in sys.stdin:\n\nline = line.decode('iso8859-1')\n\nline = line.upper()\n\nline = line.encode('utf-8')\nsys.stdout.write(line)\n\nYour code works when run in an script because Python encodes the output to whatever encoding your terminal application is using. If you are piping you must encode it yourself.\nA rule of thumb is: Always use Unicode internally. Decode what you receive, and encode what you send.\n\nAnother didactic example is a Python program to convert between ISO-8859-1 and UTF-8, making everything uppercase in between.\n\nSetting the system default encoding is a bad idea, because some modules and libraries you use can rely on the fact it is ASCII. Don't do it."
  },
  {
    "question": "Mac OS X Terminal: Map option+delete to &quot;backward delete word&quot; Tried to map it from Preferences -> Settings -> Keyboard, but the \"key\" combo box has only \"forward delete\" but no \"delete\". My keyboard on the other hand has only \"delete\" and no \"forward delete\"!\nIs there some other way to do it except from the preferences?",
    "answer": "Enable option key as meta key\n\nGo to Terminal > Settings > Profiles > Keyboard\nCheck Use option key as meta key.\n\nImage\nOn macOS High Sierra 10.13.6, captured on October 23, 2018.\n\nNotes\nMany applications (including bash and tcsh) treat Meta-Delete as \"backward delete word.\""
  },
  {
    "question": "iTerm2 keyboard shortcut - split pane navigation I have been a long time user of the standard Mac Terminal. Decided to experiment with iTerm2 after hearing good things about it from my colleagues.\nOne of the more useful features I am seeing on iTerm2 is its split panes (much like vim split buffers).\nIn vim, I can move between split buffers using Ctrl+W+arrowkeys. Is there a keyboard shortcut for iTerm2 to move between panes as well?",
    "answer": "From the documentation:\n\nCmd] and Cmd[  navigates among split panes in order of use."
  },
  {
    "question": "Switching from zsh to bash on OS X, and back again? I'm learning to develop in Rails, and have discovered the power of zsh. However, for some of my other tasks, I wish to use normal bash.\nAlthough they are the same, I just feel comfortable with the layout of bash in some situations.\nHow do I switch back and forth, or turn zsh on and off?",
    "answer": "exec bash\nexec zsh\n\nYou can just use exec to replace your current shell with a new shell:\nSwitch to bash:\n\nSwitch to zsh:\n\nThis won't affect new terminal windows or anything, but it's convenient."
  },
  {
    "question": "Linux command to list all available commands and aliases Is there a Linux command that will list all available commands and aliases for this terminal session?\nAs if you typed 'a' and pressed tab, but for every letter of the alphabet.\nOr running 'alias' but also returning commands.\nWhy?  I'd like to run the following and see if a command is available:\n\n```\nListAllCommands | grep searchstr\n\n```",
    "answer": "compgen -ac | grep searchstr\n\nYou can use the bash(1) built-in compgen\n\ncompgen -c will list all the commands you could run.\ncompgen -a will list all the aliases you could run.\ncompgen -b will list all the built-ins you could run.\ncompgen -k will list all the keywords you could run.\ncompgen -A function will list all the functions you could run.\ncompgen -A function -abck will list all the above in one go.\n\nCheck the man page for other completions you can generate.\nTo directly answer your question:\n\nshould do what you want."
  },
  {
    "question": "How to get Linux console window width in Python Is there a way in python to programmatically determine the width of the console? I mean the number of characters that fits in one line without wrapping, not the pixel width of the window.\nEdit\nLooking for a solution that works on Linux",
    "answer": ">>> import shutil\n>>> shutil.get_terminal_size((80, 20))  # pass fallback\nos.terminal_size(columns=87, lines=23)  # returns a named-tuple\n\nNot sure why it is in the module shutil, but it landed there in Python 3.3.  See:\nQuerying the size of the output terminal\n\nA low-level implementation is in the os module.  Cross-platform\u2014works under Linux, Mac OS, and Windows, probably other Unix-likes.  There's a backport as well, though no longer relevant."
  },
  {
    "question": "How do you run JavaScript script through the Terminal? For instance, if you were to run a Python script you would type python filename.py. Or, if you wanted to run a C program, make filename and then ./filename. How do you do this with .js files?",
    "answer": "> rhino filename.js\n\nYou would need a JavaScript engine (such as Mozilla's Rhino) in order to evaluate the script - exactly as you do for Python, though the latter ships with the standard distribution.\nIf you have Rhino (or alternative) installed and on your path, then running JS can indeed be as simple as\n\nIt's worth noting though that while JavaScript is simply a language in its own right, a lot of particular scripts assume that they'll be executing in a browser-like environment - and so try to access global variables such as location.href, and create output by appending DOM objects rather than calling print.\nIf you've got hold of a script which was written for a web page, you may need to wrap or modify it somewhat to allow it to accept arguments from stdin and write to stdout.  (I believe Rhino has a mode to emulate standard browser global vars which helps a lot, though I can't find the docs for this now.)"
  },
  {
    "question": "adb command not found I need to run an adb forward command before I could use the ezkeyboard application which allows user to type on the phone using browser. \nWhen I run adb forward tcp:8080 tcp:8080 command I get the adb command not found error message.\nI can run android command from terminal. Why adb is not working?",
    "answer": "Make sure adb is in your user's $PATH variable.\nor\nYou can try to locate it with whereis and run it with ./adb"
  },
  {
    "question": "How do I update zsh to the latest version? I recently switched to zsh on my Terminal.app on my OS X machine successfully. The version number of zsh is 4.3.11.",
    "answer": "brew info zsh\n\nbrew install --without-etcdir zsh\n\nsudo vim /etc/shells\n\n/usr/local/bin/zsh\n\nchsh -s /usr/local/bin/zsh\n\nIf you have Homebrew installed, you can do this."
  },
  {
    "question": "Is there a better Windows Console Window? I find working on the command line in Windows frustrating, primarily because the console window is wretched to use compared to terminal applications on linux and OS X such as \"rxvt\", \"xterm\", or \"Terminal\".  Major complaints:\n\nNo standard copy/paste.  You have to turn on \"mark\" mode and it's only available from a multi-level popup triggered by the (small) left hand corner button.  Then copy and paste need to be invoked from the same menu\nYou can't arbitrarily resize the window by dragging, you need to set a preference (back to the multi-level popup) each time you want to resize a window\nYou can only make the window so big before horizontal scroll bars enter the picture. Horizontal scroll bars suck.\nWith the cmd.exe shell, you can't navigate to folders with \\\\netpath notation (UNC?), you need to map a network drive.  This sucks when working on multiple machines that are going to have different drives mapped\n\nAre there any tricks or applications, (paid or otherwise), that address these issue?",
    "answer": "Sorry for the self-promotion, I'm the author of another Console Emulator, not mentioned here.\nConEmu is opensource console emulator with tabs, which represents multiple consoles and simple GUI applications as one customizable GUI window.\nInitially, the program was designed to work with Far Manager (my favorite shell replacement - file and archive management, command history and completion, powerful editor). But ConEmu can be used with any other console application or simple GUI tools (like PuTTY for example). ConEmu is a live project, open to suggestions.\nA brief excerpt from the long list of options:\n\nLatest versions of ConEmu may set up itself as default terminal for Windows\nUse any font installed in the system, or copied to a folder of the program (ttf, otf, fon, bdf)\nRun selected tabs as Administrator (Vista+) or as selected user\nWindows 7 Jump lists and Progress on taskbar\nIntegration with DosBox (useful in 64bit systems to run DOS applications)\nSmooth resize, maximized and fullscreen window modes\nScrollbar initially hidden, may be revealed by mouseover or checkbox in settings\nOptional settings (e.g. pallette) for selected applications\nUser friendly text and block selection (from keyboard or mouse), copy, paste, text search in console\nANSI X3.64 and Xterm 256 color\n\nFar Manager users will acquire shell style drag-n-drop, thumbnails and tiles in panles, tabs for editors and viewers, true colors and font styles (italic/bold/underline).\nPS. Far Manager supports UNC paths (\\\\server\\share\\...)"
  },
  {
    "question": "Clear the terminal in Python Does any standard \"comes with batteries\" method exist to clear the terminal screen programatically from a Python script, or do I have to use curses (the libraries, not the words)?",
    "answer": "print(chr(27) + \"[2J\")\n\nUse escape sequences:"
  },
  {
    "question": "How do I edit $PATH (.bash_profile) on OS X? I am trying to edit an entry to PATH, as I did something wrong.\nI am using Mac\u00a0OS\u00a0X v10.10.3 (Yosemite)\nI have tried:\n\n```\ntouch ~/.bash_profile; open ~/.bash_profile\n\n```\n\nBut the file editor opens with nothing inside.\nMy problem:\nI am trying to install ANDROID_HOME to my PATH\nI misspelled it, but when I closed the terminal and went back it was gone, so I tried again:\n\n\n```\nexport ANDROID_HOME=/<installation location>/android-sdk-macosx\nexport PATH=${PATH}:$ANDROID_HOME/tools:$ANDROID_HOME/platform-tools\n\n```\n\n\nThis time, I typed the command correctly but, when I closed the terminal, my settings disappeared again.\nHow do I execute my desired settings?\nIf I was to edit bash.profile, how would I enter the above code?",
    "answer": "touch ~/.bash_profile; open ~/.bash_profile\nnano ~/.bash_profile\nmate ~/.bash_profile\nvim ~/.bash_profile\nexport ANDROID_HOME=/<installation location>/android-sdk-macosx\nexport PATH=${PATH}:$ANDROID_HOME/tools:$ANDROID_HOME/platform-tools\npbpaste > ~/.bash_profile\ncat > ~/.bash_profile\n\nYou have to open that file with a text editor and then save it.\n\nIt will open the file with TextEdit, paste your things and then save it. If you open it again you'll find your edits.\nYou can use other editors:\n\nBut if you don't know how to use them, it's easier to use the open approach.\n\nAlternatively, you can rely on pbpaste. Copy\n\nin the system clipboard and then in a shell run\n\nOr alternatively you can also use cat\n\n(now cat waits for input: paste the two export definitions and then hit Ctrl + D)."
  },
  {
    "question": "Using scp to copy a file to Amazon EC2 instance? I am trying to use my Mac Terminal to scp a file from Downloads (phpMyAdmin I downloaded online) to my Amazon EC2 instance.\nThe command I used was:\n\n```\nscp -i myAmazonKey.pem phpMyAdmin-3.4.5-all-languages.tar.gz  hk22@mec2-50-17-16-67.compute-1.amazonaws.com:~/.\n\n```\n\nThe error I got:\n\n```\nWarning: Identity file myAmazonKey.pem not accessible: No such file or directory.\nPermission denied (publickey).\nlost connection\n\n```\n\nBoth my myAmazonkey.pem and phpMyAdmin-3.4.5-all-languages.tar.gz are in Downloads, so then I tried\n\n```\nscp -i /Users/Hello_Kitty22/Downloads/myAmazonKey.pem /Users/Hello_Kitty22/Downloads/phpMyAdmin-3.4.5-all-languages.tar.gz  hk22@mec2-50-17-16-67.compute-1.amazonaws.com:~/.\n\n```\n\nand the error I got:\n\n```\nWarning: Identity file /User/Hello_Kitty22/Downloads/myAmazonkey.pem not accessible: No such file or directory.\nPermission denied (publickey).\nlost connection\n\n```\n\nCan anyone please tell me how to fix my problem?\np.s. there is a similar post: scp (secure copy) to ec2 instance without password\nbut it doesn't answer my question.",
    "answer": "scp -i myAmazonKey.pem phpMyAdmin-3.4.5-all-languages.tar.gz ec2-user@mec2-50-17-16-67.compute-1.amazonaws.com:~/.\n\nTry specifying the user to be ec2-user, e.g.\n\nSee Connecting to Linux/UNIX Instances Using SSH."
  },
  {
    "question": "Expert R users, what&#39;s in your .Rprofile? I have always found startup profile files of other people both useful and instructive about the language. Moreover, while I have some customization for Bash and Vim, I have nothing for R.\nFor example, one thing I always wanted is different colors for input and output text in a window terminal, and maybe even syntax highlighting.",
    "answer": "options(\"width\"=160)                # wide display with multiple monitors\noptions(\"digits.secs\"=3)            # show sub-second time stamps\n\nr <- getOption(\"repos\")             # hard code the US repo for CRAN\nr[\"CRAN\"] <- \"http://cran.us.r-project.org\"\noptions(repos = r)\nrm(r)\n\nsetHook(packageEvent(\"grDevices\", \"onLoad\"),\nfunction(...) grDevices::X11.options(width=8, height=8,\nxpos=0, pointsize=10,\n\ntype=\"xlib\"))      # old default\n\noptions(prompt=\"R> \", digits=4, show.signif.stars=FALSE)\n\noptions(\"pdfviewer\"=\"okular\")         # on Linux, use okular as the pdf viewer\n\nHere is mine. It won't help you with the coloring but I get that from ESS and Emacs..."
  },
  {
    "question": "Temporarily change current working directory in bash to run a command I know I can use cd command to change my working directory in bash.  \nBut if I do this command:\n\n```\ncd SOME_PATH && run_some_command\n\n```\n\nThen the working directory will be changed permanently. Is there some way to change the working directory just temporarily like this?\n\n```\nPWD=SOME_PATH run_some_command\n\n```",
    "answer": "(cd SOME_PATH && exec_some_command)\npwd\n/home/abhijit\n(cd /tmp && pwd)  # directory changed in the subshell\n/tmp\npwd               # parent shell's pwd is still the same\n/home/abhijit\n\nYou can run the cd and the executable in a subshell by enclosing the command line in a pair of parentheses:\n\nDemo:"
  },
  {
    "question": "How do I clear the scrollback buffer in tmux? I want to clear all scrollback history in a particular tmux pane.",
    "answer": "bind -n C-k clear-history\n\nThis same question has been plaguing me for quite some time.  Here's the best I've come up with.  Put this into your .tmux.conf file:\n\nThis binds ctrl-k to the tmux clear-history command.  The -n after bind makes it so you don't have to issue the tmux command prefix (ctrl-b by default).  I use bash, so ctrl-l already does the equivalent of typing \"clear\" at the command line.  With these two keys I get a nice ctrl-l, ctrl-k combo, which moves all the scroll buffer off the screen (the \"clear\") and then deletes all that history (the tmux \"clear-history\" command).\nIt's not quite as nice as Terminal's, iTerm's, or Konsole's 1-key combos for clearing it out, but it's a world better than typing in clear-history all the time."
  },
  {
    "question": "npm install -g less does not work: EACCES: permission denied I'm trying to set up less on phpstorm so I can compile .less files to .css on save. I have installed node.js and the next step (according to this https://www.jetbrains.com/webstorm/help/transpiling-sass-less-and-scss-to-css.html) is running this command in the terminal \n\n```\nnpm install -g less\n\n```\n\nHowever when I do this I get these errors\n\n```\n\u2601  ~  npm install -g less\nnpm WARN install Couldn't install optional dependency: EACCES: permission denied, mkdir '/Users/brentscholl/.npm/mkdirp/0.5.1'\nnpm WARN install Couldn't install optional dependency: EACCES: permission denied, mkdir '/Users/brentscholl/.npm/graceful-fs/3.0.8'\nnpm WARN install Couldn't install optional dependency: EACCES: permission denied, mkdir '/Users/brentscholl/.npm/extend/3.0.0'\nnpm WARN install Couldn't install optional dependency: EACCES: permission denied, mkdir '/Users/brentscholl/.npm/readable-stream/2.0.4'\nnpm WARN install Couldn't install optional dependency: EACCES: permission denied, mkdir '/Users/brentscholl/.npm/chalk/1.1.1'\nnpm WARN install Couldn't install optional dependency: EACCES: permission denied, mkdir '/Users/brentscholl/.npm/xtend/4.0.1'\nnpm WARN checkPermissions Missing write access to /usr/local/lib/node_modules\n/usr/local/lib\n\u2514\u2500\u252c less@2.5.3\n  \u251c\u2500\u252c errno@0.1.4\n  \u2502 \u2514\u2500\u2500 prr@0.0.0\n  \u251c\u2500\u2500 image-size@0.3.5\n  \u251c\u2500\u2500 mime@1.3.4\n  \u251c\u2500\u252c promise@6.1.0\n  \u2502 \u2514\u2500\u2500 asap@1.0.0\n  \u2514\u2500\u252c source-map@0.4.4\n    \u2514\u2500\u2500 amdefine@1.0.0\n\nnpm ERR! Darwin 15.0.0\nnpm ERR! argv \"/usr/local/bin/node\" \"/usr/local/bin/npm\" \"install\" \"-g\" \"less\"\nnpm ERR! node v5.0.0\nnpm ERR! npm  v3.3.6\nnpm ERR! path /usr/local/lib/node_modules\nnpm ERR! code EACCES\nnpm ERR! errno -13\nnpm ERR! syscall access\n\nnpm ERR! Error: EACCES: permission denied, access '/usr/local/lib/node_modules'\nnpm ERR!     at Error (native)\nnpm ERR!  { [Error: EACCES: permission denied, access '/usr/local/lib/node_modules']\nnpm ERR!   errno: -13,\nnpm ERR!   code: 'EACCES',\nnpm ERR!   syscall: 'access',\nnpm ERR!   path: '/usr/local/lib/node_modules' }\nnpm ERR!\nnpm ERR! Please try running this command again as root/Administrator.\n\nnpm ERR! Please include the following file with any support request:\nnpm ERR!     /Users/brentscholl/npm-debug.log\n\n```\n\nI'm a complete noob here and not sure what to do next. Any help would be very appreciated!",
    "answer": "mkdir ~/.npm-global\nnpm config set prefix '~/.npm-global'\nexport PATH=~/.npm-global/bin:$PATH\nsource ~/.profile\nnpm install -g jshint\nsudo chown -R $USER ~/.npm-global\nsudo apt-get install nodejs npm\n\nUsing sudo is not recommended. It may give you permission issue later.\nWhile the above works, I am not a fan of changing folders owned by root to be writable for users, although it may only be an issue with multiple users.\nTo work around that, you could use a group, with 'npm users' but that is also more administrative overhead.\nSee here for the options to deal with permissions from the documentation:\nhttps://docs.npmjs.com/getting-started/fixing-npm-permissions\nI would go for option 2:\n\nTo minimize the chance of permissions errors, you can configure npm to\nuse a different directory. In this example, it will be a hidden\ndirectory on your home folder.\nMake a directory for global installations:\n\nConfigure npm to use the new directory path:\n\nOpen or create a ~/.profile file and add this line:\n\nBack on the command line, update your system variables:\n\nTest: Download a package globally without using sudo.\n\nIf still show permission error run (mac os):\n\nThis works with the default ubuntu install of:\n\nI recommend nvm if you want more flexibility in managing versions:\nhttps://github.com/creationix/nvm\nOn MacOS use brew, it should work without sudo out of the box if you're on a recent npm version.\nEnjoy :)"
  },
  {
    "question": "Appending a line to a file only if it does not already exist I need to add the following line to the end of a config file:\n\n```\ninclude \"/configs/projectname.conf\"\n\n```\n\nto a file called lighttpd.conf\nI am looking into using sed to do this, but I can't work out how.\nHow would I only insert it if the line doesn't already exist?",
    "answer": "grep -qxF 'include \"/configs/projectname.conf\"' foo.bar || echo 'include \"/configs/projectname.conf\"' >> foo.bar\n\nJust keep it simple :)\ngrep + echo should suffice:\n\n-q be quiet\n-x match the whole line\n-F pattern is a plain string\nhttps://linux.die.net/man/1/grep\n\nEdit:\nincorporated @cerin and @thijs-wouters suggestions."
  },
  {
    "question": "How can I fix the &quot;zsh: command not found: python&quot; error? (macOS Monterey 12.3, Python&amp;nbsp;3.10, Atom IDE, and atom-python-run 0.9.7) Since I got the macOS v12.3 (Monterey) update (not sure it's related though), I have been getting this error when I try to run my Python code in the terminal:\n\nI am using Python 3.10.3, Atom IDE, and run the code in the terminal via atom-python-run package (which used to work perfectly fine). The settings for the package go like this:\n\nThe which command in the terminal returns the following (which is odd, because earlier it would return something to just which python):\n\nI gather the error occurs because the terminal calls for python instead of python3, but I am super new to any coding and have no idea why it started now and how to fix it. Nothing of these has worked for me:\n\nI deleted and then reinstalled the Python interpreter from python.org.\nI tried alias python='python3' (which I saw in one of the threads here).\nI tried export PATH=\"/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin\" (which I found here).\nTo reset zsh and paths, I deleted all associated hidden files in /local/users/ and ran the terminal once again.\nI deleted everything and reinstalled Mac\u00a0OS\u00a0X and the Python interpreter only to get the same error.",
    "answer": "OK, after a couple of days trying, this is what has worked for me:\n\nI reinstalled Monterey (not sure it was essential, but I just figured I had messed with terminal and $PATH too much).\nI installed python via brew rather than from the official website.\nIt would still return command not found error.\nI ran echo \"alias python=/usr/bin/python3\" >> ~/.zshrc in terminal to alias python with python3.\nRelaunch the shell or run source ~/.zshrc\n\nProblem solved.\nAs far as I get it, there is no more pre-installed python 2.x in macOS as of 12.3 hence the error. I still find it odd though that atom-python-run would call for python instead of python3 despite the settings."
  },
  {
    "question": "How can I increase the cursor speed in terminal? How can I increase the cursor speed in terminal?\nI have Mac OS X by the way.\nIt would also be interesting to know it for Linux.\nI don't know what I should search for in Google (or what you like).",
    "answer": "defaults write NSGlobalDomain KeyRepeat -int 1\n\nThe original answer suggested using 0 as the value but no longer works as of 2019 / MacOS 10 onwards, it's now updated as 1\n\nIf by \"cursor speed\", you mean the repeat rate when holding down a key -  then have a look here:\nhttp://hints.macworld.com/article.php?story=20090823193018149\nTo summarize, open up a Terminal window and type the following command:\n\nMore detail from the article:\n\nEverybody knows that you can get a pretty fast keyboard repeat rate by changing a slider on the Keyboard tab of the Keyboard & Mouse System Preferences panel. But you can make it even faster! In Terminal, run this command:\ndefaults write NSGlobalDomain KeyRepeat -int 0\nThen log out and log in again. The fastest setting obtainable via System Preferences is 2 (lower numbers are faster), so you may also want to try a value of 1 if 0 seems too fast. You can always visit the Keyboard & Mouse System Preferences panel to undo your changes.\nYou may find that a few applications don't handle extremely fast keyboard input very well, but most will do just fine with it.\n\nNote: It requires a system restart"
  },
  {
    "question": "How can I set my default shell on a Mac, e.g. to Fish? I do not like to retype fish every time I start terminal. I want Fish on by default. How can I set the Fish shell as my default shell on a Mac?",
    "answer": "These are applicable to macOS v10.12.5 (Sierra) (16F73) and probably some other recent and upcoming versions of macOS.\n\nchsh is not enough to change the default shell. Make sure you press Command + , while your terminal is open and change the 'Shells open with' option to 'Default login shell.'\n\nIn case of Bash, make sure that you execute echo $BASH_VERSION to confirm you are running the intended version of Bash. bash --version does not give you correct information."
  },
  {
    "question": "Change all files and folders permissions of a directory to 644/755 How would I change all files to 644 and all folders to 755 using chmod from the linux command prompt? (Terminal)",
    "answer": "find /desired_location -type d -print0 | xargs -0 chmod 0755\nfind /desired_location -type f -print0 | xargs -0 chmod 0644\n\nOne approach could be using find:\nfor directories\n\nfor files"
  },
  {
    "question": "How can I recursively find all files in current and subfolders based on wildcard matching? How can I recursively find all files in current and subfolders based on wildcard matching?",
    "answer": "find . -name \"foo*\"\nfind . -iname \"foo*\"\n\nUse find:\n\nfind needs a starting point, so the . (dot) points to the current directory.\nIf you need case insensitive search use :"
  },
  {
    "question": "How to mkdir only if a directory does not already exist? I am writing a shell script to run under the KornShell (ksh) on AIX. I would like to use the mkdir command to create a directory. But the directory may already exist, in which case I do not want to do anything. So I want to either test to see that the directory does not exist, or suppress the \"File exists\" error that mkdir throws when it tries to create an existing directory. \nHow can I best do this?",
    "answer": "mkdir -p foo\nmkdir -p foo/bar/baz\n[ -d foo ] || mkdir foo\n\nTry mkdir -p:\n\nNote that this will also create any intermediate directories that don't exist; for instance,\n\nwill create directories foo, foo/bar, and foo/bar/baz if they don't exist.\nSome implementation like GNU mkdir include mkdir --parents as a more readable alias, but this is not specified in POSIX/Single Unix Specification and not available on many common platforms like macOS, various BSDs, and various commercial Unixes, so it should be avoided.\nIf you want an error when parent directories don't exist, and want to create the directory if it doesn't exist, then you can test for the existence of the directory first:"
  },
  {
    "question": "How to delete from a text file, all lines that contain a specific string? How would I use sed to delete all lines in a text file that contain a specific string?",
    "answer": "sed '/pattern to match/d' ./infile\nsed -i '/pattern to match/d' ./infile\nsed -i '' '/pattern to match/d' ./infile\nsed -i.bak '/pattern to match/d' ./infile\n\nTo remove the line and print the output to standard out:\n\nTo directly modify the file \u2013 does not work with BSD sed:\n\nSame, but for BSD sed (Mac OS X and FreeBSD) \u2013 does not work with GNU sed:\n\nTo directly modify the file (and create a backup) \u2013 works with BSD and GNU sed:"
  },
  {
    "question": "How do I exclude a directory when using `find`? How do I exclude a specific directory when searching for *.js files using find?\n\n```\nfind . -name '*.js'\n\n```",
    "answer": "find . -path ./misc -prune -o -name '*.txt' -print\nfind . -type d \\( -path ./dir1 -o -path ./dir2 -o -path ./dir3 \\) -prune -o -name '*.txt' -print\nfind . -type d -name node_modules -prune -o -name '*.json' -print\n\nUse the -prune primary. For example, if you want to exclude ./misc:\n\nNote that the final -print is needed to avoid printing out ./misc. See cycollins's answer for the explanation why.\nTo exclude multiple directories, OR them between parentheses.\n\nAnd, to exclude directories with a specific name at any level, use the -name primary instead of -path."
  },
  {
    "question": "Running shell command and capturing the output I want to write a function that will execute a shell command and return its output as a string, no matter, is it an error or success message. I just want to get the same result that I would have gotten with the command line.\nWhat would be a code example that would do such a thing?\nFor example:\n\n```\ndef run_command(cmd):\n    # ??????\n\nprint run_command('mysqladmin create test -uroot -pmysqladmin12')\n# Should output something like:\n# mysqladmin: CREATE DATABASE failed; error: 'Can't create database 'test'; database exists'\n\n```",
    "answer": ">>> subprocess.check_output(['ls', '-l'])\nb'total 0\\n-rw-r--r--  1 memyself  staff  0 Mar 14 11:04 files\\n'\n>>> import subprocess\n>>> result = subprocess.run(['ls', '-l'], stdout=subprocess.PIPE)\n>>> result.stdout\nb'total 0\\n-rw-r--r--  1 memyself  staff  0 Mar 14 11:04 files\\n'\n>>> result.stdout.decode('utf-8')\n'total 0\\n-rw-r--r--  1 memyself  staff  0 Mar 14 11:04 files\\n'\n>>> subprocess.run(['ls', '-l'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n'total 0\\n-rw-r--r--  1 memyself  staff  0 Mar 14 11:04 files\\n'\n>>> cmd = ['awk', 'length($0) > 5']\n>>> ip = 'foo\\nfoofoo\\n'.encode('utf-8')\n>>> result = subprocess.run(cmd, stdout=subprocess.PIPE, input=ip)\n>>> result.stdout.decode('utf-8')\n'foofoo\\n'\n>>> subprocess.run(['ls', '-l'], capture_output=True, text=True).stdout\n'total 0\\n-rw-r--r--  1 memyself  staff  0 Mar 14 11:04 files\\n'\nsubprocess.check_output(*popenargs, **kwargs)\noutput = subprocess.Popen([\"mycmd\", \"myarg\"],\nstdout=subprocess.PIPE).communicate()[0]\n>>> import subprocess\n>>> p = subprocess.Popen(['ls', '-a'], stdout=subprocess.PIPE,\n...                                    stderr=subprocess.PIPE)\n>>> out, err = p.communicate()\n>>> print out\n.\n..\nfoo\n>>> cmd = ['awk', 'length($0) > 5']\n>>> p = subprocess.Popen(cmd, stdout=subprocess.PIPE,\n...                           stderr=subprocess.PIPE,\n...                           stdin=subprocess.PIPE)\n>>> out, err = p.communicate('foo\\nfoofoo\\n')\n>>> print out\nfoofoo\n>>> subprocess.check_output('cat books/* | wc', shell=True, text=True)\n' 1299377 17005208 101299376\\n'\nrun(cmd, [stdout=etc...], input=other_output)\nPopen(cmd, [stdout=etc...]).communicate(other_output)\n\nIn all officially maintained versions of Python, the simplest approach is to use the subprocess.check_output function:\n\ncheck_output runs a single program that takes only arguments as input.1 It returns the result exactly as printed to stdout. If you need to write input to stdin, skip ahead to the run or Popen sections. If you want to execute complex shell commands, see the note on shell=True at the end of this answer.\nThe check_output function works in all officially maintained versions of Python. But for more recent versions, a more flexible approach is available.\nModern versions of Python (3.5 or higher): run\nIf you're using Python 3.5+, and do not need backwards compatibility, the new run function is recommended by the official documentation for most tasks. It provides a very general, high-level API for the subprocess module. To capture the output of a program, pass the subprocess.PIPE flag to the stdout keyword argument. Then access the stdout attribute of the returned CompletedProcess object:\n\nThe return value is a bytes object, so if you want a proper string, you'll need to decode it. Assuming the called process returns a UTF-8-encoded string:\n\nThis can all be compressed to a one-liner if desired:\n\nIf you want to pass input to the process's stdin, you can pass a bytes object to the input keyword argument:\n\nYou can capture errors by passing stderr=subprocess.PIPE (capture to result.stderr) or stderr=subprocess.STDOUT (capture to result.stdout along with regular output). If you want run to throw an exception when the process returns a nonzero exit code, you can pass check=True. (Or you can check the returncode attribute of result above.) When security is not a concern, you can also run more complex shell commands by passing shell=True as described at the end of this answer.\nLater versions of Python streamline the above further. In Python 3.7+, the above one-liner can be spelled like this:\n\nUsing run this way adds just a bit of complexity, compared to the old way of doing things. But now you can do almost anything you need to do with the run function alone.\nOlder versions of Python (3-3.4): more about check_output\nIf you are using an older version of Python, or need modest backwards compatibility, you can use the check_output function as briefly described above. It has been available since Python 2.7.\n\nIt takes takes the same arguments as Popen (see below), and returns a string containing the program's output. The beginning of this answer has a more detailed usage example. In Python 3.5+, check_output is equivalent to executing run with check=True and stdout=PIPE, and returning just the stdout attribute.\nYou can pass stderr=subprocess.STDOUT to ensure that error messages are included in the returned output. When security is not a concern, you can also run more complex shell commands by passing shell=True as described at the end of this answer.\nIf you need to pipe from stderr or pass input to the process, check_output won't be up to the task. See the Popen examples below in that case.\nComplex applications and legacy versions of Python (2.6 and below): Popen\nIf you need deep backwards compatibility, or if you need more sophisticated functionality than check_output or run provide, you'll have to work directly with Popen objects, which encapsulate the low-level API for subprocesses.\nThe Popen constructor accepts either a single command without arguments, or a list containing a command as its first item, followed by any number of arguments, each as a separate item in the list. shlex.split can help parse strings into appropriately formatted lists. Popen objects also accept a host of different arguments for process IO management and low-level configuration.\nTo send input and capture output, communicate is almost always the preferred method. As in:\n\nOr\n\nIf you set stdin=PIPE, communicate also allows you to pass data to the process via stdin:\n\nNote Aaron Hall's answer, which indicates that on some systems, you may need to set stdout, stderr, and stdin all to PIPE (or DEVNULL) to get communicate to work at all.\nIn some rare cases, you may need complex, real-time output capturing. Vartec's answer suggests a way forward, but methods other than communicate are prone to deadlocks if not used carefully.\nAs with all the above functions, when security is not a concern, you can run more complex shell commands by passing shell=True.\nNotes\n1. Running shell commands: the shell=True argument\nNormally, each call to run, check_output, or the Popen constructor executes a single program. That means no fancy bash-style pipes. If you want to run complex shell commands, you can pass shell=True, which all three functions support. For example:\n\nHowever, doing this raises security concerns. If you're doing anything more than light scripting, you might be better off calling each process separately, and passing the output from each as an input to the next, via\n\nOr\n\nThe temptation to directly connect pipes is strong; resist it. Otherwise, you'll likely see deadlocks or have to do hacky things like this."
  },
  {
    "question": "Should I put #! (shebang) in Python scripts, and what form should it take? Should I put the shebang in my Python scripts? In what form?\n\n```\n#!/usr/bin/env python \n\n```\n\nor\n\n```\n#!/usr/local/bin/python\n\n```\n\nAre these equally portable? Which form is used most?\nNote: the tornado project uses the shebang. On the other hand the Django project doesn't.",
    "answer": "The shebang line in any script determines the script's ability to be executed like a standalone executable without typing python beforehand in the terminal or when double clicking it in a file manager (when configured properly). It isn't necessary but generally put there so when someone sees the file opened in an editor, they immediately know what they're looking at. However, which shebang line you use is important.\nCorrect usage for (defaults to version 3.latest) Python 3 scripts is:\n\nCorrect usage for (defaults to version 2.latest) Python 2 scripts is:\n\nThe following should not be used (except for the rare case that you are writing code which is compatible with both Python 2.x and 3.x):\n\nThe reason for these recommendations, given in PEP 394, is that python can refer either to python2 or python3 on different systems.\nAlso, do not use:\n\n\"python may be installed at /usr/bin/python or /bin/python in those\ncases, the above #! will fail.\"\n\n\u2015\"#!/usr/bin/env python\" vs \"#!/usr/local/bin/python\""
  },
  {
    "question": "How to call shell commands from Ruby How do I call shell commands from inside of a Ruby program? How do I then get output from these commands back into Ruby?",
    "answer": "cmd = \"echo 'hi'\" # Sample string that can be used\nvalue = `echo 'hi'`\nvalue = `#{cmd}`\nvalue = %x( echo 'hi' )\nvalue = %x[ #{cmd} ]\nwasGood = system( \"echo 'hi'\" )\nwasGood = system( cmd )\nexec( \"echo 'hi'\" )\nexec( cmd ) # Note: this will never be reached because of the line above\n?.exitstatus\n\nThis explanation is based on a commented Ruby script from a friend of mine. If you want to improve the script, feel free to update it at the link.\nFirst, note that when Ruby calls out to a shell, it typically calls /bin/sh, not Bash. Some Bash syntax is not supported by /bin/sh on all systems.\nHere are ways to execute a shell script:\n\nKernel#` , commonly called backticks \u2013 `cmd`\nThis is like many other languages, including Bash, PHP, and Perl.\nReturns the result (i.e. standard output) of the shell command.\nDocs: http://ruby-doc.org/core/Kernel.html#method-i-60\n\nBuilt-in syntax, %x( cmd )\nFollowing the x character is a delimiter, which can be any character.\nIf the delimiter is one of the characters (, [, {, or <,\nthe literal consists of the characters up to the matching closing delimiter,\ntaking account of nested delimiter pairs. For all other delimiters, the\nliteral comprises the characters up to the next occurrence of the\ndelimiter character.  String interpolation #{ ... } is allowed.\nReturns the result (i.e. standard output) of the shell command, just like the backticks.\nDocs: https://docs.ruby-lang.org/en/master/syntax/literals_rdoc.html#label-Percent+Strings\n\nKernel#system\nExecutes the given command in a subshell.\nReturns true if the command was found and run successfully, false otherwise.\nDocs: http://ruby-doc.org/core/Kernel.html#method-i-system\n\nKernel#exec\nReplaces the current process by running the given external command.\nReturns none, the current process is replaced and never continues.\nDocs: http://ruby-doc.org/core/Kernel.html#method-i-exec\n\nHere's some extra advice:\n$?, which is the same as $CHILD_STATUS, accesses the status of the last system executed command if you use the backticks, system() or %x{}.\nYou can then access the exitstatus and pid properties:\n\nFor more reading see:\n\nhttp://www.elctech.com/blog/i-m-in-ur-commandline-executin-ma-commands\nhttp://blog.jayfields.com/2006/06/ruby-kernel-system-exec-and-x.html\nhttp://tech.natemurray.com/2007/03/ruby-shell-commands.html"
  },
  {
    "question": "Shell command to sum integers, one per line? I am looking for a command that will accept (as input) multiple lines of text, each line containing a single integer, and output the sum of these integers.\nAs a bit of background, I have a log file which includes timing measurements. Through grepping for the relevant lines and a bit of sed reformatting I can list all of the timings in that file.  I would like to work out the total. I can pipe this intermediate output to any command in order to do the final sum.  I have always used expr in the past, but unless it runs in RPN mode I do not think it is going to cope with this (and even then it would be tricky).\nHow can I get the summation of integers?",
    "answer": "awk '{s+=$1} END {print s}' mydatafile\nawk '{s+=$1} END {printf \"%.0f\", s}' mydatafile\n\nBit of awk should do it?\n\nNote: some versions of awk have some odd behaviours if you are going to be adding anything exceeding 2^31 (2147483647). See comments for more background. One suggestion is to use printf rather than print:"
  },
  {
    "question": "How do I put an already-running process under nohup? I have a process that is already running for a long time and don't want to end it.\nHow do I put it under nohup (that is, how do I cause it to continue running even if I close the terminal?)",
    "answer": "Using the Job Control of bash to send the process into the background:\n\nCtrl+Z to stop (pause) the program and get back to the shell.\nbg to run it in the background.\ndisown -h [job-spec] where [job-spec] is the job number (like %1 for the first running job; find about your number with the jobs command) so that the job isn't killed when the terminal closes."
  },
  {
    "question": "How to add line break to &#39;git commit -m&#39; from the command line? I am using Git from the command line and am trying to add a line break to the commit message (using git commit -m \"\") without going into Vim.\nIs this possible?",
    "answer": "git commit -m 'Message\n\ngoes\nhere'\ngit commit -F- <<EOF\nMessage\n\ngoes\nhere\nEOF\n\nCertainly, how it's done depends on your shell. In Bash, you can use single quotes around the message and can just leave the quote open, which will make Bash prompt for another line, until you close the quote. Like this:\n\nAlternatively, you can use a \"here document\" (also known as heredoc):"
  },
  {
    "question": "Count number of lines in a git repository How would I count the total number of lines present in all the files in a git repository?\ngit ls-files gives me a list of files tracked by git.\nI'm looking for a command to cat all those files. Something like\n\n```\ngit ls-files | [cat all these files] | wc -l\n\n```",
    "answer": "git ls-files | xargs cat | wc -l\ngit ls-files | xargs wc -l\n\nxargs will let you cat all the files together before passing them to wc, like you asked:\n\nBut skipping the intermediate cat gives you more information and is probably better:"
  },
  {
    "question": "Given two directory trees, how can I find out which files differ by content? If I want find the differences between two directory trees, I usually just execute:\n\n```\ndiff -r dir1/ dir2/\n\n```\n\nThis outputs exactly what the differences are between corresponding files.  I'm interested in just getting a list of corresponding files whose content differs.  I assumed that this would simply be a matter of passing a command line option to diff, but I couldn't find anything on the man page.\nAny suggestions?",
    "answer": "diff --brief --recursive dir1/ dir2/\ndiff -qr dir1/ dir2/\ndiff --brief --recursive --new-file dir1/ dir2/  # with long options\ndiff -qrN dir1/ dir2/                            # with short flag aliases\n\nTry:\n\nOr alternatively, with the short flags -qr:\n\nIf you also want to see differences for files that may not exist in either directory:"
  },
  {
    "question": "When do we need curly braces around shell variables? In shell scripts, when do we use {} when expanding variables?\nFor example, I have seen the following:\n\n```\nvar=10        # Declare variable\n\necho \"${var}\" # One use of the variable\necho \"$var\"   # Another use of the variable\n\n```\n\nIs there a significant difference, or is it just style? Is one preferred over the other?",
    "answer": "\"${foo}bar\"\n\nIn your particular example, it makes no difference. However, the {} in ${} are useful if you want to expand the variable foo in the string\n\nsince \"$foobar\" would instead expand the variable identified by foobar.\nCurly braces are also unconditionally required when:\n\nexpanding array elements, as in ${array[42]}\nusing parameter expansion operations, as in ${filename%.*} (remove extension; strips smallest match)\nexpanding positional parameters beyond 9: \"$8 $9 ${10} ${11}\"\n\nDoing this everywhere, instead of just in potentially ambiguous cases, can be considered good programming practice. This is both for consistency and to avoid surprises like $foo_$bar.jpg, where it's not visually obvious that the underscore becomes part of the variable name."
  },
  {
    "question": "How can I compare numbers in Bash? I'm unable to get numeric comparisons working:\n\n```\necho \"enter two numbers\";\nread a b;\n\necho \"a=$a\";\necho \"b=$b\";\n\nif [ $a \\> $b ];\nthen\n    echo \"a is greater than b\";\nelse\n    echo \"b is greater than a\";\nfi;\n\n```\n\nThe problem is that it compares the number from the first digit on, i.e., 9 is bigger than 10, but 1 is greater than 09.\nHow can I convert the numbers into a type to do a true comparison?",
    "answer": "if (( a > b )); then\n...\nfi\nif [ \"$a\" -gt \"$b\" ]; then\n...\nfi\n\nIn Bash, you should do your check in an arithmetic context:\n\nFor POSIX shells that don't support (()), you can use -lt and -gt.\n\nYou can get a full list of comparison operators with help test or man test."
  },
  {
    "question": "Difference between single and double quotes in Bash In Bash, what are the differences between single quotes ('') and double quotes (\"\")?",
    "answer": "echo \"$(echo \"upg\")\"\nupg\necho '$(echo \"upg\")'\n(echo \"upg\")\n\nSingle quotes won't interpolate anything, but double quotes will. For example: variables, backticks, certain \\ escapes, etc.\nExample:\n\nThe Bash manual has this to say:\n\n3.1.2.2 Single Quotes\nEnclosing characters in single quotes (') preserves the literal value of each character within the quotes. A single quote may not occur between single quotes, even when preceded by a backslash.\n3.1.2.3 Double Quotes\nEnclosing characters in double quotes (\") preserves the literal value of all characters within the quotes, with the exception of $, `, \\, and, when history expansion is enabled, !. The characters $ and ` retain their special meaning within double quotes (see Shell Expansions). The backslash retains its special meaning only when followed by one of the following characters: $, `, \", \\, or newline. Within double quotes, backslashes that are followed by one of these characters are removed. Backslashes preceding characters without a special meaning are left unmodified. A double quote may be quoted within double quotes by preceding it with a backslash. If enabled, history expansion will be performed unless an ! appearing in double quotes is escaped using a backslash. The backslash preceding the ! is not removed.\nThe special parameters * and @ have special meaning when in double quotes (see Shell Parameter Expansion)."
  },
  {
    "question": "Use grep --exclude/--include syntax to not grep through certain files I'm looking for the string foo= in text files in a directory tree. It's on a common Linux machine, I have bash shell:\n\n```\ngrep -ircl \"foo=\" *\n\n```\n\nIn the directories are also many binary files which match \"foo=\". As these results are not relevant and slow down the search, I want grep to skip searching these files (mostly JPEG and PNG images). How would I do that?\nI know there are the --exclude=PATTERN and --include=PATTERN options, but what is the pattern format? The man page of grep says:\n\n```\n--include=PATTERN     Recurse in directories only searching file matching PATTERN.\n--exclude=PATTERN     Recurse in directories skip file matching PATTERN.\n\n```\n\nSearching on grep include, grep include exclude, grep exclude and variants did not find anything relevant\nIf there's a better way of grepping only in certain files, I'm all for it; moving the offending files is not an option. I can't search only certain directories (the directory structure is a big mess, with everything everywhere). Also, I can't install anything, so I have to do with common tools (like grep or the suggested find).",
    "answer": "grep pattern -r --include=\\*.cpp --include=\\*.h rootdir\ngrep pattern -r --include=\\*.{cpp,h} rootdir\n\nUse the shell globbing syntax:\n\nThe syntax for --exclude is identical.\nNote that the star is escaped with a backslash to prevent it from being expanded by the shell (quoting it, such as --include=\"*.cpp\", would work just as well).  Otherwise, if you had any files in the current working directory that matched the pattern, the command line would expand to something like grep pattern -r --include=foo.cpp --include=bar.cpp rootdir, which would only search files named foo.cpp and bar.cpp, which is quite likely not what you wanted.\nUpdate 2021-03-04\nI've edited the original answer to remove the use of brace expansion, which is a feature provided by several shells such as Bash and zsh to simplify patterns like this; but note that brace expansion is not POSIX shell-compliant.\nThe original example was:\n\nto search through all .cpp and .h files rooted in the directory rootdir."
  },
  {
    "question": "Redirect stderr and stdout in Bash I want to redirect both standard output and standard error of a process to a single file. How do I do that in Bash?",
    "answer": "yourcommand &> filename\n\nTake a look here. It should be:\n\nIt redirects both standard output and standard error to file filename."
  },
  {
    "question": "How do I test if a variable is a number in Bash? I just can't figure out how do I make sure an argument passed to my script is a number or not.\nAll I want to do is something like this:\n\n```\ntest *isnumber* $1 && VAR=$1 || echo \"need a number\"\n\n```\n\nAny help?",
    "answer": "re='^[0-9]+$'\nif ! [[ $yournumber =~ $re ]] ; then\necho \"error: Not a number\" >&2; exit 1\nfi\n^[0-9]+([.][0-9]+)?$\n^[+-]?[0-9]+([.][0-9]+)?$\n\nOne approach is to use a regular expression, like so:\n\nIf the value is not necessarily an integer, consider amending the regex appropriately; for instance:\n\n...or, to handle numbers with a sign:"
  },
  {
    "question": "How to &#39;grep&#39; a continuous stream? Is that possible to use grep on a continuous stream?\nWhat I mean is sort of a tail -f <file> command, but with grep on the output in order to keep only the lines that interest me.\nI've tried tail -f <file> | grep pattern but it seems that grep can only be executed once tail finishes, that is to say never.",
    "answer": "tail -f file | grep --line-buffered my_pattern\n\nTurn on grep's line buffering mode when using BSD grep (FreeBSD, Mac OS X etc.)\n\nIt looks like a while ago --line-buffered didn't matter for GNU grep (used on pretty much any Linux) as it flushed by default (YMMV for other Unix-likes such as SmartOS, AIX or QNX). However, as of November 2020, --line-buffered is needed (at least with GNU grep 3.5 in openSUSE, but it seems generally needed based on comments below)."
  },
  {
    "question": "How can I reverse the order of lines in a file? I'd like to reverse the order of lines in a text file (or stdin), preserving the contents of each line.\nSo, i.e., starting with:\n\n```\nfoo\nbar\nbaz\n\n```\n\nI'd like to end up with \n\n```\nbaz\nbar\nfoo\n\n```\n\nIs there a standard UNIX commandline utility for this?",
    "answer": "tail -r myfile.txt\n\nBSD tail:\n\nReference: FreeBSD, NetBSD, OpenBSD and OS X manual pages."
  },
  {
    "question": "Using awk to print all columns from the nth to the last This line worked until I had whitespace in the second field:\n\n```\nsvn status | grep '\\!' | gawk '{print $2;}' > removedProjs\n\n```\n\nIs there a way to have awk print everything in $2 or greater?  ($3, $4.. until we don't have any more columns?)\nI'm doing this in a Windows environment with Cygwin.",
    "answer": "awk '{print $0}' somefile\nawk '{$1=\"\"; print $0}' somefile\nawk '{$1=$2=\"\"; print $0}' somefile\n\nPrint all columns:\n\nPrint all but the first column:\n\nPrint all but the first two columns:"
  },
  {
    "question": "How do I use shell variables in an awk script? I found some ways to pass external shell variables to an awk script, but I'm confused about ' and \".\nFirst, I tried with a shell script:\n\n```\n$ v=123test\n$ echo $v\n123test\n$ echo \"$v\"\n123test\n\n```\n\nThen tried awk:\n\n```\n$ awk 'BEGIN{print \"'$v'\"}'\n$ 123test\n$ awk 'BEGIN{print '\"$v\"'}'\n$ 123\n\n```\n\nWhy is the difference?\nLastly I tried this:\n\n```\n$ awk 'BEGIN{print \" '$v' \"}'\n$  123test\n$ awk 'BEGIN{print ' \"$v\" '}'\nawk: cmd. line:1: BEGIN{print\nawk: cmd. line:1:             ^ unexpected newline or end of string \n\n```\n\nI'm confused about this.",
    "answer": "variable=\"line one\\nline two\"\nawk -v var=\"$variable\" 'BEGIN {print var}'\nline one\nline two\nawk -v a=\"$var1\" -v b=\"$var2\" 'BEGIN {print a,b}'\nvariable='a\\tb\\n$c\\kd'\nawk -v var=\"$variable\" 'BEGIN{ printf \"%s\\n\", var }'\nawk: warning: escape sequence `\\k' treated as plain `k'\na       b\nckd\nawk -v var=\"${variable//\\\\/\\\\\\\\}\" 'BEGIN{ printf \"%s\\n\", var }'\na\\tb\\n$c\\kd\nawk -v time=\"$(date +\"%F %H:%M\" -d '-1 minute')\" 'BEGIN {print time}'\nawk -v var=\"$variable\" '$0 ~ var{print \"found it\"}'\nvariable=\"line one\\nline two\"\necho \"input data\" | awk '{print var}' var=\"${variable}\"\nor\nawk '{print var}' var=\"${variable}\" file\nawk '{print $0}' <<< \"$variable\"\ntest\necho \"$variable\" | awk '{print $0}'\nprintf '%s' \"$variable\" | awk '{print $0}'\nexport X=MyVar\nawk 'BEGIN{print ENVIRON[\"X\"],ENVIRON[\"SHELL\"]}'\nMyVar /bin/bash\nx=MyVar\nx=\"$x\" awk 'BEGIN{print ENVIRON[\"x\"],ENVIRON[\"SHELL\"]}'\nMyVar /bin/bash\nv=\"my data\"\nawk 'BEGIN {print ARGV[1]}' \"$v\"\nmy data\nv=\"my data\"\necho \"test\" | awk 'BEGIN{var=ARGV[1];ARGV[1]=\"\"} {print var, $0}' \"$v\"\nmy data test\nvariable=\"line one\\nline two\"\nawk 'BEGIN {print \"'\"$variable\"'\"}'\nline one\nline two\nvariable='line one\\nline two\" ; for (i=1;i<=1000;++i) print i\"'\nawk 'BEGIN {print \"'\"$variable\"'\"}'\nline one\nline two\n.\n.\ncalc() { awk -v x=\"$1\" -v z=\"$3\" 'BEGIN{ print x '\"$2\"' z }'; }\ncalc 2.7 '+' 3.4\n6.1\ncalc 2.7 '*' 3.4\n9.18\nvar=\"Line one\nThis is line two\"\n\necho $var\nLine one This is line two\n\necho \"$var\"\nLine one\nThis is line two\nvariable=\"line one\\nline two\"\nawk -v var=$variable 'BEGIN {print var}'\nawk: cmd. line:1: one\\nline\nawk: cmd. line:1:    ^ backslash not last character on line\nawk: cmd. line:1: one\\nline\nawk: cmd. line:1:    ^ syntax error\nawk -v var='$variable' 'BEGIN {print var}'\nvariable\n\n#Getting shell variables into awk\nmay be done in several ways. Some are better than others. This should cover most of them.  If you have a comment, please leave below.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0v1.5\n\nUsing -v  (The best way, most portable)\nUse the -v option: (P.S. use a space after -v or it will be less portable. E.g., awk -v var= not awk -vvar=)\n\nThis should be compatible with most awk, and the variable is available in the BEGIN block as well:\nIf you have multiple variables:\n\nWarning.  As Ed Morton writes and as seen in the above example, the shell variable is expanded by the shell before awk then sees its content as awk -v var='line one\\nline two' and so any escape sequences in the content of that shell variable will be interpreted when using -v, just like they are for every other form of assignment of a string to a variable in awk, e.g. awk 'BEGIN{var=\"line one\\nline two\"} {...}' or awk '{...}' var='line one\\nline two', and so \\n becomes a literal LineFeed character and not the 2-character string \\n. For example, given a variable like:\n\nawk would expand the escape sequences in the assignment:\n\nIf that's not what you want then, if your shell (e.g. bash) and locale (e.g. LC_ALL=C) support it then you can have backslashes treated literally by using shell parameter substitution to escape any backslashes:\n\nor by using ENVIRON[] or access it via ARGV[] (see below).\nYou cannot use -v var=\"$(printf '%q' \"$variable\")\" for this as that would also escape $s, nor can you use -v var=\"${variable@Q}\" as that would just add 's around \"$variable\" and the escape sequences would still be interpreted by awk. That's because those 2 approaches both escape chars according to shell syntax for providing command input, not awk syntax for assigning strings to variables.\nPS If you have vertical bar or other regexp meta characters as separator like |?( etc, they must be double escaped. Example 3 vertical bars ||| becomes -F'\\\\|\\\\|\\\\|'. You can also use -F\"[|][|][|]\".\n\nExample on getting data from a program/function in to awk (here date is used)\n\nExample of testing the contents of a shell variable as a regexp:\n\nVariable after code block\nHere we get the variable after the awk code. This will work fine as long as you do not need the variable in the BEGIN block:\n\nAdding multiple variables:\n\nawk '{print a,b,$0}' a=\"$var1\" b=\"$var2\" file\n\nIn this way we can also set different Field Separator FS for each file.\n\nawk 'some code' FS=',' file1.txt FS=';' file2.ext\n\nVariable after the code block will not work for the BEGIN block:\n\necho \"input data\" | awk 'BEGIN {print var}' var=\"${variable}\"\n\nHere-string\nVariable can also be added to awk using a here-string from shells that support them (including Bash):\n\nThis is the same as:\n\nP.S. this treats the variable as a file input.\n\nENVIRON input\nAs TrueY writes, you can use the ENVIRON to print Environment Variables.\nSetting a variable before running AWK, you can print it out like this:\n\nor for a non-exported variable:\n\nARGV input\nAs Steven Penny writes, you can use ARGV to get the data into awk:\n\nTo get the data into the code itself, not just the BEGIN:\n\nVariable within the code: USE WITH CAUTION\nYou can use a variable within the awk code, but it's messy and hard to read, and as Charles Duffy points out, this version may also be a victim of code injection.  If someone adds bad stuff to the variable, it will be executed as part of the awk code.\nThis works by extracting the variable within the code, so it becomes a part of it.\nIf you want to make an awk that changes dynamically with use of variables, you can do it this way, but DO NOT use it for normal variables.\n\nHere is an example of code injection:\n\nYou can add lots of commands to awk this way.  Even make it crash with non valid commands.\nOne valid use of this approach, though, is when you want to pass a symbol to awk to be applied to some input, e.g. a simple calculator:\n\nThere is no way to do that using an awk variable populated with the value of a shell variable, you NEED the shell variable to expand to become part of the text of the awk script before awk interprets it. (see comment below by Ed M.)\n\nExtra info:\nUse of double quote\nIt's always good to double quote variable \"$variable\"\nIf not, multiple lines will be added as a long single line.\nExample:\n\nOther errors you can get without double quote:\n\nAnd with single quote, it does not expand the value of the variable:\n\nMore info about AWK and variables\nRead this faq."
  },
  {
    "question": "How can I use &quot;:&quot; as an AWK field separator? Given the following command,\n\n```\necho \"1: \" | awk '/1/ -F \":\" {print $1}'\n\n```\n\nwhy does AWK output:\n\n```\n1:\n\n```\n\n?",
    "answer": "echo '1: ' | awk -F  ':' '/1/ {print $1}'\n\n-F is a command line argument, not AWK syntax. Try:"
  },
  {
    "question": "How can I shuffle the lines of a text file on the Unix command line or in a shell script? I want to shuffle the lines of a text file randomly and create a new file. The file may have several thousands of lines.\nHow can I do that with cat, awk, cut, etc?",
    "answer": "You can use shuf. On some systems at least (doesn't appear to be in POSIX).\nAs jleedev pointed out: sort -R might also be an option. On some systems at least; well, you get the picture. It has been pointed out that sort -R doesn't really shuffle but instead sort items according to their hash value.\n[Editor's note: sort -R almost shuffles, except that duplicate lines / sort keys always end up next to each other. In other words: only with unique input lines / keys is it a true shuffle. While it's true that the output order is determined by hash values, the randomness comes from choosing a random hash function - see manual.]"
  },
  {
    "question": "AWK: Access captured group from line pattern If I have an awk command\n\n```\npattern { ... }\n\n```\n\nand pattern uses a capturing group, how can I access the string so captured in the block?",
    "answer": "perl -n -e'/test(\\d+)/ && print $1'\n\nThat was a stroll down memory lane...\nI replaced awk by perl a long time ago.\nApparently the AWK regular expression engine does not capture its groups.\nyou might consider using something like :\n\nthe -n flag causes perl to loop over every line like awk does."
  },
  {
    "question": "Using multiple delimiters in awk I have a file which contain following lines:\n\n```\n/logs/tc0001/tomcat/tomcat7.1/conf/catalina.properties:app.env.server.name = demo.example.com\n/logs/tc0001/tomcat/tomcat7.2/conf/catalina.properties:app.env.server.name = quest.example.com\n/logs/tc0001/tomcat/tomcat7.5/conf/catalina.properties:app.env.server.name = www.example.com\n\n```\n\nIn above output I want to extract 3 fields (Number 2, 4 and the last one *.example.com). I am getting the following output:\n\n```\ncat file | awk -F'/' '{print $3 \"\\t\" $5}'\ntc0001   tomcat7.1\ntc0001   tomcat7.2\ntc0001   tomcat7.5\n\n```\n\nHow do I also extract last field with domain name which is after '='? How do I use multiple delimiter to extract field?",
    "answer": "awk -F'[/=]' '{print $3 \"\\t\" $5 \"\\t\" $8}' file\ntc0001   tomcat7.1    demo.example.com\ntc0001   tomcat7.2    quest.example.com\ntc0001   tomcat7.5    www.example.com\n\nThe delimiter can be a regular expression.\n\nProduces:"
  },
  {
    "question": "What are the differences between Perl, Python, AWK and sed? What are the main differences among them? And in which typical scenarios is it better to use each language?",
    "answer": "In order of appearance, the languages are sed, awk, perl, python.\nThe sed program is a stream editor and is designed to apply the actions from a script to each line (or, more generally, to specified ranges of lines) of the input file or files. Its language is based on ed, the Unix editor, and although it has conditionals and so on, it is hard to work with for complex tasks. You can work minor miracles with it - but at a cost to the hair on your head. However, it is probably the fastest of the programs when attempting tasks within its remit. (It has the least powerful regular expressions of the programs discussed - adequate for many purposes, but certainly not PCRE - Perl-Compatible Regular Expressions)\nThe awk program (name from the initials of its authors - Aho, Weinberger, and Kernighan) is a tool initially for formatting reports. It can be used as a souped-up sed; in its more recent versions, it is computationally complete. It uses an interesting idea - the program is based on 'patterns matched' and 'actions taken when the pattern matches'. The patterns are fairly powerful (Extended Regular Expressions). The language for the actions is similar to C. One of the key features of awk is that it splits the input automatically into records and each record into fields.\nPerl was written in part as an awk-killer and sed-killer. Two of the programs provided with it are a2p and s2p for converting awk scripts and sed scripts into Perl. Perl is one of the earliest of the next generation of scripting languages (Tcl/Tk can probably claim primacy). It has powerful integrated regular expression handling with a vastly more powerful language. It provides access to almost all system calls and has the extensibility of the CPAN modules. (Neither awk nor sed is extensible.) One of Perl's mottos is \"TMTOWTDI - There's more than one way to do it\" (pronounced \"tim-toady\"). Perl has 'objects', but it is more of an add-on than a fundamental part of the language.\nPython was written last, and probably in part as a reaction to Perl. It has some interesting syntactic ideas (indenting to indicate levels - no braces or equivalents). It is more fundamentally object-oriented than Perl; it is just as extensible as Perl.\nOK - when to use each?\n\nSed - when you need to do simple text transforms on files.\nAwk - when you only need simple formatting and summarisation or transformation of data.\nPerl - for almost any task, but especially when the task needs complex regular expressions.\nPython - for the same tasks that you could use Perl for.\n\nI'm not aware of anything that Perl can do that Python can't, nor vice versa. The choice between the two would depend on other factors. I learned Perl before there was a Python, so I tend to use it. Python has less accreted syntax and is generally somewhat simpler to learn. Perl 6, when it becomes available, will be a fascinating development.\n(Note that the 'overviews' of Perl and Python, in particular, are woefully incomplete; whole books could be written on the topic.)"
  },
  {
    "question": "How to show only next line after the matched one? ```\ngrep -A1 'blah' logfile\n\n```\n\nThanks to this command for every line that has 'blah' in it, I get the output of the line that contains 'blah' and the next line that follows in the logfile. It might be a simple one but I can't find a way to omit the line that has 'blah' and only show next line in the output.",
    "answer": "awk '/blah/{getline; print}' logfile\n\nyou can try with awk:"
  },
  {
    "question": "How to split a delimited string into an array in awk? How to split the string when it contains pipe symbols | in it.\nI want to split them to be in array.\nI tried\n\n```\necho \"12:23:11\" | awk '{split($0,a,\":\"); print a[3] a[2] a[1]}'\n\n```\n\nWhich works fine. If my string is like \"12|23|11\" then how do I split them into an array?",
    "answer": "echo \"12|23|11\" | awk '{split($0,a,\"|\"); print a[3],a[2],a[1]}'\n\nHave you tried:"
  },
  {
    "question": "How can I quickly sum all numbers in a file? I have a file which contains several thousand numbers, each on its own line:\n\n```\n34\n42\n11\n6\n2\n99\n...\n\n```\n\nI'm looking to write a script which will print the sum of all numbers in the file. I've got a solution, but it's not very efficient. (It takes several minutes to run.) I'm looking for a more efficient solution. Any suggestions?",
    "answer": "% perl -nle '$sum += $_ } END { print $sum'\n%  perl -MO=Deparse -nle '$sum += $_ } END { print $sum'\nBEGIN { $/ = \"\\n\"; $\\ = \"\\n\"; }\nLINE: while (defined($_ = <ARGV>)) {\nchomp $_;\nsum += $_;\n}\nsub END {\nprint $sum;\n}\n-e syntax OK\nuse 5.010;\nuse File::Map qw(map_file);\n\nmap_file my $map, $ARGV[0];\nsum += $1 while $map =~ m/(\\d+)/g;\n\nsay $sum;\n\nFor a Perl one-liner, it's basically the same thing as the awk solution in Ayman Hourieh's answer:\n\nIf you're curious what Perl one-liners do, you can deparse them:\n\nThe result is a more verbose version of the program, in a form that no one would ever write on their own:\n\nJust for giggles, I tried this with a file containing 1,000,000 numbers (in the range 0 - 9,999). On my Mac Pro, it returns virtually instantaneously. That's too bad, because I was hoping using mmap would be really fast, but it's just the same time:"
  },
  {
    "question": "How to get the second column from command output? My command's output is something like:\n\n```\n1540 \"A B\"\n   6 \"C\"\n 119 \"D\"\n\n```\n\nThe first column is always a number, followed by a space, then a double-quoted string.\nMy purpose is to get the second column only, like:\n\n```\n\"A B\"\n\"C\"\n\"D\"\n\n```\n\nI intended to use <some_command> | awk '{print $2}' to accomplish this. But the question is, some values in the second column contain space(s), which happens to be the default delimiter for awk to separate the fields. Therefore, the output is messed up:\n\n```\n\"A\n\"C\"\n\"D\"\n\n```\n\nHow do I get the second column's value (with paired quotes) cleanly?",
    "answer": "<some_command> | sed 's/^.* \\(\".*\"$\\)/\\1/'\n\nOr use sed & regex."
  },
  {
    "question": "How to grep for case insensitive string in a file? I have a file file1 which ends with\nSuccess... OR\nsuccess...\nI want to grep for the word success in a way which is not case sensitive way.\nI have written the following command but it is case sensitive\n\n```\ncat file1 | grep \"success\\.\\.\\.\"\n\n```\n\nHow can i change it so that it returns 0 with both Success... OR\nsuccess...",
    "answer": "grep -iF \"success...\" file1\n\nYou can use the -i flag which makes your pattern case insensitive:\n\nAlso, there is no need for cat. grep takes a file with the syntax grep <pattern> <file>. I also used the -F flag to search for a fixed string to avoid escaping the ellipsis."
  },
  {
    "question": "awk without printing newline I want the variable sum/NR to be printed side-by-side in each iteration. How do we avoid awk from printing newline in each iteration ? In my code a newline is printed by default in each iteration\n\n```\nfor file in cg_c ep_c is_c tau xhpl\nprintf \"\\n $file\" >> to-plot.xls\n    for f in 2.54 1.60 800 \n        awk '{sum+=$3}; END  {print  sum/NR}' ${file}_${f}_v1.xls >> to-plot-p.xls\n    done\ndone\n\n```\n\nI want the output to appear like this\n\n```\ncg_c ans1  ans2  ans3  \nep_c ans1  ans2  ans3 \nis_c ans1  ans2  ans3\ntau  ans1  ans2  ans3 \nxhpl ans1  ans2  ans3\n\n```\n\nmy current out put is like this\n\n```\n**cg_c**\nans1\nans2\nans3\n**ep_c**\nans1\nans2\nans3\n**is_c**\nans1\nans2\nans3\n**tau**\nans1\nans2\nans3\n**xhpl**\nans1\nans2\nans3\n\n```",
    "answer": "awk '{sum+=$3}; END  {printf \"%f\",sum/NR}' ${file}_${f}_v1.xls >> to-plot-p.xls\nprint will insert a newline by default. You dont want that to happen, hence use printf instead."
  },
  {
    "question": "How to merge every two lines into one from the command line? I have a text file with the following format. The first line is the \"KEY\" and the second line is the \"VALUE\". \n\n```\nKEY 4048:1736 string\n3\nKEY 0:1772 string\n1\nKEY 4192:1349 string\n1\nKEY 7329:2407 string\n2\nKEY 0:1774 string\n1\n\n```\n\nI need the value in the same line as of the key. So the output should look like this...\n\n```\nKEY 4048:1736 string 3\nKEY 0:1772 string 1\nKEY 4192:1349 string 1\nKEY 7329:2407 string 2\nKEY 0:1774 string 1\n\n```\n\nIt will be better if I could use some delimiter like $ or ,:\n\n```\nKEY 4048:1736 string , 3\n\n```\n\nHow do I merge two lines into one?",
    "answer": "awk 'NR%2{printf \"%s \",$0;next;}1' yourFile\nsed 'N;s/\\n/ /' yourFile\n\nawk:\n\nnote, there is an empty line at the end of output.\nsed:"
  },
  {
    "question": "Print second-to-last column/field in `awk` I want to print the second-to-last column or field in awk. The number of fields is the NF variable. I know that I should be able to use $NF, but I'm not sure how it can be used.\nAnd this does not seem to work:\n\n```\nawk ' { print ( $NF-- )  } '\n\n```",
    "answer": "awk '{print $(NF-1)}'\n\nShould work"
  },
  {
    "question": "Sort a text file by line length including spaces I have a CSV file that looks like this\n\n```\n\nAS2345,ASDF1232, Mr. Plain Example, 110 Binary ave.,Atlantis,RI,12345,(999)123-5555,1.56\nAS2345,ASDF1232, Mrs. Plain Example, 1121110 Ternary st.                                        110 Binary ave..,Atlantis,RI,12345,(999)123-5555,1.56\nAS2345,ASDF1232, Mr. Plain Example, 110 Binary ave.,Liberty City,RI,12345,(999)123-5555,1.56\nAS2345,ASDF1232, Mr. Plain Example, 110 Ternary ave.,Some City,RI,12345,(999)123-5555,1.56\n\n```\n\nI need to sort it by line length including spaces. The following command doesn't\ninclude spaces, is there a way to modify it so it will work for me?\n\n```\ncat $@ | awk '{ print length, $0 }' | sort -n | awk '{$1=\"\"; print $0}'\n\n```",
    "answer": "< testfile awk '{ print length, $0 }' | sort -n -s | cut -d\" \" -f2-\n< testfile awk '{ print length, $0 }' | sort -n | cut -d\" \" -f2-\necho \"hello   awk   world\" | awk '{print}'\necho \"hello   awk   world\" | awk '{$1=\"hello\"; print}'\nhello   awk   world\nhello awk world\n= $1   # force record to be reconstituted\nprint $0  # or whatever else with $0\naa A line   with     MORE    spaces\nbb The very longest line in the file\nccb\ndd equal len.  Orig pos = 1\ndd equal len.  Orig pos = 2\nccz\ncca\nee A line with  some       spaces\ndd equal len.  Orig pos = 3\nff\ndd equal len.  Orig pos = 4\ng\n\nAnswer\n\nOr, to do your original (perhaps unintentional) sub-sorting of any equal-length lines:\n\nIn both cases, we have solved your stated problem by moving away from awk for your final cut.\nLines of matching length - what to do in the case of a tie:\nThe question did not specify whether or not further sorting was wanted for lines of matching length.  I've assumed that this is unwanted and suggested the use of -s (--stable) to prevent such lines being sorted against each other, and keep them in the relative order in which they occur in the input.\n(Those who want more control of sorting these ties might look at sort's --key option.)\nWhy the question's attempted solution fails (awk line-rebuilding):\nIt is interesting to note the difference between:\n\nThey yield respectively\n\nThe relevant section of (gawk's) manual only mentions as an aside that awk is going to rebuild the whole of $0 (based on the separator, etc) when you change one field.  I guess it's not crazy behaviour.  It has this:\n\"Finally, there are times when it is convenient to force awk to rebuild the entire record, using the current value of the fields and OFS. To do this, use the seemingly innocuous assignment:\"\n\n\"This forces awk to rebuild the record.\"\nTest input including some lines of equal length:"
  },
  {
    "question": "How to delete duplicate lines in a file without sorting it in Unix Is there a way to delete duplicate lines in a file in Unix?\nI can do it with sort -u and uniq commands, but I want to use sed or awk.\nIs that possible?",
    "answer": "awk '!seen[$0]++' file.txt\n\nseen is an associative array that AWK will pass every line of the file to. If a line isn't in the array then seen[$0] will evaluate to false. The ! is the logical NOT operator and will invert the false to true. AWK will print the lines where the expression evaluates to true.\nThe ++ increments seen so that seen[$0] == 1 after the first time a line is found and then seen[$0] == 2, and so on.\nAWK evaluates everything but 0 and \"\" (empty string) to true. If a duplicate line is placed in seen then !seen[$0] will evaluate to false and the line will not be written to the output."
  },
  {
    "question": "Printing the last column of a line in a file I have a file that is constantly being written to/updated. I want to find the last line containing a particular word, then print the last column of that line.\nThe file looks something like this. More A1/B1/C1 lines will be appended to it over time.\n\n```\nA1 123 456\nB1 234 567\nC1 345 678\nA1 098 766\nB1 987 6545\nC1 876 5434\n\n```\n\nI tried to use\n\n```\ntail -f file | grep A1 | awk '{print $NF}'\n\n```\n\nto print the value 766, but nothing is output.\nIs there a way to do this?",
    "answer": "tail file | grep A1 | awk '{print $NF}'\ntail file | awk '/A1/ {print $NF}'\nawk '/A1/ {print $NF}' file\nawk '/A1/ {print $NF}' file | tail -n1\n\nYou don't see anything, because of buffering. The output is shown, when there are enough lines or end of file is reached. tail -f means wait for more input, but there are no more lines in file and so the pipe to grep is never closed.\nIf you omit -f from tail the output is shown immediately:\n\n@EdMorton is right of course. Awk can search for A1 as well, which shortens the command line to\n\nor without tail, showing the last column of all lines containing A1\n\nThanks to @MitchellTracy's comment, tail might miss the record containing A1 and thus you get no output at all. This may be solved by switching tail and awk, searching first through the file and only then show the last line:"
  },
  {
    "question": "Is there a Unix utility to prepend timestamps to stdin? I ended up writing a quick little script for this in Python, but I was wondering if there was a utility you could feed text into which would prepend each line with some text -- in my specific case, a timestamp.  Ideally, the use would be something like:\n\n```\ncat somefile.txt | prepend-timestamp\n\n```\n\n(Before you answer sed, I tried this:\n\n```\ncat somefile.txt | sed \"s/^/`date`/\"\n\n```\n\nBut that only evaluates the date command once when sed is executed, so the same timestamp is incorrectly prepended to each line.)",
    "answer": "<command> | awk '{ print strftime(\"%Y-%m-%d %H:%M:%S\"), $0; fflush(); }'\n\nCould try using awk:\n\nYou may need to make sure that <command> produces line buffered output, i.e. it flushes its output stream after each line; the timestamp awk adds will be the time that the end of the line appeared on its input pipe.\nIf awk shows errors, then try gawk instead."
  },
  {
    "question": "Save modifications in place with awk I am learning awk and I would like to know if there is an option to write changes to file, similar to sed where I would use -i option to save modifications to a file. \nI do understand that I could use redirection to write changes. However is there an option in awk to do that?",
    "answer": "gawk -i inplace '{ gsub(/foo/, \"bar\") }; { print }' file1 file2 file3\ngawk -i inplace -v INPLACE_SUFFIX=.bak '{ gsub(/foo/, \"bar\") }\n> { print }' file1 file2 file3\n\nIn GNU Awk 4.1.0 (released 2013) and later, it has the option of \"inplace\" file editing:\n\n[...] The \"inplace\" extension, built using the new facility, can be used to simulate the GNU \"sed -i\" feature. [...]\n\nExample usage:\n\nTo keep the backup:"
  },
  {
    "question": "How can I delete a newline if it is the last character in a file? I have some files that I'd like to delete the last newline if it is the last character in a file.  od -c shows me that the command I run does write the file with a trailing new line:\n\n```\n0013600   n   t  >  \\n\n\n```\n\nI've tried a few tricks with sed but the best I could think of isn't doing the trick:\n\n```\nsed -e '$s/\\(.*\\)\\n$/\\1/' abc\n\n```\n\nAny ideas how to do this?",
    "answer": "perl -pe 'chomp if eof' filename >filename2\nperl -pi -e 'chomp if eof' filename\n\nor, to edit the file in place:\n\n[Editor's note: -pi -e was originally -pie, but, as noted by several commenters and explained by @hvd, the latter doesn't work.]\nThis was described as a 'perl blasphemy' on the awk website I saw.\nBut, in a test, it worked."
  },
  {
    "question": "How to select lines between two marker patterns which may occur multiple times with awk/sed Using awk or sed how can I select lines which are occurring between two different marker patterns? There may be multiple sections marked with these patterns.\nFor example: \nSuppose the file contains: \n\n```\nabc\ndef1\nghi1\njkl1\nmno\nabc\ndef2\nghi2\njkl2\nmno\npqr\nstu\n\n```\n\nAnd the starting pattern is abc and ending pattern is mno\nSo, I need the output as: \n\n```\ndef1\nghi1\njkl1\ndef2\nghi2\njkl2\n\n```\n\nI am using sed to match the pattern once: \n\n```\nsed -e '1,/abc/d' -e '/mno/,$d' <FILE>\n\n```\n\nIs there any way in sed or awk  to do it repeatedly until the end of file?",
    "answer": "awk '/abc/{flag=1;next}/mno/{flag=0}flag' file\ndef1\nghi1\njkl1\ndef2\nghi2\njkl2\n\nUse awk with a flag to trigger the print when necessary:\n\nHow does this work?\n\n/abc/ matches lines having this text, as well as /mno/ does.\n/abc/{flag=1;next} sets the flag when the text abc is found. Then, it skips the line.\n/mno/{flag=0} unsets the flag when the text mno is found.\nThe final flag is a pattern with the default action, which is to print $0: if flag is equal 1 the line is printed.\n\nFor a more detailed description and examples, together with cases when the patterns are either shown or not, see How to select lines between two patterns?."
  },
  {
    "question": "How to print matched regex pattern using awk? Using awk, I need to find a word in a file that matches a regex pattern. \nI only want to print the word matched with the pattern.\nSo if in the line, I have:\n\n```\nxxx yyy zzz\n\n```\n\nAnd pattern:\n\n```\n/yyy/\n\n```\n\nI want to only get:\n\n```\nyyy\n\n```\n\n\nEDIT:\nthanks to kurumi i managed to write something like this:\n\n```\nawk '{\n        for(i=1; i<=NF; i++) {\n                tmp=match($i, /[0-9]..?.?[^A-Za-z0-9]/)\n                if(tmp) {\n                        print $i\n                }\n        }\n}' $1\n\n```\n\nand this is what i needed :) thanks a lot!",
    "answer": "awk '/pattern/{ print $0 }' file\nawk '{for(i=1;i<=NF;i++){ if($i==\"yyy\"){print $i} } }' file\n\nThis is the very basic\n\nask awk to search for pattern using //, then print out the line, which by default is called a record, denoted by $0. At least read up the documentation.\nIf you only want to get print out the matched word."
  },
  {
    "question": "sed or awk: delete n lines following a pattern How would I mix patterns and numeric ranges in sed (or any similar tool - awk for example)? What I want to do is match certain lines in a file, and delete the next n lines before proceeding, and I want to do that as part of a pipeline.",
    "answer": "sed -e '/pattern/,+5d' file.txt\nsed -e '/pattern/{n;N;N;N;N;d}' file.txt\n\nI'll have a go at this.\nTo delete 5 lines after a pattern (including the line with the pattern):\n\nTo delete 5 lines after a pattern (excluding the line with the pattern):"
  },
  {
    "question": "Get last field using awk substr I am trying to use awk to get the name of a file given the absolute path to the file.\nFor example, when given the input path /home/parent/child/filename I would like to get filename \nI have tried:\n\n```\nawk -F \"/\" '{print $5}' input\n\n```\n\nwhich works perfectly.\nHowever, I am hard coding $5 which would be incorrect if my input has the following structure:\n\n```\n/home/parent/child1/child2/filename\n\n```\n\nSo a generic solution requires always taking the last field (which will be the filename).\nIs there a simple way to do this with the awk substr function?",
    "answer": "awk -F \"/\" '{print $NF}' input\n/home/parent/child1/child2/child3/filename\n/home/parent/child1/child2/filename\n/home/parent/child1/filename\nawk -F\"/\" '{print $NF}' file\nfilename\nfilename\nfilename\n\nUse the fact that awk splits the lines in fields based on a field separator, that you can define. Hence, defining the field separator to / you can say:\n\nas NF refers to the number of fields of the current record, printing $NF means printing the last one.\nSo given a file like this:\n\nThis would be the output:"
  },
  {
    "question": "Printing everything except the first field with awk I have a file that looks like this:\n\n```\nAE  United Arab Emirates\nAG  Antigua & Barbuda\nAN  Netherlands Antilles\nAS  American Samoa\nBA  Bosnia and Herzegovina\nBF  Burkina Faso\nBN  Brunei Darussalam\n\n```\n\nAnd I 'd like to invert the order, printing first everything except $1 and then $1:\n\n```\nUnited Arab Emirates AE\n\n```\n\nHow can I do the \"everything except field 1\" trick?",
    "answer": "awk {'first = $1; $1=\"\"; print $0'}|sed 's/^ //g'\n\nAssigning $1 works but it will leave a leading space: awk '{first = $1; $1 = \"\"; print $0, first; }'\nYou can also find the number of columns in NF and use that in a loop.\n\nFrom Thyag: To eliminate the leading space, add sed to the end of the command:"
  },
  {
    "question": "What are the differences among grep, awk &amp; sed? I am confused about the differences between grep, awk and sed in terms of their role in Unix/Linux system administration and text processing.",
    "answer": "grep This file.txt\nEvery line containing \"This\"\nEvery line containing \"This\"\nEvery line containing \"This\"\nEvery line containing \"This\"\ncat file.txt\nEvery line containing \"This\"\nEvery line containing \"This\"\nEvery line containing \"That\"\nEvery line containing \"This\"\nEvery line containing \"This\"\nsed -i 's/cat/dog/' file.txt\nawk '{print $2}' file.txt\ncat file.txt\nA 10\nB 20\nC 60\nawk 'BEGIN {sum=0; count=0; OFS=\"\\t\"} {sum+=$2; count++} END {print \"Average:\", sum/count}' file.txt\nAverage:    30\n\nShort definition:\ngrep: search for specific terms in a file\n\nNow awk and sed are completly different than grep.\nawk and sed are text processors. Not only do they have the ability to find what you are looking for in text, they have the ability to remove, add and modify the text as well (and much more).\nawk is mostly used for data extraction and reporting. sed is a stream editor\nEach one of them has its own functionality and specialties.\nExample\nSed\n\nAwk\n\nBasic awk usage:\nCompute sum/average/max/min/etc. what ever you may need.\n\nI recommend that you read this book: Sed & Awk: 2nd Ed.\nIt will help you become a proficient sed/awk user on any unix-like environment."
  },
  {
    "question": "How to escape a single quote inside awk I want do the following\n\n```\nawk 'BEGIN {FS=\" \";} {printf \"'%s' \", $1}'\n\n```\n\nBut escaping single quote this way does not work\n\n```\nawk 'BEGIN {FS=\" \";} {printf \"\\'%s\\' \", $1}'\n\n```\n\nHow to do this? Thanks for help.",
    "answer": "awk 'BEGIN {FS=\" \";} {printf \"'\\''%s'\\'' \", $1}'\n\nThis maybe what you're looking for:\n\nThat is, with '\\'' you close the opening ', then print a literal ' by escaping it and finally open the ' again."
  },
  {
    "question": "How to use multiple arguments for awk with a shebang (i.e. #!)? I'd like to execute an gawk script with --re-interval using a shebang. The \"naive\" approach of \n\n```\n#!/usr/bin/gawk --re-interval -f\n... awk script goes here\n\n```\n\ndoes not work, since gawk is called with the first argument \"--re-interval -f\" (not splitted around the whitespace), which it does not understand. Is there a workaround for that?\nOf course you can either not call gawk directly but wrap it into a shell script that splits the first argument, or make a shell script that then calls gawk and put the script into another file, but I was wondering if there was some way to do this within one file.\nThe behaviour of shebang lines differs from system to system - at least in Cygwin it does not split the arguments by whitespaces. I just care about how to do it on a system that behaves like that; the script is not meant to be portable.",
    "answer": "arbitrary_long_name==0 \"exec\" \"/usr/bin/gawk\" \"--re-interval\" \"-f\" \"$0\" \"$@\"\n\n{ print $0 }\n\nThis seems to work for me with (g)awk.\n\nNote the #! runs /bin/sh, so this script is first interpreted as a shell script.\nAt first, I simply tried \"exec\" \"/usr/bin/gawk\" \"--re-interval\" \"-f\" \"$0\" \"$@\", but awk treated that as a command and printed out every line of input unconditionally. That is why I put in the arbitrary_long_name==0 - it's supposed to fail all the time. You could replace it with some gibberish string. Basically, I was looking for a false-condition in awk that would not adversely affect the shell script.\nIn the shell script, the arbitrary_long_name==0 defines a variable called arbitrary_long_name and sets it equal to =0."
  },
  {
    "question": "using awk with column value conditions I'm learning awk from The AWK Programming Language and I have a problem with one of the examples.\nIf I wanted to print $3 if $2 is equal to a value (e.g.1), I was using this command which works fine:\n\n```\nawk '$2==1 {print $3}' <infile> | more\n\n```\n\nBut when I substitute 1 by another searching criteria, (e.g.findtext), the command doesn't work:\n\n```\nawk '$1== findtext {print $3}' <infile> | more\n\n```\n\nIt returns no output and I'm sure that 'findtext' exist on the input file.\nI also tried this, but it does not work:\n\n```\nawk '$1== \"findtext\" {print $3}' <infile> | more\n\n```\n\nHere's my test file named 'test' and it has 9 lines and 8 fields, separated by space:\n\n```\n1 11 0.959660297 0 0.021231423 -0.0073 -0.0031 MhZisp\n2 14 0.180467091 0.800424628 0 0.0566 0.0103 ClNonZ\n3 19 0.98089172 0 0 -0.0158 0.0124 MhNonZ\n4 15 0.704883227 0.265392781 0.010615711 -0.0087 -0.0092 MhZisp\n5 22 0.010615711 0.959660297 0.010615711 0.0476 0.0061 ClNonZ\n6 23 0.715498938 0 0.265392781 -0.0013 -0.0309 Unkn\n7 26 0.927813163 0 0.053078556 -0.0051 -0.0636 MhZisp\n8 44 0.55626327 0.222929936 0.201698514 0.0053 -0.0438 MhZisp\n9 31 0.492569002 0.350318471 0.138004246 0.0485 0.0088 ClNonZ\n\n```\n\nHere's what I did and the output:\n\n```\n$awk '$8 == \"ClNonZ\" {print $3}' test \n\n$ grep ClNonZ test \n2 14 0.180467091 0.800424628 0 0.0566 0.0103 ClNonZ\n5 22 0.010615711 0.959660297 0.010615711 0.0476 0.0061 ClNonZ\n9 31 0.492569002 0.350318471 0.138004246 0.0485 0.0088 ClNonZ\n\n```\n\nI expect to see this which is the $3 that has \"ClNonZ\" in their $8.\n\n```\n0.180467091 \n0.010615711 \n0.492569002\n\n```\n\nDon't know why the awk command didn't return anything. Any thoughts?",
    "answer": "awk '$1 == \"findtext\" {print $3}'\n\nIf you're looking for a particular string, put quotes around it:\n\nOtherwise, awk will assume it's a variable name."
  },
  {
    "question": "Is there still any reason to learn AWK? I am constantly learning new tools, even old fashioned ones, because I like to use the right solution for the problem.\nNevertheless, I wonder if there is still any reason to learn some of them. awk for example is interesting to me, but for simple text processing, I can use grep, cut, sed, etc. while for complex ones, I'll go for Python.\nNow I don't mean that's it's not a powerful and handy tool. But since it takes time and energy to learn a new tool, is it worth it ?",
    "answer": "I think it depends on the environment you find yourself in. If you are a *nix person, then knowing awk is a Good Thing. The only other scripting environment that can be found on virtually every *nix is sh. So while grep, sed, etc can surely replace awk on a modern mainstream linux distro, when you move to more exotic systems, knowing a little awk is going to be Real Handy.\nawk can also be used for more than just text processing. For example one of my supervisors writes astronomy code in awk - that is how utterly old school and awesome he is. Back in his days, it was the best tool for the job... and now even though his students like me use python and what not, he sticks to what he knows and works well.\nIn closing, there is a lot of old code kicking around the world, knowing a little awk isn't going to hurt. It will also make you better *nix person :-)"
  },
  {
    "question": "Show filename and line number in grep output I am trying to search my rails directory using grep. I am looking for a specific word and I want to grep to print out the file name and line number. \nIs there a grep flag that will do this for me? I have been trying to use a combination of -n and -l but these are either printing out the file names with no numbers or just dumping out a lot of text to the terminal which can't be easily read.\nex:\n\n```\n  grep -ln \"search\" *\n\n```\n\nDo I need to pipe it to awk?",
    "answer": "grep -Hn \"search\" *\ngrep -nHo \"search\" *\n\nI think -l is too restrictive as it suppresses the output of -n. I would suggest -H (--with-filename): Print the filename for each match.\n\nIf that gives too much output, try -o to only print the part that matches."
  },
  {
    "question": "How to print last two columns using awk All I want is the last two columns printed.",
    "answer": "awk '{print $(NF-1),\"\\t\",$NF}' file\n\nYou can make use of variable NF which  is set to the total number of fields in the input record:\n\nthis assumes that you have at least 2 fields."
  },
  {
    "question": "Print all but the first three columns Too cumbersome:\n\n```\nawk '{print \" \"$4\" \"$5\" \"$6\" \"$7\" \"$8\" \"$9\" \"$10\" \"$11\" \"$12\" \"$13}' things\n\n```",
    "answer": "awk '{ for(i=4; i<NF; i++) printf \"%s\",$i OFS; if(NF) printf \"%s\",$NF; printf ORS}'\n\necho '1 2 3 4 5 6 7' |\nawk '{for(i=4;i<NF;i++)printf\"%s\",$i OFS;if(NF)printf\"%s\",$NF;printf ORS}' |\ntr ' ' '-'\n4-5-6-7\necho '1 2 3 4 5 6 7' |\nawk '{ for(i=4; i<=NF; i++) printf \"%s\",$i (i==NF?ORS:OFS) }' |\ntr ' ' '-'\n4-5-6-7\necho '1   2 3 4   5    6 7' |\nawk '{ sub(/([^ ]+ +){3}/,\"\") }1' |\ntr ' ' '-'\n4---5----6-7\necho -e ' 1   2\\t \\t3     4   5   6 7 \\t 8\\t ' |\nawk -v n=3 '{ for ( i=1; i<=n; i++) { sub(\"^[\"FS\"]*[^\"FS\"]+[\"FS\"]+\",\"\",$0);} } 1 ' |\nsed 's/ /./g;s/\\t/->/g;s/^/\"/;s/$/\"/'\n\"4...5...6.7.->.8->.\"\necho -e ' 1   2\\t \\t3     4   5   6 7 \\t 8\\t ' |\nawk -v n=3 '{ print gensub(\"[\"FS\"]*([^\"FS\"]+[\"FS\"]+){\"n\"}\",\"\",1); }' |\nsed 's/ /./g;s/\\t/->/g;s/^/\"/;s/$/\"/'\n\"4...5...6.7.->.8->.\"\necho '1 2 3 4 5 6 7' |\nawk '{for (i=3;i<=NF;i++) $(i-2)=$i; NF=NF-2; print $0}' | tr  ' ' '-'\n3-4-5-6-7\necho '1 2 3 4 5 6 7' |\nawk '{for(i=n;i<=NF;i++)$(i-(n-1))=$i;NF=NF-(n-1);print $0}' n=4 | tr ' ' '-'\n4-5-6-7\necho '1 2 3 4 5 6 7' |\nawk '{$1=$2=$3=\"\"}1' |\ntr  ' ' '-'\n---4-5-6-7\necho '1 2 3 4 5 6 7' |\nawk '{for(i=4;i<=13;i++)printf \"%s \",$i;printf \"\\n\"}' |\ntr ' ' '-'\n4-5-6-7-------\n\nA solution that does not add extra leading or trailing whitespace:\n\nSudo_O proposes an elegant improvement using the ternary operator NF?ORS:OFS\n\nEdMorton gives a solution preserving original whitespaces between fields:\n\nBinaryZebra also provides two awesome solutions:\n(these solutions even preserve trailing spaces from original string)\n\nThe solution given by larsr in the comments is almost correct:\n\nThis is the fixed and parametrized version of larsr solution:\n\nAll other answers before Sep-2013 are nice but add extra spaces:\n\nExample of answer adding extra leading spaces:\n\nExample of answer adding extra trailing space"
  },
  {
    "question": "awk partly string match (if column/word partly matches) My dummy file looks like this:\n\n```\nC1    C2    C3    \n1     a     snow   \n2     b     snowman \nsnow     c     sowman\n\n```\n\nI want to get line if there is string snow in $3. I can do this like this:  \n\n```\nawk '($3==\"snow\" || $3==\"snowman\") {print}' dummy_file\n\n```\n\nBut there should be more simpler way.",
    "answer": "awk '$3 ~ /snow/ { print }' dummy_file"
  },
  {
    "question": "Append file contents to the bottom of existing file in Bash Possible Duplicate:\nShell script to append text to each file?\nHow to append output to the end of text file in SHELL Script? \n\nI'm trying to work out the best way to insert api details into a pre-existing config. I thought about using sed to insert the contents of the api text file to the bottom of the config.inc file. I've started the script but it doesn't work and it wipes the file. \n\n```\n#!/bin/bash\n\nCONFIG=/home/user/config.inc\nAPI=/home/user/api.txt\n\nsed -e \"\\$a $API\" > $CONFIG\n\n```\n\nWhat am I doing wrong?",
    "answer": "cat \"$API\" >> \"$CONFIG\"\n\nThis should work:\n\nYou need to use the >> operator to append to a file.  Redirecting with > causes the file to be overwritten. (truncated)."
  },
  {
    "question": "Tab separated values in awk How do I select the first column from the TAB separated string?\n\n```\n# echo \"LOAD_SETTLED    LOAD_INIT       2011-01-13 03:50:01\" | awk -F'\\t' '{print $1}'\n\n```\n\nThe above will return the entire line and not just \"LOAD_SETTLED\" as expected.\nUpdate:\nI need to change the third column in the tab separated values.\nThe following does not work.\n\n```\necho $line | awk 'BEGIN { -v var=\"$mycol_new\" FS = \"[ \\t]+\" } ; { print $1 $2 var $4 $5 $6 $7 $8 $9 }' >> /pdump/temp.txt\n\n```\n\nThis however works as expected if the separator is comma instead of tab.\n\n```\necho $line | awk -v var=\"$mycol_new\" -F'\\t' '{print $1 \",\" $2 \",\" var \",\" $4 \",\" $5 \",\" $6 \",\" $7 \",\" $8 \",\" $9 \"}' >> /pdump/temp.txt\n\n```",
    "answer": "echo \"$line\" |\nawk -v var=\"$mycol_new\" -F'\\t' 'BEGIN {OFS = FS} {$3 = var; print}'\n\nYou need to set the OFS variable (output field separator) to be a tab:\n\n(make sure you quote the $line variable in the echo statement)"
  },
  {
    "question": "How to print lines between two patterns, inclusive or exclusive (in sed, AWK or Perl)? I have a file like the following and I would like to print the lines between two given patterns PAT1 and PAT2.\n\n```\n1\n2\nPAT1\n3    - first block\n4\nPAT2\n5\n6\nPAT1\n7    - second block\nPAT2\n8\n9\nPAT1\n10    - third block\n\n```\n\nI have read How to select lines between two marker patterns which may occur multiple times with awk/sed but I am curious to see all the possible combinations of this, either including or excluding the pattern.\nHow can I print all lines between two patterns?",
    "answer": "awk '/PAT1/,/PAT2/' file\nPAT1\n- first block\nPAT2\nPAT1\n- second block\nPAT2\nPAT1\n- third block\nawk '/PAT1/{flag=1} flag; /PAT2/{flag=0}' file\nawk '/PAT1/{flag=1; next} /PAT2/{flag=0} flag' file\n- first block\n- second block\n- third block\nawk '/PAT1/{flag=1} /PAT2/{flag=0} flag' file\nPAT1\n- first block\nPAT1\n- second block\nPAT1\n- third block\nawk 'flag; /PAT1/{flag=1} /PAT2/{flag=0}' file\n- first block\nPAT2\n- second block\nPAT2\n- third block\nawk 'flag{\nif (/PAT2/)\n{printf \"%s\", buf; flag=0; buf=\"\"}\nelse\nbuf = buf $0 ORS\n}\n/PAT1/ {flag=1}' file\nawk 'flag{ if (/PAT2/){printf \"%s\", buf; flag=0; buf=\"\"} else buf = buf $0 ORS}; /PAT1/{flag=1}' file\n- first block\n- second block\n\nPrint lines between PAT1 and PAT2\n\nOr, using variables:\n\nHow does this work?\n\n/PAT1/ matches lines having this text, as well as /PAT2/ does.\n/PAT1/{flag=1} sets the flag when the text PAT1 is found in a line.\n/PAT2/{flag=0} unsets the flag when the text PAT2 is found in a line.\nflag is a pattern with the default action, which is to print $0: if flag is equal 1 the line is printed. This way, it will print all those lines occurring from the time PAT1 occurs and up to the next PAT2 is seen. This will also print the lines from the last match of PAT1 up to the end of the file.\n\nPrint lines between PAT1 and PAT2 - not including PAT1 and PAT2\n\nThis uses next to skip the line that contains PAT1 in order to avoid this being printed.\nThis call to next can be dropped by reshuffling the blocks: awk '/PAT2/{flag=0} flag; /PAT1/{flag=1}' file.\nPrint lines between PAT1 and PAT2 - including PAT1\n\nBy placing flag at the very end, it triggers the action that was set on either PAT1 or PAT2: to print on PAT1, not to print on PAT2.\nPrint lines between PAT1 and PAT2 - including PAT2\n\nBy placing flag at the very beginning, it triggers the action that was set previously and hence print the closing pattern but not the starting one.\nPrint lines between PAT1 and PAT2 - excluding lines from the last PAT1 to the end of file if no other PAT2 occurs\nThis is based on a solution by Ed Morton.\n\nAs a one-liner:\n\nThis keeps all the selected lines in a buffer that gets populated from the moment PAT1 is found. Then, it keeps being filled with the following lines until PAT2 is found. In that point, it prints the stored content and empties the buffer."
  },
  {
    "question": "How to print all the columns after a particular number using awk? On shell, I pipe to awk when I need a particular column.\nThis prints column 9, for example:\n\n```\n... | awk '{print $9}'\n\n```\n\nHow can I tell awk to print all the columns including and after column 9, not just column 9?",
    "answer": "awk '{ s = \"\"; for (i = 9; i <= NF; i++) s = s $i \" \"; print s }'"
  },
  {
    "question": "How can I replace each newline (\\n) with a space using sed? How can I replace a newline (\"\\n\") with a space (\"\") using the sed command?\nI unsuccessfully tried:    \n\n```\nsed 's#\\n# #g' file\nsed 's#^$# #g' file\n\n```\n\nHow do I fix it?",
    "answer": "sed ':a;N;$!ba;s/\\n/ /g' file\nsed -e ':a' -e 'N' -e '$!ba' -e 's/\\n/ /g' file\n\nUse this solution with GNU sed:\n\nThis will read the whole file in a loop (':a;N;$!ba), then replaces the newline(s) with a space (s/\\n/ /g). Additional substitutions can be simply appended if needed.\nExplanation:\n\nsed starts by reading the first line excluding the newline into the pattern space.\nCreate a label via :a.\nAppend a newline and next line to the pattern space via N.\nIf we are before the last line, branch to the created label $!ba ($! means not to do it on the last line. This is necessary to avoid executing N again, which would terminate the script if there is no more input!).\nFinally the substitution replaces every newline with a space on the pattern space (which is the whole file).\n\nHere is cross-platform compatible syntax which works with BSD and OS X's sed (as per @Benjie comment):\n\nAs you can see, using sed for this otherwise simple problem is problematic. For a simpler and adequate solution see this answer."
  },
  {
    "question": "How can I remove the first line of a text file using bash/sed script? I need to repeatedly remove the first line from a huge text file using a bash script.\nRight now I am using sed -i -e \"1d\" $FILE - but it takes around a minute to do the deletion.\nIs there a more efficient way to accomplish this?",
    "answer": "tail -n +2 \"$FILE\"\ntail -n +2 \"$FILE\" > \"$FILE\"\ntail -n +2 \"$FILE\" > \"$FILE.tmp\" && mv \"$FILE.tmp\" \"$FILE\"\n\nTry tail:\n\n-n x: Just print the last x lines. tail -n 5 would give you the last 5 lines of the input. The + sign kind of inverts the argument and make tail print anything but the first x-1 lines. tail -n +1 would print the whole file, tail -n +2 everything but the first line, etc.\nGNU tail is much faster than sed. tail is also available on BSD and the -n +2 flag is consistent across both tools. Check the FreeBSD or OS X man pages for more.\nThe BSD version can be much slower than sed, though. I wonder how they managed that; tail should just read a file line by line while sed does pretty complex operations involving interpreting a script, applying regular expressions and the like.\nNote: You may be tempted to use\n\nbut this will give you an empty file. The reason is that the redirection (>) happens before tail is invoked by the shell:\n\nShell truncates file $FILE\nShell creates a new process for tail\nShell redirects stdout of the tail process to $FILE\ntail reads from the now empty $FILE\n\nIf you want to remove the first line inside the file, you should use:\n\nThe && will make sure that the file doesn't get overwritten when there is a problem."
  },
  {
    "question": "Find and replace in file and overwrite file doesn&#39;t work, it empties the file I would like to run a find and replace on an HTML file through the command line.\nMy command looks something like this:\n\n```\nsed -e s/STRING_TO_REPLACE/STRING_TO_REPLACE_IT/g index.html > index.html\n\n```\n\nWhen I run this and look at the file afterward, it is empty. It deleted the contents of my file.\nWhen I run this after restoring the file again:\n\n```\nsed -e s/STRING_TO_REPLACE/STRING_TO_REPLACE_IT/g index.html\n\n```\n\nThe stdout is the contents of the file, and the find and replace has been executed.\nWhy is this happening?",
    "answer": "sed -i.bak s/STRING_TO_REPLACE/STRING_TO_REPLACE_IT/g index.html\n\nWhen the shell sees  > index.html in the command line it opens the file index.html for writing, wiping off all its previous contents.\nTo fix this you need to pass the -i option to sed to make the changes inline and create a backup of the original file before it does the changes in-place:\n\nWithout the .bak the command will fail on some platforms, such as Mac OSX."
  },
  {
    "question": "sed edit file in-place How do I edit a file in a single sed command? Currently, I have to manually stream the edited content into a new file and then rename the new file to the original file name.\nI tried sed -i, but my Solaris system said that -i is an illegal option.  Is there a different way?",
    "answer": "sed -i 's/STRING_TO_REPLACE/STRING_TO_REPLACE_IT/g' filename\nsed -i '' 's/STRING_TO_REPLACE/STRING_TO_REPLACE_IT/g' filename\n\nThe -i option streams the edited content into a new file and then renames it behind the scenes, anyway.\nExample:\n\nwhile on macOS you need:"
  },
  {
    "question": "Delete empty lines using sed I am trying to delete empty lines using sed:\n\n```\nsed '/^$/d'\n\n```\n\nbut I have no luck with it.\nFor example, I have these lines:\n\n```\nxxxxxx\n\n\nyyyyyy\n\n\nzzzzzz\n\n```\n\nand I want it to be like:\n\n```\nxxxxxx\nyyyyyy\nzzzzzz\n\n```\n\nWhat should be the code for this?",
    "answer": "sed '/^[[:space:]]*$/d'\nsed -r '/^\\s*$/d'\n\nYou may have spaces or tabs in your \"empty\" line. Use POSIX classes with sed to remove all lines containing only whitespace:\n\nA shorter version that uses ERE, for example with gnu sed:\n\n(Note that sed does NOT support PCRE.)"
  },
  {
    "question": "Non greedy (reluctant) regex matching in sed? I'm trying to use sed to clean up lines of URLs to extract just the domain.\nSo from:\n\n```\nhttp://www.suepearson.co.uk/product/174/71/3816/\n\n```\n\nI want:\n\n```\nhttp://www.suepearson.co.uk/\n\n```\n\n(either with or without the trailing slash, it doesn't matter)\nI have tried:\n\n```\nsed 's|\\(http:\\/\\/.*?\\/\\).*|\\1|'\n\n```\n\nand (escaping the non-greedy quantifier)\n\n```\nsed 's|\\(http:\\/\\/.*\\?\\/\\).*|\\1|'\n\n```\n\nbut I can not seem to get the non-greedy quantifier (?) to work, so it always ends up matching the whole string.",
    "answer": "perl -pe 's|(http://.*?/).*|\\1|'\n\nNeither basic nor extended Posix/GNU regex recognizes the non-greedy quantifier; you need a later regex.  Fortunately, Perl regex for this context is pretty easy to get:"
  },
  {
    "question": "Remove first and last quote (&quot;) from a variable Below is the snippet of a shell script from a larger script. It removes the quotes from the string that is held by a variable. I am doing it using sed, but is it efficient? If not, then what is the efficient way?\n\n```\n#!/bin/sh\n\nopt=\"\\\"html\\\\test\\\\\\\"\"\ntemp=`echo $opt | sed 's/.\\(.*\\)/\\1/' | sed 's/\\(.*\\)./\\1/'`\necho $temp\n\n```",
    "answer": "temp=\"${opt%\\\"}\"\ntemp=\"${temp#\\\"}\"\necho \"$temp\"\necho \"$opt\" | sed -e 's/^\"//' -e 's/\"$//'\nsed -e 's/^\"//' -e 's/\"$//' <<<\"$opt\"\n\nThere's a simpler and more efficient way, using the native shell prefix/suffix removal feature:\n\n${opt%\\\"} will remove the suffix \" (escaped with a backslash to prevent shell interpretation).\n${temp#\\\"} will remove the prefix \" (escaped with a backslash to prevent shell interpretation).\nAnother advantage is that it will remove surrounding quotes only if there are surrounding quotes.\nBTW, your solution always removes the first and last character, whatever they may be (of course, I'm sure you know your data, but it's always better to be sure of what you're removing).\nUsing sed:\n\n(Improved version, as indicated by jfgagne, getting rid of echo)\n\nSo it replaces a leading \" with nothing, and a trailing \" with nothing too. In the same invocation (there isn't any need to pipe and start another sed. Using -e you can have multiple text processing)."
  },
  {
    "question": "sed command with -i option failing on Mac, but works on Linux I've successfully used the following sed command to search/replace text in Linux:\n\n```\nsed -i 's/old_link/new_link/g' *\n\n```\n\nHowever, when I try it on my Mac OS X, I get:\n\n\"command c expects \\ followed by text\"\n\nI thought my Mac runs a normal BASH shell.  What's up?\nEDIT:\nAccording to @High Performance, this is due to Mac sed being of a different (BSD) flavor, so my question would therefore be how do I replicate this command in BSD sed?\nEDIT: \nHere is an actual example that causes this:\n\n```\nsed -i 's/hello/gbye/g' *\n\n```",
    "answer": "sed -i 's/hello/bye/g' just_a_file.txt\nsed -i'' -e 's/hello/bye/g' my_file.txt\nbrew install gnu-sed\n\ngsed -i'' -e 's/hello/bye/g' my_file.txt\nset -Eeuo pipefail\n\nif [[ \"$OSTYPE\" == \"darwin\"* ]]; then\n\nif ! [ -x \"$(command -v gsed)\" ]; then\necho \"Error: 'gsed' is not istalled.\" >&2\necho \"If you are using Homebrew, install with 'brew install gnu-sed'.\" >&2\nexit 1\nfi\nSED_CMD=gsed\nelse\nSED_CMD=sed\nfi\n\n{SED_CMD} -i'' -e 's/hello/bye/g' my_file.txt\nPATH=\"$(brew --prefix)/opt/gnu-sed/libexec/gnubin:$PATH\" ./linux_script_using_sed.sh\nalias sed=gsed\nsed -i 's/hello/bye/g' just_a_file.txt\nset -Eeuo pipefail\n\ncase \"$OSTYPE\" in\ndarwin*|bsd*)\necho \"Using BSD sed style\"\nsed_no_backup=( -i '' )\n;;\n*)\necho \"Using GNU sed style\"\nsed_no_backup=( -i )\n;;\nesac\n\nsed ${sed_no_backup[@]} -e 's/hello/bye/g' my_file.txt\n\nPortable solution below\n\nWhy you get the error\nThe -i option (alternatively, --in-place) means that you want files edited in-place, rather than streaming the change to a new place.\nModifying a file in-place suggests a need for a backup file - and so a user-specified extension is expected after -i, but the parsing of the extension argument is handled differently under GNU sed & Mac (BSD) sed:\n\nGNU : \"If no extension is supplied, the original file is overwritten without making a backup.\" - effectively, you can omit specify a file extension altogether. The extension must be supplied immediately after the -i, with no intervening space.\nMac (BSD) : \"If a zero-length extension is given, no backup will be saved.\" - you must supply an extension, but it can be the empty string '' if you want, to disable the backup.\n\nSo GNU & Mac will interpret this differently:\n\nGNU : No extension is supplied immediately after the -i, so create no backup, use s/hello/bye/g as the text-editing command, and act on the file just_a_file.txt in-place.\nMac (BSD) : Use s/hello/bye/g as the backup file extension (!) and just_a_file.txt as the editing command (regular expression).\nResult: the command code used is j (not a valid command code for substitution, e.g. s), hence we get the error invalid command code j.\n\nPlacing the extension immediately after the -i (eg -i'' or -i'.bak', without a space) is what GNU sed expects, but macOS expect a space after -i (eg -i '' or -i '.bak').\nand is now accepted by Mac (BSD) sed too, though it wasn't tolerated by earlier versions (eg with Mac OS X v10.6, a space was required after -i, eg -i '.bak').\nThe -e parameter allows us to be explicit about where we're declaring the edit command.\nUntil Mac OS was updated in 2013, there wasn't\nStill there isn't any portable command across GNU and Mac (BSD), as these variants all failed (with an error or unexpected backup files):\n\nsed -i -e ... - works on Linux but does not work on macOS as it creates -e backups\nsed -i '' -e ... - works on macOS but fails on Linux\nsed -i='' -e ... - create = backups files on macOS and Linux\nsed -i'' -e ... - create -e backups files on macOS\n\nPortable solution\nYou have few options to achieve the same result on Linux and macOS, e.g.:\n\nUse Perl: perl -i -pe's/old_link/new_link/g' *.\n\nUse gnu-sed on macOS (Install using Homebrew)\n\nNote: On macOS, you could add the bin path of gnu-sed containing the sed command to the PATH environment variable in your shell configuration file (.zshrc).\nIt is best not to do this, since there may be scripts that rely on the macOS built-in version.\nYou can add an alias for gsed as sed using alias sed=gsed (replacing macOS sed with GNU sed) in your ~/.zshrc. This should allow you to use sed \"linux-stile\" in your shell and will have no effects on scripts unless they contain shopt -s expand_aliases.\n\nIf you are using sed in a script, you can try to automate switching to gsed:\n\nYou can temporarily set PATH to use \"gnu-sed\" sed for a script:\n\nIf you are copy/pasting linux scripts, you can alias gsed to sed in the current shell:\n\nUse -i '' on macOS and BSD or -i (GNU sed) otherwise"
  },
  {
    "question": "Replace whole line containing a string using Sed I have a text file which has a particular line something like\n\n```\nsometext sometext sometext TEXT_TO_BE_REPLACED sometext sometext sometext\n\n```\n\nI need to replace the whole line above with \n\n```\nThis line is removed by the admin.\n\n```\n\nThe search keyword is TEXT_TO_BE_REPLACED \nI need to write a shell script for this. How can I achieve this using sed?",
    "answer": "sed -i '/TEXT_TO_BE_REPLACED/c\\This line is removed by the admin.' /tmp/foo\n\nYou can use the change command to replace the entire line, and the -i flag to make the changes in-place. For example, using GNU sed:"
  },
  {
    "question": "Escape a string for a sed replace pattern In my bash script I have an external (received from user) string, which I should use in sed pattern.\n\n```\nREPLACE=\"<funny characters here>\"\nsed \"s/KEYWORD/$REPLACE/g\"\n\n```\n\nHow can I escape the $REPLACE string so it would be safely accepted by sed as a literal replacement?\nNOTE: The KEYWORD is a dumb substring with no matches etc. It is not supplied by user.",
    "answer": "ESCAPED_REPLACE=$(printf '%s\\n' \"$REPLACE\" | sed -e 's/[\\/&]/\\\\&/g')\n\nsed \"s/KEYWORD/$ESCAPED_REPLACE/g\"\nsed -e 's/[]\\/$*.^[]/\\\\&/g'\nKEYWORD=\"The Keyword You Need\";\nESCAPED_KEYWORD=$(printf '%s\\n' \"$KEYWORD\" | sed -e 's/[]\\/$*.^[]/\\\\&/g');\n\nsed \"s/$ESCAPED_KEYWORD/$ESCAPED_REPLACE/g\"\n\nWarning: This does not consider newlines. For a more in-depth answer, see this SO-question instead. (Thanks, Ed Morton & Niklas Peter)\nNote that escaping everything is a bad idea. Sed needs many characters to be escaped to get their special meaning. For example, if you escape a digit in the replacement string, it will turn in to a backreference.\nAs Ben Blank said, there are only three characters that need to be escaped in the replacement string (escapes themselves, forward slash for end of statement and & for replace all):\n\nIf you ever need to escape the KEYWORD string, the following is the one you need:\n\nAnd can be used by:\n\nRemember, if you use a character other than / as delimiter, you need replace the slash in the expressions above wih the character you are using. See PeterJCLaw's comment for explanation.\nEdited: Due to some corner cases previously not accounted for, the commands above have changed several times. Check the edit history for details."
  },
  {
    "question": "How can I output only captured groups with sed? Is there a way to tell sed to output only captured groups?\nFor example, given the input:\n\n```\nThis is a sample 123 text and some 987 numbers\n\n```\n\nAnd pattern:\n\n```\n/([\\d]+)/\n\n```\n\nCould I get only 123 and 987 output in the way formatted by back references?",
    "answer": "string='This is a sample 123 text and some 987 numbers'\necho \"$string\" | sed -rn 's/[^[:digit:]]*([[:digit:]]+)[^[:digit:]]+([[:digit:]]+)[^[:digit:]]*/\\1 \\2/p'\necho \"foobarbaz\" | sed 's/^foo\\(.*\\)baz$/\\1/'\necho \"foobarbaz\" | sed -r 's/^foo(.*)baz$/\\1/'\necho \"foobarbaz\" | sed -r 's/^foo(.*)b(.)z$/\\2 \\1 \\2/'\necho \"$string\" | grep -Po '\\d+'\necho \"$string\" | grep -Eo '\\d+'\necho \"$string\" | grep -Po '(?<=\\D )(\\d+)'\n\nThe key to getting this to work is to tell sed to exclude what you don't want to be output as well as specifying what you do want. This technique depends on knowing how many matches you're looking for. The grep command below works for an unspecified number of matches.\n\nThis says:\n\nuse extended regular expressions (-r)\ndon't default to printing each line (-n)\nexclude zero or more non-digits\ninclude one or more digits\nexclude one or more non-digits\ninclude one or more digits\nexclude zero or more non-digits\nprint the substitution (p) (on one line)\n\nIn general, in sed you capture groups using parentheses and output what you capture using a back reference:\n\nwill output \"bar\". If you use -r (-E for OS X) for extended regex, you don't need to escape the parentheses:\n\nThere can be up to 9 capture groups and their back references. The back references are numbered in the order the groups appear, but they can be used in any order and can be repeated:\n\noutputs \"a bar a\".\nIf you have GNU grep:\n\nIt may also work in BSD, including OS X:\n\nThese commands will match any number of digit sequences. The output will be on multiple lines.\nor variations such as:\n\nThe -P option enables Perl Compatible Regular Expressions. See man 3 pcrepattern or man  3 pcresyntax."
  },
  {
    "question": "Delete specific line number(s) from a text file using sed? I want to delete one or more specific line numbers from a file. How would I do this using sed?",
    "answer": "sed -e '5,10d;12d' file\nsed -i.bak -e '5,10d;12d' file\n\nIf you want to delete lines from 5 through 10 and line 12th:\n\nThis will print the results to the screen. If you want to save the results to the same file:\n\nThis will store the unmodified file as file.bak, and delete the given lines.\nNote: Line numbers start at 1. The first line of the file is 1, not 0."
  },
  {
    "question": "Error when using &#39;sed&#39; with &#39;find&#39; command on OS X: &quot;invalid command code .&quot; Being forced to use CVS for a current client and the address changed for the remote repo. The only way I can find to change the remote address in my local code is a recursive search and replace.\nHowever, with the sed command I'd expect to work:\n\n```\nfind ./ -type f -exec sed -i \"s/192.168.20.1/new.domain.com/\" {} \\;\n\n```\n\nI get an error for every file:\n\n```\nsed: 1: \".//file/path ...\": invalid command code .\n\n```\n\nI've tried to escape the periods in the sed match/replacement but that doesn't solve anything.",
    "answer": "find ./ -type f -exec sed -i '' -e \"s/192.168.20.1/new.domain.com/\" {} \\;\n\nIf you are on a OS X, this probably has nothing to do with the sed command. On the OSX version of sed, the -i option expects an extension argument so your command is actually parsed as the extension argument and the file path is interpreted as the command code.\nTry adding the -e argument explicitly and giving '' as argument to -i:\n\nSee this."
  },
  {
    "question": "Insert line after match using sed For some reason I can't seem to find a straightforward answer to this and I'm on a bit of a time crunch at the moment.  How would I go about inserting a choice line of text after the first line matching a specific string using the sed command.  I have ...\n\n```\nCLIENTSCRIPT=\"foo\"\nCLIENTFILE=\"bar\"\n\n```\n\nAnd I want insert a line after the CLIENTSCRIPT= line resulting in ...\n\n```\nCLIENTSCRIPT=\"foo\"\nCLIENTSCRIPT2=\"hello\"\nCLIENTFILE=\"bar\"\n\n```",
    "answer": "sed '/CLIENTSCRIPT=\"foo\"/a CLIENTSCRIPT2=\"hello\"' file\nsed -i '/CLIENTSCRIPT=\"foo\"/a CLIENTSCRIPT2=\"hello\"' file\nCLIENTSCRIPT=\"foo\"\nCLIENTSCRIPT2=\"hello\"\nCLIENTFILE=\"bar\"\n\nTry doing this using GNU sed:\n\nif you want to substitute in-place, use\n\nOutput\n\nDoc\n\nsee sed doc and search \\a (append)"
  },
  {
    "question": "Replace comma with newline in sed on MacOS? I have a file of strings that are comma separated.  I'm trying to replace the commas with a new line.  I've tried:\n\n```\nsed 's/,/\\n/g' file\n\n```\n\nbut it is not working.  What am I missing?",
    "answer": "tr , '\\n' < file\n\nUse tr instead:"
  },
  {
    "question": "Retrieve last 100 lines logs I need to retrieve last 100 lines of logs from the log file.\nI tried the sed command\n\n```\nsed -n -e '100,$p' logfilename\n\n```\n\nPlease let me know how can I change this command to specifically retrieve the last 100 lines.",
    "answer": "tail -100 <log file>   > newLogfile\ntail -n 100 <log file>   > newLogfile\n\nYou can use tail command as follows:\n\nNow last 100 lines will be present in newLogfile\nEDIT:\nMore recent versions of tail as mentioned by twalberg use command:"
  },
  {
    "question": "sed in-place flag that works both on Mac (BSD) and Linux Is there an invocation of sed todo in-place editing without backups that works both on Linux and Mac? While the BSD sed shipped with OS X seems to need sed -i '' \u2026, the GNU sed Linux distributions usually come with interprets the quotes as empty input file name (instead of the backup extension), and needs sed -i \u2026 instead.\nIs there any command line syntax which works with both flavors, so I can use the same script on both systems?",
    "answer": "sed -i.bak 's/foo/bar/' filename\n% sed --version | head -1\nGNU sed version 4.2.1\n% echo 'foo' > file\n% sed -i.bak 's/foo/bar/' ./file\n% ls\nfile  file.bak\n% cat ./file\nbar\n\n% sed --version 2>&1 | head -1\nsed: illegal option -- -\n% echo 'foo' > file\n% sed -i.bak 's/foo/bar/' ./file\n% ls\nfile  file.bak\n% cat ./file\nbar\n\nIf you really want to just use sed -i the 'easy' way, the following DOES work on both GNU and BSD/Mac sed:\n\nNote the lack of space and the dot.\nProof:\n\nObviously you could then just delete the .bak files."
  },
  {
    "question": "How to swap text based on patterns at once with sed? Suppose I have 'abbc' string and I want to replace:\n\nab -> bc\nbc -> ab\n\nIf I try two replaces the result is not what I want:\n\n```\necho 'abbc' | sed 's/ab/bc/g;s/bc/ab/g'\nabab\n\n```\n\nSo what sed command can I use to replace like below?\n\n```\necho abbc | sed SED_COMMAND\nbcab\n\n```\n\nEDIT:\nActually the text could have more than 2 patterns and I don't know how many replaces I will need. Since there was a answer saying that sed is a stream editor and its replaces are greedily I think that I will need to use some script language for that.",
    "answer": "sed 's/ab/~~/g; s/bc/ab/g; s/~~/bc/g'\n\nMaybe something like this:\n\nReplace ~ with a character that you know won't be in the string."
  },
  {
    "question": "How to insert a text at the beginning of a file? So far I've been able to find out how to add a line at the beginning of a file but that's not exactly what I want. I'll show it with an example:\nFile content\n\n```\nsome text at the beginning\n\n```\n\nResult\n\n```\n<added text> some text at the beginning\n\n```\n\nIt's similar but I don't want to create any new line with it...\nI would like to do this with sed if possible.",
    "answer": "sed -i '1s/^/<added text> /' file\nsed -i '1,10s/^/<added text> /' file\n{ echo -n '<added text> '; cat file; } >file.new\nmv file{.new,}\n\nsed can operate on an address:\n\nWhat is this magical 1s you see on every answer here? Line addressing!.\nWant to add <added text> on the first 10 lines?\n\nOr you can use Command Grouping:"
  },
  {
    "question": "How to use sed to replace only the first occurrence in a file? I would like to update a large number of C++ source files with an extra include directive before any existing #includes. For this sort of task, I normally use a small bash script with sed to re-write the file. \nHow do I get sed to replace just the first occurrence of a string in a file rather than replacing every occurrence?\nIf I use\n\n```\nsed s/#include/#include \"newfile.h\"\\n#include/\n\n```\n\nit replaces all #includes. \nAlternative suggestions to achieve the same thing are also welcome.",
    "answer": "1{x;s/^/first/;x;}\n1,/foo/{x;/first/s///;x;s/foo/bar/;}\nsed '0,/foo/s//bar/' file\n\nor, if you prefer: Editor's note: works with GNU sed only.\n\nSource"
  },
  {
    "question": "Environment variable substitution in sed If I run these commands from a script:\n\n```\n#my.sh\nPWD=bla\nsed 's/xxx/'$PWD'/'\n...\n$ ./my.sh\nxxx\nbla\n\n```\n\nit is fine.\nBut, if I run:\n\n```\n#my.sh\nsed 's/xxx/'$PWD'/'\n...\n$ ./my.sh\n$ sed: -e expression #1, char 8: Unknown option to `s' \n\n```\n\nI read in tutorials that to substitute environment variables from shell you need to stop, and 'out quote' the $varname part so that it is not substituted directly, which is what I did, and which works only if the variable is defined immediately before.\nHow can I get sed to recognize a $var as an environment variable as it is defined in the shell?",
    "answer": "sed 's@xxx@'\"$PWD\"'@'\n\nYour two examples look identical, which makes problems hard to diagnose.  Potential problems:\n\nYou may need double quotes, as in sed 's/xxx/'\"$PWD\"'/'\n$PWD may contain a slash, in which case you need to find a character not contained in $PWD to use as a delimiter.\n\nTo nail both issues at once, perhaps"
  },
  {
    "question": "Find and replace with sed in directory and sub directories I run this command to find and replace all occurrences of 'apple' with 'orange' in all files in root of my site:\n\n```\nfind ./ -exec sed -i 's/apple/orange/g' {} \\;\n\n```\n\nBut it doesn't go through sub directories.\nWhat is wrong with this command?\nHere are some lines of output of find ./:\n\n```\n./index.php\n./header.php\n./fpd\n./fpd/font\n./fpd/font/desktop.ini\n./fpd/font/courier.php\n./fpd/font/symbol.php\n\n```",
    "answer": "find ./ -type f -exec sed -i -e 's/apple/orange/g' {} \\;\n\nYour find should look like that to avoid sending directory names to sed:"
  },
  {
    "question": "Change multiple files The following command is correctly changing the contents of 2 files.\n\n```\nsed -i 's/abc/xyz/g' xaa1 xab1 \n\n```\n\nBut what I need to do is to change several such files dynamically and I do not know the file names. I want to write a command that will read all the files from current directory starting with xa* and sed should change the file contents.",
    "answer": "for i in xa*; do\nsed -i 's/asd/dfg/g' $i\ndone\n-bash: /bin/grep: Argument list too long\n... (output skipped)\n\nBetter yet:\n\nbecause nobody knows how many files are there, and it's easy to break command line limits.\nHere's what happens when there are too many files:"
  },
  {
    "question": "How to concatenate multiple lines of output to one line? If I run the command cat file | grep pattern, I get many lines of output. How do you concatenate all lines into one line, effectively replacing each \"\\n\" with \"\\\" \" (end with \" followed by space)?\ncat file | grep pattern | xargs sed s/\\n/ /g \nisn't working for me.",
    "answer": "grep pattern file | tr '\\n' ' '\ngrep pattern file | awk '{print}' ORS='\" '\none\ntwo\nthree\none\" two\" three\"\n\nUse tr '\\n' ' ' to translate all newline characters to spaces:\n\nNote: grep reads files, cat concatenates files. Don't cat file | grep!\nEdit:\ntr can only handle single character translations. You could use awk to change the output record separator like:\n\nThis would transform:\n\nto:"
  },
  {
    "question": "Delete all lines beginning with a # from a file All of the lines with comments in a file begin with #. How can I delete all of the lines (and only those lines) which begin with #? Other lines containing #, but not at the beginning of the line should be ignored.",
    "answer": "sed '/^#/d'\n\nThis can be done with a sed one-liner:\n\nThis says, \"find all lines that start with # and delete them, leaving everything else.\""
  },
  {
    "question": "RE error: illegal byte sequence on Mac OS X I'm trying to replace a string in a Makefile on Mac OS X for cross-compiling to iOS. The string has embedded double quotes. The command is:\n\n```\nsed -i \"\" 's|\"iphoneos-cross\",\"llvm-gcc:-O3|\"iphoneos-cross\",\"clang:-Os|g' Configure\n\n```\n\nAnd the error is:\n\n```\nsed: RE error: illegal byte sequence\n\n```\n\nI've tried escaping the double quotes, commas, dashes, and colons with no joy. For example:\n\n```\nsed -i \"\" 's|\\\"iphoneos-cross\\\"\\,\\\"llvm-gcc\\:\\-O3|\\\"iphoneos-cross\\\"\\,\\\"clang\\:\\-Os|g' Configure\n\n```\n\nI'm having a heck of a time debugging the issue. Does anyone know how to get sed to print the position of the illegal byte sequence? Or does anyone know what the illegal byte sequence is?",
    "answer": "LC_ALL=C sed -i \"\" 's|\"iphoneos-cross\",\"llvm-gcc:-O3|\"iphoneos-cross\",\"clang:-Os|g' Configure\nsed 's/.*/&/' <<<$'voil\\x{e0}'\nLC_CTYPE=C sed 's/.*/&/' <<<$'voil\\x{e0}'\niconv -f ASCII --byte-subst='\\x{%02x}' <<<$'voil\\x{e0}'\nsed 's/.*/&/' <<<\"$(iconv -f ISO-8859-1 <<<$'voil\\x{e0}')\"\nsed 's/\u00e0/\u00fc/' <<<\"$(iconv -f ISO-8859-1 <<<$'voil\\x{e0}')\"\nsed 's/\u00e0/\u00fc/' <<<\"$(iconv -f ISO-8859-1 <<<$'voil\\x{e0}')\" | iconv -t ISO-8859-1\n\nA sample command that exhibits the symptom: sed 's/./@/' <<<$'\\xfc' fails, because byte 0xfc is not a valid UTF-8 char.\nNote that, by contrast, GNU sed (Linux, but also installable on macOS) simply passes the invalid byte through, without reporting an error.\nUsing the formerly accepted answer is an option if you don't mind losing support for your true locale (if you're on a US system and you never need to deal with foreign characters, that may be fine.)\nHowever, the same effect can be had ad-hoc for a single command only:\n\nNote: What matters is an effective LC_CTYPE setting of C, so LC_CTYPE=C sed ... would normally also work, but if LC_ALL happens to be set (to something other than C), it will override individual LC_*-category variables such as LC_CTYPE. Thus, the most robust approach is to set LC_ALL.\nHowever, (effectively) setting LC_CTYPE to C treats strings as if each byte were its own character (no interpretation based on encoding rules is performed), with no regard for the - multibyte-on-demand - UTF-8 encoding that OS X employs by default, where foreign characters have multibyte encodings.\nIn a nutshell: setting LC_CTYPE to C causes the shell and utilities to only recognize basic English letters as letters (the ones in the 7-bit ASCII range), so that foreign chars. will not be treated as letters, causing, for instance, upper-/lowercase conversions to fail.\nAgain, this may be fine if you needn't match multibyte-encoded characters such as \u00e9, and simply want to pass such characters through.\nIf this is insufficient and/or you want to understand the cause of the original error (including determining what input bytes caused the problem) and perform encoding conversions on demand, read on below.\n\nThe problem is that the input file's encoding does not match the shell's.\nMore specifically, the input file contains characters encoded in a way that is not valid in UTF-8 (as @Klas Lindb\u00e4ck stated in a comment) - that's what the sed error message is trying to say by invalid byte sequence.\nMost likely, your input file uses a single-byte 8-bit encoding such as ISO-8859-1, frequently used to encode \"Western European\" languages.\nExample:\nThe accented letter \u00e0 has Unicode codepoint 0xE0 (224) - the same as in ISO-8859-1. However, due to the nature of UTF-8 encoding, this single codepoint is represented as 2 bytes - 0xC3 0xA0, whereas trying to pass the single byte 0xE0 is invalid under UTF-8.\nHere's a demonstration of the problem using the string voil\u00e0 encoded as ISO-8859-1, with the \u00e0 represented as one byte (via an ANSI-C-quoted bash string ($'...') that uses \\x{e0} to create the byte):\nNote that the sed command is effectively a no-op that simply passes the input through, but we need it to provoke the error:\n\nTo simply ignore the problem, the above LCTYPE=C approach can be used:\n\nIf you want to determine which parts of the input cause the problem, try the following:\n\nThe output will show you all bytes that have the high bit set (bytes that exceed the 7-bit ASCII range) in hexadecimal form. (Note, however, that that also includes correctly encoded UTF-8 multibyte sequences - a more sophisticated approach would be needed to specifically identify invalid-in-UTF-8 bytes.)\n\nPerforming encoding conversions on demand:\nStandard utility iconv can be used to convert to (-t) and/or from (-f) encodings; iconv -l lists all supported ones.\nExamples:\nConvert FROM ISO-8859-1 to the encoding in effect in the shell (based on LC_CTYPE, which is UTF-8-based by default), building on the above example:\n\nNote that this conversion allows you to properly match foreign characters:\n\nTo convert the input BACK to ISO-8859-1 after processing, simply pipe the result to another iconv command:"
  },
  {
    "question": "Command to get nth line of STDOUT Is there any bash command that will let you get the nth line of STDOUT?\nThat is to say, something that would take this\n\n```\n$ ls -l\n-rw-r--r--@ 1 root  wheel my.txt\n-rw-r--r--@ 1 root  wheel files.txt\n-rw-r--r--@ 1 root  wheel here.txt\n\n```\n\nand do something like\n\n```\n$ ls -l | magic-command 2\n-rw-r--r--@ 1 root  wheel files.txt\n\n```\n\nI realize this would be bad practice when writing scripts meant to be reused, BUT when working with the shell day to day it'd be useful to me to be able to filter my STDOUT in such a way.  \nI also realize this would be semi-trivial command to write (buffer STDOUT, return a specific line), but I want to know if there's some standard shell command to do this that would be available without me dropping a script into place.",
    "answer": "ls -l | sed -n 2p\nls -l | sed -n -e '2{p;q}'\nls -l | sed -n 2,4p\nls -l | sed -n -e 2,4p -e 20,30p\nls -l | sed -n -e '2,4p;20,30p'\n\nUsing sed, just for variety:\n\nUsing this alternative, which looks more efficient since it stops reading the input when the required line is printed, may generate a SIGPIPE in the feeding process, which may in turn generate an unwanted error message:\n\nI've seen that often enough that I usually use the first (which is easier to type, anyway), though ls is not a command that complains when it gets SIGPIPE.\nFor a range of lines:\n\nFor several ranges of lines:"
  },
  {
    "question": "In-place edits with sed on OS X I'd like edit a file with sed on OS X.  I'm using the following command:\n\n```\nsed 's/oldword/newword/' file.txt\n\n```\n\nThe output is sent to the terminal.  file.txt is not modified.  The changes are saved to file2.txt with this command:\n\n```\nsed 's/oldword/newword/' file1.txt > file2.txt\n\n```\n\nHowever I don't want another file.  I just want to edit file1.txt.  How can I do this?\nI've tried the -i flag.  This results in the following error:\n\n```\nsed: 1: \"file1.txt\": invalid command code f\n\n```",
    "answer": "sed -i.bu 's/oldword/newword/' file1.txt\nsed -i '' 's/oldword/newword/' file1.txt\n^ note the space\n\nYou can use the -i flag correctly by providing it with a suffix to add to the backed-up file. Extending your example:\n\nWill give you two files: one with the name file1.txt that contains the substitution, and one with the name file1.txt.bu that has the original content.\nMildly dangerous\nIf you want to destructively overwrite the original file, use something like:\n\nBecause of the way the line gets parsed, a space is required between the option flag and its argument because the argument is zero-length.\nOther than possibly trashing your original, I\u2019m not aware of any further dangers of tricking sed this way. It should be noted, however, that if this invocation of sed is part of a script, The Unix Way\u2122 would (IMHO) be to use sed non-destructively, test that it exited cleanly, and only then remove the extraneous file."
  },
  {
    "question": "sed fails with &quot;unknown option to `s&#39;&quot; error I'm trying to use \n\n```\nsed -i -e \"s/.*seb.*/    \\\"$ftp_login_template\\\"/\" $ftp_dir\n\n```\n\nhowever I get this error:\n\n```\nsed: -e expression #1, char 34: unknown option to `s'\n\n```\n\nI don't understand why since this works perfectly:\n\n```\nsed -i -e \"s/.*wbspassword.*/    \\\"wbspassword\\\": \\\"$password\\\",/\" $user_conf\n\n```\n\nAny ideas as to what I'm doing wrong? \nCould this be the problem?\n    ftp_login_template=\\${user}:${password}:24:86::\\/var\\/lib\\/clit.${user}\\/downloads:\\/bin\\/false\\\"",
    "answer": "replacement=\"/my/path\"\nsed --expression \"s@pattern@$replacement@\"\n\nThe problem is with slashes: your variable contains them and the final command will be something like sed \"s/string/path/to/something/g\", containing way too many slashes.\nSince sed can take any char as delimiter (without having to declare the new delimiter), you can try using another one that doesn't appear in your replacement string:\n\nNote that this is not bullet proof: if the replacement string later contains @ it will break for the same reason, and any backslash sequences like \\1 will still be interpreted according to sed rules. Using | as a delimiter is also a nice option as it is similar in readability to /."
  },
  {
    "question": "How to replace a whole line with sed? Suppose I have a file with lines\n\n```\naaa=bbb\n\n```\n\nNow I would like to replace them with:\n\n```\naaa=xxx\n\n```\n\nI can do that as follows:\n\n```\nsed \"s/aaa=bbb/aaa=xxx/g\"\n\n```\n\nNow I have a file with a few lines as follows:\n\n```\naaa=bbb\naaa=ccc\naaa=ddd\naaa=[something else]\n\n```\n\nHow can I replace all these lines aaa=[something] with aaa=xxx using sed?",
    "answer": "sed \"s/aaa=.*/aaa=xxx/g\"\n\nTry this:"
  },
  {
    "question": "How to use sed/grep to extract text between two words? I am trying to output a string that contains everything between two words of a string:\ninput:\n\n```\n\"Here is a String\"\n\n```\n\noutput:\n\n```\n\"is a\"\n\n```\n\nUsing: \n\n```\nsed -n '/Here/,/String/p'\n\n```\n\nincludes the endpoints, but I don't want to include them.",
    "answer": "sed -e 's/Here\\(.*\\)String/\\1/'"
  },
  {
    "question": "How to replace an entire line in a text file by line number I have a situation where I want a bash script to replace an entire line in a file.\nThe line number is always the same, so that can be a hard-coded variable.\nI'm not trying to replace some sub-string in that line, I just want to replace that line entirely with a new line.\nAre there any bash methods for doing this (or something simple that can be thrown into a .sh script).",
    "answer": "sed -i 'Ns/.*/replacement-line/' file.txt\nsed 'Ns/.*/replacement-line/' file.txt > new_file.txt\n\nNot the greatest, but this should work:\n\nwhere N should be replaced by your target line number. This replaces the line in the original file. To save the changed text in a different file, drop the -i option:"
  },
  {
    "question": "How to remove the lines which appear on file B from another file A? I have a large file A (consisting of emails), one line for each mail. I also have another file B that contains another set of mails.\nWhich command would I use to remove all the addresses that appear in file B from the file A.\nSo, if file A contained:\n\n```\nA\nB\nC\n\n```\n\nand file B contained:\n\n```\nB    \nD\nE\n\n```\n\nThen file A should be left with:\n\n```\nA\nC\n\n```\n\nNow I know this is a question that might have been asked more often, but I only found one command online that gave me an error with a bad delimiter.\nAny help would be much appreciated! Somebody will surely come up with a clever one-liner, but I'm not the shell expert.",
    "answer": "comm -23 file1 file2\n\nIf the files are sorted (they are in your example):\n\n-23 suppresses the lines that are in both files, or only in file 2. If the files are not sorted, pipe them through sort first...\nSee the man page here"
  },
  {
    "question": "how to parse a JSON String with jq (or other alternatives)? I'm trying to get jq to parse a JSON structure like:\n\n```\n{\n  \"a\" : 1,\n  \"b\" : 2,\n  \"c\" : \"{\\\"id\\\":\\\"9ee ...\\\",\\\"parent\\\":\\\"abc...\\\"}\\n\"\n}\n\n```\n\nThat is, an element in the JSON is a string with escaped json.\nSo, I have something along the lines of\n$ jq [.c] myFile.json | jq [.id]\nBut that crashes with jq: error: Cannot index string with string\nThis is because the output of .c is a string, not more JSON.\nHow do I get jq to parse this string?\nMy initial solution is to use sed to replace all the escape chars (\\\":\\\", \\\",\\\" and \\\") but that's messy, I assume there's a way built into jq to do this?\nThanks!\nedit:\nAlso, the jq version available here is:\n\n```\n$ jq --version\njq version 1.3\n\n```\n\nI guess I could update it if required.",
    "answer": "jq '.c | fromjson | .id' myFile.json\n\njq has the fromjson builtin for this:\n\nfromjson was added in version 1.4."
  },
  {
    "question": "sed one-liner to convert all uppercase to lowercase? I have a textfile in which some words are printed in ALL CAPS. I want to be able to just convert everything in the textfile to lowercase, using sed. That means that the first sentence would then read, 'i have a textfile in which some words are printed in all caps.'",
    "answer": "tr '[:upper:]' '[:lower:]' < input.txt > output.txt\ntr '[:lower:]' '[:upper:]' < input.txt > output.txt\nsed -e 's/\\(.*\\)/\\L\\1/' input.txt > output.txt\nsed -e 's/\\(.*\\)/\\U\\1/' input.txt > output.txt\n\nHere are two methods for doing the conversion using tr and sed:\nUsing tr\nConvert uppercase to lowercase\n\nConvert lowercase to uppercase\n\nUsing sed on GNU (but not BSD or Mac)\nConvert uppercase to lowercase\n\nConvert lowercase to uppercase\n\nThe reason the sed version doesn't work on BSD or Mac is because those systems don't support the \\L or \\U flags"
  },
  {
    "question": "How do I remove newlines from a text file? I have the following data, and I need to put it all into one line.\nI have this:\n\n```\n22791\n\n;\n\n14336\n\n;\n\n22821\n\n;\n\n34653\n\n;\n\n21491\n\n;\n\n25522\n\n;\n\n33238\n\n;\n\n```\n\nI need this:\n\n```\n22791;14336;22821;34653;21491;25522;33238;\n\n```\n\n\nEDIT\nNone of these commands is working perfectly.\nMost of them let the data look like this:\n\n```\n22791\n\n;14336\n\n;22821\n\n;34653\n\n;21491\n\n;25522\n\n```",
    "answer": "tr --delete '\\n' < yourfile.txt\ntr -d '\\n' < yourfile.txt\ntr -d \"\\n\\r\" < yourfile.txt\n\nIf none of the commands posted here are working, then you have something other than a newline separating your fields. Possibly you have DOS/Windows line endings in the file (although I would expect the Perl solutions to work even in that case)?\nTry:\n\nIf that doesn't work then you're going to have to inspect your file more closely (e.g., in a hex editor) to find out what characters are actually in there that you want to remove."
  },
  {
    "question": "How to use sed to remove the last n lines of a file I want to remove some n lines from the end of a file. Can this be done using sed?\nFor example, to remove lines from 2 to 4, I can use\n\n```\n$ sed '2,4d' file\n\n```\n\nBut I don't know the line numbers.  I can delete the last line using \n\n```\n$sed $d file\n\n```\n\nbut I want to know the way to remove n lines from the end. Please let me know how to do that using sed or some other method.",
    "answer": "head -n -2 myfile.txt\n\nI don't know about sed, but it can be done with head:"
  },
  {
    "question": "sed: print only matching group I want to grab the last two numbers (one int, one float; followed by optional whitespace) and print only them.\nExample:\n\n```\nfoo bar <foo> bla 1 2 3.4\n\n```\n\nShould print:\n\n```\n2 3.4\n\n```\n\nSo far, I have the following:\n\n```\nsed -n  's/\\([0-9][0-9]*[\\ \\t][0-9.]*[\\ \\t]*$\\)/replacement/p' \n\n```\n\nwill give me\n\n```\nfoo bar <foo> bla 1 replacement\n\n```\n\nHowever, if I try to replace it with group 1, the whole line is printed.\n\n```\nsed -n  's/\\([0-9][0-9]*[\\ \\t][0-9.]*[\\ \\t]*$\\)/\\1/p' \n\n```\n\nHow can I print only the section of the line that matches the regex in the group?",
    "answer": "echo \"foo bar <foo> bla 1 2 3.4\" |\nsed -n  's/.*\\([0-9][0-9]*[\\ \\t][0-9.]*[ \\t]*$\\)/\\1/p'\n3.4\n\nMatch the whole line, so add a .* at the beginning of your regex. This causes the entire line to be replaced with the contents of the group"
  },
  {
    "question": "How to insert strings containing slashes with sed? I have a Visual Studio project, which is developed locally. Code files have to be deployed to a remote server. The only problem is the URLs they contain, which are hard-coded.\nThe project contains URLs such as ?page=one. For the link to be valid on the server, it must be /page/one .\nI've decided to replace all URLs in my code files with sed before deployment, but I'm stuck on slashes.\nI know this is not a pretty solution, but it's simple and would save me a lot of time. The total number of strings I have to replace is fewer than 10. A total number of files which have to be checked is ~30.\nAn example describing my situation is below:\nThe command I'm using:\n\n```\nsed -f replace.txt < a.txt > b.txt\n\n```\n\nreplace.txt which contains all the strings:\n\n```\ns/?page=one&/pageone/g\ns/?page=two&/pagetwo/g\ns/?page=three&/pagethree/g\n\n```\n\na.txt:\n\n```\n?page=one&\n?page=two&\n?page=three&\n\n```\n\nContent of b.txt after I run my sed command:\n\n```\npageone\npagetwo\npagethree\n\n```\n\nWhat I want b.txt to contain:\n\n```\n/page/one\n/page/two\n/page/three\n\n```",
    "answer": "s:?page=one&:pageone:g\ns/\\//foo/\n\nThe easiest way would be to use a different delimiter in your search/replace lines, e.g.:\n\nYou can use any character as a delimiter that's not part of either string. Or, you could escape it with a backslash:\n\nWhich would replace / with foo. You'd want to use the escaped backslash in cases where you don't know what characters might occur in the replacement strings (if they are shell variables, for example)."
  },
  {
    "question": "Is there any sed like utility for cmd.exe? I want to programmatically edit file content using windows command line (cmd.exe). In *nix there is sed for this tasks. Are there any useful native equivalents (cmd or ps) in windows?",
    "answer": "get-content somefile.txt | where { $_ -match \"expression\"}\nselect-string somefile.txt -pattern \"expression\"\nget-content somefile.txt | %{$_ -replace \"expression\",\"replace\"}\n\nToday powershell saved me.\nFor grep there is:\n\nor\n\nand for sed there is:\n\nFor more detail about replace PowerShell function see this Microsoft article."
  },
  {
    "question": "How can I grep recursively, but only in files with certain extensions? I'm working on a script to grep certain directories:\n\n```\n{ grep -r -i CP_Image ~/path1/;\ngrep -r -i CP_Image ~/path2/;\ngrep -r -i CP_Image ~/path3/;\ngrep -r -i CP_Image ~/path4/;\ngrep -r -i CP_Image ~/path5/; }\n| mailx -s GREP email@domain.example\n\n```\n\nHow can I limit results only to extensions .h and .cpp?",
    "answer": "grep -inr --include \\*.h --include \\*.cpp CP_Image ~/path[12345] | mailx -s GREP email@domain.example\n\nJust use the --include parameter, like this:\n\nThat should do what you want.\nTo take the explanation from HoldOffHunger's answer below:\n\ngrep: command\n\n-r: recursively\n\n-i: ignore-case\n\n-n: each output line is preceded by its relative line number in the file\n\n--include \\*.cpp: all *.cpp: C++ files (escape with \\ just in case you have a directory with asterisks in the filenames)\n\n./: Start at current directory."
  },
  {
    "question": "Negative matching using grep (match lines that do not contain foo) How do I match all lines not matching a particular pattern using grep? I tried this:\n\n```\ngrep '[^foo]'\n\n```",
    "answer": "grep --help | grep invert\n\ngrep -v is your friend:\n\n-v, --invert-match        select non-matching lines\n\nAlso check out the related -L (the complement of -l).\n\n-L, --files-without-match only print FILE names containing no match"
  },
  {
    "question": "How can I exclude directories from grep -R? I want to traverse all subdirectories, except the node_modules directory.",
    "answer": "find /dir \\( -name foo -prune \\) -o \\( -name bar -prune \\) -o -name \"*.sh\" -print\nfind /dir \\( -name node_modules -prune \\) -o -name \"*.sh\" -exec grep --color -Hn \"your text to find\" {} 2>/dev/null \\;\ngrep -R --exclude-dir=node_modules 'some pattern' /path/to/search\n\nSOLUTION 1 (combine find and grep)\nThe purpose of this solution is not to deal with grep performance but to show a portable solution : should also work with busybox or GNU version older than 2.5.\nUse find, for excluding directories foo and bar :\n\nThen combine find and the non-recursive use of grep, as a portable solution :\n\nSOLUTION 2 (recursive use of grep, using the -R and --exclude-dir option of grep):\nYou know this solution already (recursive search), but I add it since it's the most recent and efficient solution. Note this is a less portable solution but more human-readable.\n\nTo exclude multiple directories, use --exclude-dir as:\n--exclude-dir={node_modules,dir1,dir2,dir3}\nNote the \"-R\" option for a deference recursive file reading, i.e. following ALL sym links, unlike \"-r\".\nSOLUTION 3 (Ag)\nIf you frequently search through code, Ag (The Silver Searcher) is a much faster alternative to grep, that's customized for searching code. For instance, it automatically ignores files and directories listed in .gitignore, so you don't have to keep passing the same cumbersome exclude options to grep or find."
  },
  {
    "question": "How can I grep Git commits for a certain word? In a Git code repository I want to list all commits that contain a certain word. I tried this\n\n```\ngit log -p | grep --context=4 \"word\"\n\n```\n\nbut it does not necessarily give me back the filename (unless it's less that five lines away from the word I searched for. I also tried\n\n```\ngit grep \"word\"\n\n```\n\nbut it gives me only present files and not the history.\nHow do I search the entire history, so I can follow changes on a particular word? I intend to search my codebase for occurrences of word to track down changes (search in files history).",
    "answer": "git log --grep=word\ngit log -Sword\ngit log -Gword\n+    return frotz(nitfol, two->ptr, 1, 0);\n...\n-    hit = frotz(nitfol, mf2.ptr, 1, 0);\ngit log -G\"searchTerm\" --patch\ngit log 3b5ab0f2a1^.. -G\"searchTerm\" --patch | grep searchTerm\n\nIf you want to find all commits where the commit message contains a given word, use\n\nIf you want to find all commits where \"word\" was added or removed in the file contents (to be more exact: where the number of occurrences of \"word\" changed), i.e., search the commit contents, use a so-called 'pickaxe' search with\n\nIn modern Git there is also\n\nto look for differences whose added or removed line matches \"word\" (also commit contents).\nA few things to note:\n\n-G by default accepts a regex, while -S accepts a string, but it can be modified to accept regexes using the --pickaxe-regex.\n-S finds commits where the number of occurrences of \"word\" changed, while -G finds commits where \"word\" appears in the diff.\nThis means that -S<regex> --pickaxe-regex and -G<regex> do not do exactly the same thing.\n\nThe git diff documentation has a nice explanation of the difference:\n\nTo illustrate the difference between -S<regex> --pickaxe-regex and -G<regex>, consider a commit with the following diff in the same file:\n\nWhile git log -G\"frotz\\(nitfol\" will show this commit, git log -S\"frotz\\(nitfol\" --pickaxe-regex will not (because the number of occurrences of that string did not change).\n\nThis will show the commits containing the search terms, but if you want to see the actual changes in those commits instead you can use --patch:\n\nThis can then be piped to grep to isolate the output just to display commit diff lines with that search term.  A common use-case is to display diff lines with that search term in commits since and including a given commit - 3b5ab0f2a1 in this example - like so:"
  },
  {
    "question": "How do I find files that do not contain a given string pattern? How do I find out the files in the current directory which do not contain the word foo (using grep)?",
    "answer": "find .  -not  -ipath '.*svn*' -exec  grep  -H -E -o -c  \"foo\"  {} \\; | grep 0\n\nThe following command gives me all the files that do not contain the pattern foo:"
  },
  {
    "question": "Delete all local Git branches I follow a development process where I create a new local branch for every new feature or story card. When finished I merge the branch into master and then push.\nWhat tends to happen over time due to a combination of laziness or forgetfulness, is that I end up with a large list of local branches, some of which (such as spikes) may not have been merged.\nI know how to list all my local branches and I know how to remove a single branch but I was wondering if there was a Git command that allows me to delete all my local branches?\nBelow is the output of the git branch --merged command.\n\n```\ncd ~/projects/application\ngit branch --merged\n\n```\n\nOutput:\n\n```\n  STORY-123-Short-Description\n  STORY-456-Another-Description\n  STORY-789-Blah-Blah\n* master\n\n```\n\nAll attempts to delete branches listed with grep -v \\* (as per the answers below) result in errors:\n\n```\nerror: branch 'STORY-123-Short-Description' not found.\nerror: branch 'STORY-456-Another-Description' not found.\nerror: branch 'STORY-789-Blah-Blah' not found.\n\n```\n\nI'm using:\n\nGit 1.7.4.1\nUbuntu 10.04 (Lucid Lynx)\nGNU Bash, version 4.1.5(1)-release\nGNU grep 2.5.4",
    "answer": "git branch -D `git branch --merged | grep -v \\* | xargs`\ngit branch --merged | grep -v \\* | xargs git branch -D\ngit for-each-ref --format '%(refname:short)' refs/heads | grep -v \"master\\|main\" | xargs git branch -D\nmkdir br\ncd br; git init\nInitialized empty Git repository in /Users/ebg/test/br/.git/\ntouch README; git add README; git commit -m 'First commit'\n[master (root-commit) 1d738b5] First commit\nfiles changed, 0 insertions(+), 0 deletions(-)\ncreate mode 100644 README\ngit branch Story-123-a\ngit branch Story-123-b\ngit branch Story-123-c\ngit branch --merged\nStory-123-a\nStory-123-b\nStory-123-c\n* master\ngit branch --merged | grep -v \\* | xargs\nStory-123-a Story-123-b Story-123-c\ngit branch --merged | grep -v \\* | xargs git branch -D\nDeleted branch Story-123-a (was 1d738b5).\nDeleted branch Story-123-b (was 1d738b5).\nDeleted branch Story-123-c (was 1d738b5).\n\nThe 'git branch -d' subcommand can delete more than one branch.  So, simplifying @sblom's answer but adding a critical xargs:\n\nor, further simplified to:\n\nImportantly, as noted by @AndrewC, using git branch for scripting is discouraged.  To avoid it use something like:\n\nCaution warranted on deletes!"
  },
  {
    "question": "How can I use grep to find a word inside a folder? In Windows, I would have done a search for finding a word inside a folder. Similarly, I want to know if a specific word occurs inside a directory containing many sub-directories and files. My searches for grep syntax shows I must specify the filename, i.e. grep string filename.\nNow, I do not know the filename, so what do I do?\nA friend suggested to do grep -nr string, but I don't know what this means and I got no results with it (there is no response until I issue a Ctrl + C).",
    "answer": "grep -nr 'yourString*' .\n-n            Show relative line number in the file\n'yourString*' String for search, followed by a wildcard character\n-r            Recursively search subdirectories listed\n.             Directory for search (current directory)\n\nThe dot at the end searches the current directory. Meaning for each parameter:\n\ngrep -nr 'MobileAppSer*' .  (Would find MobileAppServlet.java or MobileAppServlet.class or MobileAppServlet.txt; 'MobileAppASer*.*' is another way to do the same thing.)\nTo check more parameters use man grep command."
  },
  {
    "question": "Colorized grep -- viewing the entire file with highlighted matches I find grep's --color=always flag to be tremendously useful. However, grep only prints lines with matches (unless you ask for context lines). Given that each line it prints has a match, the highlighting doesn't add as much capability as it could.\nI'd really like to cat a file and see the entire file with the pattern matches highlighted.\nIs there some way I can tell grep to print every line being read regardless of whether there's a match? I know I could write a script to run grep on every line of a file, but I was curious whether this was possible with standard grep.",
    "answer": "grep --color 'pattern\\|$' file\ngrep --color -E 'pattern|$' file\negrep --color 'pattern|$' file\ngrep --color=always 'pattern\\|$' file | less -r\ngrep --color=always -E 'pattern|$' file | less -r\negrep --color=always 'pattern|$' file | less -r\n\nHere are some ways to do it:\n\nThe | symbol is the OR operator. Either escape it using \\ or tell grep that the search text has to be interpreted as regular expressions by adding -E or using the egrep command instead of grep.\nThe search text \"pattern|$\" is actually a trick, it will match lines that have pattern OR lines that have an end. Because all lines have an end, all lines are matched, but the end of a line isn't actually any characters, so it won't be colored.\nTo also pass the colored parts through pipes, e.g. towards less, provide the always parameter to --color:"
  },
  {
    "question": "How can I exclude one word with grep? I need something like:\n\n```\ngrep ^\"unwanted_word\"XXXXXXXX\n\n```",
    "answer": "grep -v \"unwanted_word\" file | grep XXXXXXXX\ngrep -v 'unwanted_word' file\n\nYou can do it using -v (for --invert-match) option of grep as:\n\ngrep -v \"unwanted_word\" file will filter the lines that have the unwanted_word and grep XXXXXXXX will list only lines with pattern XXXXXXXX.\nEDIT:\nFrom your comment it looks like you want to list all lines without the unwanted_word. In that case all you need is:"
  },
  {
    "question": "How can I grep for a string that begins with a dash/hyphen? I want to grep for the string that starts with a dash/hyphen, like -X, in a file, but it's confusing this as a command line argument. \nI've tried:\n\n```\ngrep \"-X\"\ngrep \\-X\ngrep '-X'\n\n```",
    "answer": "Use:\ngrep -- -X\nDocumentation\nRelated: What does a bare double dash mean? (thanks to nutty about natty)."
  },
  {
    "question": "Capturing Groups From a Grep RegEx I've got this script in sh (macOS 10.6) to look through an array of files:\n\n```\nfiles=\"*.jpg\"\nfor f in $files\n    do\n        echo $f | grep -oEi '[0-9]+_([a-z]+)_[0-9a-z]*'\n        name=$?\n        echo $name\n    done\n\n```\n\nSo far $name merely holds 0, 1 or 2, depending on if grep found that the filename matched the matter provided. What I'd like is to capture what's inside the parens ([a-z]+) and store that to a variable.\nI'd like to use grep only, if possible. If not, please no Python or Perl, etc. sed or something like it \u2013\u00a0I would like to attack this from the *nix purist angle.",
    "answer": "files=\"*.jpg\"\nregex=\"[0-9]+_([a-z]+)_[0-9a-z]*\" # put the regex in a variable because some patterns won't work if included literally\nfor f in $files    # unquoted in order to allow the glob to expand\ndo\nif [[ $f =~ $regex ]]\nthen\nname=\"${BASH_REMATCH[1]}\"\necho \"${name}.jpg\"    # concatenate strings\nname=\"${name}.jpg\"    # same thing stored in a variable\nelse\necho \"$f doesn't match\" >&2 # this could get noisy if there are a lot of non-matching files\nfi\ndone\n123_abc_d4e5\nxyz123_abc_d4e5\n123_abc_d4e5.xyz\nxyz123_abc_d4e5.xyz\n^[0-9]+_([a-z]+)_[0-9a-z]*\n^[0-9]+_([a-z]+)_[0-9a-z]*$\nname=$(echo \"$f\" | grep -Po '(?i)[0-9]+_\\K[a-z]+(?=_[0-9a-z]*)').jpg\n\nIf you're using Bash, you don't even have to use grep:\n\nIt's better to put the regex in a variable. Some patterns won't work if included literally.\nThis uses  =~ which is Bash's regex match operator. The results of the match are saved to an array called $BASH_REMATCH. The first capture group is stored in index 1, the second (if any) in index 2, etc. Index zero is the full match.\n\nside note #1 regarding regex anchors:\nYou should be aware that without anchors, this regex (and the one using grep) will match any of the following examples and more, which may not be what you're looking for:\n\nTo eliminate the second and fourth examples, make your regex like this:\n\nwhich says the string must start with one or more digits. The carat represents the beginning of the string. If you add a dollar sign at the end of the regex, like this:\n\nthen the third example will also be eliminated since the dot is not among the characters in the regex and the dollar sign represents the end of the string. Note that the fourth example fails this match as well.\nside note #2 regarding grep and the \\K operator:\nIf you have GNU grep (around 2.5 or later, I think, when the \\K operator was added):\n\nThe \\K operator (variable-length look-behind) causes the preceding pattern to match, but doesn't include the match in the result. The fixed-length equivalent is (?<=) - the pattern would be included before the closing parenthesis. You must use \\K if quantifiers may match strings of different lengths (e.g. +, *, {2,4}).\nThe (?=) operator matches fixed or variable-length patterns and is called \"look-ahead\". It also does not include the matched string in the result.\nIn order to make the match case-insensitive, the (?i) operator is used. It affects the patterns that follow it so its position is significant.\nThe regex might need to be adjusted depending on whether there are other characters in the filename. You'll note that in this case, I show an example of concatenating a string at the same time that the substring is captured."
  },
  {
    "question": "How can I make grep print the lines below and above each matching line? I want to search each line for the word FAILED, then print the line above and below each matching line, as well as the matching line.\n\nInput:\n\n```\nid : 15\nStatus : SUCCESS\nMessage : no problem\n\nid : 15\nStatus : FAILED\nMessage : connection error\n\n```\n\nDesired output for grep 'FAILED':\n\n```\nid : 15\nStatus : FAILED\nMessage : connection error\n\n```",
    "answer": "grep's -A 1 option will give you one line after; -B 1 will give you one line before; and -C 1 combines both to give you one line both before and after, -1 does the same."
  },
  {
    "question": "Get line number while using grep I am using grep recursive to search files for a string, and all the matched files and the lines containing that string are printed on the terminal. But is it possible to get the line numbers of those lines too?\nExample: presently, I get /var/www/file.php: $options = \"this.target\", but I am trying to get /var/www/file.php: 1142 $options = \"this.target\";, well where 1142 would be the line number containing that string.\nThe syntax I am using to grep recursively is sudo grep -r 'pattern' '/var/www/file.php'\nHow do we get results for not equal to a pattern? Like all the files, but not the ones having a certain string.",
    "answer": "grep -n SEARCHTERM file1 file2 ..."
  },
  {
    "question": "How do I grep for all non-ASCII characters? I have several very large XML files and I'm trying to find the lines that contain non-ASCII characters. I've tried the following:\n\n```\ngrep -e \"[\\x{00FF}-\\x{FFFF}]\" file.xml\n\n```\n\nBut this returns every line in the file, regardless of whether the line contains a character in the range specified.\nDo I have the syntax wrong or am I doing something else wrong?  I've also tried:\n\n```\negrep \"[\\x{00FF}-\\x{FFFF}]\" file.xml \n\n```\n\n(with both single and double quotes surrounding the pattern).",
    "answer": "LC_ALL=C  grep --color='auto' -P -n \"[\\x80-\\xFF]\" file.xml\nLC_ALL=C  grep --color='auto' -P -n \"[^\\x00-\\x7F]\" file.xml\n\nYou can use the command:\n\nThis will give you the line number, and will highlight non-ascii chars in red.\nIn  some systems, depending on your settings, the above will not work, so you can grep by the inverse\n\nNote also, that the important bit is the -P flag which equates to --perl-regexp: so it will interpret your pattern as a Perl regular expression. It also says that\n\nthis is highly experimental and grep -P may warn of unimplemented\nfeatures."
  },
  {
    "question": "How can I format my grep output to show line numbers at the end of the line, and also the hit count? I'm using grep to match string in a file. Here is an example file:\n\n```\nexample one,\nexample two null,\nexample three,\nexample four null,\n\n```\n\ngrep -i null myfile.txt returns \n\n```\nexample two null,\nexample four null,\n\n```\n\nHow can I return matched lines together with their line numbers like this:\n\n```\n  example two null, - Line number : 2\n  example four null, - Line number : 4\n  Total null count : 2\n\n```\n\nI know -c returns total matched lines, but I don't how to format it properly to add total null count in front, and I don't know how to add the line numbers.\nWhat can I do?",
    "answer": "grep -in null myfile.txt\n\n2:example two null,\n4:example four null,\ngrep -in null myfile.txt | awk -F: '{print $2\" - Line number : \"$1}'\n\nexample two null, - Line number : 2\nexample four null, - Line number : 4\necho \"Total null count :\" $(grep -ic null myfile.txt)\n\nTotal null count : 2\n\n-n returns line number.\n-i is for ignore-case. Only to be used if case matching is not necessary\n\nCombine with awk to print out the line number after the match:\n\nUse command substitution to print out the total null count:"
  },
  {
    "question": "How to check if a file contains a specific string using Bash I want to check if a file contains a specific string or not in bash. I used this script, but it doesn't work:\n\n```\n if [[ 'grep 'SomeString' $File' ]];then\n   # Some Actions\n fi\n\n```\n\nWhat's wrong in my code?",
    "answer": "if grep -q SomeString \"$File\"; then\nSome Actions # SomeString was found\nfi\necho hello | grep hi ; echo $?\necho hello | grep he ; echo $?\nhello\necho hello | grep -q he ; echo $?\nif /bin/true; then echo that is true; fi\nthat is true\nif /bin/false; then echo that is true; fi\n\nYou don't need [[ ]] here. Just run the command directly. Add -q option when you don't need the string displayed when it was found.\nThe grep command returns 0 or 1 in the exit code depending on\nthe result of search. 0 if something was found; 1 otherwise.\n\nYou can specify commands as an condition of if. If the command returns 0 in its exitcode that means that the condition is true; otherwise false.\n\nAs you can see you run here the programs directly. No additional [] or [[]]."
  },
  {
    "question": "Fast way of finding lines in one file that are not in another? I have two large files (sets of filenames). Roughly 30.000 lines in each file. I am trying to find a fast way of finding lines in file1 that are not present in file2.\nFor example, if this is file1:\n\n```\nline1\nline2\nline3\n\n```\n\nAnd this is file2:\n\n```\nline1\nline4\nline5\n\n```\n\nThen my result/output should be:\n\n```\nline2\nline3\n\n```\n\nThis works:\ngrep -v -f file2 file1\nBut it is very, very slow when used on my large files.\nI suspect there is a good way to do this using diff, but the output should be just the lines, nothing else, and I cannot seem to find a switch for that.\nCan anyone help me find a fast way of doing this, using bash and basic Linux binaries?\nEDIT: To follow up on my own question, this is the best way I have found so far using diff:\n\n```\ndiff file2 file1 | grep '^>' | sed 's/^>\\ //'\n\n```\n\nSurely, there must be a better way?",
    "answer": "diff --new-line-format=\"\" --unchanged-line-format=\"\"  file1 file2\ndiff --new-line-format=\"\" --unchanged-line-format=\"\" <(sort file1) <(sort file2)\ndiff --old-line-format=\"-%L\" --unchanged-line-format=\" %L\" \\\n--new-line-format=\"+%L\" file1 file2\nBEGIN { FS=\"\" }                         # preserve whitespace\n(NR==FNR) { ll1[FNR]=$0; nl1=FNR; }     # file1, index by lineno\n(NR!=FNR) { ss2[$0]++; }                # file2, index by string\nEND {\nfor (ll=1; ll<=nl1; ll++) if (!(ll1[ll] in ss2)) print ll1[ll]\n}\nBEGIN { FS=\"\" }\n(NR==FNR) {  # file1, index by lineno and string\nll1[FNR]=$0; ss1[$0]=FNR; nl1=FNR;\n}\n(NR!=FNR) {  # file2\nif ($0 in ss1) { delete ll1[ss1[$0]]; delete ss1[$0]; }\n}\nEND {\nfor (ll=1; ll<=nl1; ll++) if (ll in ll1) print ll1[ll]\n}\nsplit -l 20000 --filter='gawk -f linesnotin.awk - file2' < file1\n\nYou can achieve this by controlling the formatting of the old/new/unchanged lines in GNU diff output:\n\nThe input files should be sorted for this to work. With bash (and zsh) you can sort in-place with process substitution <( ):\n\nIn the above new and unchanged lines are suppressed,  so only changed (i.e. removed lines in your case) are output. You may also use a few diff options that other solutions don't offer, such as -i to ignore case, or various whitespace options (-E, -b, -v etc) for less strict matching.\n\nExplanation\nThe options --new-line-format, --old-line-format and --unchanged-line-format let you control the way diff formats the differences, similar to  printf format specifiers. These options format new (added), old (removed) and unchanged lines respectively. Setting one to empty \"\" prevents output of that kind of line.\nIf you are familiar with unified diff format, you can partly recreate it with:\n\nThe %L specifier is the line in question, and we prefix each with \"+\" \"-\" or \" \", like diff -u\n(note that it only outputs differences, it lacks the --- +++ and @@ lines at the top of each grouped change).\nYou can also use this to do other useful things like number each line with %dn.\n\nThe diff method (along with other suggestions comm and join) only produce the expected output with sorted input, though you can use <(sort ...) to sort in place. Here's a simple awk (nawk) script (inspired by the scripts linked-to in Konsolebox's answer) which accepts arbitrarily ordered input files, and outputs the missing lines in the order they occur in file1.\n\nThis stores the entire contents of file1 line by line in a line-number indexed array ll1[], and the entire contents of file2 line by line in a line-content indexed associative array ss2[]. After both files are read, iterate over ll1 and use the in operator to determine if the line in file1 is present in file2. (This will have have different output to the diff method if there are duplicates.)\nIn the event that the files are sufficiently large that storing them both causes a memory problem, you can trade CPU for memory by storing only file1 and deleting matches along the way as file2 is read.\n\nThe above stores the entire contents of file1 in two arrays, one indexed by line number ll1[], one indexed by line content ss1[]. Then as file2 is read, each matching line is deleted from ll1[] and ss1[]. At the end the remaining lines from file1 are output, preserving the original order.\nIn this case, with the problem as stated, you can also divide and conquer using GNU split (filtering is a GNU extension), repeated runs with chunks of file1 and reading file2 completely each time:\n\nNote the use and placement of - meaning stdin on the gawk command line. This is provided by split from file1 in chunks of 20000 line per-invocation.\nFor users on non-GNU systems, there is almost certainly a GNU coreutils package you can obtain, including on OSX as part of the Apple Xcode tools which provides GNU diff, awk, though only a POSIX/BSD split rather than a GNU version."
  },
  {
    "question": "How to suppress binary file matching results in grep When using grep in Linux, the result often contains a lot of \"binary file XXX matches\", which I do not care about. How to suppress this part of the results, or how to exclude binary files in grep?",
    "answer": "grep -I -n -H\n\n-I -- process a binary file as if it did not contain matching data;\n-n -- prefix each line of output with the 1-based line number within its input file\n-H -- print the file name for each match\ngrep -InH your-word *\n\nThere are three options, that you can use. -I is to exclude binary files in grep. Other are for line numbers and file names.\n\nSo this might be a way to run grep:"
  },
  {
    "question": "Display filename before matching line How can I get grep to display the filename before the matching lines in its output?",
    "answer": "grep 'pattern' file /dev/null\ngrep -n 'pattern' file /dev/null\n\nTry this little trick to coax grep into thinking it is dealing with multiple files, so that it displays the filename:\n\nTo also get the line number:"
  },
  {
    "question": "How can I find all of the distinct file extensions in a folder hierarchy? On a Linux machine I would like to traverse a folder hierarchy and get a list of all of the distinct file extensions within it.\nWhat would be the best way to achieve this from a shell?",
    "answer": "find . -type f | perl -ne 'print $1 if m/\\.([^.\\/]+)$/' | sort -u\n\nTry this (not sure if it's the best way, but it works):\n\nIt work as following:\n\nFind all files from current folder\nPrints extension of files if any\nMake a unique sorted list"
  },
  {
    "question": "Highlight text similar to grep, but don&#39;t filter out text When using grep, it will highlight any text in a line with a match to your regular expression.\nWhat if I want this behaviour, but have grep print out all lines as well? I came up empty after a quick look through the grep man page.",
    "answer": "ack --passthru 'pattern1' file_name\ncommand_here | ack --passthru 'pattern1'\ngrep --color -E '^|pattern1|pattern2' file_name\ncommand_here | grep --color -E '^|pattern1|pattern2'\n\nUse ack. Check out its --passthru option here: ack. It has the added benefit of allowing full Perl regular expressions.\n\nYou can also do it using grep like this:\n\nThis will match all lines and highlight the patterns. The ^ matches every start of the line but won't get printed/highlighted since it's not a character.\n(Note that most of the setups will use --color by default. You may not need that flag)."
  },
  {
    "question": "Count all occurrences of a string in lots of files with grep I have a bunch of log files. I need to find out how many times a string occurs in all files.\n\n```\ngrep -c string *\n\n```\n\nreturns\n\n```\n...\nfile1:1\nfile2:0\nfile3:0\n...\n\n```\n\nUsing a pipe I was able to get only files that have one or more occurrences:\n\n```\ngrep -c string * | grep -v :0\n\n...\nfile4:5\nfile5:1\nfile6:2\n...\n\n```\n\nHow can I get only the combined count? (If it returns file4:5, file5:1, file6:2, I want to get back 8.)",
    "answer": "cat * | grep -c string"
  },
  {
    "question": "How to invert a grep expression The following grep expression successfully lists all the .exe and .html files in the current directory and sub directories.  \n\n```\nls -R |grep -E .*[\\.exe]$\\|.*[\\.html]$  \n\n```\n\nHow do I invert this result to list those that aren't a .html or .exe instead. (That is, !=.)",
    "answer": "ls -R |grep -v -E .*[\\.exe]$\\|.*[\\.html]$\n\nUse command-line option -v or --invert-match,"
  },
  {
    "question": "Grep regex NOT containing a string I am passing a list of regex patterns to grep to check against a syslog file. They are usually matching an IP address and log entry;\n\n```\ngrep \"1\\.2\\.3\\.4.*Has exploded\" syslog.log\n\n```\n\nIt's just a list of patterns like the \"1\\.2\\.3\\.4.*Has exploded\" part I am passing, in a loop, so I can't pass \"-v\", for example.\nI am confused trying to do the inverse of the above, and not match lines with a certain IP address and error so \"!1.2.3.4.*Has exploded\" will match syslog lines for anything other than 1.2.3.4 telling me it has exploded. I must be able to include an IP address to not match.\nI have seen various similar posts on Stack Overflow. However, they use regex patterns that I can't seem to get to work with grep. What would be a working example for grep?\nThis is happening in a script like this;\n\n```\npatterns[1]=\"1\\.2\\.3\\.4.*Has exploded\"\npatterns[2]=\"5\\.6\\.7\\.8.*Has died\"\npatterns[3]=\"\\!9\\.10\\.11\\.12.*Has exploded\"\n\nfor i in {1..3}\ndo\n  grep \"${patterns[$i]}\" logfile.log\ndone\n\n```",
    "answer": "grep \"${PATT}\" file | grep -v \"${NOTPATT}\"\n\ngrep matches, grep -v does the inverse. If you need to \"match A but not B\" you usually use pipes:"
  },
  {
    "question": "How to get the process ID to kill a nohup process? I'm running a nohup process on the server. When I try to kill it my putty console closes instead.\nthis is how I try to find the process ID:\n\n```\nps -ef |grep nohup \n\n```\n\nthis is the command to kill\n\n```\n kill -9 1787 787\n\n```",
    "answer": "nohup my_command > my.log 2>&1 &\necho $! > save_pid.txt\nkill -9 `cat save_pid.txt`\nrm save_pid.txt\n\nWhen using nohup and you put the task in the background, the background operator (&) will give you the PID at the command prompt. If your plan is to manually manage the process, you can save that PID and use it later to kill the process if needed, via kill PID or kill -9 PID (if you need to force kill). Alternatively, you can find the PID later on by ps -ef | grep \"command name\" and locate the PID from there. Note that nohup keyword/command itself does not appear in the ps output for the command in question.\nIf you use a script, you could do something like this in the script:\n\nThis will run my_command saving all output into my.log (in a script, $! represents the PID of the last process executed). The 2 is the file descriptor for standard error (stderr) and 2>&1 tells the shell to route standard error output to the standard output (file descriptor 1). It requires &1 so that the shell knows it's a file descriptor in that context instead of just a file named 1. The 2>&1 is needed to capture any error messages that normally are written to standard error into our my.log file (which is coming from standard output). See I/O Redirection for more details on handling I/O redirection with the shell.\nIf the command sends output on a regular basis, you can check the output occasionally with tail my.log, or if you want to follow it \"live\" you can use tail -f my.log. Finally, if you need to kill the process, you can do it via:"
  },
  {
    "question": "What is makeinfo, and how do I get it? I'm trying to build GNU grep, and when I run make, I get:\n\n```\n[snip]\n/bin/bash: line 9: makeinfo: command not found\n\n```\n\nWhat is makeinfo, and how do I get it?\n(This is Ubuntu, if it makes a difference)",
    "answer": "sudo apt-get install texinfo\n\nIn (at least) Ubuntu when using bash, it tells you what package you need to install if you type in a command and its not found in your path. My terminal says you need to install 'texinfo' package."
  },
  {
    "question": "How to search contents of multiple pdf files? How could I search the contents of PDF files in a directory/subdirectory? I am looking for some command line tools. It seems that grep can't search PDF files.",
    "answer": "find /path -name '*.pdf' -exec sh -c 'pdftotext \"{}\" - | grep --with-filename --label=\"{}\" --color \"your pattern\"' \\;\n\nYour distribution should provide a utility called pdftotext:\n\nThe \"-\" is necessary to have pdftotext output to stdout, not to files.\nThe --with-filename and --label= options will put the file name in the output of grep.\nThe optional --color flag is nice and tells grep to output using colors on the terminal.\n(In Ubuntu, pdftotext is provided by the package xpdf-utils or poppler-utils.)\nThis method, using pdftotext and grep, has an advantage over pdfgrep if you want to use features of GNU grep that pdfgrep doesn't support. Note: pdfgrep-1.3.x supports -C option for printing line of context."
  },
  {
    "question": "How can I have grep not print out &#39;No such file or directory&#39; errors? I'm grepping through a large pile of code managed by git, and whenever I do a grep, I see piles and piles of messages of the form: \n\n```\n> grep pattern * -R -n\nwhatever/.git/svn: No such file or directory\n\n```\n\nIs there any way I can make those lines go away?",
    "answer": "grep pattern * -s -R -n\n\nYou can use the -s or --no-messages flag to suppress errors.\n\n-s, --no-messages         suppress error messages"
  },
  {
    "question": "Match two strings in one line with grep I am trying to use grep to match lines that contain two different strings. I have tried the following but this matches lines that contain either string1 or string2 which not what I want.\n\n```\ngrep 'string1\\|string2' filename\n\n```\n\nSo how do I match with grep only the lines that contain both strings?",
    "answer": "grep 'string1' filename | grep 'string2'\ngrep 'string1.*string2\\|string2.*string1' filename\n\nYou can use\n\nThis searches for string1 followed by string 2 on the same line, or string2 followed by string1 on the same line; it does not answer the question:"
  },
  {
    "question": "grep without showing path/file:line How do you grep and only return the matching line? i.e. The path/filename is omitted from the results.\nIn this case I want to look in all .bar files in the current directory, searching for the term FOO\n\n```\nfind . -name '*.bar' -exec grep -Hn FOO {} \\;\n\n```",
    "answer": "grep -h FOO /your/path/*.bar\n\nNo need to find. If you are just looking for a pattern within a specific directory, this should suffice:\n\nWhere -h is the parameter to hide the filename, as from man grep:\n\n-h, --no-filename\nSuppress  the  prefixing of file names on output.  This is the default\nwhen  there is only one file (or only standard input) to search.\n\nNote that you were using\n\n-H, --with-filename\nPrint the file name for each match.  This is the default when there is\nmore than one file to search."
  },
  {
    "question": "Regex (grep) for multi-line search needed I'm running a grep to find any *.sql file that has the word select followed by the word customerName followed by the word from. This select statement can span many lines and can contain tabs and newlines.\nI've tried a few variations on the following:\n\n```\n$ grep -liIr --include=\"*.sql\" --exclude-dir=\"\\.svn*\" --regexp=\"select[a-zA-Z0-\n9+\\n\\r]*customerName[a-zA-Z0-9+\\n\\r]*from\"\n\n```\n\nThis, however, just runs forever. Can anyone help me with the correct syntax please?",
    "answer": "grep -Pzo \"(?s)^(\\s*)\\N*main.*?{.*?^\\1}\" *.c\n\nWithout the need to install the grep variant pcregrep, you can do a multiline search with grep.\n\nExplanation:\n-P activate perl-regexp for grep (a powerful extension of regular expressions)\n-z Treat the input as a set of lines, each terminated by a zero byte (the ASCII NUL character) instead of a newline. That is, grep knows where the ends of the lines are, but sees the input as one big line.  Beware this also adds a trailing NUL char if used with -o, see comments.\n-o print only matching. Because we're using -z, the whole file is like a single big line, so if there is a match, the entire file would be printed; this way it won't do that.\nIn regexp:\n(?s) activate PCRE_DOTALL, which means that . finds any character or newline\n\\N find anything except newline, even with PCRE_DOTALL activated\n.*? find . in non-greedy mode, that is, stops as soon as possible.\n^ find start of line\n\\1 backreference to the first group (\\s*). This is a try to find the same indentation of method.\nAs you can imagine, this search prints the main method in a C (*.c) source file."
  },
  {
    "question": "How to find patterns across multiple lines using grep? I want to find files that have \"abc\" AND \"efg\" in that order, and those two strings are on different lines in that file. Eg: a file with content:\n\n```\nblah blah..\nblah blah..\nblah abc blah\nblah blah..\nblah blah..\nblah blah..\nblah efg blah blah\nblah blah..\nblah blah..\n\n```\n\nShould be matched.",
    "answer": "pcregrep -M 'abc.*(\\n|.)*efg' test.txt\n% sudo port install pcre2\n% brew install pcre\n% brew install pcre2\nsudo apt install pcre2-utils # PCRE2\nsudo apt install pcregrep    # Older PCRE\n\nGrep is an awkward tool for this operation.\npcregrep which is found in most of the modern Linux systems can be used as\n\nwhere -M, --multiline  allow patterns to match more than one line\nThere is a newer pcre2grep also. Both are provided by the PCRE project.\npcre2grep is available for Mac OS X via Mac Ports as part of port pcre2:\n\nand via Homebrew as:\n\nor for pcre2\n\npcre2grep is also available on Linux (Ubuntu 18.04+)"
  },
  {
    "question": "What are good grep tools for Windows? Any recommendations on grep tools for Windows? Ideally ones that could leverage 64-bit OS.\nI'm aware of Cygwin, of course, and have also found PowerGREP, but I'm wondering if there are any hidden gems out there?",
    "answer": "Based on recommendations in the comments, I've started using grepWin and it's fantastic and free.\n\n(I'm still a fan of PowerGREP, but I don't use it anymore.)\nI know you already mentioned it, but PowerGREP is awesome.\nSome of my favorite features are:\n\nRight-click on a folder to run PowerGREP on it\nUse regular expressions or literal text\nSpecify wildcards for files to include & exclude\nSearch & replace\nPreview mode is nice because you can make sure you're replacing what you intend to.\n\nNow I realize that the other grep tools can do all of the above. It's just that PowerGREP packages all of the functionality into a very easy-to-use GUI.\nFrom the same wonderful folks who brought you RegexBuddy and who I have no affiliation with beyond loving their stuff. (It should be noted that RegexBuddy includes a basic version of grep (for Windows) itself and it costs a lot less than PowerGREP.)\n\nAdditional solutions\nExisting Windows commands\n\nFINDSTR\nSelect-String in PowerShell\n\nLinux command implementations on Windows\n\nCygwin\nCash\n\nGrep tools with a graphical interface\n\nAstroGrep\nBareGrep\nGrepWin\n\nAdditional Grep tools\n\ndnGrep"
  },
  {
    "question": "Using grep to search for a string that has a dot in it I am trying to search for a string 0.49 (with dot) using the command\n\n```\ngrep -r \"0.49\" *\n\n```\n\nBut what happening is that I am also getting unwanted results which contains the string such as 0449, 0949 etc,. The thing is linux considering dot(.) as any character and bringing out all the results. But I want to get the result only for \"0.49\".",
    "answer": "grep -r \"0\\.49\" *\ngrep -r 0\\\\.49 *\ngrep -Fr 0.49 *\n\ngrep uses regexes; . means \"any character\" in a regex.  If you want a literal string, use grep -F, fgrep, or escape the . to \\..\nDon't forget to wrap your string in double quotes. Or else you should use \\\\.\nSo, your command would need to be:\n\nor\n\nor"
  },
  {
    "question": "Exploitable PHP functions I'm trying to build a list of functions that can be used for arbitrary code execution. The purpose isn't to list functions that should be blacklisted or otherwise disallowed. Rather, I'd like to have a grep-able list of red-flag keywords handy when searching a compromised server for back-doors.\nThe idea is that if you want to build a multi-purpose malicious PHP script -- such as a \"web shell\" script like c99 or r57 -- you're going to have to use one or more of a relatively small set of functions somewhere in the file in order to allow the user to execute arbitrary code. Searching for those those functions helps you more quickly narrow down a haystack of tens-of-thousands of PHP files to a relatively small set of scripts that require closer examination.\nClearly, for example, any of the following would be considered malicious (or terrible coding):\n\n```\n<? eval($_GET['cmd']); ?>\n\n<? system($_GET['cmd']); ?>\n\n<? preg_replace('/.*/e',$_POST['code']); ?>\n\n```\n\nand so forth. \nSearching through a compromised website the other day, I didn't notice a piece of malicious code because I didn't realize preg_replace could be made dangerous by the use of the /e flag (which, seriously? Why is that even there?). Are there any others that I missed?\nHere's my list so far:\nShell Execute \n\nsystem\nexec\npopen\nbacktick operator\npcntl_exec\n\nPHP Execute\n\neval\npreg_replace (with /e modifier)\ncreate_function\ninclude[_once] / require[_once] (see mario's answer for exploit details)\n\nIt might also be useful to have a list of functions that are capable of modifying files, but I imagine 99% of the time exploit code will contain at least one of the functions above. But if you have a list of all the functions capable of editing or outputting files, post it and I'll include it here. (And I'm not counting mysql_execute, since that's part of another class of exploit.)",
    "answer": "exec           - Returns last line of commands output\npassthru       - Passes commands output directly to the browser\nsystem         - Passes commands output directly to the browser and returns last line\nshell_exec     - Returns commands output\n`` (backticks) - Same as shell_exec()\npopen          - Opens read or write pipe to process of a command\nproc_open      - Similar to popen() but greater degree of control\npcntl_exec     - Executes a program\neval()\nassert()  - identical to eval()\npreg_replace('/.*/e',...) - /e does an eval() on the match\ncreate_function()\ninclude()\ninclude_once()\nrequire()\nrequire_once()\n_GET['func_name']($_GET['argument']);\nfunc = new ReflectionFunction($_GET['func_name']); $func->invoke(); or $func->invokeArgs(array());\nFunction                     => Position of callback arguments\n'ob_start'                   =>  0,\n'array_diff_uassoc'          => -1,\n'array_diff_ukey'            => -1,\n'array_filter'               =>  1,\n'array_intersect_uassoc'     => -1,\n'array_intersect_ukey'       => -1,\n'array_map'                  =>  0,\n'array_reduce'               =>  1,\n'array_udiff_assoc'          => -1,\n'array_udiff_uassoc'         => array(-1, -2),\n'array_udiff'                => -1,\n'array_uintersect_assoc'     => -1,\n'array_uintersect_uassoc'    => array(-1, -2),\n'array_uintersect'           => -1,\n'array_walk_recursive'       =>  1,\n'array_walk'                 =>  1,\n'assert_options'             =>  1,\n'uasort'                     =>  1,\n'uksort'                     =>  1,\n'usort'                      =>  1,\n'preg_replace_callback'      =>  1,\n'spl_autoload_register'      =>  0,\n'iterator_apply'             =>  1,\n'call_user_func'             =>  0,\n'call_user_func_array'       =>  0,\n'register_shutdown_function' =>  0,\n'register_tick_function'     =>  0,\n'set_error_handler'          =>  0,\n'set_exception_handler'      =>  0,\n'session_set_save_handler'   => array(0, 1, 2, 3, 4, 5),\n'sqlite_create_aggregate'    => array(2, 3),\n'sqlite_create_function'     =>  2,\nphpinfo\nposix_mkfifo\nposix_getlogin\nposix_ttyname\ngetenv\nget_current_user\nproc_get_status\nget_cfg_var\ndisk_free_space\ndisk_total_space\ndiskfreespace\ngetcwd\ngetlastmo\ngetmygid\ngetmyinode\ngetmypid\ngetmyuid\nextract - Opens the door for register_globals attacks (see study in scarlet).\nparse_str -  works like extract if only one argument is given.\nputenv\nini_set\nmail - has CRLF injection in the 3rd parameter, opens the door for spam.\nheader - on old systems CRLF injection could be used for xss or other purposes, now it is still a problem if they do a header(\"location: ...\"); and they do not die();. The script keeps executing after a call to header(), and will still print output normally. This is nasty if you are trying to protect an administrative area.\nproc_nice\nproc_terminate\nproc_close\npfsockopen\nfsockopen\napache_child_terminate\nposix_kill\nposix_mkfifo\nposix_setpgid\nposix_setsid\nposix_setuid\n// open filesystem handler\nfopen\ntmpfile\nbzopen\ngzopen\nSplFileObject->__construct\n// write to filesystem (partially in combination with reading)\nchgrp\nchmod\nchown\ncopy\nfile_put_contents\nlchgrp\nlchown\nlink\nmkdir\nmove_uploaded_file\nrename\nrmdir\nsymlink\ntempnam\ntouch\nunlink\nimagepng   - 2nd parameter is a path.\nimagewbmp  - 2nd parameter is a path.\nimage2wbmp - 2nd parameter is a path.\nimagejpeg  - 2nd parameter is a path.\nimagexbm   - 2nd parameter is a path.\nimagegif   - 2nd parameter is a path.\nimagegd    - 2nd parameter is a path.\nimagegd2   - 2nd parameter is a path.\niptcembed\nftp_get\nftp_nb_get\n// read from filesystem\nfile_exists\nfile_get_contents\nfile\nfileatime\nfilectime\nfilegroup\nfileinode\nfilemtime\nfileowner\nfileperms\nfilesize\nfiletype\nglob\nis_dir\nis_executable\nis_file\nis_link\nis_readable\nis_uploaded_file\nis_writable\nis_writeable\nlinkinfo\nlstat\nparse_ini_file\npathinfo\nreadfile\nreadlink\nrealpath\nstat\ngzfile\nreadgzfile\ngetimagesize\nimagecreatefromgif\nimagecreatefromjpeg\nimagecreatefrompng\nimagecreatefromwbmp\nimagecreatefromxbm\nimagecreatefromxpm\nftp_put\nftp_nb_put\nexif_read_data\nread_exif_data\nexif_thumbnail\nexif_imagetype\nhash_file\nhash_hmac_file\nhash_update_file\nmd5_file\nsha1_file\nhighlight_file\nshow_source\nphp_strip_whitespace\nget_meta_tags\n\nTo build this list I used 2 sources.  A Study In Scarlet and RATS.   I have also added some of my own to the mix and people on this thread have helped out.\nEdit: After posting this list I contacted the founder of RIPS and as of now this tools searches PHP code for the use of every function in this list.\nMost of these function calls are classified as Sinks. When a tainted variable (like $_REQUEST) is passed to a sink function, then you have a vulnerability.  Programs like RATS and RIPS use grep like functionality to identify all sinks in an application.  This means that programmers should take extra care when using these functions,  but if they where all banned then you wouldn't be able to get much done.\n\"With great power comes great responsibility.\"\n--Stan Lee\nCommand Execution\n\nPHP Code Execution\nApart from eval there are other ways to execute PHP code: include/require can be used for remote code execution in the form of Local File Include and Remote File Include vulnerabilities.\n\nList of functions which accept callbacks\nThese functions accept a string parameter which could be used to call a function of the attacker's choice.  Depending on the function the attacker may or may not have the ability to pass a parameter.  In that case an Information Disclosure function like phpinfo() could be used.\n\nInformation Disclosure\nMost of these function calls are not sinks.   But rather it maybe a vulnerability if any of the data returned is viewable to an attacker.  If an attacker can see phpinfo() it is definitely a vulnerability.\n\nOther\n\nFilesystem Functions\nAccording to RATS all filesystem functions in php are nasty. Some of these don't seem very useful to the attacker. Others are more useful than you might think. For instance if allow_url_fopen=On then a url can be used as a file path, so a call to copy($_GET['s'], $_GET['d']); can be used to upload a PHP script anywhere on the system.\nAlso if a site is vulnerable to a request send via GET everyone of those file system functions can be abused to channel and attack to another host through your server."
  },
  {
    "question": "How to grep a string in a directory and all its subdirectories? How to grep a string or a text in a directory and all its subdirectories'files in  LINUX ??",
    "answer": "grep -R 'string' dir/\nfind dir/ -type f -exec grep -H 'string' {} +\n\nIf your grep supports -R, do:\n\nIf not, then use find:"
  },
  {
    "question": "PowerShell equivalent to grep -f I'm looking for the PowerShell equivalent to grep --file=filename. If you don't know grep, filename is a text file where each line has a regular expression pattern you want to match.\nMaybe I'm missing something obvious, but Select-String doesn't seem to have this option.",
    "answer": "Get-Content .\\doc.txt | Select-String -Pattern (Get-Content .\\regex.txt)\n\nThe -Pattern parameter in Select-String supports an array of patterns. So the one you're looking for is:\n\nThis searches through the textfile doc.txt by using every regex(one per line) in regex.txt"
  },
  {
    "question": "Is there a Pattern Matching Utility like GREP in Windows? Is there a similar utility to grep available from the Windows Command Prompt, or is there a third party tool for it?",
    "answer": "I also found one more way of utilizing GREP like functionality in Windows 7 and above without any extra application to install and on older systems you can use install Powershell.\nIn Powershell, User can use Where-Object it has quite comprehensive set of feature that provides all the functionality of GREP plus more.\nHope It helps."
  },
  {
    "question": "grep output to show only matching file What is the option for grep that will allow me only to print the matching file and not the line within a file that matches the criteria?",
    "answer": "grep -l\n\n(That's a lowercase L)"
  },
  {
    "question": "How can I exclude all &quot;permission denied&quot; messages from &quot;find&quot;? I need to hide all permission denied messages from:\n\n```\nfind . > files_and_folders\n\n```\n\nI am experimenting when such message arises. I need to gather all folders and files, to which it does not arise. \nIs it possible to direct the permission levels to the files_and_folders file? \nHow can I hide the errors at the same time?",
    "answer": "find . > files_and_folders 2> >(grep -v 'Permission denied$' >&2)\nfind . 2>&1 >files_and_folders | grep -v 'Permission denied$' >&2\nfind . 2>&1 >files_and_folders | { grep -v 'Permission denied$' >&2; [ $? -eq 1 ]; }\n{ find . 3>&2 2>&1 1>&3 | grep -v 'Permission denied$' >&3; } 3>&2 2>&1\n\nNote:\n\nThis answer probably goes deeper than the use case warrants, and find 2>/dev/null may be good enough in many situations. It may still be of interest for a cross-platform perspective and for its discussion of some advanced shell techniques in the interest of finding a solution that is as robust as possible, even though the cases guarded against may be largely hypothetical.\n\nIf your shell is bash or zsh, there's a solution that is robust while being reasonably simple, using only POSIX-compliant find features; while bash itself is not part of POSIX, most modern Unix platforms come with it, making this solution widely portable:\n\nNote:\n\nIf your system is configured to show localized error messages, prefix the find calls below with LC_ALL=C  (LC_ALL=C find ...) to ensure that English messages are reported, so that grep -v 'Permission denied$' works as intended. Invariably, however, any error messages that do get displayed will then be in English as well.\n\n>(...) is a (rarely used) output process substitution that allows redirecting output (in this case, stderr output (2>) to the stdin of the command inside >(...).\nIn addition to bash and zsh, ksh supports them as well in principle, but trying to combine them with redirection from stderr, as is done here (2> >(...)), appears to be silently ignored (in ksh 93u+).\n\ngrep -v 'Permission denied$' filters out (-v) all lines (from the find command's stderr stream) that end with the phrase Permission denied and outputs the remaining lines to stderr (>&2).\n\nNote: There's a small chance that some of grep's output may arrive after find completes, because the overall command doesn't wait for the command inside >(...) to finish. In bash, you can prevent this by appending | cat to the command.\n\nThis approach is:\n\nrobust: grep is only applied to error messages (and not to a combination of file paths and error messages, potentially leading to false positives), and error messages other than permission-denied ones are passed through, to stderr.\n\nside-effect free: find's exit code is preserved: the inability to access at least one of the filesystem items encountered results in exit code 1 (although that won't tell you whether errors other than permission-denied ones occurred (too)).\n\nPOSIX-compliant solutions:\nFully POSIX-compliant solutions either have limitations or require additional work.\nIf find's output is to be captured in a file anyway (or suppressed altogether), then the pipeline-based solution from Jonathan Leffler's answer is simple, robust, and POSIX-compliant:\n\nNote that the order of the redirections matters: 2>&1 must come first.\nCapturing stdout output in a file up front allows 2>&1 to send only error messages through the pipeline, which grep can then unambiguously operate on.\nThe only downside is that the overall exit code will be the grep command's, not find's, which in this case means: if there are no errors at all or only permission-denied errors, the exit code will be 1 (signaling failure), otherwise (errors other than permission-denied ones) 0 - which is the opposite of the intent.\nThat said, find's exit code is rarely used anyway, as it often conveys little information beyond fundamental failure such as passing a non-existent path.\nHowever, the specific case of even only some of the input paths being inaccessible due to lack of permissions is reflected in find's exit code (in both GNU and BSD find): if a permissions-denied error occurs for any of the files processed, the exit code is set to 1.\nThe following variation addresses that:\n\nNow, the exit code indicates whether any errors other than Permission denied occurred: 1 if so, 0 otherwise.\nIn other words: the exit code now reflects the true intent of the command: success (0) is reported, if no errors at all or only permission-denied errors occurred.\nThis is arguably even better than just passing find's exit code through, as in the solution at the top.\n\ngniourf_gniourf in the comments proposes a (still POSIX-compliant) generalization of this solution using sophisticated redirections, which works even with the default behavior of printing the file paths to stdout:\n\nIn short: Custom file descriptor 3 is used to temporarily swap stdout (1) and stderr (2), so that error messages alone can be piped to grep via stdout.\nWithout these redirections, both data (file paths) and error messages would be piped to grep via stdout, and grep would then not be able to distinguish between error message Permission denied and a (hypothetical) file whose name happens to end with the phrase Permission denied.\nAs in the first solution, however, the the exit code reported will be grep's, not find's, but the same fix as above can be applied.\n\nNotes on the existing answers:\n\nThere are several points to note about Michael Brux's answer, find . ! -readable -prune -o -print:\n\nIt requires GNU find; notably, it won't work on macOS. Of course, if you only ever need the command to work with GNU find, this won't be a problem for you.\n\nSome Permission denied errors may still surface: find ! -readable -prune reports such errors for the child items of directories for which the current user does have r permission, but lacks x (executable) permission. The reason is that because the directory itself is readable, -prune is not executed, and the attempt to descend into that directory then triggers the error messages. That said, the typical case is for the r permission to be missing.\n\nNote: The following point is a matter of philosophy and/or specific use case, and you may decide it is not relevant to you and that the command fits your needs well, especially if simply printing the paths is all you do:\n\nIf you conceptualize the filtering of the permission-denied error messages a separate task that you want to be able to apply to any find command, then the opposite approach of proactively preventing permission-denied errors requires introducing \"noise\" into the find command, which also introduces complexity and logical pitfalls.\nFor instance, the most up-voted comment on Michael's answer (as of this writing) attempts to show how to extend the command by including a -name filter, as follows:\nfind . ! -readable -prune -o -name '*.txt'\nThis, however, does not work as intended, because the trailing -print action is required (an explanation can be found in this answer). Such subtleties can introduce bugs.\n\nThe first solution in Jonathan Leffler's answer, find . 2>/dev/null > files_and_folders, as he himself states,  blindly silences all error messages (and the workaround is cumbersome and not fully robust, as he also explains). Pragmatically speaking, however, it is the simplest solution, as you may be content to assume that any and all errors would be permission-related.\n\nmist's answer, sudo find . > files_and_folders, is concise and pragmatic, but ill-advised for anything other than merely printing filenames, for security reasons: because you're running as the root user, \"you risk having your whole system being messed up by a bug in find or a malicious version, or an incorrect invocation which writes something unexpectedly, which could not happen if you ran this with normal privileges\" (from a comment on mist's answer by tripleee).\n\nThe 2nd solution in viraptor's answer, find . 2>&1 | grep -v 'Permission denied$' > some_file runs the risk of false positives (due to sending a mix of stdout and stderr through the pipeline), and, potentially, instead of reporting non-permission-denied errors via stderr, captures them alongside the output paths in the output file."
  },
  {
    "question": "Find a value in a list I use the following to check if item is in my_list:\n\n```\nif item in my_list:\n    print(\"Desired item is in list\")\n\n```\n\nIs \"if item in my_list:\" the most \"pythonic\" way of finding an item in a list?\nEDIT FOR REOPENING: the question has been considered duplicate, but I'm not entirely convinced: here this question is roughly \"what is the most Pythonic way to find an element in a list\". And the first answer to the question is really extensive in all Python ways to do this.\nWhereas on the linked duplicate question and its corresponding answer, the focus is roughly only limited to the 'in' key word in Python. I think it is really limiting, compared to the current question.\nAnd I think the answer to this current question, is more relevant and elaborated that the answer of the proposed duplicate question/answer.",
    "answer": "in [1, 2, 3] # => True\nmatches = [x for x in lst if fulfills_some_condition(x)]\nmatches = (x for x in lst if x > 6)\nmatches = filter(fulfills_some_condition, lst)\nnext(x for x in lst if ...)\nnext((x for x in lst if ...), [default value])\n[1,2,3].index(2) # => 1\n[1,2,3].index(4) # => ValueError\n[1,2,3,2].index(2) # => 1\n[i for i,x in enumerate([1,2,3,2]) if x==2] # => [1, 3]\n\nAs for your first question: \"if item is in my_list:\" is perfectly fine and should work if item equals one of the elements inside my_list. The item must exactly match an item in the list. For instance, \"abc\" and \"ABC\" do not match. Floating point values in particular may suffer from inaccuracy. For instance, 1 - 1/3 != 2/3.\nAs for your second question: There's actually several possible ways if \"finding\" things in lists.\nChecking if something is inside\nThis is the use case you describe: Checking whether something is inside a list or not. As you know, you can use the in operator for that:\n\nFiltering a collection\nThat is, finding all elements in a sequence that meet a certain condition. You can use list comprehension or generator expressions for that:\n\nThe latter will return a generator which you can imagine as a sort of lazy list that will only be built as soon as you iterate through it. By the way, the first one is exactly equivalent to\n\nin Python 2. Here you can see higher-order functions at work. In Python 3, filter doesn't return a list, but a generator-like object.\nFinding the first occurrence\nIf you only want the first thing that matches a condition (but you don't know what it is yet), it's fine to use a for loop (possibly using the else clause as well, which is not really well-known). You can also use\n\nwhich will return the first match or raise a StopIteration if none is found. Alternatively, you can use\n\nFinding the location of an item\nFor lists, there's also the index method that can sometimes be useful if you want to know where a certain element is in the list:\n\nHowever, note that if you have duplicates, .index always returns the lowest index:......\n\nIf there are duplicates and you want all the indexes then you can use enumerate() instead:"
  },
  {
    "question": "Does a &quot;Find in project...&quot; feature exist in Eclipse IDE? Does Eclipse have a way to search a whole project for some text like Xcode's \"find in project\" feature?",
    "answer": "1. Ctrl + H\n2. Choose File Search for plain text search in workspace/selected projects\nFor specific expression searches, choose the relevant tab (such as Java Search which allows you to search for specific identifiers)\nFor whole project search:\n3. Scope (in the form section) > Enclosing project (Radio button selection)."
  },
  {
    "question": "Find duplicate lines in a file and count how many time each line was duplicated? Suppose I have a file similar to the following:\n\n```\n123 \n123 \n234 \n234 \n123 \n345\n\n```\n\nI would like to find how many times '123' was duplicated, how many times '234' was duplicated, etc.\nSo ideally, the output would be like:\n\n```\n123  3 \n234  2 \n345  1\n\n```",
    "answer": "sort <file> | uniq -c\nsort <file> | uniq --count\n\nAssuming there is one number per line:\n\nYou can use the more verbose --count flag too with the GNU version, e.g., on Linux:"
  },
  {
    "question": "How can I get a recursive full-path listing, one line per file? How can I spit out a flat list of recursive one-per-line paths?\nFor example, I just want a flat listing of files with their full paths:\n\n```\n/home/dreftymac/.\n/home/dreftymac/foo.txt\n/home/dreftymac/bar.txt\n/home/dreftymac/stackoverflow\n/home/dreftymac/stackoverflow/alpha.txt\n/home/dreftymac/stackoverflow/bravo.txt\n/home/dreftymac/stackoverflow/charlie.txt\n\n```\n\nls -a1 almost does what I need, but I do not want path fragments, I want full paths.",
    "answer": "ls -R /path | awk '\n/:$/&&f{s=$0;f=0}\n/:$/&&!f{sub(/:$/,\"\");s=$0;f=1;next}\nNF&&f{ print s\"/\"$0 }'\n\nIf you really want to use ls, then format its output using awk:"
  },
  {
    "question": "Find a file by name in Visual Studio Code How can I find a file by name (or path) in Visual Studio Code?\nA Visual Studio shortcut I'm used to is CTRL+,, but it does not work here.",
    "answer": "When you have opened a folder in a workspace you can do Ctrl+P (Cmd+P on Mac) and start typing the filename, or extension to filter the list of filenames\nif you have:\n\nplugin.ts\npage.css\nplugger.ts\n\nYou can type css and press enter and it will open the page.css. If you type .ts the list is filtered and contains two items."
  },
  {
    "question": "Using find to locate files that match one of multiple patterns I was trying to get a list of all python and html files in a directory with the command find Documents -name \"*.{py,html}\".\nThen along came the man page:\n\nBraces within the pattern (\u2018{}\u2019) are not considered to be special (that is, find . -name 'foo{1,2}' matches a file named foo{1,2}, not the files foo1 and foo2.\n\nAs this is part of a pipe-chain, I'd like to be able to specify which extensions it matches at runtime (no hardcoding).  If find just can't do it, a perl one-liner (or similar) would be fine.\nEdit: The answer I eventually came up with include all sorts of crap, and is a bit long as well, so I posted it as an answer to the original itch I was trying to scratch.  Feel free to hack that up if you have better solutions.",
    "answer": "find Documents \\( -name \"*.py\" -o -name \"*.html\" \\)\nls **/*.py **/*.html\n\nUse -o, which means \"or\":\n\nYou'd need to build that command line programmatically, which isn't that easy.\nAre you using bash (or Cygwin on Windows)?  If you are, you should be able to do this:\n\nwhich might be easier to build programmatically."
  },
  {
    "question": "How to use regex with find command? I have some images named with generated uuid1 string. For example 81397018-b84a-11e0-9d2a-001b77dc0bed.jpg. I want to find out all these images using \"find\" command:\n\n```\nfind . -regex \"[a-f0-9\\-]\\{36\\}\\.jpg\".\n\n```\n\nBut it doesn't work. Is something wrong with the regex?",
    "answer": "find . -regextype sed -regex \".*/[a-f0-9\\-]\\{36\\}\\.jpg\"\nsusam@nifty:~/so$ find . -name \"*.jpg\"\n./foo-111.jpg\n./test/81397018-b84a-11e0-9d2a-001b77dc0bed.jpg\n./81397018-b84a-11e0-9d2a-001b77dc0bed.jpg\nsusam@nifty:~/so$\nsusam@nifty:~/so$ find . -regextype sed -regex \".*/[a-f0-9\\-]\\{36\\}\\.jpg\"\n./test/81397018-b84a-11e0-9d2a-001b77dc0bed.jpg\n./81397018-b84a-11e0-9d2a-001b77dc0bed.jpg\nfind --version\nfind (GNU findutils) 4.4.2\nCopyright (C) 2007 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.\n\nWritten by Eric B. Decker, James Youngman, and Kevin Dalley.\nBuilt using GNU gnulib version e5573b1bad88bfabcda181b9e0125fb0c52b7d3b\nFeatures enabled: D_TYPE O_NOFOLLOW(enabled) LEAF_OPTIMISATION FTS() CBO(level=0)\nsusam@nifty:~/so$\nsusam@nifty:~/so$ find . -regextype foo -regex \".*/[a-f0-9\\-]\\{36\\}\\.jpg\"\nfind: Unknown regular expression type `foo'; valid types are `findutils-default', `awk', `egrep', `ed', `emacs', `gnu-awk', `grep', `posix-awk', `posix-basic', `posix-egrep', `posix-extended', `posix-minimal-basic', `sed'.\n\nNote that you need to specify .*/ in the beginning because find matches the whole path.\nExample:\n\nMy version of find:"
  },
  {
    "question": "Make xargs handle filenames that contain spaces ```\n$ ls *mp3 | xargs mplayer  \n\nPlaying Lemon.  \nFile not found: 'Lemon'  \nPlaying Tree.mp3.  \nFile not found: 'Tree.mp3'  \n\nExiting... (End of file)  \n\n```\n\nMy command fails because the file \"Lemon Tree.mp3\" contains spaces and so xargs thinks it's two files. Can I make find + xargs work with filenames like this?",
    "answer": "ls *.mp3 | xargs -d '\\n' mplayer\nls *.mp3 | tr \\\\n \\\\0 | xargs -0 mplayer\nmplayer *.mp3\n\nThe xargs command takes white space characters (tabs, spaces, new lines) as delimiters.\nYou can narrow it down only for the new line characters ('\\n') with -d option like this:\n\nIt works only with GNU xargs.\nFor MacOS:\n\nThe more simplistic and practically useful approach (when don't need to process the filenames further):"
  },
  {
    "question": "How can I find elements by text content with jQuery? Can anyone tell me if it's possible to find an element based on its content rather than by an ID or class?\nI am attempting to find elements that don't have distinct classes or IDs. (Then I then need to find that element's parent.)",
    "answer": "('div:contains(\"test\")').css('background-color', 'red');\n<div>This is a test</div>\n<div>Another Div</div>\n\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/1.12.4/jquery.min.js\"></script>\n\nYou can use the :contains selector to get elements based on their content.\nDemo here"
  },
  {
    "question": "How do I find all of the symlinks in a directory tree? I'm trying to find all of the symlinks within a directory tree for my website. I know that I can use find to do this but I can't figure out how to recursively check the directories.\nI've tried this command:\n\n```\nfind /var/www/ -type l\n\n```\n\n\u2026 and later I discovered that the contents in /var/www are symlinks, so I've changed the command to: \n\n```\nfind -L /var/www/ -type l\n\n```\n\nit take a while to run, however I'm getting no matches.\nHow do I get this to check subdirectories?",
    "answer": "ls -lR /path/to/folder | grep '^l'\n-L     Follow symbolic links.  When find examines or prints information\nabout files, the information used shall be taken from the  prop\u2010\nerties  of  the file to which the link points, not from the link\nitself (unless it is a broken symbolic link or find is unable to\nexamine  the file to which the link points).  Use of this option\nimplies -noleaf.  If you later use the -P option,  -noleaf  will\nstill  be  in  effect.   If -L is in effect and find discovers a\nsymbolic link to a subdirectory during its search, the subdirec\u2010\ntory pointed to by the symbolic link will be searched.\n\nWhen the -L option is in effect, the -type predicate will always\nmatch against the type of the file that a symbolic  link  points\nto rather than the link itself (unless the symbolic link is bro\u2010\nken).  Using -L causes the -lname and -ilname predicates  always\nto return false.\nfind -L /var/www/ -type l\nl      symbolic link; this is never true if the -L option or the\n-follow option is in effect, unless the symbolic link  is\nbroken.  If you want to search for symbolic links when -L\nis in effect, use -xtype.\nfind -L /var/www/ -xtype l\n\nThis will recursively traverse the /path/to/folder directory and list only the symbolic links:\n\nIf your intention is to follow the symbolic links too, you should use your find command but you should include the -L option; in fact the find man page says:\n\nThen try this:\n\nThis will probably work: I found in the find man page this diamond: if you are using the -type option you have to change it to the -xtype option:\n\nThen:"
  },
  {
    "question": "How to only get file name with Linux &#39;find&#39;? I'm using find to all files in directory, so I get a list of paths. However, I need only file names. i.e. I get ./dir1/dir2/file.txt and I want to get file.txt",
    "answer": "find /dir1 -type f -printf \"%f\\n\"\n\nIn GNU find you can use -printf parameter for that, e.g.:"
  },
  {
    "question": "Find the files that have been changed in last 24 hours E.g., a MySQL server is running on my Ubuntu machine. Some data has been changed during the last 24 hours.\nWhat (Linux) scripts can find the files that have been changed during the last 24 hours?\nPlease list the file names, file sizes, and modified time.",
    "answer": "find /directory_path -mtime -1 -ls\n\nTo find all files modified in the last 24 hours (last full day) in a particular specific directory and its sub-directories:\n\nShould be to your liking\nThe - before 1 is important - it means anything changed one day or less ago.\nA + before 1 would instead mean anything changed at least one day ago, while having nothing before the 1 would have meant it was changed exacted one day ago, no more, no less."
  },
  {
    "question": "How to check if a value exists in a dictionary? I have the following dictionary in python: \n\n```\nd = {'1': 'one', '3': 'three', '2': 'two', '5': 'five', '4': 'four'}\n\n```\n\nI need a way to find if a value such as \"one\" or \"two\" exists in this dictionary.\nFor example, if I wanted to know if the index \"1\" existed I would simply have to type: \n\n```\n\"1\" in d\n\n```\n\nAnd then python would tell me if that is true or false, however I need to do that same exact thing except to find if a value exists.",
    "answer": ">>> d = {'1': 'one', '3': 'three', '2': 'two', '5': 'five', '4': 'four'}\n>>> 'one' in d.values()\nTrue\n>>> T(lambda : 'one' in d.itervalues()).repeat()\n[0.28107285499572754, 0.29107213020324707, 0.27941107749938965]\n>>> T(lambda : 'one' in d.values()).repeat()\n[0.38303399085998535, 0.37257885932922363, 0.37096405029296875]\n>>> T(lambda : 'one' in d.viewvalues()).repeat()\n[0.32004380226135254, 0.31716084480285645, 0.3171098232269287]\n>>> T(lambda : 'four' in d.itervalues()).repeat()\n[0.41178202629089355, 0.3959040641784668, 0.3970959186553955]\n>>> T(lambda : 'four' in d.values()).repeat()\n[0.4631338119506836, 0.43541407585144043, 0.4359898567199707]\n>>> T(lambda : 'four' in d.viewvalues()).repeat()\n[0.43414998054504395, 0.4213531017303467, 0.41684913635253906]\n>>> type(d.viewvalues())\n<type 'dict_values'>\n>>> type(d.values())\n<type 'list'>\n>>> type(d.itervalues())\n<type 'dictionary-valueiterator'>\n\nOut of curiosity, some comparative timing:\n\nThe reason is that each of the above returns a different type of object, which may or may not be well suited for lookup operations:"
  },
  {
    "question": "find: missing argument to -exec I was helped out today with a command, but it doesn't seem to be working. This is the command:\n\n```\nfind /home/me/download/ -type f -name \"*.rm\" -exec ffmpeg -i {} -sameq {}.mp3 && rm {}\\;\n\n```\n\nThe shell returns\n\n```\nfind: missing argument to `-exec'\n\n```\n\nWhat I am basically trying to do is go through a directory recursively (if it has other directories) and run the ffmpeg command on the .rm file types and convert them to .mp3 file types. Once this is done, remove the .rm file that has just been converted.",
    "answer": "find . -type f -name \"*.rm\" -exec ffmpeg -i {} -sameq {}.mp3 \\; -exec rm {} \\;\n\nI figured it out now. When you need to run two commands in exec in a find you need to actually have two separate execs. This finally worked for me."
  },
  {
    "question": "How to use &#39;find&#39; to search for files created on a specific date? How do I use the UNIX command find to search for files created on a specific date?",
    "answer": "find . -type f -newermt 2007-06-07 ! -newermt 2007-06-08\nfind . -type f -newerat 2008-09-29 ! -newerat 2008-09-30\nfind . -type f -newerct 2008-09-29 ! -newerct 2008-09-30\n\nAs pointed out by Max, you can't, but checking files modified or accessed is not all that hard.  I wrote a tutorial about this, as late as today. The essence of which is to use -newerXY and ! -newerXY:\nExample: To find all files modified on the 7th of June, 2007:\n\nTo find all files accessed on the 29th of september, 2008:\n\nOr, files which had their permission changed on the same day:\n\nIf you don't change permissions on the file, 'c' would normally correspond to the creation date, though."
  },
  {
    "question": "&quot;find: paths must precede expression:&quot; How do I specify a recursive search that also finds files in the current directory? I am having a hard time getting find to look for matches in the current directory as well as its subdirectories. \nWhen I run find *test.c it only gives me the matches in the current directory. (does not look in subdirectories)\nIf I try find . -name *test.c I would expect the same results, but instead it gives me only matches that are in a subdirectory. When there are files that should match in the working directory, it gives me: find: paths must precede expression: mytest.c \nWhat does this error mean, and how can I get the matches from both the current directory and its subdirectories?",
    "answer": "find . -name bobtest.c cattest.c snowtest.c\nfind . -name '*test.c'\n\nTry putting it in quotes -- you're running into the shell's wildcard expansion, so what you're acually passing to find will look like:\n\n...causing the syntax error. So try this instead:\n\nNote the single quotes around your file expression -- these will stop the shell (bash) expanding your wildcards."
  },
  {
    "question": "How can I find WPF controls by name or type? I need to search a WPF control hierarchy for controls that match a given name or type. How can I do this?",
    "answer": "/// <summary>\n/// Finds a Child of a given item in the visual tree.\n/// </summary>\n/// <param name=\"parent\">A direct parent of the queried item.</param>\n/// <typeparam name=\"T\">The type of the queried item.</typeparam>\n/// <param name=\"childName\">x:Name or Name of child. </param>\n/// <returns>The first parent item that matches the submitted type parameter or null if not found</returns>\npublic static T FindChild<T>(DependencyObject parent, string childName)\nwhere T : DependencyObject\n{\n// Confirm parent and childName are valid.\nif (parent == null) return null;\n\nT foundChild = null;\n\nint childrenCount = VisualTreeHelper.GetChildrenCount(parent);\nfor (int i = 0; i < childrenCount; i++)\n{\nvar child = VisualTreeHelper.GetChild(parent, i);\n// If the child is not of the request child type child\nT childType = child as T;\nif (childType == null)\n{\n// recursively drill down the tree\nfoundChild = FindChild<T>(child, childName);\n\n// If the child is found, break so we do not overwrite the found child.\nif (foundChild != null) break;\n}\nelse if (!string.IsNullOrEmpty(childName))\n{\nvar frameworkElement = child as FrameworkElement;\n// If the child's name is set for search\nif (frameworkElement != null && frameworkElement.Name == childName)\n{\n// if the child's name is of the request name\nfoundChild = (T)child;\nbreak;\n}\n}\nelse\n{\n// child element found.\nfoundChild = (T)child;\nbreak;\n}\n}\n\nreturn foundChild;\n}\nTextBox foundTextBox =\nUIHelper.FindChild<TextBox>(Application.Current.MainWindow, \"myTextBoxName\");\n\nI combined the template format used by John Myczek and Tri Q's algorithm above to create a findChild Algorithm that can be used on any parent. Keep in mind that recursively searching a tree downwards could be a lengthy process. I've only spot-checked this on a WPF application, please comment on any errors you might find and I'll correct my code.\nWPF Snoop is a useful tool in looking at the visual tree - I'd strongly recommend using it while testing or using this algorithm to check your work.\nThere is a small error in Tri Q's Algorithm. After the child is found, if childrenCount is > 1 and we iterate again we can overwrite the properly found child. Therefore I added a if (foundChild != null) break; into my code to deal with this condition.\n\nCall it like this:\n\nNote Application.Current.MainWindow can be any parent window."
  },
  {
    "question": "find without recursion Is it possible to use the find command in some way that it will not recurse into the sub-directories? For example,\n\n```\nDirsRoot\n  |-->SubDir1\n  |    |-OtherFile1\n  |-->SubDir2\n  |    |-OtherFile2\n  |-File1\n  |-File2\n\n```\n\nAnd the result of something like find DirsRoot --do-not-recurse -type f will be only File1, File2?",
    "answer": "-maxdepth levels\nDescend at most levels (a non-negative integer) levels of direc-\ntories below the command line arguments.   `-maxdepth  0'  means\nonly  apply the tests and actions to the command line arguments.\nfind DirsRoot/* -maxdepth 0 -type f\nfind DirsRoot/ -maxdepth 1 -type f\n\nI think you'll get what you want with the -maxdepth 1 option, based on your current command structure. If not, you can try looking at the man page for find.\nRelevant entry (for convenience's sake):\n\nYour options basically are:\n\nOr:"
  },
  {
    "question": "find filenames NOT ending in specific extensions on Unix? Is there a simple way to recursively find all files in a directory hierarchy, that do not end in a list of extensions? E.g. all files that are not *.dll or *.exe\nUNIX/GNU find, powerful as it is, doesn't seem to have an exclude mode (or I'm missing it), and I've always found it hard to use regular expressions to find things that don't match a particular expression.\nI'm in a Windows environment (using the GnuWin32 port of most GNU tools), so I'm equally open for Windows-only solutions.",
    "answer": "find . -not -name \"*.exe\" -not -name \"*.dll\"\nfind . -not -name \"*.exe\" -not -name \"*.dll\" -not -type d\nfind . -not -name \"*.exe\" -not -name \"*.dll\" -type f\n\nOr without ( and the need to escape it:\n\nand to also exclude the listing of directories\n\nor in positive logic ;-)"
  },
  {
    "question": "How to recursively find the latest modified file in a directory? It seems that ls doesn't sort the files correctly when doing a recursive call:\n\n```\nls -altR . | head -n 3\n\n```\n\nHow can I find the most recently modified file in a directory (including subdirectories)?",
    "answer": "find . -type f -printf '%T@ %p\\n' \\\n| sort -n | tail -1 | cut -f2- -d\" \"\n\nFor a huge tree, it might be hard for sort to keep everything in memory.\n%T@ gives you the modification time like a unix timestamp, sort -n sorts numerically, tail -1 takes the last line (highest timestamp), cut -f2 -d\" \" cuts away the first field (the timestamp) from the output.\nEdit: Just as -printf is probably GNU-only, ajreals usage of stat -c is too. Although it is possible to do the same on BSD, the options for formatting is different (-f \"%m %N\" it would seem)\nAnd I missed the part of plural; if you want more then the latest file, just bump up the tail argument."
  },
  {
    "question": "How to use &#39;-prune&#39; option of &#39;find&#39; in sh? I don't quite understand the example given from the man find, can anyone give me some examples and explanations? Can I combine regular expression in it? \n\nThe more detailed question is like this: \nWrite a shell script, changeall, which has an interface like changeall [-r|-R] \"string1\" \"string2\". It will find all files with an suffix of .h, .C, .cc, or .cpp and change all occurrences of string1 to string2. -r is option for staying in current dir only or including subdir's.\nNOTE: \n\nFor non-recursive case, ls is NOT allowed, we could only use find and sed.\nI tried find -depth but it was NOT supported. That's why I was wondering if -prune could help, but didn't understand the example from man find. \n\n\nEDIT2: I was doing assignment, I didn't ask question in great details because I would like to finish it myself. Since I already done it and hand it in, now I can state the whole question. Also, I managed to finish the assignment without using -prune, but would like to learn it anyway.",
    "answer": "find [path] [conditions to prune] -prune -o \\\n[your usual conditions] [actions to perform]\nfind . -name .snapshot -prune -o -name '*.foo' -print\nfind . -name .snapshot -prune -o -name '*.foo'              # DON'T DO THIS\nfind . \\( -name .snapshot -prune -o -name '*.foo' \\) -print # DON'T DO THIS\nfind . -name .snapshot -prune -o -name '*.foo' -print       # DO THIS\nfind . -name '.git*' -prune -o -type f -print               # DON'T DO THIS\nfind . -name '.git*' -type d -prune -o -type f -print       # DO THIS\n\nThe thing I'd found confusing about -prune is that it's an action (like -print), not a test (like -name). It alters the \"to-do\" list, but always returns true.\nThe general pattern for using -prune is this:\n\nYou pretty much always want the -o (logical OR) immediately after -prune, because that first part of the test (up to and including -prune) will return false for the stuff you actually want (ie: the stuff you don't want to prune out).\nHere's an example:\n\nThis will find the \"*.foo\" files that aren't under \".snapshot\" directories. In this example, -name .snapshot makes up the [conditions to prune], and -name '*.foo' -print is [your usual conditions] and [actions to perform].\nImportant notes:\n\nIf all you want to do is print the results you might be used to leaving out the -print action. You generally don't want to do that when using -prune.\nThe default behavior of find is to \"and\" the entire expression with the -print action if there are no actions other than -prune (ironically) at the end. That means that writing this:\n\nis equivalent to writing this:\n\nwhich means that it'll also print out the name of the directory you're pruning, which usually isn't what you want. Instead it's better to explicitly specify the -print action if that's what you want:\n\nIf your \"usual condition\" happens to match files that also match your prune condition, those files will not be included in the output. The way to fix this is to add a -type d predicate to your prune condition.\nFor example, suppose we wanted to prune out any directory that started with .git (this is admittedly somewhat contrived -- normally you only need to remove the thing named exactly .git), but other than that wanted to see all files, including files like .gitignore. You might try this:\n\nThis would not include .gitignore in the output. Here's the fixed version:\n\nExtra tip: if you're using the GNU version of find, the texinfo page for find has a more detailed explanation than its manpage (as is true for most GNU utilities)."
  },
  {
    "question": "How to exclude this / current / dot folder from find &quot;type d&quot; ```\nfind . -type d\n\n```\n\ncan be used to find all directories below some start point. But it returns the current directory (.) too, which may be undesired. How can it be excluded?",
    "answer": "find . ! -path . -type d\nD=\"long_name\"\nfind \"$D\" ! -path \"$D\" -type d\n\nPOSIX 7 solution:\n\nFor this particular case (.), golfs better than the mindepth solution (24 vs 26 chars), although this is probably slightly harder to type because of the !.\nTo exclude other directories, this will golf less well and requires a variable for DRYness:\n\nMy decision tree between ! and -mindepth:\n\nscript? Use ! for portability.\ninteractive session on GNU?\n\nexclude .? Throw a coin.\nexclude long_name? Use -mindepth."
  },
  {
    "question": "&#39;find -exec&#39; a shell function in Linux Is there a way to get find to execute a function I define in the shell?\nFor example:\n\n```\ndosomething () {\n  echo \"Doing something with $1\"\n}\nfind . -exec dosomething {} \\;\n\n```\n\nThe result of that is:\n\n```\nfind: dosomething: No such file or directory\n\n```\n\nIs there a way to get find's -exec to see dosomething?",
    "answer": "export -f dosomething\nfind . -exec bash -c 'dosomething \"$0\"' {} \\;\n\nSince only the shell knows how to run shell functions, you have to run a shell to run a function.  You also need to mark your function for export with export -f, otherwise the subshell won't inherit them:"
  },
  {
    "question": "How to pipe list of files returned by find command to cat to view all the files I am doing a find to get a list of files.\nHow do I pipe it to another utility like cat so that cat displays the contents of all those files?\nAfterwards, I'd use grep on that to search some text in those files.",
    "answer": "command1 | command2\nfind . -name '*.foo' -exec cat {} \\;\ncommand2 `command1`\ncat `find . -name '*.foo' -print`\n\nPiping to another process (although this won't accomplish what you said you are trying to do):\n\nThis will send the output of command1 as the input of command2.\n\n-exec on a find (this will do what you want to do, but it's specific to find):\n\nEverything between find and -exec are the find predicates you were already using. {} will substitute the particular file you found into the command (cat {} in this case); the \\; is to end the -exec command.\n\nSend output of one process as command line arguments to another process:\n\nFor example:\n\nNote these are backquotes not regular quotes (they are under the tilde ~ on my keyboard).\nThis will send the output of command1 into command2 as command line arguments. It's called command substitution. Note that file names containing spaces (newlines, etc) will be broken into separate arguments, though."
  },
  {
    "question": "Chmod recursively I have an archive, which is archived by someone else, and I want to automatically, after I download it, to change a branch of the file system within the extracted files to gain read access. (I can't change how archive is created).\nI've looked into this thread: chmod: How to recursively add execute permissions only to files which already have execute permission as into some others, but no joy.\nThe directories originally come with multiple but all wrong flags, they may appear as:\n\n```\ndrwx------\nd---r-x---\ndrwxrwxr-x\ndr--r-xr--\n\n```\n\nThose are just the few I've discovered so far, but could be more.\nfind errors when tries to look into a directory with no x permission, and so doesn't pass it to chmod. What I've been doing so far, is manually change permissions on the parent directory, then go into the child directories and do the same for them and so on. But this is a lot of hand labour. Isn't there some way to do this automatically?\nI.e. how I am doing it now:\ndo:\n\n```\n$ chmod -R +x\n$ chmod -R +r\n\n```\n\nuntil I get no errors, then\n\n```\n$ find -type f -exec chmod -x {} +\n\n```\n\nBut there must be a better way.",
    "answer": "find . -type d -exec chmod +rx {} \\;\n\nYou need read access, in addition to execute access, to list a directory. If you only have execute access, then you can find out the names of entries in the directory, but no other information (not even types, so you don't know which of the entries are subdirectories). This works for me:"
  },
  {
    "question": "Find all files with a filename beginning with a specified string? I have a directory with roughly 100000 files in it, and I want to perform some function on all files beginning with a specified string, which may match tens of thousands of files.\nI have tried\n\n```\nls mystring*\n\n```\n\nbut this returns with the bash error 'Too many arguments'.  My next plan was to use\n\n```\nfind ./mystring* -type f\n\n```\n\nbut this has the same issue.  \nThe code needs to look something like\n\n```\nfor FILE in `find ./mystring* -type f`\ndo\n    #Some function on the file\ndone\n\n```",
    "answer": "find . -name 'mystring*'\n\nUse find with a wildcard:"
  },
  {
    "question": "Using semicolon (;) vs plus (+) with exec in find Why is there a difference in output between using \n\n```\nfind . -exec ls '{}' \\+\n\n```\n\nand\n\n```\nfind . -exec ls '{}' \\;\n\n```\n\nI got:\n\n```\n$ find . -exec ls  \\{\\} \\+\n./file1  ./file2\n\n.:\nfile1  file2  testdir1\n\n./testdir1:\ntestdir2\n\n./testdir1/testdir2:\n\n\n$ find . -exec ls  \\{\\} \\;\nfile1  file2  testdir1\ntestdir2\n./file2\n./file1\n\n```",
    "answer": "file1\nfile2\nfile3\nls file1\nls file2\nls file3\nls file1 file2 file3\n\nThis might be best illustrated with an example. Let's say that find turns up these files:\n\nUsing -exec with a semicolon (find . -exec ls '{}' \\;), will execute\n\nBut if you use a plus sign instead (find . -exec ls '{}' \\+), as many  filenames as possible are passed as arguments to a single command:\n\nThe number of filenames is only limited by the system's maximum command line length. If the command exceeds this length, the command will be called multiple times."
  },
  {
    "question": "How can I get `find` to ignore .svn directories? I often use the find command to search through source code, delete files, whatever. Annoyingly, because Subversion stores duplicates of each file in its .svn/text-base/ directories my simple searches end up getting lots of duplicate results. For example, I want to recursively search for uint in multiple messages.h and messages.cpp files:\n\n```\n# find -name 'messages.*' -exec grep -Iw uint {} +\n./messages.cpp:            Log::verbose << \"Discarding out of date message: id \" << uint(olderMessage.id)\n./messages.cpp:    Log::verbose << \"Added to send queue: \" << *message << \": id \" << uint(preparedMessage->id)\n./messages.cpp:                Log::error << \"Received message with invalid SHA-1 hash: id \" << uint(incomingMessage.id)\n./messages.cpp:            Log::verbose << \"Received \" << *message << \": id \" << uint(incomingMessage.id)\n./messages.cpp:            Log::verbose << \"Sent message: id \" << uint(preparedMessage->id)\n./messages.cpp:        Log::verbose << \"Discarding unsent message: id \" << uint(preparedMessage->id)\n./messages.cpp:        for (uint i = 0; i < 10 && !_stopThreads; ++i) {\n./.svn/text-base/messages.cpp.svn-base:            Log::verbose << \"Discarding out of date message: id \" << uint(olderMessage.id)\n./.svn/text-base/messages.cpp.svn-base:    Log::verbose << \"Added to send queue: \" << *message << \": id \" << uint(preparedMessage->id)\n./.svn/text-base/messages.cpp.svn-base:                Log::error << \"Received message with invalid SHA-1 hash: id \" << uint(incomingMessage.id)\n./.svn/text-base/messages.cpp.svn-base:            Log::verbose << \"Received \" << *message << \": id \" << uint(incomingMessage.id)\n./.svn/text-base/messages.cpp.svn-base:            Log::verbose << \"Sent message: id \" << uint(preparedMessage->id)\n./.svn/text-base/messages.cpp.svn-base:        Log::verbose << \"Discarding unsent message: id \" << uint(preparedMessage->id)\n./.svn/text-base/messages.cpp.svn-base:        for (uint i = 0; i < 10 && !_stopThreads; ++i) {\n./virus/messages.cpp:void VsMessageProcessor::_progress(const string &fileName, uint scanCount)\n./virus/messages.cpp:ProgressMessage::ProgressMessage(const string &fileName, uint scanCount)\n./virus/messages.h:    void _progress(const std::string &fileName, uint scanCount);\n./virus/messages.h:    ProgressMessage(const std::string &fileName, uint scanCount);\n./virus/messages.h:    uint        _scanCount;\n./virus/.svn/text-base/messages.cpp.svn-base:void VsMessageProcessor::_progress(const string &fileName, uint scanCount)\n./virus/.svn/text-base/messages.cpp.svn-base:ProgressMessage::ProgressMessage(const string &fileName, uint scanCount)\n./virus/.svn/text-base/messages.h.svn-base:    void _progress(const std::string &fileName, uint scanCount);\n./virus/.svn/text-base/messages.h.svn-base:    ProgressMessage(const std::string &fileName, uint scanCount);\n./virus/.svn/text-base/messages.h.svn-base:    uint        _scanCount;\n\n```\n\nHow can I tell find to ignore the .svn directories?\n\nUpdate: If you upgrade  your SVN client to version 1.7 this is no longer an issue.\n\nA key feature of the changes introduced in Subversion 1.7 is the centralization of working copy metadata storage into a single location. Instead of a .svn directory in every directory in the working copy, Subversion 1.7 working copies have just one .svn directory\u2014in the root of the working copy. This directory includes (among other things) an SQLite-backed database which contains all of the metadata Subversion needs for that working copy.",
    "answer": "For searching, can I suggest you look at ack ? It's a source-code aware find, and as such will automatically ignore many file types, including source code repository info such as the above."
  },
  {
    "question": "How to go to each directory and execute a command? How do I write a bash script that goes through each directory inside a parent_directory and executes a command in each directory.\nThe directory structure is as follows:\n\nparent_directory (name could be anything - doesnt follow a pattern)\n\n001 (directory names follow this pattern)\n\n0001.txt (filenames follow this pattern)\n0002.txt\n0003.txt\n\n\n002\n\n0001.txt\n0002.txt\n0003.txt\n0004.txt\n\n\n003\n\n0001.txt\n\n\n\nthe number of directories is unknown.",
    "answer": "for d in [0-9][0-9][0-9]\ndo\n( cd \"$d\" && your-command-here )\ndone\n\nYou can do the following, when your current directory is parent_directory:\n\nThe ( and ) create a subshell, so the current directory isn't changed in the main script."
  },
  {
    "question": "How can I check if character in a string is a letter? (Python) I know about islower and isupper, but can you check whether or not that character is a letter?\nFor Example:\n\n```\n>>> s = 'abcdefg'\n>>> s2 = '123abcd'\n>>> s3 = 'abcDEFG'\n>>> s[0].islower()\nTrue\n\n>>> s2[0].islower()\nFalse\n\n>>> s3[0].islower()\nTrue\n\n```\n\nIs there any way to just ask if it is a character besides doing .islower() or .isupper()?",
    "answer": "s = 'a123b'\n\nfor char in s:\nprint(char, char.isalpha())\na True\nFalse\nFalse\nFalse\nb True\n\nYou can use str.isalpha().\nFor example:\n\nOutput:"
  },
  {
    "question": "Search for executable files using find command What type of parameter/flag can I use with the Unix find command so that I search executables?",
    "answer": "find . -type f -executable -print\nfind . -type f -perm +111 -print\n\nOn GNU versions of find you can use -executable:\n\nFor BSD (and MacOS) versions of find, you can use -perm with + and an octal mask:\n\nIn this context \"+\" means \"any of these bits are set\" and 111 is the execute bits.\nNote that this is not identical to the -executable predicate in GNU find. In particular, -executable tests that the file can be executed by the current user, while -perm +111 just tests if any execute permissions are set.\nOlder versions of GNU find also support the -perm +111 syntax, but as of 4.5.12 this syntax is no longer supported. Instead, you can use -perm /111 to get this behavior."
  },
  {
    "question": "How to use find command to find all files with extensions from list? I need to find all image files from directory (gif, png, jpg, jpeg).\n\n```\nfind /path/to/ -name \"*.jpg\" > log\n\n```\n\nHow to modify this string to find not only .jpg files?",
    "answer": "find /path/to -regex \".*\\.\\(jpg\\|gif\\|png\\|jpeg\\)\" > log"
  },
  {
    "question": "Find and copy files Why does the following does not copy the files to the destination folder?\n\n```\n# find /home/shantanu/processed/ -name '*2011*.xml' -exec cp /home/shantanu/tosend {} \\;\n\ncp: omitting directory `/home/shantanu/tosend'\ncp: omitting directory `/home/shantanu/tosend'\ncp: omitting directory `/home/shantanu/tosend'\n\n```",
    "answer": "find /home/shantanu/processed/ -name '*2011*.xml' -exec cp \"{}\" /home/shantanu/tosend  \\;\n\nIf your intent is to copy the found files into /home/shantanu/tosend, you have the order of the arguments to cp reversed:\n\nPlease note: the find command uses {} as placeholder for the matched file."
  },
  {
    "question": "Find files containing a given text In bash I want to return file name (and the path to the file) for every file of type .php|.html|.js containing the case-insensitive string \"document.cookie\" | \"setcookie\"\nHow would I do that?",
    "answer": "egrep -ir --include=*.{php,html,js} \"(document.cookie|setcookie)\" .\negrep -lir --include=*.{php,html,js} \"(document.cookie|setcookie)\" .\n\nThe r flag means to search recursively (search subdirectories). The i flag means case insensitive.\nIf you just want file names add the l (lowercase L) flag:"
  },
  {
    "question": "Command line: piping find results to rm I'm trying to work out a command which deletes sql files older than 15 days.\nThe find part is working but not the rm.\n\n```\nrm -f | find -L /usr/www2/bar/htdocs/foo/rsync/httpdocs/db_backups -type f  \\( -name '*.sql' \\) -mtime +15\n\n```\n\nIt kicks out a list of exactly the files I want deleted but is not deleting them.  The paths are correct.  \n\n```\nusage: rm [-f | -i] [-dIPRrvW] file ...\n       unlink file\n/usr/www2/bar/htdocs/foo/rsync/httpdocs/db_backups/20120601.backup.sql\n...\n/usr/www2/bar/htdocs/foo/rsync/httpdocs/db_backups/20120610.backup.sql\n\n```\n\nWhat am I doing wrong?",
    "answer": "find -type f -name '*.sql' -mtime +15 | xargs rm\nfind -type f -name '*.sql' -mtime +15 -print0 | xargs -0 rm\nfind -type f -name '*.sql' -mtime +15 -delete\nWarnings:  Don't  forget that the find command line is evaluated\nas an expression, so putting -delete first will make find try to\ndelete everything below the starting points you specified.  When\ntesting a find command line that you later intend  to  use  with\n-delete,  you should explicitly specify -depth in order to avoid\nlater surprises.  Because -delete  implies  -depth,  you  cannot\nusefully use -prune and -delete together.\n\nYou are actually piping rm's output to the input of find. What you want is to use the output of find as arguments to rm:\n\nxargs is the command that \"converts\" its standard input into arguments of another program, or, as they more accurately put it on the man page,\n\nbuild and execute command lines from standard input\n\nNote that if file names can contain whitespace characters, you should correct for that:\n\nBut actually, find has a shortcut for this: the -delete option:\n\nPlease be aware of the following warnings in man find:\n\nP.S. Note that piping directly to rm isn't an option, because rm doesn't expect filenames on standard input. What you are currently doing is piping them backwards."
  },
  {
    "question": "Check if one list contains element from the other I have two lists with different objects in them.\n\n```\nList<Object1> list1;\nList<Object2> list2;\n\n```\n\nI want to check if element from list1 exists in list2, based on specific attribute (Object1 and Object2 have (among others),  one mutual attribute (with type Long), named attributeSame).\nright now, I do it like this:\n\n```\nboolean found = false;\nfor(Object1 object1 : list1){\n   for(Object2 object2: list2){\n       if(object1.getAttributeSame() == object2.getAttributeSame()){\n           found = true;\n           //also do something\n       }\n    }\n    if(!found){\n        //do something\n    }\n    found = false;\n}\n\n```\n\nBut I think there is a better and faster way to do this :)\nCan someone propose it?\nThanks!",
    "answer": "!Collections.disjoint(list1, list2);\nlist1.stream()\n.map(Object1::getProperty)\n.anyMatch(\nlist2.stream()\n.map(Object2::getProperty)\n.collect(toSet())\n::contains)\n\nIf you just need to test basic equality, this can be done with the basic JDK without modifying the input lists in the one line\n\nIf you need to test a specific property, that's harder.  I would recommend, by default,\n\n...which collects the distinct values in list2 and tests each value in list1 for presence."
  },
  {
    "question": "How can I store the &quot;find&quot; command results as an array in Bash I am trying to save the result from find as arrays.\nHere is my code:\n\n```\n#!/bin/bash\n\necho \"input : \"\nread input\n\necho \"searching file with this pattern '${input}' under present directory\"\narray=`find . -name ${input}`\n\nlen=${#array[*]}\necho \"found : ${len}\"\n\ni=0\n\nwhile [ $i -lt $len ]\ndo\necho ${array[$i]}\nlet i++\ndone\n\n```\n\nI get 2 .txt files under current directory.\nSo I expect '2' as result of ${len}. However, it prints 1.\nThe reason is that it takes all result of find as one elements.\nHow can I fix this?\nP.S\nI found several solutions on StackOverFlow about a similar problem. However, they are a little bit different so I can't apply in my case. I need to store the results in a variable before the loop. Thanks again.",
    "answer": "array=()\nwhile IFS=  read -r -d $'\\0'; do\narray+=(\"$REPLY\")\ndone < <(find . -name \"${input}\" -print0)\narray=()\nfind . -name \"${input}\" -print0 >tmpfile\nwhile IFS=  read -r -d $'\\0'; do\narray+=(\"$REPLY\")\ndone <tmpfile\nrm -f tmpfile\nset +m\nshopt -s lastpipe\narray=()\nfind . -name \"${input}\" -print0 | while IFS=  read -r -d $'\\0'; do array+=(\"$REPLY\"); done; declare -p array\narray=`find . -name \"${input}\"`\narray=(`find . -name \"${input}\"`)  # don't do this\nmapfile -d $'\\0' array < <(find . -name \"${input}\" -print0)\n\nUpdate 2020 for Linux Users:\nIf you have an up-to-date version of bash (4.4-alpha or better), as you probably do if you are on Linux, then you should be using Benjamin W.'s answer.\nIf you are on Mac OS, which \u2014last I checked\u2014 still used bash 3.2, or are otherwise using an older bash, then continue on to the next section.\nAnswer for bash 4.3 or earlier\nHere is one solution for getting the output of find into a bash array:\n\nThis is tricky because, in general, file names can have spaces, new lines, and other script-hostile characters.  The only way to use find and have the file names safely separated from each other is to use -print0 which prints the file names separated with a null character.  This would not be much of an inconvenience if bash's readarray/mapfile functions supported null-separated strings but they don't.  Bash's read does and that leads us to the loop above.\n[This answer was originally written in 2014.  If you have a recent version of bash, please see the update below.]\nHow it works\n\nThe first line creates an empty array: array=()\nEvery time that the read statement is executed, a null-separated file name is read from standard input.  The -r option tells read to leave backslash characters alone.  The -d $'\\0' tells read that the input will be null-separated.  Since we omit the name to read, the shell puts the input into the default name: REPLY.\nThe array+=(\"$REPLY\") statement appends the new file name to the array array.\nThe final line combines redirection and command substitution to provide the output of find to the standard input of the while loop.\n\nWhy use process substitution?\nIf we didn't use process substitution, the loop could be written as:\n\nIn the above the output of find is stored in a temporary file and that file is used as standard input to the while loop.  The idea of process substitution is to make such temporary files unnecessary.  So, instead of having the while loop get its stdin from tmpfile, we can have it get its stdin from <(find . -name ${input} -print0).\nProcess substitution is widely useful.  In many places where a command wants to read from a file, you can specify process substitution, <(...), instead of a file name.  There is an analogous form, >(...), that can be used in place of a file name where the command wants to write to the file.\nLike arrays, process substitution is a feature of bash and other advanced shells.  It is not part of the POSIX standard.\nAlternative: lastpipe\nIf desired, lastpipe can be used instead of process substitution (hat tip: Caesar):\n\nshopt -s lastpipe tells bash to run the last command in the pipeline in the current shell (not the background).  This way, the array remains in existence after the pipeline completes.  Because lastpipe only takes effect if job control is turned off, we run set +m.  (In a script, as opposed to the command line, job control is off by default.)\nAdditional notes\nThe following command creates a shell variable, not a shell array:\n\nIf you wanted to create an array, you would need to put parens around the output of find.  So, naively, one could:\n\nThe problem is that the shell performs word splitting on the results of find so that the elements of the array are not guaranteed to be what you want.\nUpdate 2019\nStarting with version 4.4-alpha, bash now supports a -d option so that the above loop is no longer necessary.  Instead, one can use:\n\nFor more information on this, please see (and upvote) Benjamin W.'s answer."
  },
  {
    "question": "How do I get the find command to print out the file size with the file name? If I issue the find command as follows:\n\n```\nfind . -name *.ear\n\n```\n\nIt prints out:\n\n```\n./dir1/dir2/earFile1.ear\n./dir1/dir2/earFile2.ear\n./dir1/dir3/earFile1.ear\n\n```\n\nI want to 'print' the name and the size to the command line:\n\n```\n./dir1/dir2/earFile1.ear  5000 KB\n./dir1/dir2/earFile2.ear  5400 KB\n./dir1/dir3/earFile1.ear  5400 KB\n\n```",
    "answer": "find . -name '*.ear' -print0 | xargs -0 ls -lhS\n\njust the h extra from jer.drab.org's reply. saves time converting to MB mentally ;)"
  },
  {
    "question": "How to strip leading &quot;./&quot; in unix &quot;find&quot;? ```\nfind . -type f -print\n\n```\n\nprints out\n\n```\n./file1\n./file2\n./file3\n\n```\n\nAny way to make it print\n\n```\nfile1\nfile2\nfile3\n\n```\n\n?",
    "answer": "If they're only in the current directory\nfind * -type f -print\nIs that what you want?"
  },
  {
    "question": "How can I search for a multiline pattern in a file? I needed to find all the files that contained a specific string pattern. The first solution that comes to mind is using find piped with xargs grep:\n\n```\nfind . -iname '*.py' | xargs grep -e 'YOUR_PATTERN'\n\n```\n\nBut if I need to find patterns that spans on more than one line, I'm stuck because vanilla grep can't find multiline patterns.",
    "answer": "find . -iname '*.py' | xargs pcregrep -M '_name.*\\n.*_description'\n\nSo I discovered pcregrep which stands for Perl Compatible Regular Expressions GREP.\n\nthe -M option makes it possible to search for patterns that span line boundaries.\n\nFor example, you need to find files where the '_name' variable is followed on the next line by the '_description' variable:\n\nTip: you need to include the line break character in your pattern. Depending on your platform, it could be '\\n', \\r', '\\r\\n', ..."
  },
  {
    "question": "Ruby Array find_first object? Am I missing something in the Array documentation?  I have an array which contains up to one object satisfying a certain criterion.  I'd like to efficiently find that object.  The best idea I have from the docs is this:\n\n```\ncandidates = my_array.select { |e| e.satisfies_condition? }\nfound_it = candidates.first if !candidates.empty?\n\n```\n\nBut I am unsatisfied for two reasons:\n\nThat select made me traverse the whole array, even though we could have bailed after the first hit.\nI needed a line of code (with a condition) to flatten the candidates.  \n\nBoth operations are wasteful with foreknowledge that there's 0 or 1 satisfying objects.\nWhat I'd like is something like:\n\n```\narray.find_first(block)\n\n```\n\nwhich returns nil or the first object for which the block evaluates to true, ending the traversal at that object.\nMust I write this myself?  All those other great methods in Array make me think it's there and I'm just not seeing it.",
    "answer": "Either I don't understand your question, or Enumerable#find is the thing you were looking for."
  },
  {
    "question": "Delete files older than 10 days using shell script in Unix I want to delete scripts in a folder from the current date back to 10 days.\nThe scripts looks like:\n\n```\n2012.11.21.09_33_52.script\n2012.11.21.09_33_56.script\n2012.11.21.09_33_59.script\n\n```\n\nThe script will run in every 10 day with Crontab, that's why I need the current date.",
    "answer": "find ./my_dir -mtime +10 -type f -delete\n\nfind is the common tool for this kind of task :\n\nEXPLANATIONS\n\n./my_dir your directory (replace with your own)\n-mtime +10 older than 10 days\n-type f only files\n-delete no surprise. Remove it to test your find filter before executing the whole command\n\nAnd take care that ./my_dir exists to avoid bad surprises !"
  },
  {
    "question": "Find all elements on a page whose element ID contains a certain text using jQuery I'm trying to find all elements on a page whose element ID contains a certain text. I'll then need to filter the found elements based on whether they are hidden or not. Any help is greatly appreciated.",
    "answer": "('*[id*=mytext]:visible').each(function() {\n(this).doStuff();\n});\n\nNote the asterisk '*' at the beginning of the selector matches all elements.\nSee the Attribute Contains Selectors, as well as the :visible and :hidden selectors."
  },
  {
    "question": "How to Configure Multiple SSH Private Keys for Different Servers Efficiently? I am working with multiple SSH private keys to connect to different servers or services (e.g., system administration, Git usage) on the same machine. Currently, I am using the -i option like so:\nApparently a straightforward way to do this is to use the command\n\n```\nssh -i <path_to_private_key> user@server.example.com\n\n```\n\nHowever, managing connections this way feels cumbersome, especially when switching between different keys for different servers.\nIs there a more efficient way to manage and use multiple SSH private keys without specifying the -i flag each time? I am looking for a configuration or tool-based approach that allows me to streamline this process.\nWhat I've tried:\nStacking the keys in the id_rsa file, which didn't work.\nManually specifying the private key file with the -i flag, but this approach isn't very scalable.\nDesired outcome:\nI'd like to set up my SSH configuration so that the correct private key is automatically used for the corresponding server or service.",
    "answer": "Host myshortname realname.example.com\nHostName realname.example.com\nIdentityFile ~/.ssh/realname_rsa # private key for realname\nUser remoteusername\n\nHost myother realname2.example.org\nHostName realname2.example.org\nIdentityFile ~/.ssh/realname2_rsa  # different private key for realname2\nUser remoteusername\n\nFrom my .ssh/config:\n\nThen you can use the following to connect:\nssh myshortname\nssh myother\nAnd so on."
  },
  {
    "question": "How to solve Permission denied (publickey) error when using Git? I'm on Mac Snow Leopard and I just installed git.\nI just tried \n\n```\ngit clone git@thechaw.com:cakebook.git\n\n```\n\nbut that gives me this error:\n\n```\nInitialized empty Git repository in `/Users/username/Documents/cakebook/.git/`\nPermission denied (publickey).\nfatal: The remote end hung up unexpectedly\n\n```\n\nWhat am I missing?\nI've also tried doing ssh-keygen with no passphase but still same error.",
    "answer": "If the user has not generated a ssh public/private key pair set before\nThis info is working on theChaw but can be applied to all other git repositories which support SSH pubkey authentications. (See [gitolite][1], gitlab or github for example.)\n\nFirst start by setting up your own public/private key pair set. This\ncan use either DSA or RSA, so basically any key you setup will work.\nOn most systems you can use ssh-keygen.\n\nFirst you'll want to cd into your .ssh directory. Open up the terminal and run:\n\ncd ~/.ssh && ssh-keygen\n\nNext you need to copy this to your clipboard.\nOn OS X run: cat id_rsa.pub | pbcopy\nOn Linux run: cat id_rsa.pub | xclip\nOn Windows (via Cygwin/Git Bash) run: cat id_rsa.pub | clip\nOn Windows (Powershell) run: Get-Content id_rsa.pub | Set-Clipboard (Thx to @orion elenzil)\nAdd your key to your account via the website.\nFinally setup your .gitconfig.\ngit config --global user.name \"bob\"\ngit config --global user.email bob@...\n(don't forget to restart your command line to make sure the config is reloaded)\n\nThat's it you should be good to clone and checkout.\n\nFurther information can be found at https://help.github.com/articles/generating-ssh-keys (thanks to @Lee Whitney)\n[1]: https://github.com/sitaramc/gitolite\n-\nIf the user has generated a ssh public/private key pair set before\n\ncheck which key have been authorized on your github or gitlab account settings\ndetermine which corresponding private key must be associated from your local computer\n\neval $(ssh-agent -s)\n\ndefine where the keys are located\n\nssh-add ~/.ssh/id_rsa"
  },
  {
    "question": "Calculate RSA key fingerprint I need to do the SSH key audit for GitHub, but I am not sure how do find my RSA key fingerprint. I originally followed a guide to generate an SSH key on Linux.\nWhat is the command I need to enter to find my current RSA key fingerprint?",
    "answer": "ssh-keygen -lf /path/to/ssh/key\nssh-keygen -lf ~/.ssh/id_rsa.pub\n00:11:22:33:44:55:66:77:88:99:aa:bb:cc:dd:ee:ff /Users/username/.ssh/id_rsa.pub (RSA)\nssh-keygen -E md5 -lf <fileName>\nfind /etc/ssh /home/*/.ssh /Users/*/.ssh -name '*.pub' -o -name 'authorized_keys' -o -name 'known_hosts'\n\nRun the following command to retrieve the SHA256 fingerprint of your SSH key (-l means \"list\" instead of create a new key, -f means \"filename\"):\n\nSo for example, on my machine the command I ran was (using RSA public key):\n\nTo get the GitHub (MD5) fingerprint format with newer versions of ssh-keygen, run:\n\nBonus information:\nssh-keygen -lf also works on known_hosts and authorized_keys files.\nTo find most public keys on Linux/Unix/OS\u00a0X systems, run\n\n(If you want to see inside other users' homedirs, you'll have to be root or sudo.)\nThe ssh-add -l is very similar, but lists the fingerprints of keys added to your agent. (OS\u00a0X users take note that magic passwordless SSH via Keychain is not the same as using ssh-agent.)"
  },
  {
    "question": "&quot;UNPROTECTED PRIVATE KEY FILE!&quot; Error using SSH into Amazon EC2 Instance (AWS) I've created a new linux instance on Amazon EC2, and as part of that downloaded the .pem file to allow me to SSH in.\nWhen I tried to ssh with:\n\n```\nssh -i myfile.pem <public dns>\n\n```\n\nI got:\n\n```\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@         WARNING: UNPROTECTED PRIVATE KEY FILE!          @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\nPermissions 0644 for 'amazonec2.pem' are too open.\nIt is recommended that your private key files are NOT accessible by others.\nThis private key will be ignored.\nbad permissions: ignore key: amazonec2.pem\nPermission denied (publickey).\n\n```\n\nFollowing this post I tried to chmod +600 the .pem file, but now when I ssh I just get\n\n```\nPermission denied (publickey).\n\n```\n\nWhat school-boy error am I making here?\nThe .pem file is in my home folder (in macOS). Its permissions look like this:\n\n```\n-rw-------@   1 mattroberts  staff    1696 19 Nov 11:20 amazonec2.pem\n\n```",
    "answer": "The problem is wrong set of permissions on the file.\nEasily solved by executing -\nchmod 400 mykey.pem\nTaken from AWS instructions -\n\nYour key file must not be publicly viewable for SSH to work. Use this\ncommand if needed:  chmod 400 mykey.pem\n\n400 protects it by making it read only and only for the owner."
  },
  {
    "question": "How do I access my SSH public key? I've just generated my RSA key pair, and I wanted to add that key to GitHub.\nI tried cd id_rsa.pub and id_rsa.pub, but no luck. How can I access my SSH public key?",
    "answer": "cat ~/.ssh/id_rsa.pub or cat ~/.ssh/id_dsa.pub or cat ~/.ssh/id_ed25519.pub\nYou can list all the public keys you have by doing:\n$ ls ~/.ssh/*.pub"
  },
  {
    "question": "SSH Key - Still asking for password and passphrase I've been somewhat 'putting up' with GitHub always asking for my username and password when I clone a repository. I want to bypass this step because it is an annoyance within my workflow.\nI tried setting up an SSH key (which I successfully did) using this guide. https://help.github.com/articles/generating-ssh-keys and I was successful.\nMy problem is that I am still asked for my GitHub password and passphrase when cloning a repository (using SSH). My understanding was that after I set up this SSH key, I would no longer have to do that.\nI want to be able to clone repositories without having to put in my GitHub information all the time.\nWhat am I missing with my SSH key? If anyone can provide some guidance or resources I would appreciate it, because I've always felt a little lost when it came to SSH authentication in GitHub.\nFrom my knowledge, this is a command that tests if things are working properly, here are the output from my console:\n\n```\n~ $ ssh -T git@github.com\nSaving password to keychain failed\nEnter passphrase for key '/Users/MYNAME/.ssh/id_rsa':\nHi MYNAME! You've successfully authenticated, but GitHub does not provide shell access.\n\n```\n\nWhen I input my password, should that fail first? Then, when I enter my passphrase, it passes.",
    "answer": "git remote set-url origin git@github.com:USERNAME/REPOSITORY.git\n\nIf you work with HTTPs urls, it'll always ask for your username / password. This could be solved using @Manavalan Gajapathy's comment (copying here):\nSee this github doc to convert remote's URL from https to ssh. To check if remote's URL is ssh or https, use git remote -v. To switch from https to ssh:\n\nIf you're correctly using SSH when cloning / setting remotes: make sure you have a ssh-agent to remember your password (see this answer by @Komu). That way, you'll only enter your passphrase once by terminal session.\nIf it is still too annoying, then simply set a ssh-key without passphrase."
  },
  {
    "question": "How to download a file from server using SSH? I need to download a file from server to my desktop. (UBUNTU 10.04) I don't have a web access to the server, just ssh.\nIf it helps, my OS is Mac OS X and iTerm 2 as a terminal.",
    "answer": "scp your_username@remotehost.edu:foobar.txt /local/dir\nscp -i key_file.pem your_username@remotehost.edu:/remote/dir/foobar.txt /local/dir\n\nIn your terminal, type:\n\nreplacing the username, host, remote filename, and local directory as appropriate.\nIf you want to access EC2 (or other service that requires authenticating with a private key), use the -i option:\n\nFrom: http://www.hypexr.org/linux_scp_help.php"
  },
  {
    "question": "SSH Key: \u201cPermissions 0644 for &#39;id_rsa.pub&#39; are too open.\u201d on mac I generate a ssh key pair on my mac and add the public key to my ubuntu server(in fact, it is a virtual machine on my mac),but when I try to login the ubuntu server,it says:\n\n```\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@         WARNING: UNPROTECTED PRIVATE KEY FILE!          @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\nPermissions 0644 for '/Users/tudouya/.ssh/vm/vm_id_rsa.pub' are too open.\nIt is required that your private key files are NOT accessible by others.\nThis private key will be ignored.\nbad permissions: ignore key: /Users/tudouya/.ssh/vm/vm_id_rsa.pub\nPermission denied (publickey,password).\n\n```\n\nI have tried many ways to solve this, change the key file mode, change the folder mode,as some answer on stackoverflow,but it doesn't work.\nthe key file permission:\n\n```\nvm dir:\ndrwxr-xr-x   4 tudouya  staff    136  4 29 10:37 vm\n\nkey file:\n-rw-------  1 tudouya  staff  1679  4 29 10:30 vm_id_rsa\n-rw-r--r--  1 tudouya  staff   391  4 29 10:30 vm_id_rsa.pub\n\n```\n\nplease give me some idea...\n=========================================\nI write the host infomation to ssh_config:\n\n```\nHost ubuntuvm\n    Hostname 10.211.55.17\n    PreferredAuthentications publickey\n    IdentityFile /Users/tudouya/.ssh/vm/vm_id_rsa.pub\n\n```\n\nI run command \"ssh -v ubuntuvm\",it displays:\n\n```\nssh -v ubuntuvm\nOpenSSH_6.2p2, OSSLShim 0.9.8r 8 Dec 2011\ndebug1: Reading configuration data /etc/ssh_config\ndebug1: /etc/ssh_config line 20: Applying options for *\ndebug1: /etc/ssh_config line 103: Applying options for *\ndebug1: /etc/ssh_config line 175: Applying options for ubuntuvm\ndebug1: Connecting to 10.211.55.17 [10.211.55.17] port 22.\ndebug1: Connection established.\ndebug1: identity file /Users/tudouya/.ssh/vm/vm_id_rsa.pub type 1\ndebug1: identity file /Users/tudouya/.ssh/vm/vm_id_rsa.pub-cert type -1\ndebug1: Enabling compatibility mode for protocol 2.0\ndebug1: Local version string SSH-2.0-OpenSSH_6.2\ndebug1: Remote protocol version 2.0, remote software version OpenSSH_6.6.1p1 Ubuntu-8\ndebug1: match: OpenSSH_6.6.1p1 Ubuntu-8 pat OpenSSH*\ndebug1: SSH2_MSG_KEXINIT sent\ndebug1: SSH2_MSG_KEXINIT received\ndebug1: kex: server->client aes128-ctr hmac-md5-etm@openssh.com none\ndebug1: kex: client->server aes128-ctr hmac-md5-etm@openssh.com none\ndebug1: SSH2_MSG_KEX_DH_GEX_REQUEST(1024<1024<8192) sent\ndebug1: expecting SSH2_MSG_KEX_DH_GEX_GROUP\ndebug1: SSH2_MSG_KEX_DH_GEX_INIT sent\ndebug1: expecting SSH2_MSG_KEX_DH_GEX_REPLY\ndebug1: Server host key: RSA 55:6d:4f:0f:23:51:ac:8e:70:01:ec:0e:62:9e:1c:10\ndebug1: Host '10.211.55.17' is known and matches the RSA host key.\ndebug1: Found key in /Users/tudouya/.ssh/known_hosts:54\ndebug1: ssh_rsa_verify: signature correct\ndebug1: SSH2_MSG_NEWKEYS sent\ndebug1: expecting SSH2_MSG_NEWKEYS\ndebug1: SSH2_MSG_NEWKEYS received\ndebug1: Roaming not allowed by server\ndebug1: SSH2_MSG_SERVICE_REQUEST sent\ndebug1: SSH2_MSG_SERVICE_ACCEPT received\ndebug1: Authentications that can continue: publickey,password\ndebug1: Next authentication method: publickey\ndebug1: Offering RSA public key: /Users/tudouya/.ssh/vm/vm_id_rsa.pub\ndebug1: Server accepts key: pkalg ssh-rsa blen 279\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@         WARNING: UNPROTECTED PRIVATE KEY FILE!          @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\nPermissions 0644 for '/Users/tudouya/.ssh/vm/vm_id_rsa.pub' are too open.\nIt is required that your private key files are NOT accessible by others.\nThis private key will be ignored.\nbad permissions: ignore key: /Users/tudouya/.ssh/vm/vm_id_rsa.pub\ndebug1: No more authentication methods to try.\nPermission denied (publickey,password).\n\n```",
    "answer": "debug1: identity file /Users/tudouya/.ssh/vm/vm_id_rsa.pub type 1\nIdentityFile .../.ssh/vm/vm_id_rsa.pub\nIdentityFile .../.ssh/vm/vm_id_rsa\n\nIt appears that you're trying to use the wrong key file. The file with the \".pub\" extension contains the public portion of the key. The corresponding file without the \".pub\" extension contains the private part of the key. When you run an ssh client to connect to a remote server, you have to provide the private key file to the ssh client.\nYou probably have a line in the your .ssh/config file (or /etc/ssh_config) which looks like this:\n\nYou need to remove the \".pub\" extension from the filename:"
  },
  {
    "question": "How to permanently add a private key with ssh-add on Ubuntu? I have a private key protected with a password to access a server via SSH. \nI have 2 linux (ubuntu 10.04) machines and the behavior of ssh-add command is different in both of them. \nIn one machine, once I use \"ssh-add .ssh/identity\" and entered my password, the key was added permanently, i.e., every time I shutdown the computer and login again, the key is already added.\nIn the other one, I have to add the key every time I login. \nAs far as I remember, I did the same thing on both. The only difference is that the key was created on the one that is added permanently.\nDoes anyone know how to add it permanently to the other machine as well?",
    "answer": "Host *\nIdentityFile ~/.ssh/gitHubKey\nIdentityFile ~/.ssh/id_rsa_buhlServer\nnano ~/.ssh/config\nHost github.com\nUser git\nIdentityFile ~/.ssh/githubKey\n\nA solution would be to force the key files to be kept permanently, by adding them in your ~/.ssh/config file:\n\nIf you do not have a 'config' file in the ~/.ssh directory, then you should create one. It does not need root rights, so simply:\n\n...and enter the lines above as per your requirements.\nFor this to work the file needs to have chmod 600. You can use the command chmod 600 ~/.ssh/config.\nIf you want all users on the computer to use the key put these lines into /etc/ssh/ssh_config and the key in a folder accessible to all.\nAdditionally if you want to set the key specific to one host, you can do the following in your ~/.ssh/config :\n\nThis has the advantage when you have many identities that a server doesn't reject you because you tried the wrong identities first. Only the specific identity will be tried."
  },
  {
    "question": "Why are connections to GitHub over SSH throwing an error &quot;Warning: Remote Host Identification Has Changed&quot;? Just sometime ago I started getting this warning when pushing to GitHub.\n\n```\nWARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!\n\nIT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!\nSomeone could be eavesdropping on you right now (man-in-the-middle attack)!\nIt is also possible that a host key has just been changed.\n\n```\n\nIs this normal and how do I resolve it?",
    "answer": "ssh-keygen -R github.com\n\nThis happened because on the 24th of March 2023, GitHub updated their RSA SSH host key used to secure Git operations for GitHub.com because the private key was briefly exposed in a public GitHub repository.  You will get that message if you had remembered GitHub\u2019s previous key fingerprint in your SSH client before that date.\nAs per per the linked blog post, the solution is to remove the old key by running this command:\n\nNow the next git connection (pull, push or clone) should ask if you trust the new SSH key.  Before entering yes, ensure the shown new key is valid, using the list:\nhttps://docs.github.com/en/authentication/keeping-your-account-and-data-secure/githubs-ssh-key-fingerprints\nRefer to the blog post for other ways to fix the issue."
  },
  {
    "question": "How to SSH to a VirtualBox guest externally through a host? I have a Ubuntu VM running on my Windows 7 machine. How do I set it up so that I can access the webserver externally through SSH?\nI found steps (Setup SSH access between VirtualBox Host and Guest VMs) to be able to ssh to my guest from my host, but that still leaves me with the problem of accessing it through my router.\nI suppose that I could install an SSH server on my Windows machine and then tunnel a few times (though I'm not 100% sure what to use in terms of local, dynamic, etc. or how to set up multiple tunnels?), but is there a way to make the VM directly accessible to my router so I could directly port forward to it?",
    "answer": "VBoxManage modifyvm myserver --natpf1 \"ssh,tcp,,3022,,22\"\nVBoxManage showvminfo myserver | grep 'Rule'\nsudo apt-get install openssh-server\nssh -p 3022 user@127.0.0.1\n\nThe best way to login to a guest Linux VirtualBox VM is port forwarding. By default, you should have one interface already which is using NAT. Then go to the Network settings and click the Port Forwarding button.  Add a new Rule. As the rule name, insert \"ssh\". As \"Host port\", insert 3022. As \"Guest port\", insert 22. Everything else of the rule can be left blank.\nor from the command line\n\nwhere 'myserver' is the name of the created VM. Check the added rules:\n\nThat's all! Please be sure you don't forget to install an SSH server in the VM:\n\nTo SSH into the guest VM, write:\n\nWhere user is your username within the VM."
  },
  {
    "question": "Git error: &quot;Host Key Verification Failed&quot; when connecting to remote repository I am trying to connect to a remote Git repository that resides on my web server and clone it to my machine.\nI am using the following format for my command:\n\n```\ngit clone ssh://username@domain.example/repository.git\n\n```\n\nThis has worked fine for most of my team members. Usually after running this command Git will prompt for the user's password, and then run the cloning. However, when running on one of my machines I get the following error:\n\nHost key verification failed.\nfatal: Could not read from remote\nrepository.\n\nWe are not using SSH keys to connect to this repository, so I'm not sure why Git is checking for one on this particular machine.",
    "answer": "ssh-keygen -R domain.example\nssh-keyscan -t rsa domain.example >> ~/.ssh/known_hosts\nThe authenticity of host 'domain.example (a.b.c.d)' can't be established.\nRSA key fingerprint is XX:XX:...:XX.\nAre you sure you want to continue connecting (yes/no)?\n\nYou are connecting via the SSH protocol, as indicated by the ssh:// prefix on your clone URL. Using SSH, every host has a key. Clients remember the host key associated with a particular address and refuse to connect if a host key appears to change. This prevents man in the middle attacks.\nThe host key for domain.example has changed. If this does not seem fishy to you, remove the old key from your local cache by editing ${HOME}/.ssh/known_hosts to remove the line for domain.example or letting an SSH utility do it for you with\n\nFrom here, record the updated key either by doing it yourself with\n\nor, equivalently, let ssh do it for you next time you connect with git fetch, git pull, or git push (or even a plain ol\u2019 ssh domain.example) by answering yes when prompted\n\nThe reason for this prompt is domain.example is no longer in your known_hosts after deleting it and presumably not in the system\u2019s /etc/ssh/ssh_known_hosts, so ssh has no way to know whether the host on the other end of the connection is really domain.example. (If the wrong key is in /etc, someone with administrative privileges will have to update the system-wide file.)\nI strongly encourage you to consider having users authenticate with keys as well. That way, ssh-agent can store key material for convenience (rather than everyone having to enter her password for each connection to the server), and passwords do not go over the network."
  },
  {
    "question": "Is it possible to specify a different ssh port when using rsync? I have been attempting the following command:  \n\n```\nrsync -rvz --progress --remove-sent-files ./dir user@host:2222/path\n\n```\n\nSSH is running on port 2222, but rsync still tries to use port 22 and then complains about not finding the path, cause of course it does not exist.  \nI would like to know if it is possible to rsync to a remote host on a non-standard ssh port.",
    "answer": "cat ~/.ssh/config\nHost host\nPort 2222\nrsync -rvz --progress --remove-sent-files ./dir user@host:/path\n\nAnother option, in the host you run rsync from, set the port in the ssh config file, ie:\n\nThen rsync over ssh will talk to port 2222:"
  },
  {
    "question": "Specify an SSH key for git push for a given domain I have the following use case: I would like to be able to push to git@git.company.com:gitolite-admin using the private key of user gitolite-admin, while I want to push to git@git.company.com:some_repo using 'my own' private key. AFAIK, I can't solve this using ~/.ssh/config, because the user name and server name are identical in both cases. As I mostly use my own private key, I have that defined in ~/.ssh/config for git@git.company.com. Does anyone know of a way to override the key that is used for a single git invocation?\n(Aside: gitolite distinguishes who is doing the pushing based on the key, so it's not a problem, in terms of access, ownership and auditing, that the user@server string is identical for different users.)",
    "answer": "Host gitolite-as-alice\nHostName git.company.com\nUser git\nIdentityFile /home/whoever/.ssh/id_rsa.alice\nIdentitiesOnly yes\n\nHost gitolite-as-bob\nHostName git.company.com\nUser git\nIdentityFile /home/whoever/.ssh/id_dsa.bob\nIdentitiesOnly yes\ngit remote add alice git@gitolite-as-alice:whatever.git\ngit remote add bob git@gitolite-as-bob:whatever.git\n\nEven if the user and host are the same, they can still be distinguished in ~/.ssh/config.  For example, if your configuration looks like this:\n\nThen you just use gitolite-as-alice and gitolite-as-bob instead of the hostname in your URL:\n\nNote\nYou want to include the option IdentitiesOnly yes to prevent the use of default ids. Otherwise, if you also have id files matching the default names, they will get tried first because unlike other config options (which abide by \"first in wins\") the IdentityFile option appends to the list of identities to try. See: https://serverfault.com/questions/450796/how-could-i-stop-ssh-offering-a-wrong-key/450807#450807"
  },
  {
    "question": "Adding a public key to ~/.ssh/authorized_keys does not log me in automatically I added the public SSH key to the authorized_keys file.  ssh localhost should log me in without asking for the password.\nI did that and tried typing ssh localhost, but it still asks me to type in the password. Is there another setting that I have to go through to make it work?\nI have followed the instructions for changing permissions:\nBelow is the result if I do ssh -v localhost.\n\n```\ndebug1: Reading configuration data /home/john/.ssh/config\ndebug1: Reading configuration data /etc/ssh/ssh_config\ndebug1: Applying options for *\ndebug1: Connecting to localhost [127.0.0.1] port 22.\ndebug1: Connection established.\ndebug1: identity file /home/john/.ssh/identity type 1\ndebug1: identity file /home/john/.ssh/id_rsa type -1\ndebug1: identity file /home/john/.ssh/id_dsa type -1\ndebug1: Remote protocol version 2.0, remote software version OpenSSH_4.7p1 Debian-8ubuntu3\ndebug1: match: OpenSSH_4.7p1 Debian-8ubuntu3 pat OpenSSH*\ndebug1: Enabling compatibility mode for protocol 2.0\ndebug1: Local version string SSH-2.0-OpenSSH_4.7p1 Debian-8ubuntu3\ndebug1: SSH2_MSG_KEXINIT sent\ndebug1: SSH2_MSG_KEXINIT received\ndebug1: kex: server->client aes128-cbc hmac-md5 none\ndebug1: kex: client->server aes128-cbc hmac-md5 none\ndebug1: SSH2_MSG_KEX_DH_GEX_REQUEST(1024<1024<8192) sent\ndebug1: expecting SSH2_MSG_KEX_DH_GEX_GROUP\ndebug1: SSH2_MSG_KEX_DH_GEX_INIT sent\ndebug1: expecting SSH2_MSG_KEX_DH_GEX_REPLY\ndebug1: Host 'localhost' is known and matches the RSA host key.\ndebug1: Found key in /home/john/.ssh/known_hosts:12\ndebug1: ssh_rsa_verify: signature correct\ndebug1: SSH2_MSG_NEWKEYS sent\ndebug1: expecting SSH2_MSG_NEWKEYS\ndebug1: SSH2_MSG_NEWKEYS received\ndebug1: SSH2_MSG_SERVICE_REQUEST sent\ndebug1: SSH2_MSG_SERVICE_ACCEPT received\ndebug1: Authentications that can continue: publickey,password\ndebug1: Next authentication method: publickey\ndebug1: Offering public key: /home/john/.ssh/identity\ndebug1: Server accepts key: pkalg ssh-rsa blen 149\ndebug1: PEM_read_PrivateKey failed\ndebug1: read PEM private key done: type <unknown>\n\n```\n\nThen it asks for a passphase after the above log. Why isn't it logging me in without a password?",
    "answer": "chmod 700 ~/.ssh\nchmod 600 ~/.ssh/authorized_keys\nchmod go-w ~\n\nYou need to verify the permissions of the authorized_keys file and the folder / parent folders in which it is located.\n\nFor more information see this page.\nYou may also need to change/verify the permissions of your home directory to remove write access for the group and others."
  },
  {
    "question": "Pseudo-terminal will not be allocated because stdin is not a terminal I am trying to write a shell script that creates some directories on a remote server and then uses scp to copy files from my local machine onto the remote. Here's what I have so far:\n\n```\nssh -t user@server<<EOT\nDEP_ROOT='/home/matthewr/releases'\ndatestamp=$(date +%Y%m%d%H%M%S)\nREL_DIR=$DEP_ROOT\"/\"$datestamp\nif [ ! -d \"$DEP_ROOT\" ]; then\n    echo \"creating the root directory\"\n    mkdir $DEP_ROOT\nfi\nmkdir $REL_DIR\nexit\nEOT\n\nscp ./dir1 user@server:$REL_DIR\nscp ./dir2 user@server:$REL_DIR\n\n```\n\nWhenever I run it I get this message:\n\n```\nPseudo-terminal will not be allocated because stdin is not a terminal.\n\n```\n\nAnd the script just hangs forever.\nMy public key is trusted on the server and I can run all the commands outside of the script just fine. Any ideas?",
    "answer": "-T      Disable pseudo-tty allocation.\n\n-t      Force pseudo-tty allocation.  This can be used to execute arbitrary\nscreen-based programs on a remote machine, which can be very useful,\ne.g. when implementing menu services.  Multiple -t options force tty\nallocation, even if ssh has no local tty.\n\nTry ssh -t -t(or ssh -tt for short) to force pseudo-tty allocation even if stdin isn't a terminal.\nSee also: Terminating SSH session executed by bash script\nFrom ssh manpage:"
  },
  {
    "question": "Starting ssh-agent on Windows 10 fails: &quot;unable to start ssh-agent service, error :1058&quot; When I try to start the ssh-agent on Windows 10 via PowerShell (with elevated right or without) by entering Start-Service ssh-agent I get the error\n\nunable to start ssh-agent service, error :1058\n\nWhen I check of the service is running via Get-Service ssh-agent is returns that the service is stopped.\nHow can I get the ssh-agent running?",
    "answer": "I solved the problem by changing the StartupType of the ssh-agent to Manual via Set-Service ssh-agent -StartupType Manual.\nThen I was able to start the service via Start-Service ssh-agent or just ssh-agent.exe."
  },
  {
    "question": "scp (secure copy) to ec2 instance without password I have an EC2 instance running (FreeBSD 9 AMI ami-8cce3fe5), and I can ssh into it using my amazon-created key file without password prompt, no problem.\nHowever, when I want to copy a file to the instance using scp I am asked to enter a password:\n\n```\nscp somefile.txt -i mykey.pem root@my.ec2.id.amazonaws.com:/\n\nPassword:\n\n```\n\nAny ideas why this is happening/how it can be prevented?",
    "answer": "scp -i mykey.pem somefile.txt root@my.ec2.id.amazonaws.com:/\n\nI figured it out.  I had the arguments in the wrong order.  This works:"
  },
  {
    "question": ".bashrc at ssh login When I ssh into my ubuntu-box running Hardy 8.04, the environment variables in my .bashrc are not set.\nIf I do a source .bashrc, the variables are properly set, and all is well.\nHow come .bashrc isn't run at login?",
    "answer": "if [ -f ~/.bashrc ]; then\n. ~/.bashrc\nfi\n\n.bashrc is not sourced when you log in using SSH. You need to source it in your .bash_profile like this:"
  },
  {
    "question": "What is the cleanest way to ssh and run multiple commands in Bash? I already have an ssh agent set up, and I can run commands on an external server in Bash script doing stuff like:\n\n```\nssh blah_server \"ls; pwd;\"\n\n```\n\nNow, what I'd really like to do is run a lot of long commands on an external server. Enclosing all of these in between quotation marks would be quite ugly, and I'd really rather avoid ssh'ing multiple times just to avoid this.\nSo, is there a way I can do this in one go enclosed in parentheses or something? I'm looking for something along the lines of:\n\n```\nssh blah_server (\n   ls some_folder;\n   ./someaction.sh;\n   pwd;\n)\n\n```\n\nBasically, I'll be happy with any solution as long as it's clean.\nEdit\nTo clarify, I'm talking about this being part of a larger bash script. Other people might need to deal with the script down the line, so I'd like to keep it clean. I don't want to have a bash script with one line that looks like:\n\n```\nssh blah_server \"ls some_folder; ./someaction.sh 'some params'; pwd; ./some_other_action 'other params';\"\n\n```\n\nbecause it is extremely ugly and difficult to read.",
    "answer": "ssh otherhost << EOF\nls some_folder;\n./someaction.sh 'some params'\npwd\n./some_other_action 'other params'\nEOF\nssh otherhost /bin/bash << EOF\nssh otherhost /bin/bash << EOF\ntouch \"/tmp/${NAME}\"\nEOF\n\nHow about a Bash Here Document:\n\nTo avoid the problems mentioned by @Globalz in the comments, you may be able to (depending what you're doing on the remote site) get away with replacing the first line with\n\nNote that you can do variable substitution in the Here document, but you may have to deal with quoting issues. For instance, if you quote the \"limit string\" (ie. EOF in the above), then you can't do variable substitutions. But without quoting the limit string, variables are substituted. For example, if you have defined $NAME above in your shell script, you could do\n\nand it would create a file on the destination otherhost with the name of whatever you'd assigned to $NAME. Other rules about shell script quoting also apply, but are too complicated to go into here."
  },
  {
    "question": "git clone with HTTPS or SSH remote? git clone supports both HTTPS and SSH remote URLs. Which should I use? What are the advantages of each?\nGitHub's docs don't make a recommendation either way. I recall in 2013 GitHub used to recommend SSH (archive link). Why was that?",
    "answer": "GitHub have changed their recommendation several times (example).\nIt appears that they currently recommend HTTPS because it is the easiest to set up on the widest range of networks and platforms, and by users who are new to all this.\nThere is no inherent flaw in SSH (if there was they would disable it) -- in the links below, you will see that they still provide details about SSH connections too:\n\nHTTPS is less likely to be blocked by a firewall.\nhttps://docs.github.com/en/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls\n\nThe https:// clone URLs are available on all repositories, regardless of visibility. https:// clone URLs work even if you are behind a firewall or proxy.\n\nAn HTTPS connection allows credential.helper to cache your password.\nhttps://docs.github.com/en/get-started/quickstart/set-up-git#connecting-over-https-recommended\n\nIf you clone with HTTPS, you can cache your GitHub credentials in Git using a credential helper. For more information, see \"Cloning with HTTPS urls\" and \"Caching your GitHub credentials in Git.\""
  },
  {
    "question": "How do I verify/check/test/validate my SSH passphrase? I think I forgot the passphrase for my SSH key, but I have a hunch what it might be.  How do I check if I'm right?",
    "answer": "You can verify your SSH key passphrase by attempting to load it into your SSH agent. With OpenSSH this is done via ssh-add.\nOnce you're done, remember to unload your SSH passphrase from the terminal by running ssh-add -d."
  },
  {
    "question": "Copying files using rsync from remote server to local machine Once I've ssh'd into my remote server, what would the command be to copy all files from a directory to a local directory on my machine?",
    "answer": "rsync -chavzP --stats user@remote.host:/path/to/copy /path/to/local/storage\nrsync -chavzP -e \"ssh -p $portNumber\" user@remote.host:/path/to/copy /local/path\nrsync -chavzP --stats /path/to/copy user@host.remoted.from:/path/to/local/storage\n\nFrom your local machine:\n\nFrom your local machine with a non standard ssh port:\n\nOr from the remote host, assuming you really want to work this way and your local machine is listening on SSH:\n\nSee man rsync for an explanation of my usual switches."
  },
  {
    "question": "How to fix &quot;ssh: connect to host github.com port 22: Connection timed out&quot; for git push/pull/... commands? I am under a proxy and I am pushing in to git successfully for quite a while.\nNow I am not able to push into git all of a sudden.\nI have set the RSA key and the proxy and double checked them, with no avail and git is throwing me the error shown in the title of the page.",
    "answer": "ssh -T git@github.com\ngit config --local -e\nurl = git@github.com:username/repo.git\nurl = https://github.com/username/repo.git\n\nThe reason could be the firewall modification since you are on a network, in which case they may deliberately block some ports.\nTo double check if this is the reason, run:\n\nThis should timeout.  If that's the case, then use the http protocol instead of ssh.  Change your url in the config file to http:\n\nand change:\n\nto"
  },
  {
    "question": "Start ssh-agent on login I have a site as a remote Git repo pulling from Bitbucket.com using an SSH alias. I can manually start the ssh-agent on my server but I have to do this every time I login via SSH.\nI manually start the ssh-agent:\n\n```\neval ssh-agent $SHELL\n\n```\n\nThen I add the agent:\n\n```\nssh-add ~/.ssh/bitbucket_id\n\n```\n\nThen it shows up when I do:\n\n```\nssh-add -l\n\n```\n\nAnd I'm good to go. Is there any way to automate this process so I don't have to do it every time I login? The server is running RedHat 6.2 (Santiago).",
    "answer": "SSH_ENV=\"$HOME/.ssh/agent-environment\"\n\nfunction start_agent {\necho \"Initialising new SSH agent...\"\n/usr/bin/ssh-agent | sed 's/^echo/#echo/' >\"$SSH_ENV\"\necho succeeded\nchmod 600 \"$SSH_ENV\"\n. \"$SSH_ENV\" >/dev/null\n/usr/bin/ssh-add;\n}\n\nif [ -f \"$SSH_ENV\" ]; then\n. \"$SSH_ENV\" >/dev/null\n\nps -ef | grep $SSH_AGENT_PID | grep ssh-agent$ >/dev/null || {\nstart_agent\n}\nelse\nstart_agent\nfi\n\nPlease go through this article. You may find this very useful:\nhttps://web.archive.org/web/20210506080335/https://mah.everybody.org/docs/ssh\nJust in case the above link vanishes some day, I am capturing the main piece of the solution below:\n\nThis solution from Joseph M. Reagle by way of Daniel Starin:\nAdd this following to your .bash_profile\n\nThis version is especially nice since it will see if you've already started ssh-agent  and, if it can't find it, will start it up and store the settings so that they'll be usable the next time you start up a shell."
  },
  {
    "question": "git remote add with other SSH port In Git, how can I add a remote origin server when my host uses a different SSH port?  \n\n```\ngit remote add origin ssh://user@host/srv/git/example\n\n```",
    "answer": "git remote add origin ssh://user@host:1234/srv/git/example\n\nYou can just do this:\n\n1234 is the ssh port being used"
  },
  {
    "question": "Vagrant stuck connection timeout retrying My vagrant was working perfectly fine last night. I've just turned the PC on, hit vagrant up, and this is what I get:\n\n```\n==> default: Clearing any previously set network interfaces...\n==> default: Preparing network interfaces based on configuration...\n    default: Adapter 1: nat\n    default: Adapter 2: hostonly\n==> default: Forwarding ports...\n    default: 22 => 2222 (adapter 1)\n==> default: Booting VM...\n==> default: Waiting for machine to boot. This may take a few minutes...\n    default: SSH address: 127.0.0.1:2222\n    default: SSH username: vagrant\n    default: SSH auth method: private key\n    default: Error: Connection timeout. Retrying...\n    default: Error: Connection timeout. Retrying...\n    default: Error: Connection timeout. Retrying...\n    default: Error: Connection timeout. Retrying...\n    default: Error: Connection timeout. Retrying...\n    default: Error: Connection timeout. Retrying...\n    default: Error: Connection timeout. Retrying...\n    default: Error: Connection timeout. Retrying...\n    default: Error: Connection timeout. Retrying...\n    default: Error: Connection timeout. Retrying...\n    default: Error: Connection timeout. Retrying...\n    default: Error: Connection timeout. Retrying...\n    default: Error: Connection timeout. Retrying...\n\n```\n\nHas anyone had this before? vagrant isn't widely covered on the web yet and I can't find a reason why this is occurring.",
    "answer": "config.vm.provider :virtualbox do |vb|\nvb.gui = true\nend\n\nI solved this problem, and will answer in case anyone else has a similar issue.\nWhat I did was: I enabled the GUI of Virtual box to see that it was waiting for input on startup to select whether I wanted to boot directly to ubuntu or safemode etc.\nTo turn on the GUI you have to put this in your vagrant config Vagrantfile:"
  },
  {
    "question": "Is it possible to create a remote repo on GitHub from the CLI without opening browser? I created a new local Git repository:\n\n```\n~$ mkdir projectname\n~$ cd projectname\n~$ git init\n~$ touch file1\n~$ git add file1\n~$ git commit -m 'first commit'\n\n```\n\nIs there any git command to create a new remote repo and push my commit to GitHub from here? I know it's no big deal to just fire up a browser and head over to Create a New Repository, but if there is a way to achieve this from the CLI I would be happy.\nI read a vast amount of articles but none that I found mention how to create a remote repo from the CLI using git commands. Tim Lucas's nice article Setting up a new remote git repository is the closest I found, but GitHub does not provide shell access.",
    "answer": "gh repo create\nmkdir project\ncd project\ngit init\ntouch file\ngit add file\ngit commit -m 'Initial commit'\ngh repo create\ngit push -u origin master\n\nWith Github's official new command line interface:\n\nSee additional details and options and installation instructions.\n\nFor instance, to complete your git workflow:"
  },
  {
    "question": "How do I get git to default to ssh and not https for new repositories These days when I create a new repository on GitHub on the setup page I get:\n\n```\ngit remote add origin https://github.com/nikhilbhardwaj/abc.git\ngit push -u origin master\n\n```\n\nAnd whenever I have to push a commit I need to enter my GitHub username and password.\nI can manually change that to \n\n```\ngit@github.com:nikhilbhardwaj/abc.git\n\n```\n\nin the .git/config. I find this quite irritating - is there some way I can configure git to use SSH by default?",
    "answer": "git remote add origin git@github.com:nikhilbhardwaj/abc.git\n[remote \"origin\"]\nfetch = +refs/heads/*:refs/remotes/origin/*\n-url = https://github.com/nikhilbhardwaj/abc.git\n+url = git@github.com:nikhilbhardwaj/abc.git\ngit remote set-url origin git@github.com:nikhilbhardwaj/abc.git\n\nSet up a repository's origin branch to be SSH\nThe GitHub repository setup page is just a suggested list of commands (and GitHub now suggests using the HTTPS protocol). Unless you have administrative access to GitHub's site, I don't know of any way to change their suggested commands.\nIf you'd rather use the SSH protocol, simply add a remote branch like so (i.e. use this command in place of GitHub's suggested command). To modify an existing branch, see the next section.\n\nModify a pre-existing repository\nAs you already know,  to switch a pre-existing repository to use SSH instead of HTTPS, you can change the remote url within your .git/config file.\n\nA shortcut is to use the set-url command:\n\nMore information about the SSH-HTTPS switch\n\n\"Why is Git always asking for my password?\" - GitHub help page.\nGitHub's switch to Smart HTTP - relevant StackOverflow question\nCredential Caching for Wrist-Friendly Git Usage - GitHub blog post about HTTPS, and how to avoid re-entering your password"
  },
  {
    "question": "Getting ssh to execute a command in the background on target machine This is a follow-on question to the How do you use ssh in a shell script? question.  If I want to execute a command on the remote machine that runs in the background on that machine, how do I get the ssh command to return?  When I try to just include the ampersand (&) at the end of the command it just hangs.  The exact form of the command looks like this:\n\n```\nssh user@target \"cd /some/directory; program-to-execute &\"\n\n```\n\nAny ideas?  One thing to note is that logins to the target machine always produce a text banner and I have SSH keys set up so no password is required.",
    "answer": "nohup myprogram > foo.log 2> foo.err < /dev/null &\nnohup myscript.sh >myscript.log 2>&1 </dev/null &\n\nThis should solve your problem:\n\nThe syntax and unusual use of < /dev/null are explained especially well in this answer, quoted here for your convenience.\n\n< /dev/null is used to instantly send EOF to the program, so that it\ndoesn't wait for input (/dev/null, the null device, is a special file\nthat discards all data written to it, but reports that the write\noperation succeeded, and provides no data to any process that reads\nfrom it, yielding EOF immediately).\nSo the command:\n\nwill move to background the command, outputting both stdout and\nstderr to myscript.log without waiting for any input.\n\nSee also the wikipedia artcle on nohup, also quoted here for your convenience.\n\nNohuping backgrounded jobs is for\nexample useful when logged in via SSH,\nsince backgrounded jobs can cause the\nshell to hang on logout due to a race\ncondition. This problem can also\nbe overcome by redirecting all three\nI/O streams."
  },
  {
    "question": "Multiple GitHub Accounts &amp; SSH Config I'm having some trouble getting two different SSH keys/GitHub accounts to play well together.  I have the following setup:\nRepos accessible from one account using git@github.com:accountname\nRepos accessible from another account using git@github.com:anotheraccount\nEach account has its own SSH key.  Both SSH keys have been added and I have created a config file.  I don't believe the config file is correct though.  I'm not quite sure how to specify that repos accessed using git@github.com:accountname should use id_rsa and git@github.com:anotheraccount should use id_rsa_anotheraccount.",
    "answer": "Host me.github.com\nHostName github.com\nPreferredAuthentications publickey\nIdentityFile ~/.ssh/me_rsa\n\nHost work.github.com\nHostName github.com\nPreferredAuthentications publickey\nIdentityFile ~/.ssh/work_rsa\nGIT_SSH_COMMAND='ssh -v' git pull --verbose\n\nAndy Lester's response is accurate but I found an important extra step I needed to make to get this to work. In trying to get two profiles set up, one for personal and one for work, my ~/.ssh/config was roughly as follows:\n\nMy work profile didn't take until I did a ssh-add ~/.ssh/work_rsa. After that connections to github used the correct profile. Previously they defaulted to the first public key.\n\nIf you encounter problems, they are likely from using the wrong account. You can troubleshoot things from the command line with a command like this (use ssh -vvv for max verbosity):\n\nFor Could not open a connection to your authentication agent when using ssh-add,check https://stackoverflow.com/a/17695338/1760313\nFor  defaulted to the first public key, add IdentitiesOnly to the Host * section at the bottom of your ~/.ssh/config file. This tells ssh to only use specified identities rather than all unlocked keys in your agent.\nIf your Host * section has an IdentityFile line, this will not work. Remove that line and add it to a new Host !work.github.com * secion.\nIf you use ControlMaster and you try to use different Github accounts before the ControlPersist timeout expires, it will use the persisted connection, which may be for the other account. Change your ControlPath to include %k (the host key alias if specified, otherwise this is identical to %h, the hostname). If you prefer, you can limit this to just one Github account's path with e.g. ControlPath ~/.ssh/control-%C-2 solely in the Host work.github.com stanza."
  },
  {
    "question": "How can I ssh directly to a particular directory? I often have to login to one of several servers and go to one of several directories on those machines.  Currently I do something of this sort:\n\n```\n\nlocalhost ~]$ ssh somehost\n\nWelcome to somehost!\n\nsomehost ~]$ cd /some/directory/somewhere/named/Foo\nsomehost Foo]$ \n\n```\n\nI have scripts that can determine which host and which directory I need to get into but I cannot figure out a way to do this:\n\n```\n\nlocalhost ~]$ go_to_dir Foo\n\nWelcome to somehost!\n\nsomehost Foo]$\n\n```\n\nIs there an easy, clever or any way to do this?",
    "answer": "ssh -t xxx.xxx.xxx.xxx \"cd /directory_wanted ; bash --login\"\n\nYou can do the following:\n\nThis way, you will get a login shell right on the directory_wanted.\n\nExplanation\n\n-t Force pseudo-terminal allocation.  This can be used to execute arbitrary screen-based programs on a remote machine, which can be very useful, e.g. when implementing menu services.\nMultiple -t options force tty allocation, even if ssh has no local tty.\n\nIf you don't use -t then no prompt will appear.\nIf you don't add ; bash then the connection will get closed and return control to your local machine\nIf you don't add bash --login then it will not use your configs because it's not a login shell"
  },
  {
    "question": "How to solve &quot;sign_and_send_pubkey: signing failed: agent refused operation&quot;? Configuring a new Digital Ocean droplet with SSH keys. When I run ssh-copy-id this is what I get:\n\n```\nssh-copy-id user@012.345.67.89\n/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed\n/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys\nsign_and_send_pubkey: signing failed: agent refused operation\nuser@012.345.67.89's password: \n\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   \"ssh 'user@012.345.67.89'\"\nand check to make sure that only the key(s) you wanted were added.\n\n```\n\nHowever, when I then attempt to ssh in, this happens:\n\n```\nssh user@012.345.67.89\nsign_and_send_pubkey: signing failed: agent refused operation\nuser@012.345.67.89's password: \n\n```\n\nUpon entering the password, I am logged in just fine, but this of course defeats the purpose of creating the SSH key in the first place. I decided to take a look at the ssh-agent server-side and here's what I get:\n\n```\nuser@012.345.67.89:~# eval `ssh-agent -s`\nAgent pid 5715\nuser@012.345.67.89:~# ssh-add -l\nThe agent has no identities.\n\n```\n\nuser/.ssh/authorized_keys does contain an ssh-rsa key entry, as well, but find -name \"keynamehere\" returns nothing.",
    "answer": "Run ssh-add on the client machine, that will add the SSH key to the agent.\nConfirm with ssh-add -l (again on the client) that it was indeed added."
  },
  {
    "question": "Permission denied (publickey) when SSH Access to Amazon EC2 instance I want to use my Amazon ec2 instance but faced the following error: \n\n```\nPermission denied (publickey).\n\n```\n\nI have created my key pair and downloaded .pem file.\nGiven:               \n\n```\nchmod  600 pem file.\n\n```\n\nThen, this command\n\n```\nssh -i /home/kashif/serverkey.pem  ubuntu@ec2-54-227-242-179.compute-1.amazonaws.com\n\n```\n\nBut have this error:\n\n```\nPermission denied (publickey)\n\n```\n\nAlso, how can I connect with filezilla to upload/download files?",
    "answer": "This error message means you failed to authenticate.\nThese are common reasons that can cause that:\n\nTrying to connect with the wrong key. Are you sure this instance is using this keypair?\nTrying to connect with the wrong username. ubuntu is the username for the ubuntu based AWS distribution, but on some others it's ec2-user (or admin on some Debians, according to Bogdan Kulbida's answer)(can also be root, fedora, see below)\nTrying to connect the wrong host. Is that the right host you are trying to log in to?\n\nNote that 1. will also happen if you have messed up the /home/<username>/.ssh/authorized_keys file on your EC2 instance.\nAbout 2., the information about which username you should use is often lacking from the AMI Image description. But you can find some in AWS EC2 documentation, bullet point 4. :\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AccessingInstancesLinux.html\n\nUse the ssh command to connect to the instance. You'll specify the private key (.pem) file and user_name@public_dns_name. For Amazon Linux, the user name is ec2-user. For RHEL5, the user name is either root or ec2-user. For Ubuntu, the user name is ubuntu. For Fedora, the user name is either fedora or ec2-user. For SUSE Linux, the user name is root. Otherwise, if ec2-user and root don't work, check with your AMI provider.\n\nFinally, be aware that there are many other reasons why authentication would fail. SSH is usually pretty explicit about what went wrong if you care to add the -v option to your SSH command and read the output, as explained in many other answers to this question."
  },
  {
    "question": "vim: how to delete a newline/linefeed character(s)? Is there a way to delete the newline at the end of a line in Vim, so that the next line is appended to the current line?\nFor example:\n\n```\nEvaluator<T>():\n    _bestPos(){\n}\n\n```\n\nI'd like to put this all on one line without copying lines and pasting them into the previous one.  It seems like I should be able to put my cursor to the end of each line, press a key, and have the next line jump onto the same one the cursor is on.\nEnd result:\n\n```\nEvaluator<T>(): _bestPos(){ }\n\n```\n\nIs this possible in Vim?",
    "answer": "If you are on the first line, pressing (upper case) J will join that line and the next line together, removing the newline. You can also combine this with a count, so pressing 3J will combine all 3 lines together."
  },
  {
    "question": "How to prevent a background process from being stopped after closing SSH client in Linux I'm working on a Linux machine through SSH (Putty). I need to leave a process running during the night, so I thought I could do that by starting the process in background (with an ampersand at the end of the command) and redirecting stdout to a file.\nThat doesn't work. As soon as I close the Putty window, the process is stopped.\nHow can I prevent that?",
    "answer": "Check out the \"nohup\" program."
  },
  {
    "question": "Repository access denied. access via a deployment key is read-only After successfully cloning my repo from heroku and added another remote\n\n```\n1/ git clone git@heroku.com:[APP].git\n2/ git remote add bitbucket ssh://git@bitbucket.org/[ACCOUNT]/[REPO].git\n3/ git push bitbucket master\n\n```\n\nI am still getting this error after running line (3) or using SourceTree\n\n```\nconq: repository access denied. access via a deployment key is read-only.\n\n```\n\nFirst I don't understand what this message means in practice. And that's shame.\nI did create ssh key pair and added to heroku :\n\n```\nssh-keygen -t rsa \nheroku keys:add ./id_rsa.pub \n\n```\n\nI also added my key in deployment keys section in BitBucket. But I must be missing something. This question is not out of laziness, I have been reading various docs including BitBuckets guides. But it still don't get around this issue.\nThis post is related to Can I import my heroku git repo into bitbuket? and how? \nADDITIONAL FACTS:\n\n```\nssh -T hg@bitbucket.org\nconq: authenticated via a deploy key.\n\nYou can use git or hg to connect to Bitbucket. Shell access is disabled.\n\n\n$ ssh -v git@bitbucket.org\nOpenSSH_5.6p1, OpenSSL 0.9.8r 8 Feb 2011\ndebug1: Reading configuration data /Users/joel/.ssh/config\ndebug1: Applying options for bitbucket.org\ndebug1: Reading configuration data /etc/ssh_config\ndebug1: Applying options for *\ndebug1: Connecting to bitbucket.org [207.223.240.181] port 22.\ndebug1: Connection established.\ndebug1: identity file /Users/joel/.ssh/id_rsa type 1\ndebug1: identity file /Users/joel/.ssh/id_rsa-cert type -1\ndebug1: Remote protocol version 2.0, remote software version OpenSSH_5.3\ndebug1: match: OpenSSH_5.3 pat OpenSSH*\ndebug1: Enabling compatibility mode for protocol 2.0\ndebug1: Local version string SSH-2.0-OpenSSH_5.6\ndebug1: SSH2_MSG_KEXINIT sent\ndebug1: SSH2_MSG_KEXINIT received\ndebug1: kex: server->client aes128-ctr hmac-md5 none\ndebug1: kex: client->server aes128-ctr hmac-md5 none\ndebug1: SSH2_MSG_KEX_DH_GEX_REQUEST(1024<1024<8192) sent\ndebug1: expecting SSH2_MSG_KEX_DH_GEX_GROUP\ndebug1: SSH2_MSG_KEX_DH_GEX_INIT sent\ndebug1: expecting SSH2_MSG_KEX_DH_GEX_REPLY\ndebug1: Host 'bitbucket.org' is known and matches the RSA host key.\ndebug1: Found key in /Users/joel/.ssh/known_hosts:5\ndebug1: ssh_rsa_verify: signature correct\ndebug1: SSH2_MSG_NEWKEYS sent\ndebug1: expecting SSH2_MSG_NEWKEYS\ndebug1: SSH2_MSG_NEWKEYS received\ndebug1: Roaming not allowed by server\ndebug1: SSH2_MSG_SERVICE_REQUEST sent\ndebug1: SSH2_MSG_SERVICE_ACCEPT received\ndebug1: Authentications that can continue: publickey\ndebug1: Next authentication method: publickey\ndebug1: Offering RSA public key: /Users/joel/.ssh/id_rsa\ndebug1: Remote: Forced command: conq deploykey:13907\ndebug1: Remote: Port forwarding disabled.\ndebug1: Remote: X11 forwarding disabled.\ndebug1: Remote: Agent forwarding disabled.\ndebug1: Remote: Pty allocation disabled.\ndebug1: Server accepts key: pkalg ssh-rsa blen 279\ndebug1: read PEM private key done: type RSA\ndebug1: Remote: Forced command: conq deploykey:13907\ndebug1: Remote: Port forwarding disabled.\ndebug1: Remote: X11 forwarding disabled.\ndebug1: Remote: Agent forwarding disabled.\ndebug1: Remote: Pty allocation disabled.\ndebug1: Authentication succeeded (publickey).\nAuthenticated to bitbucket.org ([207.223.240.181]:22).\ndebug1: channel 0: new [client-session]\ndebug1: Requesting no-more-sessions@openssh.com\ndebug1: Entering interactive session.\ndebug1: Sending environment.\ndebug1: Sending env LC_CTYPE = UTF-8\nPTY allocation request failed on channel 0\n\n```\n\nLooks like all is fine.",
    "answer": "First confusion on my side was about where exactly to set SSH Keys in BitBucket.\nI am new to BitBucket and I was setting a Deployment Key which gives read-access only.\nSo make sure you are setting your rsa pub key in your BitBucket Account Settings.\nClick your BitBucket avatar and select Bitbucket Settings(Manage account). There you'll be able to set SSH Keys.\nI simply deleted the Deployment Key, I don't need any for now. And it worked"
  },
  {
    "question": "Getting permission denied (public key) on gitlab My problem is that I can't push or fetch from GitLab. However, I can clone (via HTTP or via SSH). I get this error when I try to push :\n\nPermission denied (publickey) fatal : Could not read from remote repository\n\nFrom all the threads I've looked, here is what I have done :\n\nSet up an SSH key on my computer and added the public key to GitLab\nDone the config --global for username and email\nCloned via SSH and via HTTP to check if it would resolve the issue\nDone the ssh -T git@gitlab.com command\n\nIf you have any insight about how to resolve my issue, it would be greatly appreciated.",
    "answer": "I found this after searching a lot. It will work perfectly fine for me.\n\nGo to \"Git Bash\" just like cmd. Right click and \"Run as Administrator\".\nType ssh-keygen\nPress enter.\nIt will ask you to save the key to the specific directory.\nPress enter. It will prompt you to type password or enter without password.\nThe public key will be created to the specific directory.\nNow go to the directory and open .ssh folder.\nYou'll see a file id_rsa.pub. Open it on notepad. Copy all text from it.\nGo to https://gitlab.com/-/profile/keys or\nPaste here in the \"key\" textfield.\nNow click on the \"Title\" below. It will automatically get filled.\nThen click \"Add key\".\n\nNow give it a shot and it will work for sure."
  },
  {
    "question": "&#39;heroku&#39; does not appear to be a git repository When I try to push my app to Heroku I get this response:\n\n```\nfatal: 'heroku' does not appear to be a git repository\nfatal: Could not read from remote repository.\n\nPlease make sure you have the correct access rights\nand the repository exists.\n\n```\n\nI have tried 'heroku keys:add' but still comes up with the same result.\nI already have an ssh key for my GitHub account.",
    "answer": "To add a Heroku app as a Git remote, you need to execute heroku git:remote -a yourapp.\nSource: Deploying with Git"
  },
  {
    "question": "How to set ssh timeout? I'm executing a script connecting via password-less SSH on a remote host. I want to set a timeout, so that if the remote host is taking an infinite time to run, I want to come out of that ssh session and continue other lines in my sh script.\nHow can I set a timeout?",
    "answer": "ssh -o ConnectTimeout=10  <hostName>\n\nWhere 10 is time in seconds.  This Timeout applies only to the creation of the connection."
  },
  {
    "question": "Add Keypair to existing EC2 instance I was given AWS Console access to an account with 2 instances running that I cannot shut down (in production). I would, however, like to gain SSH access to these instances, is it possible to create a new Keypair and apply it to the instances so I can SSH in? Obtaining the existing pem file for the keypair the instances were created under is currently not an option.\nIf this isn't possible is there some other way I can get into the instances?",
    "answer": "instance_a=i-XXXXXXXX\n\nvolume=$(ec2-describe-instances $instance_a |\negrep '^BLOCKDEVICE./dev/sda1' | cut -f3)\ninstance_b=i-YYYYYYYY\nec2-stop-instances $instance_a\nec2-detach-volume $volume\nec2-attach-volume --instance $instance_b --device /dev/sdj $volume\nssh ...instance b...\n\nsudo mkdir -p 000 /vol-a\nsudo mount /dev/sdj /vol-a\nsudo umount /vol-a\nsudo rmdir /vol-a\nec2-detach-volume $volume\nec2-attach-volume --instance $instance_a --device /dev/sda1 $volume\nec2-start-instances $instance_a\n\nYou can't apply a keypair to a running instance. You can only use the new keypair to launch a new instance.\nFor recovery, if it's an EBS boot AMI, you can stop it, make a snapshot of the volume. Create a new volume based on it. And be able to use it back to start the old instance, create a new image, or recover data.\nThough data at ephemeral storage will be lost.\n\nDue to the popularity of this question and answer, I wanted to capture the information in the link that Rodney posted on his comment.\nCredit goes to Eric Hammond for this information.\nFixing Files on the Root EBS Volume of an EC2 Instance\nYou can examine and edit files on the root EBS volume on an EC2 instance even if you are in what you considered a disastrous situation like:\n\nYou lost your ssh key or forgot your password\nYou made a mistake editing the /etc/sudoers file and can no longer\ngain root access with sudo to fix it\nYour long running instance is hung for some reason, cannot be\ncontacted, and fails to boot properly\nYou need to recover files off of the instance but cannot get to it\n\nOn a physical computer sitting at your desk, you could simply boot the system with a CD or USB stick, mount the hard drive, check out and fix the files, then reboot the computer to be back in business.\nA remote EC2 instance, however, seems distant and inaccessible when you are in one of these situations. Fortunately, AWS provides us with the power and flexibility to be able to recover a system like this, provided that we are running EBS boot instances and not instance-store.\nThe approach on EC2 is somewhat similar to the physical solution, but we\u2019re going to move and mount the faulty \u201chard drive\u201d (root EBS volume) to a different instance, fix it, then move it back.\nIn some situations, it might simply be easier to start a new EC2 instance and throw away the bad one, but if you really want to fix your files, here is the approach that has worked for many:\nSetup\nIdentify the original instance (A) and volume that contains the broken root EBS volume with the files you want to view and edit.\n\nIdentify the second EC2 instance (B) that you will use to fix the files on the original EBS volume. This instance must be running in the same availability zone as instance A so that it can have the EBS volume attached to it. If you don\u2019t have an instance already running, start a temporary one.\n\nStop the broken instance A (waiting for it to come to a complete stop), detach the root EBS volume from the instance (waiting for it to be detached), then attach the volume to instance B on an unused device.\n\nssh to instance B and mount the volume so that you can access its file system.\n\nFix It\nAt this point your entire root file system from instance A is available for viewing and editing under /vol-a on instance B. For example, you may want to:\n\nPut the correct ssh keys in /vol-a/home/ubuntu/.ssh/authorized_keys\nEdit and fix /vol-a/etc/sudoers\nLook for error messages in /vol-a/var/log/syslog\nCopy important files out of /vol-a/\u2026\n\nNote: The uids on the two instances may not be identical, so take care if you are creating, editing, or copying files that belong to non-root users. For example, your mysql user on instance A may have the same UID as your postfix user on instance B which could cause problems if you chown files with one name and then move the volume back to A.\nWrap Up\nAfter you are done and you are happy with the files under /vol-a, unmount the file system (still on instance-B):\n\nNow, back on your system with ec2-api-tools, continue moving the EBS volume back to it\u2019s home on the original instance A and start the instance again:\n\nHopefully, you fixed the problem, instance A comes up just fine, and you can accomplish what you originally set out to do. If not, you may need to continue repeating these steps until you have it working.\nNote: If you had an Elastic IP address assigned to instance A when you stopped it, you\u2019ll need to reassociate it after starting it up again.\nRemember! If your instance B was temporarily started just for this process, don\u2019t forget to terminate it now."
  },
  {
    "question": "How can I remove an SSH key? I currently have an old SSH key uploaded on a server. The problem is I lost my ~/.ssh directory (with the original id_rsa and id_rsa.pub files).\nConsequently, I want to remove the old SSH key directly on the server and upload a new one.\nI tried the following command without success:\n\n```\n$> ssh-add -D\n\n```\n\n\nIs there a way to completely remove an SSH key?",
    "answer": "Note that there are at least two bug reports for ssh-add -d/-D not removing keys:\n\n\"Debian Bug report #472477: ssh-add -D does not remove SSH key from gnome-keyring-daemon memory\"\n\"Ubuntu: ssh-add -D deleting all identities does not work. Also, why are all identities auto-added?\"\n\nThe exact issue is:\n\nssh-add -d/-D deletes only manually added keys from gnome-keyring.\nThere is no way to delete automatically added keys.\nThis is the original bug, and it's still definitely present.\nSo, for example, if you have two different automatically-loaded ssh identities associated with two different GitHub accounts -- say for work and for home -- there's no way to switch between them. GitHubtakes the first one which matches, so you always appear as your 'home' user to GitHub, with no way to upload things to work projects.\nAllowing ssh-add -d to apply to automatically-loaded keys (and ssh-add -t X to change the lifetime of automatically-loaded keys), would restore the behavior most users expect.\n\nMore precisely, about the issue:\n\nThe culprit is gpg-keyring-daemon:\n\nIt subverts the normal operation of ssh-agent, mostly just so that it can pop up a pretty box into which you can type the passphrase for an encrypted ssh key.\nAnd it paws through your .ssh directory, and automatically adds any keys it finds to your agent.\nAnd it won't let you delete those keys.\n\nHow do we hate this? Let's not count the ways -- life's too short.\nThe failure is compounded because newer ssh clients automatically try all the keys in your ssh-agent when connecting to a host.\nIf there are too many, the server will reject the connection.\nAnd since gnome-keyring-daemon has decided for itself how many keys you want your ssh-agent to have, and has autoloaded them, AND WON'T LET YOU DELETE THEM, you're toast.\n\nThis bug is still confirmed in Ubuntu 14.04.4, as recently as two days ago (August 21st, 2014)\n\nA possible workaround:\n\nDo ssh-add -D to delete all your manually added keys. This also locks\nthe automatically added keys, but is not much use since gnome-keyring will ask you to unlock them anyways when you try doing a git push.\nNavigate to your ~/.ssh folder and move all your key files except the one you want to identify with into a separate folder called backup. If necessary you can also open seahorse and delete the keys from there.\nNow you should be able to do git push without a problem.\n\nAnother workaround:\n\nWhat you really want to do is to turn off gpg-keyring-daemon altogether.\nGo to System --> Preferences --> Startup Applications, and unselect the \"SSH Key Agent (Gnome Keyring SSH Agent)\" box -- you'll need to scroll down to find it.\nYou'll still get an ssh-agent, only now it will behave sanely: no keys autoloaded, you run ssh-add to add them, and if you want to delete keys, you can. Imagine that.\n\nThis comments actually suggests:\n\nThe solution is to keep gnome-keyring-manager from ever starting up, which was strangely difficult by finally achieved by removing the program file's execute permission.\n\nRyan Lue adds another interesting corner case in the comments:\n\nIn case this helps anyone: I even tried deleting the id_rsa and id_rsa.pub files altogether, and the key was still showing up.\nTurns out gpg-agent was caching them in a ~/.gnupg/sshcontrol file; I had to manually delete them from there.\n\nThat is the case when the keygrip has been added as in here."
  },
  {
    "question": "Why does an SSH remote command get fewer environment variables then when run manually? I have a command that runs fine if I ssh to a machine and run it, but fails when I try to run it using a remote ssh command like : \n\n```\nssh user@IP <command>\n\n```\n\nComparing the output of \"env\" using both methods resutls in different environments. When I manually login to the machine and run env, I get much more environment variables then when I run :\n\n```\nssh user@IP \"env\"\n\n```\n\nAny idea why ?",
    "answer": "A  login  shell  is  one whose first character of argument\nzero is a -, or one started with the --login option.\n\nAn interactive shell is  one  started  without  non-option\narguments  and  without the -c option whose standard input\nand error are both connected to terminals  (as  determined\nby  isatty(3)), or one started with the -i option.  PS1 is\nset and $- includes i if bash is interactive,  allowing  a\nshell script or a startup file to test this state.\n\nThe  following  paragraphs  describe how bash executes its\nstartup files.  If any of the files exist  but  cannot  be\nread,  bash reports an error.  Tildes are expanded in file\nnames as described below  under  Tilde  Expansion  in  the\nEXPANSION section.\n\nWhen  bash is invoked as an interactive login shell, or as\na non-interactive shell with the --login option, it  first\nreads and executes commands from the file /etc/profile, if\nthat file exists.  After reading that file, it  looks  for\n~/.bash_profile,  ~/.bash_login,  and  ~/.profile, in that\norder, and reads and executes commands from the first  one\nthat  exists  and is readable.  The --noprofile option may\nbe used when the shell is started to inhibit  this  behav\u00ad\nior.\n\nWhen a login shell exits, bash reads and executes commands\nfrom the file ~/.bash_logout, if it exists.\n\nWhen an interactive shell that is not  a  login  shell  is\nstarted,  bash reads and executes commands from ~/.bashrc,\nif that file exists.  This may be inhibited by  using  the\n--norc  option.   The --rcfile file option will force bash\nto  read  and  execute  commands  from  file  instead   of\n~/.bashrc.\n\nWhen  bash  is  started  non-interactively, to run a shell\nscript, for example, it looks for the variable BASH_ENV in\nthe  environment,  expands  its value if it appears there,\nand uses the expanded value as the name of a file to  read\nand  execute.   Bash  behaves  as if the following command\nwere executed:\nif [ -n \"$BASH_ENV\" ]; then . \"$BASH_ENV\"; fi\nbut the value of the PATH variable is not used  to  search\nfor the file name.\n\nThere are different types of shells. The SSH command execution shell is a non-interactive shell, whereas your normal shell is either a login shell or an interactive shell. Description follows, from man bash:"
  },
  {
    "question": "ssh -L forward multiple ports I'm currently running a bunch of:\n\n```\nsudo ssh -L PORT:IP:PORT root@IP\n\n```\n\nwhere IP is the target of a secured machine, and PORT represents the ports I'm forwarding.\nThis is because I use a lot of applications which I cannot access without this forwarding. After performing this, I can access through localhost:PORT.\nThe main problem occured now that I actually have 4 of these ports that I have to forward.\nMy solution is to open 4 shells and constantly search my history backwards to look for exactly which ports need to be forwarded etc, and then run this command - one in each shell (having to fill in passwords etc).\nIf only I could do something like:\n\n```\nsudo ssh -L PORT1+PORT2+PORT+3:IP:PORT+PORT2+PORT3 root@IP\n\n```\n\nthen that would already really help.\nIs there a way to make it easier to do this?",
    "answer": "The -L option can be specified multiple times within the same command. Every time with different ports. I.e. ssh -L localPort0:ip:remotePort0 -L localPort1:ip:remotePort1 ..."
  },
  {
    "question": "Is there a way to continue broken scp (secure copy) command process in Linux? I am copying 7.5 GB file to a remote server using scp command. At some point in time file transfer breaks and I have to start all over again.\nIs the temporary amount of file being transferred completely lost ? Can I somehow restart the transfer from where it has stopped at previous attempt ? If not, is there some standard Unix command line file transfer command for doing that ?",
    "answer": "rsync --partial --progress --rsh=ssh local_file user@host:remote_file\nrsync -P -e ssh local_file user@host:remote_file\nrsync [options] SRC DEST\n\nIf you need to resume an scp transfer from local to remote, try with rsync:\n\nShort version, as pointed out by @aurelijus-rozenas:\n\nIn general the order of args for rsync is"
  },
  {
    "question": "Converting a Postman request to curl I am calling my Java webservice (POST request) via Postman in the following manner which works perfectly fine (i.e. I can see my records getting inserted into the database):\n\nAnd, here's how the contents inside the Headers(1) tab look like:\n\nInstead of calling it via Postman, I have to call the same request in PHP using cURL. I am wondering if there's a way to export this command to a curl command so that I could use it in my PHP code? I have found the opposite approach at many places online where someone is asking to convert a curl based request to Postman but couldn't figure out how to do the opposite.\nI found this question for curl to Postman: Simulate a specific CURL in PostMan",
    "answer": "You can see the button </> Code icon in right side of the Postman app (attached screenshot).\nPress it and you can get your code in many different languages including PHP cURL"
  },
  {
    "question": "How to capture cURL output to a file? I have a text document that contains a bunch of URLs in this format:\n\n```\nURL = \"sitehere.com\"\n\n```\n\nWhat I'm looking to do is to run curl -K myfile.txt, and get the output of the response cURL returns, into a file.\nHow can I do this?",
    "answer": "curl -K myconfig.txt -o output.txt\ncurl -K myconfig.txt >> output.txt\ncurl \\\n-X POST \\\n-H \"Content-Type: application/octet-stream\" \\\n--data-binary \"@/home/path/file.xyz\" \\\n\"https://xample.org:8080/v1/?filename=file.xyz&food=1&z=bee\" \\\n>out.txt 2>err.txt\n\nWrites the first output received in the file you specify (overwrites if an old one exists).\n\nAppends all output you receive to the specified file.\nNote: The -K is optional.\nIf you are posting to a URL like https://example.org/?foo=1&baz=4\nthen you need to put double quotes around the URL:"
  },
  {
    "question": "Unable to resolve &quot;unable to get local issuer certificate&quot; using git on Windows with self-signed certificate I am using Git on Windows. I installed the msysGit package. My test repository has a self signed certificate at the server. I can access and use the repository using HTTP without problems. Moving to HTTPS gives the error:\n\nSSL Certificate problem: unable to get local issuer certificate.  \n\nI have the self signed certificate installed in the Trusted Root Certification Authorities of my Windows 7 - client machine. I can browse to the HTTPS repository URL in Internet Explorer with no error messages.\nThis blog post by Philip Kelley explained that cURL does not use the client machine's certificate store. I followed the blog post's advice to create a private copy of curl-ca-bundle.crt and configure Git to use it. I am sure Git is using my copy. If I rename the copy; Git complains the file is missing.\nI pasted in my certificate, as mentioned in the blog post, I still get the message \"unable to get local issuer certificate\".\nI verified that Git was still working by cloning a GitHub Repository via HTTPS.\nThe only thing I see that's different to the blog post is that my certificate is the root - there is no chain to reach it. My certificate originally came from clicking the IIS8 IIS Manager link 'Create Self Signed Certificate'. Maybe that makes a certificate different in some way to what cURL expects.\nHow can I get Git/cURL to accept the self signed certificate?",
    "answer": "An answer to  Using makecert for Development SSL fixed this for me.\nI do not know why, but the certificate created by the simple 'Create Self Signed Certificate' link in IIS Manager does not do the trick.  I followed the approach in the linked question of creating and installing a self-signed CA Root; then using that to issue a Server Authentication Certificate for my server.  I installed both of them in IIS.\nThat gets my situation the same as the blog post referenced in the original question.  Once the root certificate was copy/pasted into curl-ca-bundle.crt the git/curl combo were satisfied."
  },
  {
    "question": "How do I install and use cURL on Windows? I am having trouble getting cURL to run on Windows.\nI have downloaded a cURL zip file from here, but it seems to contain source code, not an executable.\nDo I need to compile cURL to run it? If yes, then how do I do that?\nWhere can I find .exe downloads for cURL ?\nI have looked for documentation on installing cURL, but there is little to be found.",
    "answer": "Assuming you got it from https://curl.haxx.se/download.html, just unzip it wherever you want. No need to install. If you are going to use SSL, you need to download the OpenSSL DLLs, available from curl's website."
  },
  {
    "question": "JSONDecodeError: Expecting value: line 1 column 1 (char 0) I am getting error Expecting value: line 1 column 1 (char 0) when trying to decode JSON.\nThe URL I use for the API call works fine in the browser, but gives this error when done through a curl request. The following is the code I use for the curl request.\nThe error happens at return simplejson.loads(response_json)\n\n```\nresponse_json = self.web_fetch(url)\nresponse_json = response_json.decode('utf-8')\nreturn json.loads(response_json)\n\n\ndef web_fetch(self, url):\n    buffer = StringIO()\n    curl = pycurl.Curl()\n    curl.setopt(curl.URL, url)\n    curl.setopt(curl.TIMEOUT, self.timeout)\n    curl.setopt(curl.WRITEFUNCTION, buffer.write)\n    curl.perform()\n    curl.close()\n    response = buffer.getvalue().strip()\n    return response\n\n```\n\nTraceback:\n\n```\nFile \"/Users/nab/Desktop/myenv2/lib/python2.7/site-packages/django/core/handlers/base.py\" in get_response\n  111.                         response = callback(request, *callback_args, **callback_kwargs)\nFile \"/Users/nab/Desktop/pricestore/pricemodels/views.py\" in view_category\n  620.     apicall=api.API().search_parts(category_id= str(categoryofpart.api_id), manufacturer = manufacturer, filter = filters, start=(catpage-1)*20, limit=20, sort_by='[[\"mpn\",\"asc\"]]')\nFile \"/Users/nab/Desktop/pricestore/pricemodels/api.py\" in search_parts\n  176.         return simplejson.loads(response_json)\nFile \"/Users/nab/Desktop/myenv2/lib/python2.7/site-packages/simplejson/__init__.py\" in loads\n  455.         return _default_decoder.decode(s)\nFile \"/Users/nab/Desktop/myenv2/lib/python2.7/site-packages/simplejson/decoder.py\" in decode\n  374.         obj, end = self.raw_decode(s)\nFile \"/Users/nab/Desktop/myenv2/lib/python2.7/site-packages/simplejson/decoder.py\" in raw_decode\n  393.         return self.scan_once(s, idx=_w(s, idx).end())\n\nException Type: JSONDecodeError at /pricemodels/2/dir/\nException Value: Expecting value: line 1 column 1 (char 0)\n\n```",
    "answer": "import httpx\n\nresponse = httpx.get(url)\nresponse.raise_for_status()  # raises exception when not a 2xx response\nif response.status_code != 204:\nreturn response.json()\nif (\nresponse.status_code != 204 and\nresponse.headers[\"content-type\"].strip().startswith(\"application/json\")\n):\ntry:\nreturn response.json()\nexcept ValueError:\n\nYour code produced an empty response body; you'd want to check for that or catch the exception raised. It is possible the server responded with a 204 No Content response, or a non-200-range status code was returned (404 Not Found, etc.). Check for this.\nNote:\n\nThere is no need to decode a response from UTF8 to Unicode, the  json.loads() method can handle UTF8-encoded data natively.\n\npycurl has a very archaic API. Unless you have a specific requirement for using it, there are better choices.\n\nEither requests or httpx offer much friendlier APIs, including JSON support.\nIf you can, replace your call with the following httpx code:\n\nOf course, this won't protect you from a URL that doesn't comply with HTTP standards; when using arbitrary URLs where this is a possibility, check if the server intended to give you JSON by checking the Content-Type header, and for good measure catch the exception:"
  },
  {
    "question": "Is there a way to follow redirects with command line cURL? I know that in a php script:\n\n```\ncurl_setopt($ch, CURLOPT_FOLLOWLOCATION, true);\n\n```\n\nwill follow redirects. Is there a way to follow redirects with command line cURL?",
    "answer": "curl -L <URL>\ncurl --location <URL>  # or (same thing)\n\nUse the location header flag:"
  },
  {
    "question": "PHP, cURL, and HTTP POST example? Can anyone show me how to do a PHP cURL with an HTTP POST?\nI want to send data like this:\n\n```\nusername=user1, password=passuser1, gender=1\n\n```\n\nTo www.example.com\nI expect the cURL to return a response like result=OK.  Are there any examples?",
    "answer": "url = \"http://www.example.com/tester.phtml\";\npost_data = array('postvar1' => 'value1');\nch = curl_init($url);\n// return the response instead of sending it to stdout:\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER, true);\n// set the POST data, corresponding method and headers:\ncurl_setopt($ch, CURLOPT_POSTFIELDS, http_build_query($post_data));\n// send the request and get the response\nserver_output = curl_exec($ch);\n\nA very simple PHP example that sends an HTTP POST request to a remote site"
  },
  {
    "question": "How to set the authorization header using cURL How do I pass authorization header using cURL? ( executable in /usr/bin/curl).",
    "answer": "curl --user name:password http://www.example.com\ncurl --proxy-user proxyuser:proxypassword curl.haxx.se\n\nhttp://curl.se/docs/httpscripting.html\nSee part 6. HTTP Authentication\n\nHTTP Authentication\n\nHTTP Authentication is the ability to tell the server your username and\npassword so that it can verify that you're allowed to do the request you're\ndoing. The Basic authentication used in HTTP (which is the type curl uses by\ndefault) is plain text based, which means it sends username and password\nonly slightly obfuscated, but still fully readable by anyone that sniffs on\nthe network between you and the remote server.\n\nTo tell curl to use a user and password for authentication:\n\nThe site might require a different authentication method (check the headers\nreturned by the server), and then --ntlm, --digest, --negotiate or even\n--anyauth might be options that suit you.\n\nSometimes your HTTP access is only available through the use of a HTTP\nproxy. This seems to be especially common at various companies. A HTTP proxy\nmay require its own user and password to allow the client to get through to\nthe Internet. To specify those with curl, run something like:\n\nIf your proxy requires the authentication to be done using the NTLM method,\nuse --proxy-ntlm, if it requires Digest use --proxy-digest.\n\nIf you use any one these user+password options but leave out the password\npart, curl will prompt for the password interactively.\n\nDo note that when a program is run, its parameters might be possible to see\nwhen listing the running processes of the system. Thus, other users may be\nable to watch your passwords if you pass them as plain command line\noptions. There are ways to circumvent this.\n\nIt is worth noting that while this is how HTTP Authentication works, very\nmany web sites will not use this concept when they provide logins etc. See\nthe Web Login chapter further below for more details on that."
  },
  {
    "question": "wget/curl large file from google drive I'm trying to download a file from google drive in a script, and I'm having a little trouble doing so. The files I'm trying to download are here.\nI've looked online extensively and I finally managed to get one of them to download. I got the UIDs of the files and the smaller one (1.6MB) downloads fine, however the larger file (3.7GB) always redirects to a page which asks me whether I want to proceed with the download without a virus scan. Could someone help me get past that screen?\nHere's how I got the first file working -\n\n```\ncurl -L \"https://docs.google.com/uc?export=download&id=0Bz-w5tutuZIYeDU0VDRFWG9IVUE\" > phlat-1.0.tar.gz\n\n```\n\nWhen I run the same on the other file,\n\n```\ncurl -L \"https://docs.google.com/uc?export=download&id=0Bz-w5tutuZIYY3h5YlMzTjhnbGM\" > index4phlat.tar.gz\n\n```\n\nI get the the following output -\n\nI notice on the third-to-last line in the link, there a &confirm=JwkK which is a random 4 character string but suggests there's a way to add a confirmation to my URL. One of the links I visited suggested &confirm=no_antivirus but that's not working.",
    "answer": "wget https://googledrive.com/host/LARGEPUBLICFOLDERID/index4phlat.tar.gz\n\nWARNING: This functionality is deprecated.  See warning below in comments.\n\nHave a look at this question: Direct download from Google Drive using Google Drive API\nBasically you have to create a public directory and access your files by relative reference with something like\n\nAlternatively, you can use this script: https://github.com/circulosmeos/gdown.pl"
  },
  {
    "question": "performing HTTP requests with cURL (using PROXY) I have this proxy address: 125.119.175.48:8909\nHow can I perform a HTTP request using cURL like curl http://www.example.com, but specifying the proxy address of my network?",
    "answer": "export http_proxy=http://your.proxy.server:port/\nexport https_proxy=https://your.proxy.server:port/\n\nGeneral way:\n\nThen you can connect through proxy from (many) application.\nAnd, as per comment below, for https:"
  },
  {
    "question": "Using cURL to upload POST data with files I would like to use cURL to not only send data parameters in HTTP POST but to also upload files with specific form name. How should I go about doing that ?\nHTTP Post parameters:\nuserid = 12345\nfilecomment = This is an image file\nHTTP File upload:\nFile location = /home/user1/Desktop/test.jpg\nForm name for file = image (correspond to the $_FILES['image'] at the PHP side)\nI figured part of the cURL command as follows:\n\n```\ncurl -d \"userid=1&filecomment=This is an image file\" --data-binary @\"/home/user1/Desktop/test.jpg\" localhost/uploader.php\n\n```\n\nThe problem I am getting is as follows:\n\n```\nNotice: Undefined index: image in /var/www/uploader.php\n\n```\n\nThe problem is I am using $_FILES['image'] to pick up files in the PHP script.\nHow do I adjust my cURL commands accordingly ?",
    "answer": "curl \\\n-F \"userid=1\" \\\n-F \"filecomment=This is an image file\" \\\n-F \"image=@/home/user1/Desktop/test.jpg\" \\\nlocalhost/uploader.php\n\nYou need to use the -F option:\n-F/--form <name=content> Specify HTTP multipart POST data (H)\nTry this:"
  },
  {
    "question": "How to do a PUT request with cURL? How do I test a RESTful PUT (or DELETE) method using cURL?",
    "answer": "curl -X PUT -d argument=value -d argument2=value2 http://localhost:8080\n\nUsing the uppercase -X flag with whatever HTTP verb you want:\n\nThis example also uses the -d flag to provide arguments with your PUT request."
  },
  {
    "question": "How to urlencode data for curl command? I am trying to write a bash script for testing that takes a parameter and sends it through curl to web site.  I need to url encode the value to make sure that special characters are processed properly.  What is the best way to do this?  \nHere is my basic script so far:\n\n```\n#!/bin/bash\nhost=${1:?'bad host'}\nvalue=$2\nshift\nshift\ncurl -v -d \"param=${value}\" http://${host}/somepath $@\n\n```",
    "answer": "curl \\\n--data-urlencode \"paramName=value\" \\\n--data-urlencode \"secondParam=value\" \\\nhttp://example.com\ncurl --get \\\n--data-urlencode \"p1=value 1\" \\\n--data-urlencode \"p2=value 2\" \\\nhttp://example.com\n\nUse curl --data-urlencode; from man curl:\n\nThis posts data, similar to the other --data options with the exception that this performs URL-encoding. To be CGI-compliant, the <data> part should begin with a name followed by a separator and a content specification.\n\nExample usage:\n\nSee the man page for more info.\nThis requires curl 7.18.0 or newer (released January 2008). Use  curl -V to check which version you have.\nYou can as well encode the query string:"
  },
  {
    "question": "PHP cURL custom headers I'm wondering if/how you can add custom headers to a cURL HTTP request in PHP. I'm trying to emulate how iTunes grabs artwork and it uses these non-standard headers:\n\n```\nX-Apple-Tz: 0\nX-Apple-Store-Front: 143444,12\n\n```\n\nHow could I add these headers to a request?",
    "answer": "curl_setopt($ch, CURLOPT_HTTPHEADER, [\n'X-Apple-Tz: 0',\n'X-Apple-Store-Front: 143444,12'\n]);\n\nhttps://www.php.net/manual/en/function.curl-setopt.php"
  },
  {
    "question": "How can you debug a CORS request with cURL? How can you debug CORS requests using cURL? So far I couldn't find a way to \"simulate\" the preflight request.",
    "answer": "curl -H \"Origin: http://example.com\" --verbose \\\nhttps://www.googleapis.com/discovery/v1/apis?fields=\ncurl -H \"Origin: http://example.com\" \\\n-H \"Access-Control-Request-Method: POST\" \\\n-H \"Access-Control-Request-Headers: X-Requested-With\" \\\n-X OPTIONS --verbose \\\nhttps://www.googleapis.com/discovery/v1/apis?fields=\n\nHere's how you can debug CORS requests using curl.\nSending a regular CORS request using cUrl:\n\nThe -H \"Origin: http://example.com\" flag is the third party domain making the request. Substitute in whatever your domain is.\nThe --verbose flag prints out the entire response so you can see the request and response headers.\nThe URL I'm using above is a sample request to a Google API that supports CORS, but you can substitute in whatever URL you are testing.\nThe response should include the Access-Control-Allow-Origin header.\nSending a preflight request using cUrl:\n\nThis looks similar to the regular CORS request with a few additions:\nThe -H flags send additional preflight request headers to the server\nThe -X OPTIONS flag indicates that this is an HTTP OPTIONS request.\nIf the preflight request is successful, the response should include the Access-Control-Allow-Origin, Access-Control-Allow-Methods, and  Access-Control-Allow-Headers response headers.  If the preflight request was not successful, these headers shouldn't appear, or the HTTP response won't be 200.\nYou can also specify additional headers, such as User-Agent, by using the -H flag."
  },
  {
    "question": "Run cURL commands from Windows console Is there a way to install cURL in Windows in order to run cURL commands from the command prompt?",
    "answer": "If you are not into Cygwin, you can use native Windows builds. Some are here: curl Download Wizard."
  },
  {
    "question": "How to count items in JSON object using command line? I'm getting this kind of JSON reply from a curl command:\n\n```\n[\n  {\n    \"cid\": 49,\n    \"pyn\": \"yi4\",\n    \"hans\": \"\u4ebf\",\n    \"hant\": \"\u5104\",\n    \"tid\": 68,\n    \"l10n\": \"cent million\",\n    \"pid\": 1,\n    \"pos\": \"num\",\n    \"pos_txt\": \"\"\n  },\n  {\n    \"cid\": 50,\n    \"pyn\": \"yi4\",\n    \"hans\": \"\u4ebf\",\n    \"hant\": \"\u5104\",\n    \"tid\": 69,\n    \"l10n\": \"100 millions\",\n    \"pid\": 1,\n    \"pos\": \"num\",\n    \"pos_txt\": \"\"\n  }\n]\n\n```\n\nHow can I count the number of items in the array (here 2), using Bash or a command line (e.g. underscore) ?",
    "answer": "jq length /tmp/test.json\n\nJust throwing another solution in the mix...\nTry jq, a lightweight and flexible command-line JSON processor:\n\nPrints the length of the array of objects."
  },
  {
    "question": "Call to undefined function curl_init()? When i am going to implement Authorize.net payment gateway. However, I got this error: \n\nCall to undefined function curl_init()\n\nPlease let me know what is wrong in it.",
    "answer": ";extension=php_curl.dll\nsudo apt-get install php-curl\nsudo apt-get install php5-curl\nsudo apt-get install php5.6-curl\nsudo service apache2 restart\n\nIf you're on Windows:\nGo to your php.ini file and remove the ; mark from the beginning of the following line:\n\nAfter you have saved the file you must restart your HTTP server software (e.g. Apache) before this can take effect.\n\nFor Ubuntu 13.0 and above, simply use the debundled package. In a terminal type the following to install it and do not forgot to restart server.\n\nOr if you're using the old PHP5\n\nor\n\nThen restart apache to activate the package with"
  },
  {
    "question": "How to use cURL to send Cookies? I read that sending cookies with cURL works, but not for me.\nI have a REST endpoint like this:\n\n```\nclass LoginResource(restful.Resource):\n    def get(self):\n        print(session)\n        if 'USER_TOKEN' in session:\n            return 'OK'\n        return 'not authorized', 401\n\n```\n\nWhen I try to access the endpoint, it refuses:\n\n```\ncurl -v -b ~/Downloads/cookies.txt -c ~/Downloads/cookies.txt http://127.0.0.1:5000/\n* About to connect() to 127.0.0.1 port 5000 (#0)\n*   Trying 127.0.0.1...\n* connected\n* Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)\n> GET / HTTP/1.1\n> User-Agent: curl/7.27.0\n> Host: 127.0.0.1:5000\n> Accept: */*\n>\n* HTTP 1.0, assume close after body\n< HTTP/1.0 401 UNAUTHORIZED\n< Content-Type: application/json\n< Content-Length: 16\n< Server: Werkzeug/0.8.3 Python/2.7.2\n< Date: Sun, 14 Apr 2013 04:45:45 GMT\n<\n* Closing connection #0\n\"not authorized\"%\n\n```\n\nWhere my ~/Downloads/cookies.txt is:\n\n```\ncat ~/Downloads/cookies.txt\nUSER_TOKEN=in\n\n```\n\nand the server receives nothing:\n\n```\n127.0.0.1 - - [13/Apr/2013 21:43:52] \"GET / HTTP/1.1\" 401 -\n127.0.0.1 - - [13/Apr/2013 21:45:30] \"GET / HTTP/1.1\" 401 -\n<SecureCookieSession {}>\n<SecureCookieSession {}>\n127.0.0.1 - - [13/Apr/2013 21:45:45] \"GET / HTTP/1.1\" 401 -\n\n```\n\nWhat is it that I am missing?",
    "answer": "curl -v --cookie \"USER_TOKEN=Yes\" http://127.0.0.1:5000/\nprint(request.cookies)\n\nThis worked for me:\n\nI could see the value in backend using"
  },
  {
    "question": "How to define the basic HTTP authentication using cURL correctly? I'm learning Apigility (Apigility docu -> REST Service Tutorial) and trying to send a POST request with basic authentication via cURL:\n\n```\n$ curl -X POST -i -H \"Content-Type: application/hal+json\" -H \"Authorization: Basic YXBpdXNlcjphcGlwd2Q=\" http://apigilityhw.sandbox.loc/status\n\n```\n\nYXBpdXNlcjphcGlwd2Q= is the base 64 encoded string with my credentials apiuser:apipwd. The credentials are saved in the /data/htpasswd (apiuser:$apr1$3J4cyqEw$WKga3rQMkxvnevMuBaekg/).\nThe looks like this:\n\n```\nHTTP/1.1 401 Unauthorized\nServer: nginx/1.4.7\nDate: Mon, 22 Sep 2014 07:48:47 GMT\nContent-Type: application/problem+json\nTransfer-Encoding: chunked\nConnection: keep-alive\nX-Powered-By: PHP/5.5.12-1~dotdeb.1\nWWW-Authenticate: Basic realm=\"api\"\n\n```\n\nWhere is the mistake here? How to get it work?",
    "answer": "curl -u username:password http://\ncurl -u username http://\n\nFrom the documentation page:\n\n-u, --user <user:password>\nSpecify the user name and password to use for server authentication.\nOverrides -n, --netrc and --netrc-optional.\nIf you simply specify the user name, curl will prompt for a password.\nThe user name and passwords are split up on the first colon, which\nmakes it impossible to use a colon in the user name with this option.\nThe password can, still.\nWhen using Kerberos V5 with a Windows based server you should include\nthe Windows domain name in the user name, in order for the server to\nsuccesfully obtain a Kerberos Ticket. If you don't then the initial\nauthentication handshake may fail.\nWhen using NTLM, the user name can be specified simply as the user\nname, without the domain, if there is a single domain and forest in\nyour setup for example.\nTo specify the domain name use either Down-Level Logon Name or UPN\n(User Principal Name) formats. For example, EXAMPLE\\user and\nuser@example.com respectively.\nIf you use a Windows SSPI-enabled curl binary and perform Kerberos V5,\nNegotiate, NTLM or Digest authentication then you can tell curl to\nselect the user name and password from your environment by specifying\na single colon with this option: \"-u :\".\nIf this option is used several times, the last one will be used.\n\nhttp://curl.haxx.se/docs/manpage.html#-u\nNote that you do not need --basic flag as it is the default."
  },
  {
    "question": "Curl to return http status code along with the response I use curl to get http headers to find http status code and also return response. I get the http headers with the command\n\n```\ncurl -I http://localhost\n\n```\n\nTo get the response, I use the command \n\n```\ncurl http://localhost\n\n```\n\nAs soon as use the -I flag, I get only the headers and the response is no longer there. Is there a way to get both the http response and the headers/http status code in in one command?",
    "answer": "curl -o - -I http://localhost\ncurl -o /dev/null -s -w \"%{http_code}\\n\" http://localhost\n\nI was able to get a solution by looking at the curl doc which specifies to use - for the output to get the output to stdout.\n\nTo get the response with just the http return code, I could just do"
  },
  {
    "question": "Passing a URL with brackets to curl If I try to pass a URL to curl that contains brackets, it fails with an error:\n\n```\n$ curl 'http://www.google.com/?TEST[]=1'\ncurl: (3) [globbing] illegal character in range specification at pos 29\n\n```\n\nHowever, if I escape both brackets, it appears to work:\n\n```\n$ curl 'http://www.google.com/?TEST\\[\\]=1'\n\n```\n\nHow do I fix this? Is there an argument that will escape URLs automatically, or a description of the characters that need to be escaped before passing to curl?",
    "answer": "-g, --globoff\nThis option switches off the \"URL globbing parser\". When you set this option, you can\nspecify URLs that contain the letters {}[] without having curl itself interpret them.\nNote that these letters are not normal legal URL contents but they should be encoded\naccording to the URI standard.\n\nExample:\ncurl -g \"https://example.com/{[]}}}}\"\n\nAdd -g to your command:\n\ncurl.se/docs/manpage.html#-g"
  },
  {
    "question": "Can PHP cURL retrieve response headers AND body in a single request? Is there any way to get both headers and body for a cURL request using PHP? I found that this option:\n\n```\ncurl_setopt($ch, CURLOPT_HEADER, true);\n\n```\n\nis going to return the body plus headers, but then I need to parse it to get the body. Is there any way to get both in a more usable (and secure) way?\nNote that for \"single request\" I mean avoiding issuing a HEAD request prior of GET/POST.",
    "answer": "ch = curl_init();\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);\ncurl_setopt($ch, CURLOPT_HEADER, 1);\n// ...\nresponse = curl_exec($ch);\n\n// Then, after your curl_exec call:\nheader_size = curl_getinfo($ch, CURLINFO_HEADER_SIZE);\nheader = substr($response, 0, $header_size);\nbody = substr($response, $header_size);\n\nOne solution to this was posted in the PHP documentation comments: http://www.php.net/manual/en/function.curl-exec.php#80442\nCode example:\n\nWarning: As noted in the comments below, this may not be reliable when used with proxy servers or when handling certain types of redirects. @Geoffrey's answer may handle these more reliably."
  },
  {
    "question": "List all indexes on ElasticSearch server? I would like to list all indexes present on an ElasticSearch server. I tried this:\n\n```\ncurl -XGET localhost:9200/\n\n```\n\nbut it just gives me this:\n\n```\n{\n  \"ok\" : true,\n  \"status\" : 200,\n  \"name\" : \"El Aguila\",\n  \"version\" : {\n    \"number\" : \"0.19.3\",\n    \"snapshot_build\" : false\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n\n```\n\nI want a list of all indexes..",
    "answer": "curl http://localhost:9200/_aliases\ncurl http://localhost:9200/_aliases?pretty=true\n{\n\"old_deuteronomy\" : {\n\"aliases\" : { }\n},\n\"mungojerrie\" : {\n\"aliases\" : {\n\"rumpleteazer\" : { },\n\"that_horrible_cat\" : { }\n}\n}\n}\n\nFor a concise list of all indices in your cluster, call\n\nthis will give you a list of indices and their aliases.\nIf you want it pretty-printed, add pretty=true:\n\nThe result will look something like this, if your indices are called old_deuteronomy and mungojerrie:"
  },
  {
    "question": "Send request to cURL with post data sourced from a file I need to make a POST request via cURL from the command line. Data for this request is located in a file. I know that via PUT this could be done with the --upload-file option.\n\n```\ncurl host:port/post-file -H \"Content-Type: text/xml\" --data \"contents_of_file\"\n\n```",
    "answer": "curl -i -X POST host:port/post-file \\\n-H \"Content-Type: text/xml\" \\\n--data-binary \"@path/to/file\"\n\nYou're looking for the --data-binary argument:\n\nIn the example above, -i prints out all the headers so that you can see what's going on, and -X POST makes it explicit that this is a post.  Both of these can be safely omitted without changing the behaviour on the wire.  The path to the file needs to be preceded by an @ symbol, so curl knows to read from a file."
  },
  {
    "question": "POST XML file using cURL command line How can I POST an XML file to a local server http://localhost:8080 using cURL from the command line?\nWhat command should I use?",
    "answer": "curl -X POST -d '<run>...</run>' \\\nhttp://user:pass@myhost:myport/path/of/url\ncurl -X POST -d @myfilename http://user:pass@myhost:myport/path/of/url\n\nIf that question is connected to your other Hudson questions use the command they provide.  This way with XML from the command line:\n\nYou need to change it a little bit to read from a file:\n\nRead the manpage. following an abstract for -d Parameter.\n\n-d/--data\n(HTTP) Sends the specified data in a\nPOST request to the HTTP server, in\nthe same way that a browser does when\na user has filled in an HTML form and\npresses the submit button. This will\ncause curl to pass the data to the\nserver using the content-type\napplication/x-www-form-urlencoded.\nCompare to -F/--form.\n-d/--data is the same as --data-ascii. To post data purely binary, you should\ninstead use the --data-binary option.\nTo URL-encode the value of a form\nfield you may use --data-urlencode.\nIf any of these options is used more\nthan once on the same command line,\nthe data pieces specified will be\nmerged together with a separating\n&-symbol. Thus, using '-d name=daniel\n-d skill=lousy' would generate a post chunk that looks like\n'name=daniel&skill=lousy'.\nIf you start the data with the letter\n@, the rest should be a file name to\nread the data from, or - if you want\ncurl to read the data from stdin. The\ncontents of the file must already be\nURL-encoded. Multiple files can also\nbe specified. Posting data from a file\nnamed 'foobar' would thus be done with\n--data @foobar."
  },
  {
    "question": "Simulate a specific CURL in PostMan I am using Postman to test some Curl requests to an API server. The API developers gave us the curl command, but I can't send it from the Postman. How to make such a request from the Postman?\n\n```\ncurl -X POST \"https://api-server.com/API/index.php/member/signin\" -d \"{\"description\":\"\",\"phone\":\"\",\"lastname\":\"\",\"app_version\":\"2.6.2\",\"firstname\":\"\",\"password\":\"my_pass\",\"city\":\"\",\"apikey\":\"213\",\"lang\":\"fr\",\"platform\":\"1\",\"email\":\"email@example.com\",\"pseudo\":\"example\"}\"\n\n--0xKhTmLbOuNdArY\nContent-Disposition: form-data; name=\"userfile\"; filename=\"profil.jpg\"\nContent-Type: image/jpeg\nContent-Transfer-Encoding: binary\n\n<ffd8ffe0 00104a46 49460001 01010048 ... a00fffd9>\n\n\u20140xKhTmLbOuNdArY\u2014\n\n```",
    "answer": "A simpler approach would be:\n\nOpen POSTMAN\nClick on \"import\" tab on the upper left side.\nSelect the Raw Text option and paste your cURL command.\nHit import and you will have the command in your Postman builder!\nClick Send to post the command"
  },
  {
    "question": "Automatically add newline at end of curl response body If the HTTP response body for a curl request doesn't contain a trailing newline, I end up with this really annoying condition where the shell prompt is in the middle of the line, and escaping is messed up enough that when I put the last curl command on the screen, deleting characters from that curl command deletes the wrong characters.\nFor example:\n\n```\n[root@localhost ~]# curl jsonip.com\n{\"ip\":\"10.10.10.10\",\"about\":\"/about\"}[root@localhost ~]#\n\n```\n\nIs there a trick I can use to automatically add a newline at the end of a curl response, to get the prompt back on the left edge of the screen?",
    "answer": "curl -w 'We downloaded %{size_download} bytes\\n' www.download.com\n-w \"\\n\"\n\nFrom the man file:\n\nTo better allow script programmers to get to know about the progress of\ncurl, the -w/--write-out option was introduced. Using this, you can specify\nwhat information from the previous transfer you want to extract.\nTo display the amount of bytes downloaded together with some text and an\nending newline:\n\nSo try adding the following to your ~/.curlrc file:"
  },
  {
    "question": "cURL error 60: SSL certificate: unable to get local issuer certificate I am trying to send an API request using Stripe but get the error message:\n\ncURL error 60: SSL certificate problem: unable to get local issuer certificate\n\nThis is the code I am running:\n\n```\npublic function chargeStripe()\n{\n    $stripe = new Stripe;\n    $stripe = Stripe::make(env('STRIPE_PUBLIC_KEY'));\n\n    $charge = $stripe->charges()->create([\n        'amount'   => 2900,\n        'customer' => Input::get('stripeEmail'),\n        'currency' => 'EUR',\n    ]);\n\n    return Redirect::route('step1');\n}\n\n```\n\nI searched a lot on Google and lots of people are suggesting that I download this file: cacert.pem, put it somewhere and reference it in my php.ini. This is the part in my php.ini:\n\n```\ncurl.cainfo = \"C:\\Windows\\cacert.pem\"\n\n```\n\nYet, even after restarting my server several times and changing the path, I get the same error message.\nI have the ssl_module enabled in Apache, and I have php_curl enabled in my php.ini.\nI have also tried another fix which suggests that I add these lines to my cURL options:\n\n```\ncurl_setopt($process, CURLOPT_CAINFO, dirname(__FILE__) . '/cacert.pem');\ncurl_setopt($process, CURLOPT_SSL_VERIFYPEER, true);\n\n```\n\nWhere do I add options to my cURL? Apparently not through the command line, since my CLI doesn't find the command \"curl_setopt\"",
    "answer": "[curl]\ncurl.cainfo = \"C:\\xampp\\php\\extras\\ssl\\cacert.pem\"\n\n[openssl]\nopenssl.cafile = \"C:\\xampp\\php\\extras\\ssl\\cacert.pem\"\n\nHow to solve this problem:\n\ndownload and extract cacert.pem following the instructions at https://curl.se/docs/caextract.html\n\nsave it on your filesystem somewhere (for example, XAMPP users might use C:\\xampp\\php\\extras\\ssl\\cacert.pem)\n\nin your php.ini, put this file location in the [curl] section (putting it in the [openssl] section is also a good idea):\n\nrestart your webserver (e.g. Apache) and PHP FPM server if applicable\n\n(Reference: https://laracasts.com/discuss/channels/general-discussion/curl-error-60-ssl-certificate-problem-unable-to-get-local-issuer-certificate)"
  },
  {
    "question": "curl: (35) error:1408F10B:SSL routines:ssl3_get_record:wrong version number When I try to connect to any server (e.g. google.com) using curl (or libcurl) I get the error message:\n\ncurl: (35) error:1408F10B:SSL routines:ssl3_get_record:wrong version number\n\nVerbose output:\n\n```\n$ curl www.google.com --verbose  \n* Rebuilt URL to: www.google.com/  \n* Uses proxy env variable no_proxy == 'localhost,127.0.0.1,localaddress,.localdomain.com'  \n* Uses proxy env variable http_proxy == 'https://proxy.in.tum.de:8080'  \n*   Trying 131.159.0.2...  \n* TCP_NODELAY set  \n* Connected to proxy.in.tum.de (131.159.0.2) port 8080 (#0)  \n* successfully set certificate verify locations:  \n*   CAfile: /etc/ssl/certs/ca-certificates.crt  \n  CApath: none  \n* TLSv1.3 (OUT), TLS handshake, Client hello (1):  \n* error:1408F10B:SSL routines:ssl3_get_record:wrong version number  \n* Closing connection 0  \ncurl: (35) error:1408F10B:SSL routines:ssl3_get_record:wrong version number'  \n\n```\n\nFor some reason curl seems to use TLSv1.3 even if I force it to use TLSv1.2 with the command --tlsv1.2  (it will still print TLSv1.3 (OUT), ...\" \nI am using the newest version of both Curl and OpenSSL :\n\n```\n$ curl -V  \ncurl 7.61.0-DEV (x86_64-pc-linux-gnu) libcurl/7.61.0-DEV OpenSSL/1.1.1 zlib/1.2.8  \nRelease-Date: [unreleased]  \nProtocols: dict file ftp ftps gopher http https imap imaps pop3 pop3s rtsp smb smbs smtp smtps telnet tftp  \nFeatures: AsynchDNS IPv6 Largefile NTLM NTLM_WB SSL libz TLS-SRP UnixSockets HTTPS-proxy  \n\n```\n\nI think this is a problem related to my installation of the programms.\nCan somebody explain to me what this error message means?",
    "answer": "* Uses proxy env variable http_proxy == 'https://proxy.in.tum.de:8080'\n^^^^^\n\nThe https:// is wrong, it should be http://. The proxy itself should be accessed by HTTP and not HTTPS even though the target URL is HTTPS. The proxy will nevertheless properly handle HTTPS connection and keep the end-to-end encryption. See HTTP CONNECT method for details how this is done."
  },
  {
    "question": "How to send file contents as body entity using cURL I am using cURL command line utility to send HTTP POST to a web service.  I want to include a file's contents as the body entity of the POST.  I have tried using -d </path/to/filename> as well as other variants with type info like --data </path/to/filename> --data-urlencode </path/to/filename> etc... the file is always attached.  I need it as the body entity.",
    "answer": "curl --data \"@/path/to/filename\" http://...\ncurl --data-binary \"@/path/to/filename\" http://...\n\nI believe you're looking for the @filename syntax, e.g.:\nstrip new lines\n\nkeep new lines\n\ncurl will strip all newlines from the file. If you want to send the file with newlines intact, use --data-binary in place of --data"
  },
  {
    "question": "Save file to specific folder with curl command In a shell script, I want to download a file from some URL and save it to a specific folder. What is the specific CLI flag I should use to download files to a specific folder with the curl command, or how else do I get that result?",
    "answer": "cd target/path && { curl -O URL ; cd -; }\n(cd target/path && curl -O URL)\ncurl -o target/path/filename URL\n\nI don't think you can give a path to curl, but you can CD to the location, download and CD back.\n\nOr using subshell.\n\nBoth ways will only download if path exists. -O keeps remote file name. After download it will return to original location.\nIf you need to set filename explicitly, you can use small -o option:"
  },
  {
    "question": "How do I make a request using HTTP basic authentication with PHP curl? I'm building a REST web service client in PHP and at the moment I'm using curl to make requests to the service.\nHow do I use curl to make authenticated (http basic) requests? Do I have to add the headers myself?",
    "answer": "curl_setopt($ch, CURLOPT_USERPWD, $username . \":\" . $password);\nch = curl_init($host);\ncurl_setopt($ch, CURLOPT_HTTPHEADER, array('Content-Type: application/xml', $additionalHeaders));\ncurl_setopt($ch, CURLOPT_HEADER, 1);\ncurl_setopt($ch, CURLOPT_USERPWD, $username . \":\" . $password);\ncurl_setopt($ch, CURLOPT_TIMEOUT, 30);\ncurl_setopt($ch, CURLOPT_POST, 1);\ncurl_setopt($ch, CURLOPT_POSTFIELDS, $payloadName);\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER, TRUE);\nreturn = curl_exec($ch);\ncurl_close($ch);\n\nYou want this:\n\nZend has a REST client and zend_http_client and I'm sure PEAR has some sort of wrapper.\nBut its easy enough to do on your own.\nSo the entire request might look something like this:"
  },
  {
    "question": "CURL Command Line URL Parameters I am trying to send a DELETE request with a url parameter using CURL. I am doing:\n\n```\ncurl -H application/x-www-form-urlencoded -X DELETE http://localhost:5000/locations` -d 'id=3'\n\n```\n\nHowever, the server is not seeing the parameter id = 3. I tried using some GUI application and when I pass the url as: http://localhost:5000/locations?id=3, it works. I really would rather use CURL rather than this GUI application. Can anyone please point out what I'm doing wrong?",
    "answer": "curl -X DELETE \"http://localhost:5000/locations?id=3\"\ncurl -X GET \"http://localhost:5000/locations?id=3\"\n\nThe application/x-www-form-urlencoded Content-type header is not required (well, kinda depends).   Unless the request handler expects parameters coming from the form body.   Try it out:\n\nor"
  },
  {
    "question": "What is the right way to POST multipart/form-data using curl? I used this syntax to post a file along with some parameters:\n\n```\ncurl -v -include --form \"key1=value1\" --form upload=localfilename URL\n\n```\n\nThe file is around 500K in size. First of all, I see content length to be 254 on the transmit side. Later the server response's content length is 0. \nWhere am I going wrong? \nHere is the complete trace of the command.\n\n```\n* Couldn't find host xxx.xxx.xxx.xxx in the _netrc file; using defaults\n* About to connect() to xxx.xxx.xxx.xxx port yyyy (#0)\n*   Trying xxx.xxx.xxx.xxx...\n* Adding handle: conn: 0x4b96a0\n* Adding handle: send: 0\n* Adding handle: recv: 0\n* Curl_addHandleToPipeline: length: 1\n* - Conn 0 (0x4b96a0) send_pipe: 1, recv_pipe: 0\n* Connected to xxx.xxx.xxx.xxx (xxx.xxx.xxx.xxx) port yyyy (#0)\n* POST /zzzzzz/UploadFile HTTP/1.1\n* User-Agent: curl/7.32.0\n* Host: xxx.xxx.xxx.xxx:yyyy\n* Accept: */*\n* Content-Length: 254\n* Expect: 100-continue\n* Content-Type: multipart/form-data; boundary=------------------------948a6137eef50079\n*\n* HTTP/1.1 100 Continue\n* HTTP/1.1 100 Continue\n\n* HTTP/1.1 200 OK\n* HTTP/1.1 200 OK\n* Server Apache-Coyote/1.1 is not blacklisted\n* Server: Apache-Coyote/1.1\n* Server: Apache-Coyote/1.1\n* Added cookie JSESSIONID=\"C1D7DD042E250211D9DEA82688876F88\" for domain xxx.xxx.xxx.xxx, path /zzzzz/, expire 0\n* Set-Cookie: JSESSIONID=C1D7DD042E250211D9DEA82688876F88; Path=/zzzzzz/;\n* HttpOnly\n* Set-Cookie: JSESSIONID=C1D7DD042E250211D9DEA82688876F88; Path=/zzzzzz/; HttpOnly\n* Content-Type: text/html;charset=ISO-8859-1\nContent-Type: text/html;charset=ISO-8859-1\n* Content-Length: 0\n* Content-Length: 0\n* Date: Tue, 01 Oct 2013 11:54:24 GMT\n* Date: Tue, 01 Oct 2013 11:54:24 GMT\n* Connection #0 to host xxx.xxx.xxx.xxx left intact\n\n```",
    "answer": "curl -v -F key1=value1 -F upload=@localfilename URL\n\nThe following syntax fixes it for you:"
  },
  {
    "question": "Composer install error - requires ext_curl when it&#39;s actually enabled I'm trying to install Facebook PHP SDK with Composer. This is what I get\n\n```\n$ composer install\nLoading composer repositories with package information\nInstalling dependencies (including require-dev)\nYour requirements could not be resolved to an installable set of packages.\n\n    Problem 1\n        - Installation request for facebook/php-sdk dev-master -> satisfiable by facebook/php-sdk[dev-master].\n        - facebook/php-sdk dev-master requires ext-curl * -> the requested PHP extension curl is missing from your system.\n\n```\n\nProblem is, I have curl extension enabled (uncommented in php.ini). When I run phpinfo(), it says it's enabled.\nOnly clue I have is that when I run $ php -m, 'curl' line is missing but I don't know what to do about it.\nI have wamp 2.4 on Win8 and I'm running composer in cmd.exe.",
    "answer": "sudo apt-get install php5-curl\nsudo apt-get install php7.0-curl\nsudo apt-get install php7.1-curl\nsudo apt-get install php7.2-curl\nsudo apt-get install php7.3-curl\nsudo apt-get install php7.4-curl\nsudo apt-get install php8.0-curl\nsudo apt-get install php8.1-curl\nsudo apt-get install php-curl\n\nThis is caused because you don't have a library php5-curl installed in your system,\nOn Ubuntu its just simple run the line code below, in your case on Xamp take a look in Xamp documentation\n\nFor anyone who uses php7.0\n\nFor those who uses php7.1\n\nFor those who use php7.2\n\nFor those who use php7.3\n\nFor those who use php7.4\n\nFor those who use php8.0\n\nFor those who use php8.1\n\nOr simply run below command to install by your version:"
  },
  {
    "question": "How do I deal with certificates using cURL while trying to access an HTTPS url? I am getting the following error using curl:\n\n```\n\ncurl: (77) error setting certificate verify locations:\n  CAfile: /etc/ssl/certs/ca-certificates.crt\n  CApath: none\n\n```\n\nHow do I set this certificate verify locations?",
    "answer": "This error is related to a missing package: ca-certificates. Install it.\nIn Ubuntu Linux (and similar distro):\n\nIn CygWin via Apt-Cyg\n\nIn Arch Linux (Raspberry Pi)\n\nThe documentation tells:\n\nThis package includes PEM files of CA certificates to allow SSL-based applications to check for the authenticity of SSL connections.\n\nAs seen at: Debian -- Details of package ca-certificates in squeeze"
  },
  {
    "question": "How to pass payload via JSON file for curl? I can successfully create a place via curl executing the following command:\n\n```\n$ curl -vX POST https://server/api/v1/places.json -d \"\n  auth_token=B8dsbz4HExMskqUa6Qhn& \\\n  place[name]=Fuelstation Central& \\\n  place[city]=Grossbeeren& \\\n  place[address]=Buschweg 1& \\\n  place[latitude]=52.3601& \\\n  place[longitude]=13.3332& \\\n  place[washing]=true& \\\n  place[founded_at_year]=2000& \\\n  place[products][]=diesel& \\\n  place[products][]=benzin \\\n\"\n\n```\n\nThe server returns HTTP/1.1 201 Created.\nNow I want to store the payload in a JSON file which looks like this:\n\n```\n// testplace.json\n{\n  \"auth_token\" : \"B8dsbz4HExMskqUa6Qhn\",\n  \"name\" : \"Fuelstation Central\",\n  \"city\" : \"Grossbeeren\",\n  \"address\" : \"Buschweg 1\",\n  \"latitude\" : 52.3601,\n  \"longitude\" : 13.3332,\n  \"washing\" : true,\n  \"founded_at_year\" : 2000,\n  \"products\" : [\"diesel\",\"benzin\"]\n}\n\n```\n\nSo I modify the command to be executed like this:\n\n```\n$ curl -vX POST http://server/api/v1/places.json -d @testplace.json\n\n```\n\nThis fails returning HTTP/1.1 401 Unauthorized. Why?",
    "answer": "curl -vX POST http://server/api/v1/places.json -d @testplace.json \\\n--header \"Content-Type: application/json\"\n\ncurl sends POST requests with the default content type of application/x-www-form-urlencoded. If you want to send a JSON request, you will have to specify the correct content type header:\n\nBut that will only work if the server accepts json input. The .json at the end of the url may only indicate that the output is json, it doesn't necessarily mean that it also will handle json input. The API documentation should give you a hint on whether it does or not.\nThe reason you get a 401 and not some other error is probably because the server can't extract the auth_token from your request."
  },
  {
    "question": "Using curl POST with variables defined in bash script functions When I echo I get this, which runs when I enter it into the terminal\n\n```\ncurl -i \\\n-H \"Accept: application/json\" \\\n-H \"Content-Type:application/json\" \\\n-X POST --data '{\"account\":{\"email\":\"akdgdtk@test.com\",\"screenName\":\"akdgdtk\",\"type\":\"NIKE\",\"passwordSettings\":{\"password\":\"Starwars1\",\"passwordConfirm\":\"Starwars1\"}},\"firstName\":\"Test\",\"lastName\":\"User\",\"middleName\":\"ObiWan\",\"locale\":\"en_US\",\"registrationSiteId\":\"520\",\"receiveEmail\":\"false\",\"dateOfBirth\":\"1984-12-25\",\"mobileNumber\":\"9175555555\",\"gender\":\"male\",\"fuelActivationDate\":\"2010-10-22\",\"postalCode\":\"10022\",\"country\":\"US\",\"city\":\"Beverton\",\"state\":\"OR\",\"bio\":\"This is a test user\",\"jpFirstNameKana\":\"unsure\",\"jpLastNameKana\":\"ofthis\",\"height\":\"80\",\"weight\":\"175\",\"distanceUnit\":\"MILES\",\"weightUnit\":\"POUNDS\",\"heightUnit\":\"FT/INCHES\"}' https://xxx:xxxxx@xxxx-www.xxxxx.com/xxxxx/xxxx/xxxx\n\n```\n\nBut when run in the bash script file, I get this error\n\n```\ncurl: (6) Could not resolve host: application; nodename nor servname provided, or not known\ncurl: (6) Could not resolve host: is; nodename nor servname provided, or not known\ncurl: (6) Could not resolve host: a; nodename nor servname provided, or not known\ncurl: (6) Could not resolve host: test; nodename nor servname provided, or not known\ncurl: (3) [globbing] unmatched close brace/bracket at pos 158\n\n```\n\nthis is the code in the file\n\n```\ncurl -i \\\n-H '\"'Accept: application/json'\"' \\\n-H '\"'Content-Type:application/json'\"' \\\n-X POST --data \"'\"'{\"account\":{\"email\":\"'$email'\",\"screenName\":\"'$screenName'\",\"type\":\"'$theType'\",\"passwordSettings\":{\"password\":\"'$password'\",\"passwordConfirm\":\"'$password'\"}},\"firstName\":\"'$firstName'\",\"lastName\":\"'$lastName'\",\"middleName\":\"'$middleName'\",\"locale\":\"'$locale'\",\"registrationSiteId\":\"'$registrationSiteId'\",\"receiveEmail\":\"'$receiveEmail'\",\"dateOfBirth\":\"'$dob'\",\"mobileNumber\":\"'$mobileNumber'\",\"gender\":\"'$gender'\",\"fuelActivationDate\":\"'$fuelActivationDate'\",\"postalCode\":\"'$postalCode'\",\"country\":\"'$country'\",\"city\":\"'$city'\",\"state\":\"'$state'\",\"bio\":\"'$bio'\",\"jpFirstNameKana\":\"'$jpFirstNameKana'\",\"jpLastNameKana\":\"'$jpLastNameKana'\",\"height\":\"'$height'\",\"weight\":\"'$weight'\",\"distanceUnit\":\"MILES\",\"weightUnit\":\"POUNDS\",\"heightUnit\":\"FT/INCHES\"}'\"'\" \"https://xxx:xxxxx@xxxx-www.xxxxx.com/xxxxx/xxxx/xxxx\"\n\n```\n\nI assume there's an issue with my quotation marks, but I've played with them a lot and I've gotten similar errors.  All the variables are defined with different functions in the actual script",
    "answer": "generate_post_data()\n{\ncat <<EOF\n{\n\"account\": {\n\"email\": \"$email\",\n\"screenName\": \"$screenName\",\n\"type\": \"$theType\",\n\"passwordSettings\": {\n\"password\": \"$password\",\n\"passwordConfirm\": \"$password\"\n}\n},\n\"firstName\": \"$firstName\",\n\"lastName\": \"$lastName\",\n\"middleName\": \"$middleName\",\n\"locale\": \"$locale\",\n\"registrationSiteId\": \"$registrationSiteId\",\n\"receiveEmail\": \"$receiveEmail\",\n\"dateOfBirth\": \"$dob\",\n\"mobileNumber\": \"$mobileNumber\",\n\"gender\": \"$gender\",\n\"fuelActivationDate\": \"$fuelActivationDate\",\n\"postalCode\": \"$postalCode\",\n\"country\": \"$country\",\n\"city\": \"$city\",\n\"state\": \"$state\",\n\"bio\": \"$bio\",\n\"jpFirstNameKana\": \"$jpFirstNameKana\",\n\"jpLastNameKana\": \"$jpLastNameKana\",\n\"height\": \"$height\",\n\"weight\": \"$weight\",\n\"distanceUnit\": \"MILES\",\n\"weightUnit\": \"POUNDS\",\n\"heightUnit\": \"FT/INCHES\"\n}\nEOF\n}\ncurl -i \\\n-H \"Accept: application/json\" \\\n-H \"Content-Type:application/json\" \\\n-X POST --data \"$(generate_post_data)\" \"https://xxx:xxxxx@xxxx-www.xxxxx.com/xxxxx/xxxx/xxxx\"\n\nYou don't need to pass the quotes enclosing the custom headers to curl. Also, your variables in the middle of the data argument should be quoted.\nFirst, write a function that generates the post data of your script. This saves you from all sort of headaches concerning shell quoting and makes it easier to read and maintain the script than feeding the post data on curl's invocation line as in your attempt:\n\nIt is then easy to use that function in the invocation of curl:\n\nThis said, here are a few clarifications about shell quoting rules:\nThe double quotes in the -H arguments (as in -H \"foo bar\") tell bash to keep what's inside as a single argument (even if it contains spaces).\nThe single quotes in the --data argument (as in --data 'foo bar') do the same, except they pass all text verbatim (including double quote characters and the dollar sign).\nTo insert a variable in the middle of a single quoted text, you have to end the single quote, then concatenate with the double quoted variable, and re-open the single quote to continue the text: 'foo bar'\"$variable\"'more foo'."
  },
  {
    "question": "How do I install the ext-curl extension with PHP 7? I've installed PHP 7 using this repo, but when I try to run composer install, it's giving this error:\n\n\n[package] requires ext-curl * -> the requested PHP extension curl is missing from your system.\n\n\nWith PHP 5, you can easily install it by running the yum or apt-get install php5-curl command, but I can't find how to install the PHP 7 equivalent.\nHow do I install ext-curl for PHP 7?",
    "answer": "sudo apt-get install php-curl\nsudo service apache2 restart\n\nWell I was able to install it by :\n\non my system. This will install a dependency package, which depends on the default php version.\nAfter that restart apache"
  },
  {
    "question": "PHP - Debugging Curl I'd like to see what the post fields in the request are before I send it. (For debugging purposes).\nThe PHP library (class) I am using is already made (not by me), so I am trying to understand it.\nAs far as I can tell, it uses curl_setopt() to set different options like headers and such and then it uses curl_exec() to send the request.\nIdeas on how to see what post fields are being sent?",
    "answer": "curl_setopt($curlhandle, CURLOPT_VERBOSE, true);\n\nYou can enable the CURLOPT_VERBOSE option:\n\nWhen CURLOPT_VERBOSE is set, output is written to STDERR or the file specified using CURLOPT_STDERR. The output is very informative.\nYou can also use tcpdump or wireshark to watch the network traffic."
  },
  {
    "question": "Getting HTTP code in PHP using curl I'm using CURL to get the status of a site, if it's up/down or redirecting to another site. I want to get it as streamlined as possible, but it's not working well.\n\n```\n<?php\n$ch = curl_init($url);\ncurl_setopt($ch,CURLOPT_RETURNTRANSFER,1);\ncurl_setopt($ch,CURLOPT_TIMEOUT,10);\n$output = curl_exec($ch);\n$httpcode = curl_getinfo($ch, CURLINFO_HTTP_CODE);\ncurl_close($ch);\n\nreturn $httpcode;\n?>\n\n```\n\nI have this wrapped in a function. It works fine but performance is not the best because it downloads the whole page, thing in if I remove $output = curl_exec($ch); it returns 0 all the time.\nDoes anyone know how to make the performance better?",
    "answer": "if(!$url || !is_string($url) || ! preg_match('/^http(s)?:\\/\\/[a-z0-9-]+(.[a-z0-9-]+)*(:[0-9]+)?(\\/.*)?$/i', $url)){\nreturn false;\n}\n@curl_setopt($ch, CURLOPT_HEADER  , true);  // we want headers\n@curl_setopt($ch, CURLOPT_NOBODY  , true);  // we don't need body\nurl = 'http://www.example.com';\nch = curl_init($url);\ncurl_setopt($ch, CURLOPT_HEADER, true);    // we want headers\ncurl_setopt($ch, CURLOPT_NOBODY, true);    // we don't need body\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER,1);\ncurl_setopt($ch, CURLOPT_TIMEOUT,10);\noutput = curl_exec($ch);\nhttpcode = curl_getinfo($ch, CURLINFO_HTTP_CODE);\ncurl_close($ch);\n\necho 'HTTP code: ' . $httpcode;\n\nFirst make sure if the URL is actually valid (a string, not empty, good syntax), this is quick to check server side. For example, doing this first could save a lot of time:\n\nMake sure you only fetch the headers, not the body content:\n\nFor more details on getting the URL status http code I refer to another post I made (it also helps with following redirects):\n\nHow can I check if a URL exists via PHP?\n\nAs a whole:"
  },
  {
    "question": "PHP - SSL certificate error: unable to get local issuer certificate I'm running PHP Version 5.6.3 as part of XAMPP on Windows 7.\nWhen I try to use the Mandrill API, I'm getting the following error:\n\nUncaught exception 'Mandrill_HttpError' with message 'API call to messages/send-template failed: SSL certificate problem: unable to get local issuer certificate'\n\nI already tried everything I read on StackOverflow, including adding the following to the php.ini file:\n\n```\ncurl.cainfo = \"C:\\xampp\\php\\cacert.pem\"\n\n```\n\nAnd ofcourse downloaded to that location the cacert.pem file from http://curl.haxx.se/docs/caextract.html\nbut after all that, restarted XAMPP and Apache server but still getting the same error.\nI really don't know what else to try.\nCan anyone advise on what else can I try?",
    "answer": "curl.cainfo=\"C:/wamp/cacert.pem\"\nopenssl.cafile=\"C:/wamp/cacert.pem\"\n\nFinally got this to work!\n\nDownload the certificate bundle.\nPut it somewhere. In my case, that was c:\\wamp\\ directory (if you are using Wamp 64 bit then it's c:\\wamp64\\).\nEnable mod_ssl in Apache and php_openssl.dll in php.ini (uncomment them by removing ; at the beginning). But be careful, my problem was that I had two php.ini files and I need to do this in both of them. One is the one you get from your WAMP taskbar icon, and another one is, in my case, in C:\\wamp\\bin\\php\\php5.5.12\\\nAdd these lines to your cert in both php.ini files:\n\nRestart Wamp services."
  },
  {
    "question": "Execute bash script from URL Say I have a file at the URL http://mywebsite.example/myscript.txt that contains a script:\n\n```\n#!/bin/bash\necho \"Hello, world!\"\nread -p \"What is your name? \" name\necho \"Hello, ${name}!\"\n\n```\n\nAnd I'd like to run this script without first saving it to a file. How do I do this?\nNow, I've seen the syntax:\n\n```\nbash < <(curl -s http://mywebsite.example/myscript.txt)\n\n```\n\nBut this doesn't seem to work like it would if I saved to a file and then executed. For example readline doesn't work, and the output is just:\n\n```\n$ bash < <(curl -s http://mywebsite.example/myscript.txt)\nHello, world!\n\n```\n\nSimilarly, I've tried:\n\n```\ncurl -s http://mywebsite.example/myscript.txt | bash -s --\n\n```\n\nWith the same results.\nOriginally I had a solution like:\n\n```\ntimestamp=`date +%Y%m%d%H%M%S`\ncurl -s http://mywebsite.example/myscript.txt -o /tmp/.myscript.${timestamp}.tmp\nbash /tmp/.myscript.${timestamp}.tmp\nrm -f /tmp/.myscript.${timestamp}.tmp\n\n```\n\nBut this seems sloppy, and I'd like a more elegant solution.\nI'm aware of the security issues regarding running a shell script from a URL, but let's ignore all of that for right now.",
    "answer": "source <(curl -s http://mywebsite.example/myscript.txt)\nbash <(curl -s http://mywebsite.example/myscript.txt)\n\nought to do it. Alternately, leave off the initial redirection on yours, which is redirecting standard input; bash takes a filename to execute just fine without redirection, and <(command) syntax provides a path.\n\nIt may be clearer if you look at the output of echo <(cat /dev/null)"
  },
  {
    "question": "What is cURL in PHP? In PHP, I see the word cURL in many PHP projects.  What is it?  How does it work?\nReference Link: cURL",
    "answer": "// Make a HTTP GET request and print it (requires allow_url_fopen to be enabled)\nprint file_get_contents('http://www.example.com/');\n\ncURL is a library that lets you make HTTP requests in PHP. Everything you need to know about it (and most other extensions) can be found in the PHP manual.\n\nIn order to use PHP's cURL functions\nyou need to install the \u00bb libcurl\npackage. PHP requires that you use\nlibcurl 7.0.2-beta or higher. In PHP\n4.2.3, you will need libcurl version 7.9.0 or higher. From PHP 4.3.0, you will need a libcurl version that's\n7.9.8 or higher. PHP 5.0.0 requires a libcurl version 7.10.5 or greater.\n\nYou can make HTTP requests  without cURL, too, though it requires allow_url_fopen to be enabled in your php.ini file."
  },
  {
    "question": "How to specify the download location with wget? I need files to be downloaded to /tmp/cron_test/. My wget code is\n\n```\nwget --random-wait -r -p -nd -e robots=off -A\".pdf\" -U mozilla http://math.stanford.edu/undergrad/\n\n```\n\nSo is there some parameter to specify the directory?",
    "answer": "-P prefix\n--directory-prefix=prefix\nSet directory prefix to prefix.  The directory prefix is the\ndirectory where all other files and sub-directories will be\nsaved to, i.e. the top of the retrieval tree.  The default\nis . (the current directory).\n\nFrom the manual page:\n\nSo you need to add -P /tmp/cron_test/ (short form) or --directory-prefix=/tmp/cron_test/ (long form) to your command. Also note that if the directory does not exist it will get created."
  },
  {
    "question": "How to change filename of a file downloaded with wget? I am downloading a file from www.examplesite.com/textfile.txt\nWhen running the following command\n\n```\nwget www.examplesite.com/textfile.txt\n\n```\n\nthe file is saved as textfile. How can I save it as newfile.txt?",
    "answer": "wget google.com\n...\n16:07:52 (538.47 MB/s) - `index.html' saved [10728]\nwget -O foo.html google.com\n...\n16:08:00 (1.57 MB/s) - `foo.html' saved [10728]\n\nUse the -O file option.\nE.g.\n\nvs."
  },
  {
    "question": "Using wget to recursively fetch a directory with arbitrary files in it I have a web directory where I store some config files. I'd like to use wget to pull those files down and maintain their current structure. For instance, the remote directory looks like:\n\n```\nhttp://mysite.com/configs/.vim/\n\n```\n\n.vim holds multiple files and directories. I want to replicate that on the client using wget. Can't seem to find the right combo of wget flags to get this done. Any ideas?",
    "answer": "wget --recursive --no-parent http://example.com/configs/.vim/\nwget -r -np -R \"index.html*\" http://example.com/configs/.vim/\n\nYou have to pass the -np/--no-parent option to wget (in addition to -r/--recursive, of course), otherwise it will follow the link in the directory index on my site to the parent directory. So the command would look like this:\n\nTo avoid downloading the auto-generated index.html files, use the -R/--reject option:"
  },
  {
    "question": "Downloading Java JDK on Linux via wget is shown license page instead When I try to download Java from Oracle I instead end up downloading a page telling me that I need agree to the OTN license terms.\n\nSorry!\nIn order to download products from Oracle Technology Network you must agree to the OTN license terms.\nBe sure that...\n\nYour browser has \"cookies\" and JavaScript enabled.\nYou clicked on \"Accept License\" for the product you wish to download.\nYou attempt the download within 30 minutes of accepting the license.\n\n\nHow can I download and install Java?",
    "answer": "wget --no-check-certificate -c --header \"Cookie: oraclelicense=accept-securebackup-cookie\" https://download.oracle.com/java/17/archive/jdk-17.0.1_linux-x64_bin.rpm\nwget --no-check-certificate -c --header \"Cookie: oraclelicense=accept-securebackup-cookie\" https://download.oracle.com/otn-pub/java/jdk/16.0.2%2B7/d4a915d82b4c4fbb9bde534da945d746/jdk-16.0.2_linux-x64_bin.rpm\nwget --no-check-certificate -c --header \"Cookie: oraclelicense=accept-securebackup-cookie\" https://download.oracle.com/otn-pub/java/jdk/15.0.1+9/51f4f36ad4ef43e39d0dfdbaf6549e32/jdk-15.0.1_linux-x64_bin.rpm\nwget --no-check-certificate -c --header \"Cookie: oraclelicense=accept-securebackup-cookie\" https://download.oracle.com/otn-pub/java/jdk/14.0.1+7/664493ef4a6946b186ff29eb326336a2/jdk-14.0.1_linux-x64_bin.rpm -O ~/Downloads/jdk-14.0.1_linux-x64_bin.rpm\nwget --no-check-certificate -c --header \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/11+28/55eed80b163941c8885ad9298e6d786a/jdk-11_linux-x64_bin.tar.gz\nwget --no-check-certificate -c --header \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/10.0.2+13/19aef61b38124481863b1413dce1855f/jdk-10.0.2_linux-x64_bin.tar.gz\nwget --no-check-certificate -c --header \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/10.0.1+10/fb4372174a714e6b8c52526dc134031e/jdk-10.0.1_linux-x64_bin.tar.gz\nwget http://download.java.net/java/GA/jdk9/9/binaries/jdk-9+181_linux-x64_bin.tar.gz\nwget --no-cookies --no-check-certificate --header \"Cookie: gpw_e24=http%3a%2F%2Fwww.oracle.com%2Ftechnetwork%2Fjava%2Fjavase%2Fdownloads%2Fjdk8-downloads-2133151.html; oraclelicense=accept-securebackup-cookie;\" \"https://download.oracle.com/otn-pub/java/jdk/8u191-b12/2787e4a523244c269598db4e85c51e0c/jdk-8u191-linux-x64.tar.gz\"\nwget --no-cookies --no-check-certificate --header \"Cookie: gpw_e24=http%3a%2F%2Fwww.oracle.com%2Ftechnetwork%2Fjava%2Fjavase%2Fdownloads%2Fjdk8-downloads-2133151.html; oraclelicense=accept-securebackup-cookie;\" \"https://download.oracle.com/otn-pub/java/jdk/8u191-b12/2787e4a523244c269598db4e85c51e0c/jdk-8u191-linux-x64.rpm\"\nwget -c --header \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/8u131-b11/d54c1d3a095b4ff2b6607d096fa80163/jdk-8u131-linux-x64.rpm\nwget -c --header \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/8u131-b11/d54c1d3a095b4ff2b6607d096fa80163/jdk-8u131-linux-x64.tar.gz\ncurl -v -j -k -L -H \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/8u131-b11/d54c1d3a095b4ff2b6607d096fa80163/jdk-8u131-linux-x64.rpm > jdk-8u112-linux-x64.rpm\nwget --no-check-certificate --no-cookies --header \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/7u79-b15/jdk-7u79-linux-x64.tar.gz\ncurl -v -j -k -L -H \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/7u79-b15/jdk-7u79-linux-x64.rpm > jdk-7u79-linux-x64.rpm\nwget --no-cookies --header \"Cookie: gpw_e24=http%3A%2F%2Fwww.oracle.com\" \"http://download.oracle.com/otn-pub/java/jdk/7/jdk-7-linux-x64.tar.gz\"\n\nWorks as of December 23rd, 2021 for JDK 17\n\nWorks as of July 27th, 2021 for JDK 16\n\nWorks as of November 5th, 2020 for JDK 15\n\nWorks as of 07-11-2020 for JDK 14\n\nPS: Alf added this ( me ) :-) this, I couldn't figured out how to just commented at the end... Enjoy it.\nUPDATED FOR Oracle JDK 11\n\nUPDATED FOR JDK 10.0.2\n\nUPDATED FOR JDK 10.0.1\n\nUPDATED FOR JDK 9\nit looks like you can download it now directly from java.net without sending a header\n\nUPDATED FOR JDK 8u191\nTAR GZ:\n\nRPM:\n\nUPDATED FOR JDK 8u131\nRPM:\n\nTAR GZ:\n\nRPM using curl:\n\nIn all cases above, subst 'i586' for 'x64' to download the 32-bit build.\n\n-j -> junk cookies\n-k -> ignore certificates\n-L -> follow redirects\n-H [arg] -> headers\n\ncurl can be used in place of wget.\nUPDATE FOR JDK 7u79\nTAR GZ:\n\nRPM using curl:\n\nOnce again, make sure you specify the correct URL for the version you are downloading. You can find the URL here: Oracle JDK download site\nORIGINAL ANSWER FROM 9th June 2012\nIf you are looking to download the Oracle JDK from the command line using wget, there is a workaround. Run the wget command as follows:\n\nBe sure to replace the download link with the correct one for the version you are downloading."
  },
  {
    "question": "How to download HTTP directory with all files and sub-directories as they appear on the online files/folders list? There is an online HTTP directory that I have access to. I have tried to download all sub-directories and files via wget. But, the problem is that when wget downloads sub-directories it downloads the index.html file which contains the list of files in that directory without downloading the files themselves.\nIs there a way to download the sub-directories and files without depth limit (as if the directory I want to download is just a folder which I want to copy to my computer).",
    "answer": "wget -r -np -nH --cut-dirs=3 -R index.html http://hostname/aaa/bbb/ccc/ddd/\n\nSolution:\n\nExplanation:\n\nIt will download all files and subfolders in ddd directory\n-r : recursively\n-np : not going to upper directories, like ccc/\u2026\n-nH : not saving files to hostname folder\n--cut-dirs=3 : but saving it to ddd by omitting\nfirst 3 folders aaa, bbb, ccc\n-R index.html : excluding index.html\nfiles\n\nReference: http://bmwieczorek.wordpress.com/2008/10/01/wget-recursively-download-all-files-from-certain-directory-listed-by-apache/"
  },
  {
    "question": "How to install wget in macOS? I try to install wget in MAC OS 10.11.1 but when I run ./configure --with-ssl=openssl I get this error:\n\n```\nconfigure: error: --with-ssl=openssl was given, but SSL is not available.\n\n```\n\nHow to resolve this problem in OSX 10.11.1?",
    "answer": "/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nbrew install wget\nsudo port install wget\n\nUsing brew\nFirst install brew:\n\nAnd then install wget with brew:\n\nUsing MacPorts\nFirst, download and run MacPorts installer (.pkg)\nAnd then install wget:"
  },
  {
    "question": "How to get past the login page with Wget? I am trying to use Wget to download a page, but I cannot get past the login screen.\nHow do I send the username/password using post data on the login page and then download the actual page as an authenticated user?",
    "answer": "wget --save-cookies cookies.txt \\\n--keep-session-cookies \\\n--post-data 'user=foo&password=bar' \\\n--delete-after \\\nhttp://server.com/auth.php\n\nwget --load-cookies cookies.txt \\\nhttp://server.com/interesting/article.php\n\nBased on the manual page:\n\nMake sure the --post-data parameter is properly percent-encoded (especially ampersands!) or the request will probably fail. Also make sure that user and password are the correct keys; you can find out the correct keys by sleuthing the HTML of the login page (look into your browser\u2019s \u201cinspect element\u201d feature and find the name attribute on the username and password fields)."
  },
  {
    "question": "Download a working local copy of a webpage I would like to download a local copy of a web page and get all of the css, images, javascript, etc.  \nIn previous discussions (e.g. here and here, both of which are more than two years old), two suggestions are generally put forward: wget -p and httrack.  However, these suggestions both fail.  I would very much appreciate help with using either of these tools to accomplish the task; alternatives are also lovely.  \n\nOption 1: wget -p\nwget -p successfully downloads all of the web page's prerequisites (css, images, js).  However, when I load the local copy in a web browser, the page is unable to load the prerequisites because the paths to those prerequisites haven't been modified from the version on the web. \nFor example:\n\nIn the page's html, <link rel=\"stylesheet href=\"/stylesheets/foo.css\" /> will need to be corrected to point to the new relative path of foo.css\nIn the css file, background-image: url(/images/bar.png) will similarly need to be adjusted.\n\nIs there a way to modify wget -p so that the paths are correct?\n\nOption 2: httrack\nhttrack seems like a great tool for mirroring entire websites, but it's unclear to me how to use it to create a local copy of a single page.  There is a great deal of discussion in the httrack forums about this topic (e.g. here) but no one seems to have a bullet-proof solution.\n\nOption 3: another tool?\nSome people have suggested paid tools, but I just can't believe there isn't a free solution out there.",
    "answer": "wget -p -k http://www.example.com/\n\u2018-k\u2019\n\u2018--convert-links\u2019\nAfter the download is complete, convert the links in the document to make them\nsuitable for local viewing. This affects not only the visible hyperlinks, but\nany part of the document that links to external content, such as embedded images,\nlinks to style sheets, hyperlinks to non-html content, etc.\n\nEach link will be changed in one of the two ways:\n\nThe links to files that have been downloaded by Wget will be changed to refer\nto the file they point to as a relative link.\n\nExample: if the downloaded file /foo/doc.html links to /bar/img.gif, also\ndownloaded, then the link in doc.html will be modified to point to\n\u2018../bar/img.gif\u2019. This kind of transformation works reliably for arbitrary\ncombinations of directories.\n\nThe links to files that have not been downloaded by Wget will be changed to\ninclude host name and absolute path of the location they point to.\n\nExample: if the downloaded file /foo/doc.html links to /bar/img.gif (or to\n../bar/img.gif), then the link in doc.html will be modified to point to\nhttp://hostname/bar/img.gif.\n\nBecause of this, local browsing works reliably: if a linked file was downloaded,\nthe link will refer to its local name; if it was not downloaded, the link will\nrefer to its full Internet address rather than presenting a broken link. The fact\nthat the former links are converted to relative links ensures that you can move\nthe downloaded hierarchy to another directory.\n\nNote that only at the end of the download can Wget know which links have been\ndownloaded. Because of that, the work done by \u2018-k\u2019 will be performed at the end\nof all the downloads.\n\nwget is capable of doing what you are asking. Just try the following:\n\nThe -p will get you all the required elements to view the site correctly (css, images, etc).\nThe -k will change all links (to include those for CSS & images) to allow you to view the page offline as it appeared online.\nFrom the Wget docs:"
  },
  {
    "question": "How do I fix certificate errors when running wget on an HTTPS URL in Cygwin? For example, running wget https://www.dropbox.com results in the following errors:\n\n```\nERROR: The certificate of `www.dropbox.com' is not trusted.\nERROR: The certificate of `www.dropbox.com' hasn't got a known issuer.\n\n```",
    "answer": "Looking at current hacky solutions in here, I feel I have to describe a proper solution after all.\nFirst, you need to install the cygwin package ca-certificates via Cygwin's setup.exe to get the certificates.\nDo NOT use curl or similar hacks to download certificates (as a neighboring answer advices) because that's fundamentally insecure and may compromise the system.\nSecond, you need to tell wget where your certificates are, since it doesn't pick them up by default in Cygwin environment. If you can do that either with the command-line parameter --ca-directory=/usr/ssl/certs (best for shell scripts) or by adding ca_directory = /usr/ssl/certs to ~/.wgetrc file.\nYou can also fix that by running ln -sT /usr/ssl /etc/ssl as pointed out in another answer, but that will work only if you have administrative access to the system. Other solutions I described do not require that."
  },
  {
    "question": "Get final URL after curl is redirected I need to get the final URL after a page redirect preferably with curl or wget.\nFor example http://google.com may redirect to http://www.google.com.\nThe contents are easy to get(ex. curl --max-redirs 10 http://google.com -L), but I'm only interested in the final url (in the former case http://www.google.com).\nIs there any way of doing this by using only Linux built-in tools? (command line only)",
    "answer": "curl -Ls -o /dev/null -w %{url_effective} https://example.com\n-L         Follow redirects\n-s         Silent mode. Don't output anything\n-o FILE    Write output to <file> instead of stdout\n-w FORMAT  What to output after completion\n\ncurl's -w option and the sub variable url_effective is what you are\nlooking for.\nSomething like\n\nMore info\n\nMore\nYou might want to add -I (that is an uppercase i) as well, which will make the command not download any \"body\", but it then also uses the HEAD method, which is not what the question included and risk changing what the server does. Sometimes servers don't respond well to HEAD even when they respond fine to GET."
  },
  {
    "question": "How can I use Python&#39;s Requests to fake a browser visit a.k.a and generate User Agent? I want to get the content from this website.\nIf I use a browser like Firefox or Chrome, I could get the real website page I want, but if I use the Python Requests package (or wget command) to get it, it returns a totally different HTML page.\nI thought the developer of the website had made some blocks for this.\nHow do I fake a browser visit by using Python's Requests or command wget?",
    "answer": "import requests\n\nurl = 'http://www.ichangtou.com/#company:data_000008.html'\nheaders = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n\nresponse = requests.get(url, headers=headers)\nprint(response.content)\n>>> from fake_useragent import UserAgent\n>>> ua = UserAgent()\n>>> ua.chrome\nu'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1667.0 Safari/537.36'\n>>> ua.random\nu'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.67 Safari/537.36'\n\nProvide a User-Agent header:\n\nFYI, here is a list of User-Agent strings for different browsers:\n\nList of all Browsers\n\nAs a side note, there is a pretty useful third-party package called fake-useragent that provides a nice abstraction layer over user agents:\n\nfake-useragent\nUp to date simple useragent faker with real world database\n\nDemo:"
  },
  {
    "question": "How to download an entire directory and subdirectories using wget? I am trying to download the files for a project using wget, as the SVN server for that project isn't running anymore and I am only able to access the files through a browser. The base URLs for all the files is the same like \n\nhttp://abc.tamu.edu/projects/tzivi/repository/revisions/2/raw/tzivi/*\n\nHow can I use wget (or any other similar tool) to download all the files in this repository, where the \"tzivi\" folder is the root folder and there are several files and sub-folders (upto 2 or 3 levels) under it?",
    "answer": "wget -r --no-parent http://abc.tamu.edu/projects/tzivi/repository/revisions/2/raw/tzivi/\n-r     //recursive Download\n--no-parent // Don\u00b4t download something from the parent directory\n-l1 just download the directory (tzivi in your case)\n\n-l2 download the directory and all level 1 subfolders ('tzivi/something' but not 'tivizi/somthing/foo')\n\nYou may use this in shell:\n\nThe Parameters are:\n\nand\n\nIf you don't want to download the entire content, you may use:\n\nAnd so on. If you insert no -l option, wget will use -l 5 automatically.\nIf you insert a -l 0 you\u00b4ll download the whole Internet, because wget will follow every link it finds."
  },
  {
    "question": "How to download all files (but not HTML) from a website using wget? How to use wget and get all the files from website?\nI need all files except the webpage files like HTML, PHP, ASP etc.",
    "answer": "wget -A pdf,jpg -m -p -E -k -K -np http://site/path/\nwget --accept pdf,jpg --mirror --page-requisites --adjust-extension --convert-links --backup-converted --no-parent http://site/path/\n\nTo filter for specific file extensions:\n\nOr, if you prefer long option names:\n\nThis will mirror the site, but the files without jpg or pdf extension will be automatically removed."
  },
  {
    "question": "How to `wget` a list of URLs in a text file? Let's say I have a text file of hundreds of URLs in one location, e.g.\n\n```\nhttp://url/file_to_download1.gz\nhttp://url/file_to_download2.gz\nhttp://url/file_to_download3.gz\nhttp://url/file_to_download4.gz\nhttp://url/file_to_download5.gz\n....\n\n```\n\nWhat is the correct way to download each of these files with wget? I suspect there's a command like wget -flag -flag text_file.txt",
    "answer": "Quick man wget gives me the following:\n\n[..]\n-i file\n--input-file=file\nRead URLs from a local or external file. If - is specified as file, URLs are read from the standard input. (Use ./- to read from a file literally named -.)\nIf this function is used, no URLs need be present on the command line. If there are URLs both on the command line and in an input file, those on the command lines will be the first ones to be retrieved. If --force-html is not specified, then file should consist of a series of URLs, one per line.\n[..]\n\nSo: wget -i text_file.txt"
  },
  {
    "question": "How do I request a file but not save it with Wget? I'm using Wget to make http requests to a fresh web server.  I am doing this to warm the MySQL cache.  I do not want to save the files after they are served.\n\n```\nwget -nv -do-not-save-file $url\n\n```\n\nCan I do something like -do-not-save-file with wget?",
    "answer": "./app &>  file # redirect error and standard output to file\n./app >   file # redirect standard output to file\n./app 2>  file # redirect error output to file\nwget -O/dev/null -q $url\n\nUse q flag for quiet mode, and tell wget to output to stdout with O- (uppercase o) and redirect to /dev/null to discard the output:\nwget -qO- $url &> /dev/null\n> redirects application output (to a file). if > is preceded by ampersand, shell redirects all outputs (error and normal) to the file right of >. If you don't specify ampersand, then only normal output is redirected.\n\nif file is /dev/null then all is discarded.\nThis works as well, and simpler:"
  },
  {
    "question": "How do I use Wget to download all images into a single folder, from a URL? I am using wget to download all images from a website and it works fine but it stores the original hierarchy of the site with all the subfolders and so the images are dotted around. Is there a way so that it will just download all the images into a single folder? The syntax I'm using at the moment is:\n\n```\nwget -r -A jpeg,jpg,bmp,gif,png http://www.somedomain.com\n\n```",
    "answer": "wget -nd -r -P /save/location -A jpeg,jpg,bmp,gif,png http://www.somedomain.com\n\nTry this:\n\nHere is some more information:\n-nd prevents the creation of a directory hierarchy (i.e. no directories).\n-r enables recursive retrieval. See Recursive Download for more information.\n-P sets the directory prefix where all files and directories are saved to.\n-A sets a whitelist for retrieving only certain file types. Strings and patterns are accepted, and both can be used in a comma separated list (as seen above). See Types of Files for more information."
  },
  {
    "question": "How to hide wget output in Linux? I don't want to see any message when I use wget. I want to suppress all the output it normally produces on the screen.\nHow can I do it?",
    "answer": "-q\n--quiet\nTurn off Wget's output.\nwget www.google.com\n--2015-05-08 14:07:42--  http://www.google.com/\nResolving www.google.com (www.google.com)...\n(...)\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/html]\nSaving to: \u2018index.html\u2019\n\n[ <=>                                                                                       ] 17,913      --.-K/s   in 0.01s\n\n2015-05-08 14:07:42 (1.37 MB/s) - \u2018index.html\u2019 saved [17913]\nwget -q www.google.com\n\nWhy don't you use -q?\nFrom man wget:\n\nTest\n\nAnd:"
  },
  {
    "question": "How to get the contents of a webpage in a shell variable? In Linux how can I fetch an URL and get its contents in a variable in shell script?",
    "answer": "content=$(wget google.com -q -O -)\necho $content\ncontent=$(curl -L google.com)\necho $content\n\nYou can use wget command to download the page and read it into a variable as:\n\nWe use the -O option of wget which allows us to specify the name of the file into which wget dumps the page contents. We specify - to get the dump onto standard output and collect that into the variable content. You can add the -q quiet option to turn off's wget output.\nYou can use the curl command for this aswell as:\n\nWe need to use the -L option as the page we are requesting might have moved. In which case we need to get the page from the new location. The -L or --location option helps us with this."
  },
  {
    "question": "How to run wget inside Ubuntu Docker image? I'm trying to download a Debian package inside a Ubuntu container as follows:\n\n```\nsudo docker run ubuntu:14.04 wget https://downloads-packages.s3.amazonaws.com/ubuntu-14.04/gitlab_7.8.2-omnibus.1-1_amd64.deb\n\n```\n\nI get\n\n```\nexec: \"wget\": executable file not found in $PATH\n\n```\n\nI've already installed wget with docker as follows:\n\n```\nrun ubuntu:14.04 apt-get install wget\n\n```\n\nHow can I download a file?",
    "answer": "FROM ubuntu:14.04\nRUN  apt-get update \\\n&& apt-get install -y wget \\\n&& rm -rf /var/lib/apt/lists/*\ndocker build -t my-ubuntu .\ndocker run my-ubuntu wget https://downloads-packages.s3.amazonaws.com/ubuntu-14.04/gitlab_7.8.2-omnibus.1-1_amd64.deb\n\nYou need to install it first. Create a new Dockerfile, and install wget in it:\n\nThen, build that image:\n\nFinally, run it:"
  },
  {
    "question": "Python equivalent of a given wget command I'm trying to create a Python function that does the same thing as this wget command:\n\n```\nwget -c --read-timeout=5 --tries=0 \"$URL\"\n\n```\n\n-c - Continue from where you left off if the download is interrupted.\n--read-timeout=5 - If there is no new data coming in for over 5 seconds, give up and try again.  Given -c this mean it will try again from where it left off.\n--tries=0 - Retry forever.\nThose three arguments used in tandem results in a download that cannot fail.\nI want to duplicate those features in my Python script, but I don't know where to begin...",
    "answer": "urllib.request should work.\nJust set it up in a while(not done) loop, check if a localfile already exists, if it does send a GET with a RANGE header, specifying how far you got in downloading the localfile.\nBe sure to use read() to append to the localfile until an error occurs.\nThis is also potentially a duplicate of Python urllib2 resume download doesn't work when network reconnects"
  },
  {
    "question": "How can I show the wget progress bar only? For example:\n\n```\nwget http://somesite.com/TheFile.jpeg\n\n    downloading: TheFile.tar.gz ...\n    --09:30:42--  http://somesite.com/TheFile.jpeg\n               => `/home/me/Downloads/TheFile.jpeg'\n    Resolving somesite.co... xxx.xxx.xxx.xxx.\n    Connecting to somesite.co|xxx.xxx.xxx.xxx|:80... connected.\n    HTTP request sent, awaiting response... 200 OK\n    Length: 1,614,820 (1.5M) [image/jpeg]\n\n    25% [======>                              ] 614,424      173.62K/s    ETA 00:14\n\n```\n\nHow can I get it to look like the following?\n\n```\n    downloading: TheFile.jpeg ...\n    25% [======>                              ] 614,424      173.62K/s    ETA 00:14\n\n```\n\nI know curl can do that. However, I need to get wget to do that job.",
    "answer": "progressfilt ()\n{\nlocal flag=false c count cr=$'\\r' nl=$'\\n'\nwhile IFS='' read -d '' -rn 1 c\ndo\nif $flag\nthen\nprintf '%s' \"$c\"\nelse\nif [[ $c != $cr && $c != $nl ]]\nthen\ncount=0\nelse\n((count++))\nif ((count > 1))\nthen\nflag=true\nfi\nfi\nfi\ndone\n}\nwget --progress=bar:force http://somesite.com/TheFile.jpeg 2>&1 | progressfilt\n100%[======================================>] 15,790      48.8K/s   in 0.3s\n\n2011-01-13 22:09:59 (48.8 KB/s) - 'TheFile.jpeg' saved [15790/15790]\n\nYou can use the following filter:\n\nUsage:\n\nThis function depends on a sequence of 0x0d0x0a0x0d0x0a0x0d being sent right before the progress bar is started. This behavior may be implementation dependent."
  },
  {
    "question": "How do I download a tarball from GitHub using cURL? I am trying to download a tarball from GitHub using cURL, but it does not seem to be redirecting:\n\n```\n$ curl --insecure https://github.com/pinard/Pymacs/tarball/v0.24-beta2\n<html><body>You are being <a href=\"https://nodeload.github.com/pinard/Pymacs/tarball/v0.24-beta2\">redirected</a>.</body></html>\n\n```\n\nNote: wget works for me:\n\n```\n$ wget --no-check-certificate https://github.com/pinard/Pymacs/tarball/v0.24-beta2\n\n```\n\nHowever I want to use cURL because ultimately I want to untar it inline with something like:\n\n```\n$ curl --insecure https://github.com/pinard/Pymacs/tarball/v0.24-beta2 | tar zx\n\n```\n\nI found that the URL after redirecting turned out to be https://download.github.com/pinard-Pymacs-v0.24-beta1-0-gcebc80b.tar.gz, but I would like cURL to be smart enough to figure this out.",
    "answer": "curl -L https://github.com/pinard/Pymacs/tarball/v0.24-beta2 | tar zx\n\nUse the -L option to follow redirects:"
  },
  {
    "question": "Does WGET timeout? I'm running a PHP script via cron using Wget, with the following command:\n\n```\nwget -O - -q -t 1 http://www.example.com/cron/run\n\n```\n\nThe script will take a maximum of 5-6 minutes to do its processing. Will WGet wait for it and give it all the time it needs, or will it time out?",
    "answer": "-T seconds\n--timeout=seconds\n--dns-timeout=seconds\n--connect-timeout=seconds\n--read-timeout=seconds\nwget -O - -q -t 1 --timeout=600 http://www.example.com/cron/run\n\nAccording to the man page of wget, there are a couple of options related to timeouts -- and there is a default read timeout of 900s -- so I say that, yes, it could timeout.\n\nHere are the options in question :\n\nSet the network timeout to seconds\nseconds.  This is equivalent to\nspecifying --dns-timeout,\n--connect-timeout, and\n--read-timeout, all at the same\ntime.\n\nAnd for those three options :\n\nSet the DNS lookup timeout to seconds\nseconds.  DNS lookups that don't\ncomplete within the specified time\nwill fail. By default, there is no\ntimeout on DNS lookups, other than\nthat implemented by system libraries.\n\nSet the connect timeout to seconds\nseconds. TCP connections that take\nlonger to establish will be aborted.\nBy default, there is no connect\ntimeout, other than that implemented\nby system libraries.\n\nSet the read (and write) timeout to\nseconds seconds. The \"time\" of\nthis timeout refers to idle time: if,\nat any point in the download, no data\nis received for more than the\nspecified number of seconds, reading\nfails and the download is restarted.\nThis option does not directly\naffect the duration of the entire\ndownload.\n\nI suppose using something like\n\nshould make sure there is no timeout before longer than the duration of your script.\n(Yeah, that's probably the most brutal solution possible ^^ )"
  },
  {
    "question": "POST request with Wget? I want to use Wget to upload a picture to a server's test folder using an authentication token, AUTH_1624582364932749DFHDD.\nThis command doesn't work, I get an \"authorization failed\" error:\n\n```\nwget --post-file=nature.jpg http://ipadress:8080/v1/AUTH_test/test/ --post-data=\"AUTH_1624582364932749DFHDD\"\n\n```",
    "answer": "--post-data=string\n--post-file=file\nwget --post-data 'user=foo&password=bar' http://example.com/auth.php\n\nWget does not support sending \"multipart/form-data\" data. --post-file is not for transmitting files as form attachments, it expects data in this form: key=value&otherkey=example. It is actually possible to POST other formats (like JSON) if you send the corresponding header.\n--post-data and --post-file work the same way: the only difference is that --post-data allows you to specify the data in the command line, while --post-file allows you to specify the path of the file that contains the data to send.\nHere's the documentation:\n\nUse POST as the method for all HTTP requests and send the specified data in the request body. --post-data sends string as data, whereas --post-file sends the contents of file. Other than that, they work in exactly the same way. In particular, they both expect content of the form key1=value1&key2=value2, with percent-encoding for special characters; the only difference is that one expects its content as a command-line parameter and the other accepts its content from a file. In particular, --post-file is not for transmitting files as form attachments: those must appear as key=value data (with appropriate percent-coding) just like everything else. Wget does not currently support multipart/form-data for transmitting POST data; only application/x-www-form-urlencoded. Only one of --post-data and --post-file should be specified.\n\nRegarding your authentication token, it should either be provided in the header, in the path of the URL, or in the data itself. This must be indicated somewhere in the documentation of the service you use. In a POST request, as in a GET request, you must specify the data using keys and values. This way the server will be able to receive multiple pieces of information with specific names. It's similar with variables.\nHence, you can't just send a magic token to the server, you also need to specify the name of the key. If the key is \"token\", then it should be token=YOUR_TOKEN.\n\nAlso, you should consider using curl if you can because it is easier to send files using it."
  },
  {
    "question": "How to download all links to .zip files on a given web page using wget/curl? A page contains links to a set of .zip files, all of which I want to download. I know this can be done by wget and curl. How is it done?",
    "answer": "wget -r -np -l 1 -A zip http://example.com/download/\n-r,  --recursive          specify recursive download.\n-np, --no-parent          don't ascend to the parent directory.\n-l,  --level=NUMBER       maximum recursion depth (inf or 0 for infinite).\n-A,  --accept=LIST        comma-separated list of accepted extensions.\n\nThe command is:\n\nOptions meaning:"
  },
  {
    "question": "Parallel wget in Bash I am getting a bunch of relatively small pages from a website and was wondering if I could somehow do it in parallel in Bash. Currently my code looks like this, but it takes a while to execute (I think what is slowing me down is the latency in the connection).\n\n```\nfor i in {1..42}\ndo\n    wget \"https://www.example.com/page$i.html\"\ndone\n\n```\n\nI have heard of using xargs, but I don't know anything about that and the man page is very confusing.  Any ideas? Is it even possible to do this in parallel? Is there another way I could go about attacking this?",
    "answer": "echo $URL_LIST | xargs -n 1 -P 8 wget -q\n\nMuch preferrable to pushing wget into the background using & or -b, you can use xargs to the same effect, and better.\nThe advantage is that xargs will synchronize properly with no extra work. Which means that you are safe to access the downloaded files (assuming no error occurs). All downloads will have completed (or failed) once xargs exits, and you know by the exit code whether all went well. This is much preferrable to busy waiting with sleep and testing for completion manually.\nAssuming that URL_LIST is a variable containing all the URLs (can be constructed with a loop in the OP's example, but could also be a manually generated list), running this:\n\nwill pass one argument at a time (-n 1) to wget, and execute at most 8 parallel wget processes at a time (-P 8). xarg returns after the last spawned process has finished, which is just what we wanted to know. No extra trickery needed.\nThe \"magic number\" of 8 parallel downloads that I've chosen is not set in stone, but it is probably a good compromise. There are two factors in \"maximising\" a series of downloads:\nOne is filling \"the cable\", i.e. utilizing the available bandwidth. Assuming \"normal\" conditions (server has more bandwidth than client), this is already the case with one or at most two downloads. Throwing more connections at the problem will only result in packets being dropped and TCP congestion control kicking in, and N downloads with asymptotically 1/N bandwidth each, to the same net effect (minus the dropped packets, minus window size recovery). Packets being dropped is a normal thing to happen in an IP network, this is how congestion control is supposed to work (even with a single connection), and normally the impact is practically zero. However, having an unreasonably large number of connections amplifies this effect, so it can be come noticeable. In any case, it doesn't make anything faster.\nThe second factor is connection establishment and request processing. Here, having a few extra connections in flight really helps. The problem one faces is the latency of two round-trips (typically 20-40ms within the same geographic area, 200-300ms inter-continental) plus the odd 1-2 milliseconds that the server actually needs to process the request and push a reply to the socket. This is not a lot of time per se, but multiplied by a few hundred/thousand requests, it quickly adds up.\nHaving anything from half a dozen to a dozen requests in-flight hides most or all of this latency (it is still there, but since it overlaps, it does not sum up!). At the same time, having only a few concurrent connections does not have adverse effects, such as causing excessive congestion, or forcing a server into forking new processes."
  },
  {
    "question": "Unable to establish SSL connection, how do I fix my SSL cert? I'm trying to wget to my own box, and it can't be an internal address in the wget (so says another developer).\nWhen I wget, I get this:\n\n```\nwget http://example.com\n--2013-03-01 15:03:30--  http://example.com/\nResolving example.com... 172.20.0.224\nConnecting to example.com|172.20.0.224|:80... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://www.example.com/ [following]\n--2013-03-01 15:03:30--  https://www.example.com/\nResolving www.example.com... 172.20.0.224\nConnecting to www.example.com|172.20.0.224|:443... connected.\nOpenSSL: error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown protocol\nUnable to establish SSL connection.\n\n```\n\nI believe it is because I do not have the certificate setup properly. Using openssl:\n\n```\nopenssl s_client -connect example.com:443\nCONNECTED(00000003)\n15586:error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown protocol:s23_clnt.c:588:\n\n```\n\nWhile if I do the same command on another site, it shows the entire cert. \nPerhaps the ssl cert was never setup in the conf file on Apache for that domain?\nIf so, what should I be specifying in the virtualhost? Is there any alternative other than specifying --no-check-certificate because I don't want to do that?",
    "answer": "SSL23_GET_SERVER_HELLO:unknown protocol\n\nThis error happens when OpenSSL receives something other than a ServerHello in a protocol version it understands from the server.  It can happen if the server answers with a plain (unencrypted) HTTP.  It can also happen if the server only supports e.g. TLS 1.2 and the client does not understand that protocol version.  Normally, servers are backwards compatible to at least SSL 3.0 / TLS 1.0, but maybe this specific server isn't (by implementation or configuration).\nIt is unclear whether you attempted to pass --no-check-certificate or not.  I would be rather surprised if that would work.\nA simple test is to use wget (or a browser) to request http://example.com:443 (note the http://, not https://); if it works, SSL is not enabled on port 443.  To further debug this, use openssl s_client with the -debug option, which right before the error message dumps the first few bytes of the server response which OpenSSL was unable to parse.  This may help to identify the problem, especially if the server does not answer with a ServerHello message.  To see what exactly OpenSSL is expecting, check the source: look for SSL_R_UNKNOWN_PROTOCOL in ssl/s23_clnt.c.\nIn any case, looking at the apache error log may provide some insight too."
  },
  {
    "question": "Spider a Website and Return URLs Only I'm looking for a way to pseudo-spider a website. The key is that I don't actually want the content, but rather a simple list of URIs. I can get reasonably close to this idea with Wget using the --spider option, but when piping that output through a grep, I can't seem to find the right magic to make it work:\n\n```\nwget --spider --force-html -r -l1 http://somesite.com | grep 'Saving to:'\n\n```\n\nThe grep filter seems to have absolutely no affect on the wget output. Have I got something wrong or is there another tool I should try that's more geared towards providing this kind of limited result set?\nUPDATE\nSo I just found out offline that, by default, wget writes to stderr. I missed that in the man pages (in fact, I still haven't found it if it's in there). Once I piped the return to stdout, I got closer to what I need:\n\n```\nwget --spider --force-html -r -l1 http://somesite.com 2>&1 | grep 'Saving to:'\n\n```\n\nI'd still be interested in other/better means for doing this kind of thing, if any exist.",
    "answer": "wget --spider --force-html -r -l2 $url 2>&1 \\\n| grep '^--' | awk '{ print $3 }' \\\n| grep -v '\\.\\(css\\|js\\|png\\|gif\\|jpg\\)$' \\\n> urls.m3u\n\nThe absolute last thing I want to do is download and parse all of the content myself (i.e. create my own spider). Once I learned that Wget writes to stderr by default, I was able to redirect it to stdout and filter the output appropriately.\n\nThis gives me a list of the content resource (resources that aren't images, CSS or JS source files) URIs that are spidered. From there, I can send the URIs off to a third party tool for processing to meet my needs.\nThe output still needs to be streamlined slightly (it produces duplicates as it's shown above), but it's almost there and I haven't had to do any parsing myself."
  },
  {
    "question": "get file size of a file to wget before wget-ing it? I'm wondering if there is a way to check ahead of time the size of a file I might download via wget? I know that using the --spider option tells me if a file exists or not, but I'm interested in finding the size of that file as well.",
    "answer": "wget --spider http://henning.makholm.net/\nSpider mode enabled. Check if remote file exists.\n--2011-08-08 19:39:48--  http://henning.makholm.net/\nResolving henning.makholm.net (henning.makholm.net)... 85.81.19.235\nConnecting to henning.makholm.net (henning.makholm.net)|85.81.19.235|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 9535 (9.3K) [text/html]     <-------------------------\nRemote file exists and could contain further links,\nbut recursion is disabled -- not retrieving.\n\nHmm.. for me --spider does display the size:\n\n(But beware that not all web servers will inform clients of the length of the data except by closing the connection when it's all been sent.)\nIf you're concerned about wget changing the format it reports the length in, you might use wget --spider --server-response and look for a Content-Length header in the output."
  },
  {
    "question": "How do I mirror a directory with wget without creating parent directories? I want to mirror a folder via FTP, like this:\n\n```\nwget --mirror --user=x --password=x ftp://ftp.site.com/folder/subfolder/evendeeper\n\n```\n\nBut I do not want to create a directory structure like this:\n\nftp.site.com -> folder -> subfolder -> evendeeper\n\nI just want:\n\nevendeeper\n\nAnd anything below it to be the resulting structure. It would also be acceptable for the contents of evendeeper to wind up in the current directory as long as subdirectories are created for subdirectories of evendeeper on the server.\nI am aware of the -np option, according to the documentation that just keeps it from following links to parent pages (a non-issue for the binary files I'm mirroring via FTP). I am also aware of the -nd option, but this prevents creating any directory structure at all, even for subdirectories of evendeeper.\nI would consider alternatives as long as they are command-line-based, readily available as Ubuntu packages and easily automated like wget.",
    "answer": "For a path like: ftp.site.com/a/b/c/d\n-nH would download all files to the directory a/b/c/d in the current directory, and -nH --cut-dirs=3 would download all files to the directory d in the current directory."
  },
  {
    "question": "Download a file from google drive using wget I want to download the file that is viewable at this address to a linux remote:\nhttps://drive.google.com/file/d/0Bz7KyqmuGsilT0J5dmRCM0ROVHc/view?usp=sharing\nI'm hoping I can do this with wget.\nI tried \n\n```\nwget https://drive.google.com/file/d/0Bz7KyqmuGsilT0J5dmRCM0ROVHc/vgg16_weights.h5\n\n```\n\nand the response was a 404.\nIs it possible to wget a google drive file? If so, what is the path to provide? If not, are there any alternatives (bash or other) so I can avoid downloading the file to my local and transferring it to the remote?",
    "answer": "wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=FILEID' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=FILEID\" -O FILENAME && rm -rf /tmp/cookies.txt\n\nInsert your file ID into this URL (https://drive.google.com/uc?export=download&id=), then surround the URL with quotes so that Bash doesn't misinterpret the &, like so:\nwget \"https://drive.google.com/uc?export=download&id=0Bz7KyqmuGsilT0J5dmRCM0ROVHc\"\nReference here.\n\nWhen downloading big files, Google Drive adds a security warning that breaks the script above. In that case, you can download the file using:\n\n(Script taken from here)"
  },
  {
    "question": "How to download a file into a directory using curl or wget? I know I can use the following 2 commands to download a file:\n\n```\ncurl -O example.com/file.zip\nwget example.com/file.zip\n\n```\n\nBut I want them to go into a specific directory. So I can do the following:\n\n```\ncurl -o mydir/file.zip example.com/file.zip\nwget -O mydir/file.zip example.com/file.zip\n\n```\n\nIs there a way to not have to specify the filename? Something like this:\n\n```\ncurl -dir mydir example.com/file.zip\n\n```",
    "answer": "wget -P /home/test www.xyz.com\n\nThe following line will download all the files to a directory mentioned by you.\n\nHere the files will be downloaded to /home/test directory"
  },
  {
    "question": "Sites not accepting wget user agent header When I run this command:\n\n```\nwget --user-agent=\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.8; rv:21.0) Gecko/20100101 Firefox/21.0\"  http://yahoo.com\n\n```\n\n...I get this result (with nothing else in the file):\n\n```\n<!-- hw147.fp.gq1.yahoo.com uncompressed/chunked Wed Jun 19 03:42:44 UTC 2013 -->\n\n```\n\nBut when I run wget http://yahoo.com with no --user-agent option, I get the full page.\nThe user agent is the same header that my current browser sends. Why does this happen? Is there a way to make sure the user agent doesn't get blocked when using wget?",
    "answer": "wget  --header=\"Accept: text/html\" --user-agent=\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.8; rv:21.0) Gecko/20100101 Firefox/21.0\"  http://yahoo.com\n\nIt seems Yahoo server does some heuristic based on User-Agent in a case Accept header is set to */*.\n\nAccept: text/html\n\ndid the trick for me.\ne.g.\n\nNote: if you don't declare Accept header then wget automatically adds Accept:*/* which means give me anything you have."
  },
  {
    "question": "wget ssl alert handshake failure I am trying to download files from an https site and keep getting the following error:\n\n```\nOpenSSL: error:14077410:SSL routines:SSL23_GET_SERVER_HELLO:sslv3 alert handshake failure\nUnable to establish SSL connection.\n\n```\n\nFrom reading blogs online I gather I have to provide the server cert and the client cert. I have found steps on how to download the server cert but not the client cert. Does anyone have a complete set of steps to use wget with SSL? I also tried the --no-check-certificate option but that did not work.\n\n```\nwget version: wget-1.13.4\nopenssl version: OpenSSL 1.0.1f 6 Jan 2014\n\n```\n\ntrying to download all lecture resources from a course's webpage on coursera.org. So, the URL would look something like this: https://class.coursera.org/matrix-002/lecture\nAccessing this webpage online requires form authentication, not sure if that is causing the failure.",
    "answer": "wget https://class.coursera.org\n...\nHTTP request sent, awaiting response...\nHTTP/1.1 302 Found\n...\nLocation: https://www.coursera.org/ [following]\n...\nConnecting to www.coursera.org (www.coursera.org)|54.230.46.78|:443... connected.\nOpenSSL: error:14077410:SSL routines:SSL23_GET_SERVER_HELLO:sslv3 alert handshake failure\nUnable to establish SSL connection.\n\nIt works from here with same OpenSSL version, but a newer version of wget (1.15). Looking at the Changelog there is the following significant change regarding your problem:\n\n1.14: Add support for TLS Server Name Indication.\n\nNote that this site does not require SNI. But www.coursera.org requires it.\nAnd if you would call wget with -v --debug (as I've explicitly recommended in my comment!) you will see:\n\nSo the error actually happens with www.coursera.org and the reason is missing support for SNI. You need to upgrade your version of wget."
  },
  {
    "question": "How to download a Google Drive url via curl or wget Is there a way to download a publicly-viewable Google Drive url via curl or wget? For example, being able to do something like:\n\n```\ncurl -O myfile.xls https://drive.google.com/uc?export=download&id=1Wb2NfKTQr_dLoFJH0GfM0cx-t4r07IVl\n\n```\n\nNote, I'm looking to do this on a publicly-viewable file without having to sign into my Google account (or have someone else sign into their account, etc.).\nIf helpful, the cors headers I have are:\n\n```\n \"kind\": \"drive#file\",\n \"id\": \"1Wb2NfKTQr_dLoFJH0GfM0cx-t4r07IVl\",\n\n```",
    "answer": "<a id=\"uc-download-link\" class=\"goog-inline-block jfk-button jfk-button-action\" href=\"/uc?export=download&amp;confirm=ABCD&amp;id=### file ID ###\">download</a>\nfileid=\"### file id ###\"\nfilename=\"MyFile.csv\"\ncurl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=${fileid}\" > /dev/null\ncurl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=${fileid}\" -o ${filename}\nfileid=\"### file id ###\"\nfilename=\"MyFile.csv\"\nhtml=`curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=${fileid}\"`\ncurl -Lb ./cookie \"https://drive.google.com/uc?export=download&`echo ${html}|grep -Po '(confirm=[a-zA-Z0-9\\-_]+)'`&id=${fileid}\" -o ${filename}\n<form id=\"downloadForm\" action=\"https://drive.google.com/uc?export=download&amp;id={fileId}&amp;confirm={value for downloading}\" method=\"post\">\ncurl -L \"https://drive.google.com/uc?export=download&id=### fileId ###\" -o sampleoutput.csv\nhttps://drive.usercontent.google.com/download?id={fileId}&confirm=xxx\ncurl \"https://drive.usercontent.google.com/download?id={fileId}&confirm=xxx\" -o filename\ncurl -L \"https://drive.usercontent.google.com/download?id={fileId}&confirm=xxx\" -o filename\n\nHow about this method? When the file is such large size, Google returns a code for downloading the file. You can download the file using the code. When such large file is downloaded using curl, you can see the code as follows.\n\nThe query with confirm=ABCD is important for downloading the file. This code is also included in the cookie. At the cookie, you can see it as follows.\n\nIn this case, \"ABCD\" is the code. In order to retrieve the code from the cookie and download the file, you can use the following script.\nSample script :\n\nIf this was not useful for you, I'm sorry.\n\nUpdated at February 17, 2022\nRecently, it seems that the specification of this flow has been changed. So I updated this answer. In order to download a publicly shared file of large size from Google Drive, you can use the following script.\n\nIn this case, the ID for downloading is retrieved from the HTML data as follows.\n\nWhen you want to download a publicly shared file of small size from Google Drive, you can use the following command.\n\nUpdated at January 21, 2024\nFrom the following @excellproj 's comment,\n\nJanuary 2024\ncurl -L \"https://drive.usercontent.google.com/download?id=${fileId}&export=download&confirm=t\" -o \"file.zip\"\nworking great for me. Small and large files\n\nI checked this endpoint. By this, the following result is obtained. In the current stage, the following endpoint can be used.\n\nIt seems that in the current stage, various values can be used to xxx of confirm=xxx. So, even confirm=xxx and confirm=yy and confirm=z can be used.\nThe sample curl command is as follows.\n\nAnd/or,"
  },
  {
    "question": "How to unzip a piped zip file (from &quot;wget -qO-&quot;)? Any ideas on how to unzip a piped zip file like this:\n\n```\nwget -qO- http://downloads.wordpress.org/plugin/akismet.2.5.3.zip\n\n```\n\nI wished to unzip the file to a directory, like we used to do with a normal file:\n\n```\nwget -qO- http://downloads.wordpress.org/plugin/akismet.2.5.3.zip | unzip -d ~/Desktop\n\n```",
    "answer": "wget -q -O tmp.zip http://downloads.wordpress.org/plugin/akismet.2.5.3.zip && unzip tmp.zip && rm tmp.zip"
  },
  {
    "question": "What headers are automatically sent by wget? I know that you can manually set some headers with the --header option, but I want to know what headers it sends without interaction.",
    "answer": "---request begin---\nGET / HTTP/1.0\nUser-Agent: Wget/1.12 (cygwin)\nAccept: */*\nHost: www.uml.edu\nConnection: Keep-Alive\n\n---request end---\n\nUsing the -d (--debug) option I see it set:"
  },
  {
    "question": "What is the correct wget command syntax for HTTPS with username and password? I would like to download a file remotely with this URL using wget:\n\n```\nhttps://test.mydomain.com/files/myfile.zip\n\n```\n\nThe site test.mydomain.com requires a login. I would like to download that file in my another server using this command but it does not work (does not completely download the file):\n\n```\nwget --user=myusername --password=mypassword https://test.mydomain.com/files/myfile.zip\n\n```\n\nIf my username is myusername and password is mypassword what would be the correct wget syntax?\nThe following are the return messages after I type the above command:\n\n```\nResolving test.mydomain.com (test.mydomain.com)... 123.456.789\nConnecting to test.mydomain.com (test.mydomain.com)|123.456.789|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://test.mydomain.com/login/unauthorized [following]\n--2013-01-30 02:01:32--  https://test.mydomain.com/login/unauthorized\nReusing existing connection to test.mydomain.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://test.mydomain.com/login [following]\n--2013-01-30 02:01:32--  https://test.mydomain.com/login\nReusing existing connection to test.mydomain.com:443.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/html]\nSaving to: `myfile.zip'\n\n```\n\nAm I missing something? Please help. Thanks.",
    "answer": "wget --user=username --ask-password https://xyz.com/changelog-6.40.txt\n\nBy specifying the option --user and --ask-password wget will ask for the credentials. Below is an example. Change the username and download link to your needs."
  },
  {
    "question": "Using WGET to run a cronjob PHP I tried to do a cron and run a url every 5 mintues.\nI tried to use WGET however I dont want to download the files on the server, all I want is just to run it.\nThis is what I used (crontab):\n\n```\n*/5 * * * * wget http://www.example.com/cronit.php\n\n```\n\nIs there any other command to use other than wget to just run the url and not downlaod it?",
    "answer": "wget --spider http://www.example.com/cronit.php\nwget -O /dev/null http://www.example.com/cronit.php\nwget -q --spider http://www.example.com/cronit.php\n\nYou could tell wget to not download the contents in a couple of different ways:\n\nwhich will just perform a HEAD request but probably do what you want\n\nwhich will save the output to /dev/null (a black hole)\nYou might want to look at wget's -q switch too which prevents it from creating output\nI think that the best option would probably be:\n\nthat's unless you have some special logic checking the HTTP method used to request the page"
  },
  {
    "question": "wget - Download a sub directory How do I download only a sub directory using wget?\nCan I specify the subdirectory that I need to download?\nThanks!",
    "answer": "-r: recursive retrieving\n-l1: sets the maximum recursion depth to be 1\n--no-parent: does not ascend to the parent; only downloads from the specified subdirectory and downwards hierarchy\n\nYou can do:\nwget -r -l1 --no-parent http://www.domain.com/subdirectory/\nwhere:"
  },
  {
    "question": "How can I make a Docker healthcheck with wget instead of curl? Docker healthcheck document shows this for curl:\n\n```\nHEALTHCHECK --interval=5m --timeout=3s \\\n  CMD curl -f http://localhost/ || exit 1 \n\n```\n\nI want a one-line equivalent in wget that would exit 1 when HTTP 200 is not returned.\nThis is because the docker image has wget installed and I don't want to have to install curl.",
    "answer": "HEALTHCHECK  --interval=5m --timeout=3s \\\nCMD wget --no-verbose --tries=1 --spider http://localhost/ || exit 1\nhealthcheck:\ntest: wget --no-verbose --tries=1 --spider http://localhost || exit 1\ninterval: 5m\ntimeout: 3s\nretries: 3\nstart_period: 2m\n\nThe following seems to be the equivalent:\n\nWhere:\n\n--no-verbose - Turn off verbose without being completely quiet (use -q for that), which means that error messages and basic information still get printed.\n--tries=1 - If not set, some wget implementations will retry indefinitely when HTTP 200 response is not returned.\n--spider - Behave as a Web spider, which means that it will not download the pages, just check that they are there.\nexit 1 - Ensures exit code 1 on failure. Heathcheck only expects the following:\n\n0: success - the container is healthy and ready for use\n1: unhealthy - the container is not working correctly\n2: reserved - do not use this exit code\n\nDocker compose example:\n\nhttps://docs.docker.com/compose/compose-file/compose-file-v3/#healthcheck"
  },
  {
    "question": "How can I download a file from an S3 bucket with wget? I can push some content to an S3 bucket with my credentials through S3cmd tool with s3cmd put contentfile S3://test_bucket/test_file\nI am required to download the content from this bucket in other computers that don't have s3cmd installed on them, BUT they have wget installed.\nwhen I try to download some content from my bucket with wget I get this:\n\n```\n https://s3.amazonaws.com/test_bucket/test_file\n--2013-08-14 18:17:40--  `https`://s3.amazonaws.com/test_bucket/test_file\nResolving s3.amazonaws.com (s3.amazonaws.com)... [ip_here]\nConnecting to s3.amazonaws.com (s3.amazonaws.com)|ip_here|:port... connected.\nHTTP request sent, awaiting response... 403 Forbidden\n`2013`-08-14 18:17:40 ERROR 403: Forbidden.\n\n```\n\nI have manually made this bucket public through the Amazon AWS web console.\nHow can I download content from an S3 bucket with wget into a local txt file?",
    "answer": "s3cmd setacl --acl-public --guess-mime-type s3://test_bucket/test_file\n\nGot it ... If you upload a file in an S3 bucket with S3CMD with the --acl public flag then one shall be able to download the file from S3 with wget easily ...\nConclusion: In order to download with wget, first of one needs to upload the content in S3 with s3cmd put --acl public --guess-mime-type <test_file> s3://test_bucket/test_file\nalternatively you can try:\n\nnotice the setacl flag above. THAT WILL set the file in s3 accessible publicly\nthen you can execute the wget http://s3.amazonaws.com/test_bucket/test_file"
  },
  {
    "question": "Check wget&#39;s return value I'm writing a script to download a bunch of files, and I want it to inform when a particular file doesn't exist.\n\n```\nr=`wget -q www.someurl.com`\nif [ $r -ne 0 ]\n  then echo \"Not there\"\n  else echo \"OK\"\nfi\n\n```\n\nBut it gives the following error on execution:\n./file: line 2: [: -ne: unary operator expected\nWhat's wrong?",
    "answer": "$r is the text output of wget (which you've captured with backticks). To access the return code, use the $? variable."
  },
  {
    "question": "BASH script: Downloading consecutive numbered files with wget I have a web server that saves the logs files of a web application numbered. A file name example for this would be:\n\n```\ndbsclog01s001.log\ndbsclog01s002.log\ndbsclog01s003.log\n\n```\n\nThe last 3 digits are the counter and they can get sometime up to 100.\nI usually open a web browser, browse to the file like:\n\n```\nhttp://someaddress.com/logs/dbsclog01s001.log\n\n```\n\nand save the files. This of course gets a bit annoying when you get 50 logs.\nI tried to come up with a BASH script for using wget and passing\n\n```\nhttp://someaddress.com/logs/dbsclog01s*.log\n\n```\n\nbut I am having problems with my the script. \nAnyway, anyone has a sample on how to do this?\nthanks!",
    "answer": "if [ $# -lt 3 ]; then\necho \"Usage: $0 url_format seq_start seq_end [wget_args]\"\nexit\nfi\n\nurl_format=$1\nseq_start=$2\nseq_end=$3\nshift 3\n\nprintf \"$url_format\\\\n\" `seq $seq_start $seq_end` | wget -i- \"$@\"\n./seq_wget http://someaddress.com/logs/dbsclog01s%03d.log 1 50\nwget http://someaddress.com/logs/dbsclog01s{001..050}.log\n\nSave the above as seq_wget, give it execution permission (chmod +x seq_wget), and then run, for example:\n\nOr, if you have Bash 4.0, you could just type\n\nOr, if you have curl instead of wget, you could follow Dennis Williamson's answer."
  },
  {
    "question": "Unable to establish SSL connection upon wget on Ubuntu 14.04 LTS I tried to download an image through wget but I got an error: Unable to establish SSL connection. \n\n```\nwget https://www.website.com/image.jpg\n--2015-02-26 01:30:17--  https://www.website.com/image.jpg\nResolving www.website.com (www.website.com)... xx.xxx.xx.xx\nConnecting to www.website.com (www.website.com)|xx.xxx.xx.xx|:443... connected.\nUnable to establish SSL connection.\n\n```\n\nMy test case:\n\nUsing Ubuntu 12.04.4 LTS (GNU/Linux 3.8.0-44-generic x86_64), GNU\nWget 1.13.4 built on linux-gnu, I was able to download the image\nusing the code above. No error.\nUsing Ubuntu 14.04 LTS (GNU/Linux 3.13.0-24-generic x86_64), GNU Wget 1.15 built on linux-gnu, I was not able to download the image\nusing the code above.\n\nAnother variable is that the www.website.com uses TLS 1.0. I don't have an idea how this affects wget. But if I wget an image from TLS 1.2 websites I don't get any ssl connection errors from both test cases.\nIs Ubuntu 14.04 or wget 1.15 not compatible with TLS 1.0 websites?\nDo I need to install/download any library/software to enable this connection?",
    "answer": "... right now it happens only to the website I'm testing. I can't post it here because it's confidential.\n\nThen I guess it is one of the sites which is incompatible with TLS1.2. The openssl as used in 12.04 does not use TLS1.2 on the client side while with 14.04 it uses TLS1.2 which might explain the difference. To work around try to explicitly use\n--secure-protocol=TLSv1. If this does not help check if you can access the site with openssl s_client -connect ... (probably not) and with openssl s_client -tls1 -no_tls1_1, -no_tls1_2 ....\nPlease note that it might be other causes, but this one is the most probable and without getting access to the site everything is just speculation anyway.\nThe assumed problem in detail: Usually clients use the most compatible handshake to access a server. This is the SSLv23 handshake which is compatible to older SSL versions but announces the best TLS version the client supports, so that the server can pick the best version. In this case wget would announce TLS1.2. But there are some broken servers which never assumed that one day there would be something like TLS1.2 and which refuse the handshake if the client announces support for this hot new version (from 2008!) instead of just responding with the best version the server supports. To access these broken servers the client has to lie and claim that it only supports TLS1.0 as the best version.\n\nIs Ubuntu 14.04 or wget 1.15 not compatible with TLS 1.0 websites? Do I need to install/download any library/software to enable this connection?\n\nThe problem is the server, not the client.\nMost browsers work around these broken servers by retrying with a lower version. Most other applications fail permanently if the first connection attempt fails, i.e. they don't downgrade by itself and one has to enforce another version by some application specific settings."
  },
  {
    "question": "Why does wget ignore the query string in the URL? I want to use wget to download the following 18 HTML files:\n\n```\nhttp://www.ted.com/talks/quick-list?sort=date&order=desc&page=18\nhttp://www.ted.com/talks/quick-list?sort=date&order=desc&page=17\n...\nhttp://www.ted.com/talks/quick-list?sort=date&order=desc&page=1\n\n```\n\nNo matter what comes after page=, it always downloads the first page of the listing. Do I have to escape some characters in the URLs? How?",
    "answer": "wget \"http://www.ted.com/talks/quick-list?sort=date&order=desc&page=18\"\n\n& is a special character in most shell environments. You can use double quotes to quote the URL to pass the whole thing in as the parameter to wget:"
  },
  {
    "question": "Downloading a file with wget using multiple connections How to download a file with wget using multiple connection in which each connection downloads a part of the file?",
    "answer": "aria2c -x 16 [url] #where 16 is the number of connections\n\nuse aria2\n\nOR\nJust repeat the wget -r -np -N [url] for as many threads as you need. This isn\u2019t pretty and there are surely better ways to do this, but if you want something quick and dirty it should do the trick."
  },
  {
    "question": "wget -O for non-existing save path? I can't wget while there is no path already to save. I mean, wget doens't work for the non-existing save paths. For e.g:\n\n```\nwget -O /path/to/image/new_image.jpg http://www.example.com/old_image.jpg\n\n```\n\nIf /path/to/image/ is not previously existed, it always returns:\n\n```\nNo such file or directory\n\n```\n\nHow can i make it work to automatically create the path and save?",
    "answer": "curl http://www.site.org/image.jpg --create-dirs -o /path/to/save/images.jpg\n\nTry curl"
  },
  {
    "question": "How to ignore specific type of files to download in wget? How do I ignore .jpg, .png files in wget as I wanted to include only .html files.\nI am trying:\n\n```\nwget  -R index.html,*tiff,*pdf,*jpg -m http://example.com/\n\n```\n\nbut it's not working.",
    "answer": "--reject jpg,png  --accept html\n\nUse the\n\noptions to exclude/include files with certain extensions, see http://www.gnu.org/software/wget/manual/wget.html#Recursive-Accept_002fReject-Options.\nPut patterns with wildcard characters in quotes, otherwise your shell will expand them, see http://www.gnu.org/software/wget/manual/wget.html#Types-of-Files"
  },
  {
    "question": "How does `scp` differ from `rsync`? An article about setting up Ghost blogging says to use scp to copy from my local machine to a remote server:\n\n```\nscp -r ghost-0.3 root@*your-server-ip*:~/\n\n```\n\nHowever, Railscast 339: Chef Solo Basics uses scp to copy in the opposite direction (from the remote server to the local machine):\n\n```\nscp -r root@178.xxx.xxx.xxx:/var/chef .\n\n```\n\nIn the same Railscast, when the author wants to copy files to the remote server (same direction as the first example), he uses rsync:\n\n```\nrsync -r . root@178.xxx.xxx.xxx:/var/chef\n\n```\n\nWhy use the rsync command if scp will copy in both directions? How does scp differ from rsync?",
    "answer": "rsync A host:B\n\nThe major difference between these tools is how they copy files.\nscp basically reads the source file and writes it to the destination. It performs a plain linear copy, locally, or over a network.\nrsync also copies files locally or over a network. But it employs a special delta transfer algorithm and a few optimizations to make the operation a lot faster. Consider the call.\n\nrsync will check files sizes and modification timestamps of both A and B, and skip any further processing if they match.\n\nIf the destination file B already exists, the delta transfer algorithm will make sure only differences between A and B are sent over the wire.\n\nrsync will write data to a temporary file T, and then replace the destination file B with T to make the update look \"atomic\" to processes that might be using B.\n\nAnother difference between them concerns invocation. rsync has a plethora of command line options, allowing the user to fine tune its behavior. It supports complex filter rules, runs in batch mode, daemon mode, etc. scp has only a few switches.\nIn summary, use scp for your day to day tasks. Commands that you type once in a while on your interactive shell. It's simpler to use, and in those cases rsync optimizations won't help much.\nFor recurring tasks, like cron jobs, use rsync. As mentioned, on multiple invocations it will take advantage of data already transferred, performing very quickly and saving on resources. It is an excellent tool to keep two directories synchronized over a network.\nAlso, when dealing with large files, use rsync with the -P option. If the transfer is interrupted, you can resume it where it stopped by reissuing the command. See Sid Kshatriya's answer.\nFinally, note that rsync:// the protocol is similar to plain HTTP: unencrypted and no integrity checks. Be sure to always use rsync via SSH (as in the examples from the question above), not via the rsync protocol, unless you really know what you're doing. scp will always use SSH as underlying transport mechanism which has both integrity and confidentiality guarantees, so that is another difference between the two utilities."
  },
  {
    "question": "rsync error: failed to set times on &quot;/foo/bar&quot;: Operation not permitted I'm getting a confusing error from rsync and the initial things I'm finding from web searches (as well as all the usual chmod'ing) are not solving it:\n\n```\nrsync: failed to set times on \"/foo/bar\": Operation not permitted (1)\nrsync error: some files could not be transferred (code 23) \n  at /SourceCache/rsync/rsync-35.2/rsync/main.c(992) [sender=2.6.9]\n\n```\n\nIt seems to be working despite that error, but it would be nice to get rid of that.",
    "answer": "If /foo/bar is on NFS (or possibly some FUSE filesystem), that might be the problem.\nEither way, adding -O / --omit-dir-times to your command line will avoid it trying to set modification times on directories."
  },
  {
    "question": "How can I configure rsync to create target directory on remote server? I would like to rsync from local computer to server. On a directory that does not exist, and I want rsync to create that directory on the server first.\nHow can I do that?",
    "answer": "rsync -a --rsync-path=\"mkdir -p /tmp/x/y/z/ && rsync\" $source user@remote:/tmp/x/y/z/\nrsync -a --relative /new/x/y/z/ user@remote:/pre_existing/dir/\nrsync -a --relative /new/x/./y/z/ user@remote:/pre_existing/dir/\n\nIf you have more than the last leaf directory to be created, you can either run a separate ssh ... mkdir -p first, or use the --rsync-path trick as explained here :\n\nOr use the --relative option as suggested by Tony. In that case, you only specify the root of the destination, which must exist, and not the directory structure of the source, which will be created:\n\nThis way, you will end up with /pre_existing/dir/new/x/y/z/\nAnd if you want to have \"y/z/\" created, but not inside \"new/x/\", you can add ./ where you want --relativeto begin:\n\nwould create /pre_existing/dir/y/z/."
  },
  {
    "question": "rsync copy over only certain types of files using include option I use the following bash script to copy only files of certain extension(in this case *.sh), however it still copies over all the files.  what's wrong?\n\n```\n\nfrom=$1\nto=$2\n\nrsync -zarv  --include=\"*.sh\" $from $to\n\n```",
    "answer": "rsync -zarv --include=\"*/\" --include=\"*.sh\" --exclude=\"*\" \"$from\" \"$to\"\nrsync -zarv  --include \"*/\" --exclude=\"*\" --include=\"*.sh\" \"$from\" \"$to\"\n\nI think --include is used to include a subset of files that are otherwise excluded by --exclude, rather than including only those files.\nIn other words: you have to think about include meaning don't exclude.\nTry instead:\n\nFor rsync versions below 3.0.6, the order needs to be modified as follows (see comments):\n\nAdding the -m flag will avoid creating empty directory structures in the destination. Tested in  version 3.1.2.\nSo if we only want *.sh files we have to exclude all files --exclude=\"*\", include all directories --include=\"*/\" and include all *.sh files --include=\"*.sh\".\nYou can find some good examples in the section Include/Exclude Pattern Rules of the man page"
  },
  {
    "question": "Rsync copy directory contents but not directory itself I'm trying to synchronize two contents of folders with different name:\n\n```\nrsync -av ~/foo user@remote.com:/var/www/bar\n\n```\n\nI'd like to copy the content of foo into bar at remote host, but not the directory foo itself. I've tried something like foo/*, but rsync doesn't support that.\nrsync always creates \n\n```\n/var/www/bar/foo\n\n```",
    "answer": "rsync interprets a directory with no trailing slash as copy this directory, and a directory with a trailing slash as copy the contents of this directory.\nTry rsync -av ~/foo/ user@remote.com:/var/www/bar/"
  },
  {
    "question": "How to resume scp with partially copied files? I'm using the scp shell command to copy huge folder of files.\nBut at some point in time I had to kill the running command (by Ctrl+C or kill).\nTo my understanding scp copies files sequentially, so there should be only one partially copied file.\nHow can same scp command be resumed to not overwrite successfully copied files and to properly handle partially copied files?\nP.S. I know I can do this kind of stuff in rsync, but scp is faster for me for some reason and I use it instead.",
    "answer": "rsync -P -e ssh remoteuser@remotehost:/remote/path /local/path\n\nYou should use rsync over ssh\n\nThe key option is -P, which is the same as --partial --progress\n\nBy default, rsync will delete any partially transferred file if the transfer is interrupted. In some circumstances it is more desirable to keep partially transferred files. Using the --partial option tells rsync to keep the partial file which should make a subsequent transfer of the rest of the file much faster.\n\nOther options, such -a (for archive mode), and -z (to enable compression) can also be used.\nThe manual: https://download.samba.org/pub/rsync/rsync.html"
  },
  {
    "question": "rsync: difference between --size-only and --ignore-times I'm trying to understand what the difference is between two options \n\n```\nrsync --size-only\n\n```\n\nand\n\n```\nrsync --ignore-times\n\n```\n\nIt is my understanding that by default rsync will compare both the timestamps and the file sizes in order to decide whether or not a file should be synchronized or not. The options above allow the user to influence this behavior.\nBoth options seem, at least verbally to result in the same thing: comparing by size only.\nAm I missing something subtle here?",
    "answer": "There are several ways rsync compares files -- the authoritative source is the rsync algorithm description: https://www.andrew.cmu.edu/course/15-749/READINGS/required/cas/tridgell96.pdf. The wikipedia article on rsync is also very good.\nFor local files, rsync compares metadata and if it looks like it doesn't need to copy the file because size and timestamp match between source and destination it doesn't look further. If they don't match, it cp's the file. However, what if the metadata do match but files aren't actually the same? Then rsync probably didn't do what you intended.\nFiles that are the same size may still have changed. One simple example is a text file where you correct a typo -- like changing \"teh\" to \"the\". The file size is the same, but the corrected file will have a newer timestamp. --size-only says \"don't look at the time; if size matches assume files match\", which would be the wrong choice in this case.\nOn the other hand, suppose you accidentally did a big cp -r A B yesterday, but you forgot to preserve the time stamps, and now you want to do the operation in reverse rsync B A. All those files you cp'ed have yesterday's time stamp, even though they weren't really modified yesterday, and rsync will by default end up copying all those files, and updating the timestamp to yesterday too. --size-only may be your friend in this case (modulo the example above).\n--ignore-times says to compare the files regardless of whether the files have the same modify time. Consider the typo example above, but then not only did you correct the typo but you used touch to make the corrected file have the same modify time as the original file -- let's just say you're sneaky that way. Well --ignore-times will do a diff of the files even though the size and time match."
  },
  {
    "question": "What is the simplest way to remove a trailing slash from each parameter? What is the simplest way to remove a trailing slash from each parameter in the '$@' array, so that rsync copies  the directories by name?\n\n```\nrsync -a --exclude='*~' \"$@\" \"$dir\"\n\n```\n\n\nThe title has been changed for clarification. To understand the comments and answer about multiple trailing slashes see the edit history.",
    "answer": "echo ${@%/}\n\nYou can use the ${parameter%word} expansion that is detailed here.  Here is a simple test script that demonstrates the behavior:"
  },
  {
    "question": "How to pass password automatically for rsync SSH command? I need to do rsync by ssh and want to do it automatically without the need of passing password for ssh manually.",
    "answer": "You should use a keyfile without passphrase for scripted ssh logins. This is obviously a security risk, take care that the keyfile itself is adequately secured.\nFrom Instructions for setting up passwordless ssh access:\n\nRun ssh-keygen\nIt will ask for a path to the key file, a passphrase, and a repeat of the same passphrase. Answer all three by just pressing Enter (accepting the defaults).\n\nRun ssh-copy-id -i ~/.ssh/id_rsa.pub <remote-host>\nThe path ~/.ssh/id_rsa.pub to the public key file may be different if you chose another path in step 1. Replace <remote-host> with the IP or hostname of the remote host you want to log in to.\n\nRun ssh <remote-host>\nThe remote host should not ask for a password, and you should be logged in to the remote host."
  },
  {
    "question": "What does f+++++++++ mean in rsync logs? I'm using rsync to make a backup of my server files, and I have two questions:\n\nIn the middle of the process I need to stop and start rsync again.\nWill rsync start from the point where it stopped or it will restart from the beginning?\nIn the log files I see \"f+++++++++\". What does it mean?\n\ne.g.:\n\n```\n2010/12/21 08:28:37 [4537] >f.st...... iddd/logs/website-production-access_log\n2010/12/21 08:29:11 [4537] >f.st...... iddd/web/website/production/shared/log/production.log\n2010/12/21 08:29:14 [4537] .d..t...... iddd/web/website/production/shared/sessions/\n2010/12/21 08:29:14 [4537] >f+++++++++ iddd/web/website/production/shared/sessions/ruby_sess.017a771cc19b18cd\n2010/12/21 08:29:14 [4537] >f+++++++++ iddd/web/website/production/shared/sessions/ruby_sess.01eade9d317ca79a\n\n```",
    "answer": "> - the item is received\nf - it is a regular file\ns - the file size is different\nt - the time stamp is different\n. - the item is not being updated (though it might have attributes\nthat are being modified)\nd - it is a directory\nt - the time stamp is different\n> - the item is received\nf - a regular file\n+++++++++ - this is a newly created item\n\nLet's take a look at how rsync works and better understand the cryptic result lines:\n1 - A huge advantage of rsync is that after an interruption the next time it continues smoothly.\nThe next rsync invocation will not transfer the files again, that it had already transferred, if they were not changed in the meantime. But it will start checking all the files again from the beginning to find out, as it is not aware that it had been interrupted.\n2 - Each character is a code that can be translated if you read the section for -i, --itemize-changes in man rsync\nDecoding your example log file from the question:\n>f.st......\n\n.d..t......\n\n>f+++++++++\n\nThe relevant part of the rsync man page:\n\n-i, --itemize-changes\nRequests a simple itemized list of the changes that are being made to\neach file, including attribute changes. This is exactly the same as\nspecifying --out-format='%i %n%L'. If you repeat the option, unchanged\nfiles will also be output, but only if the receiving rsync is at least\nversion 2.6.7 (you can use -vv with older versions of rsync, but that\nalso turns on the output of other verbose messages).\nThe \"%i\" escape has a cryptic output that is 11 letters long. The\ngeneral format is like the string YXcstpoguax, where Y is replaced by\nthe type of update being done, X is replaced by the file-type, and the\nother letters represent attributes that may be output if they are\nbeing modified.\nThe update types that replace the Y are as follows:\n\nA < means that a file is being transferred to the remote host (sent).\nA > means that a file is being transferred to the local host (received).\nA c means that a local change/creation is occurring for the item (such as the creation of a directory or the changing of a symlink,\netc.).\nA h means that the item is a hard link to another item (requires --hard-links).\nA . means that the item is not being updated (though it might have attributes that are being modified).\nA * means that the rest of the itemized-output area contains a message (e.g. \"deleting\").\n\nThe file-types that replace the X are: f for a file, a d for a\ndirectory, an L for a symlink, a D for a device, and a S for a\nspecial file (e.g. named sockets and fifos).\nThe other letters in the string above are the actual letters that will\nbe output if the associated attribute for the item is being updated or\na \".\" for no change. Three exceptions to this are: (1) a newly created\nitem replaces each letter with a \"+\", (2) an identical item replaces\nthe dots with spaces, and (3) an unknown attribute replaces each\nletter with a \"?\" (this can happen when talking to an older rsync).\nThe attribute that is associated with each letter is as follows:\n\nA c means either that a regular file has a different checksum (requires --checksum) or that a symlink, device, or special file has a\nchanged value. Note that if you are sending files to an rsync prior to\n3.0.1, this change flag will be present only for checksum-differing regular files.\nA s means the size of a regular file is different and will be updated by the file transfer.\nA t means the modification time is different and is being updated to the sender\u2019s value (requires --times). An alternate value of T\nmeans that the modification time will be set to the transfer time,\nwhich happens when a file/symlink/device is updated without --times\nand when a symlink is changed and the receiver can\u2019t set its time.\n(Note: when using an rsync 3.0.0 client, you might see the s flag\ncombined with t instead of the proper T flag for this time-setting\nfailure.)\nA p means the permissions are different and are being updated to the sender\u2019s value (requires --perms).\nAn o means the owner is different and is being updated to the sender\u2019s value (requires --owner and super-user privileges).\nA g means the group is different and is being updated to the sender\u2019s value (requires --group and the authority to set the group).\nThe u slot is reserved for future use.\nThe a means that the ACL information changed.\nThe x means that the extended attribute information changed.\n\nOne other output is possible: when deleting files, the \"%i\" will\noutput the string \"*deleting\" for each item that is being removed\n(assuming that you are talking to a recent enough rsync that it logs\ndeletions instead of outputting them as a verbose message)."
  },
  {
    "question": "How to rsync only a specific list of files? I've about 50 or so files in various sub-directories that I'd like to push to a remote server.  I figured rsync would be able to do this for me using the --include-from option. Without the --exclude=\"*\" option, all the files in the directory are being synced, with the option, no files are.\n\n```\nrsync -avP -e ssh --include-from=deploy/rsync_include.txt --exclude=* ./ root@0.0.0.0:/var/www/ --dry-run\n\n```\n\nI'm running it as dry initially and 0.0.0.0 is obviously replaced by the IP of the remote server. The contents of rsync_include.txt is a new line separated list of relative paths to the files I want to upload.\nIs there a better way of doing this that is escaping me on a Monday morning?",
    "answer": "Edit: atp's answer below is better.  Please use that one!\nYou might have an easier time, if you're looking for a specific list of files, putting them directly on the command line instead:\n\nThis is assuming, however, that your list isn't so long that the command line length will be a problem and that the rsync_include.txt file contains just real paths (i.e. no comments, and no regexps)."
  },
  {
    "question": "How do you use an identity file with rsync? How do you use an identity file with rsync?\nThis is the syntax I think I should be using with rsync to use an identity file to connect:\n\n```\n\nrsync -avz -e 'ssh -p1234  -i ~/.ssh/1234-identity'  \\\n\"/local/dir/\" remoteUser@22.33.44.55:\"/remote/dir/\"\n\n```\n\nBut it's giving me an error:\n\n```\n\nWarning: Identity file ~/.ssh/1234-identity not accessible: No such file or directory.\n\n```\n\nThe file is fine, permissions are set correctly, it works when doing ssh - just not with rsync - at least in my syntax. What am I doing wrong? Is it trying to look for the identity file on the remote machine? If so, how do I specify that I want to use an identity file on my local machine?",
    "answer": "eval $(ssh-agent) # Create agent and environment variables\nssh-add ~/.ssh/1234-identity\nhost 22.33.44.55\nIdentityFile ~/.ssh/1234-identity\nPort 1234\n\nYou may want to use ssh-agent and ssh-add to load the key into memory.  ssh will try identities from ssh-agent automatically if it can find them. Commands would be\n\nssh-agent is a user daemon which holds unencrypted ssh keys in memory.  ssh finds it based on environment variables which ssh-agent outputs when run.  Using eval to evaluate this output creates the environment variables.  ssh-add is the command which manages the keys memory.  The agent can be locked using ssh-add.  A default lifetime for a key can be specified when ssh-agent is started, and or specified for a key when it is added.\nYou might also want to setup a ~/.ssh/config file to supply the port and key definition. (See `man ssh_config for more options.)\n\nSingle quoting the ssh command will prevent shell expansion which is needed for ~ or $HOME.  You could use the full or relative path to the key in single quotes."
  },
  {
    "question": "How to RSYNC a single file? Currently I am only RSync-ing Directories as in:\n\n```\n* * * * * rsync -avz /var/www/public_html/images root@<remote-ip>:/var/www/public_html\n\n```\n\nSo how do I rsync one single file like, /var/www/public_html/.htaccess?",
    "answer": "rsync -avz --progress /var/www/public_html/.htaccess root@<remote-ip>:/var/www/public_html/\n\nYou do it the same way as you would a directory, but you specify the full path to the filename as the source. In your example:\n\nAs mentioned in the comments: since -a includes recurse, one little typo can make it kick off a full directory tree transfer, so a more fool-proof approach might to just use -vz, or replace it with -lptgoD."
  },
  {
    "question": "How do I synchronize in both directions? I want to use rsync to synchronize two directories in both directions.\nI refer to synchronization in classical sense\n(not how it is meant in rsync manuals):\nI want to update the directories in both directions,\ndepending on which of them is newer.\nCan this be done by rsync (preferable in a Linux-way)?\nIf not, what other solutions exist?",
    "answer": "rsync -rtuv /path/to/dir_a/* /path/to/dir_b\nrsync -rtuv /path/to/dir_b/* /path/to/dir_a\n\nJust run it twice, with \"newer\" mode (-u or --update flag) plus -t (to copy file modified time), -r (for recursive folders), and -v (for verbose output to see what it is doing):\n\nThis won't handle deletes, but I'm not sure there is a good solution to that problem with only periodic sync'ing."
  },
  {
    "question": "Copy or rsync command The following command is working as expected...\n\n```\ncp -ur /home/abc/* /mnt/windowsabc/\n\n```\n\nDoes rsync has any advantage over it? Is there a better way to keep to backup folder in sync every 24 hours?",
    "answer": "Rsync is better since it will only copy only the updated parts of the updated file, instead of the whole file. It also uses compression and encryption if you want. Check out this tutorial."
  },
  {
    "question": "Using Rsync include and exclude options to include directory and file by pattern I'm having problems getting my rsync syntax right and I'm wondering if my scenario can actually be handled with rsync. First, I've confirmed that rsync is working just fine between my local host and my remote host. Doing a straight sync on a directory is successful.\nHere's what my filesystem looks like:\n\n```\nuploads/\n  1260000000/\n    file_11_00.jpg\n    file_11_01.jpg\n    file_12_00.jpg\n  1270000000/\n    file_11_00.jpg\n    file_11_01.jpg\n    file_12_00.jpg\n  1280000000/\n    file_11_00.jpg\n    file_11_01.jpg\n    file_12_00.jpg\n\n```\n\nWhat I want to do is run rsync only on files that begin with \"file_11_\" in the subdirectories and I want to be able to run just one rsync job to sync all of these files in the subdirectories.\nHere's the command that I'm trying:\nrsync -nrv --include=\"**/file_11*.jpg\" --exclude=\"*\" /Storage/uploads/ /website/uploads/\nThis results in 0 files being marked for transfer in my dry run. I've tried various other combinations of --include and --exclude statements, but either continued to get no results or got everything as if no include or exclude options were set.\nAnyone have any idea how to do this?",
    "answer": "rsync -nrv --include=\"*/\" --include=\"file_11*.jpg\" --exclude=\"*\" /Storage/uploads/ /website/uploads/\nrsync -nrv --include=\"/[0-9][0-9][0-9]0000000/\" --include=\"file_11*.jpg\" --exclude=\"*\" /Storage/uploads/ /website/uploads/\n\nThe problem is that --exclude=\"*\" says to exclude (for example) the 1260000000/ directory, so rsync never examines the contents of that directory, so never notices that the directory contains files that would have been matched by your --include.\nI think the closest thing to what you want is this:\n\n(which will include all directories, and all files matching file_11*.jpg, but no other files), or maybe this:\n\n(same concept, but much pickier about the directories it will include).\nIn either case, note that the --include=... option needs to come before the --exclude=... option, because we need the former to take precedence over the latter when a file matches both patterns."
  },
  {
    "question": "Symbolic links and synced folders in Vagrant I want to use Vagrant to provide a common development environment to my team. The hosts are completely different:\n\nSome use OS X, some Linux, and some Windows.\nSome use VMware, some use VirtualBox.\n\nInside of the VM we want to run Linux.\nSo far, everything is fine.\nNow our idea was that each developer shall be able use the IDE of their choice, and hence we have introduced a synced folder that shares the source code between the host and the VM. This basically, works as well \u2026 except for symbolic links.\nInside of our source code we actually do have a few symbolic links, which is not a problem within the Linux inside the VM, but on Windows as host this causes problems. The only thing that we can not do is get rid of the symbolic links, so we need another way to deal with this.\nSo far, we have tried a number of options:\n\nThere is a workaround mentioned in an issue of Vagrant, unfortunately this is VirtualBox-only and does not help those who run VMware. So far, we haven't found a way of running code in the Vagrantfile depending on the provider used.\nInstead of using a standard shared folder we now have tried using the rsync type. This works on Windows, but crashes on OS X with a number of errors telling us that the symlink has no referent (one error per symbolic link).\nWe thought about NFS, but that only works if you do not use Windows as host.\nWe also though about SMB, but this again only works on Windows as host.\n\nI can not imagine that we are the only or the first persons on this planet to experience problems with multi-platform hosts and symbolic links within the shared folder.\nHow can you solve this issue, so that we can keep symbolic links, but still use different host operating systems?",
    "answer": "config.vm.provider \"virtualbox\" do |v|\nv.customize [\"setextradata\", :id, \"VBoxInternal2/SharedFoldersEnableSymlinksCreate/v-root\", \"1\"]\nend\n\nVirtualbox does not allow symlinks on shared folders for security reasons. To enable symlinks the following line needs to be added to the vm provider config block in the Vagrantfile:\n\nAdditionally, on windows vagrant up needs to be executed in a shell with admin rights.\nNo workarounds necessary."
  },
  {
    "question": "Two way sync with rsync I have a folder a/ and a remote folder A/.\nI now run something like this on a Makefile:\n\n```\nget-music:\n rsync -avzru server:/media/10001/music/ /media/Incoming/music/\n\nput-music:\n rsync -avzru /media/Incoming/music/ server:/media/10001/music/\n\nsync-music: get-music put-music\n\n```\n\nwhen I make sync-music, it first gets all the diffs from server to local and then the opposite, sending all the diffs from local to server.\nThis works very well only if there are just updates or new files on the future. If there are deletions, it doesn't do anything.\nIn rsync there is --delete and --delete-after options to help accomplish what I want but thing is, it doesn't work on a 2-way-sync.\nIf I want to delete server files on a syn, when local files have been deleted, it works, but if, for some reason (explained after) I have some files that aren't in the server but exist locally and they were deleted, I want locally to remove them and not server copied (as it happens).\nThing is I have 3 machines in context:\n\ndesktop\nnotebook\nhome-server\n\nSo, sometimes, server will have files that were deleted with a notebook sync, for example and then, when I run a sync with my desktop (where the deleted server files still exist on) I want these files to be deleted and not to be copied again to the server.\nI guess this is only possible with a database and track of operations :P\nAny simpler solutions?\nThank you.",
    "answer": "unison dirA/ dirB/\n\nTry Unison: http://www.cis.upenn.edu/~bcpierce/unison/\nSyntax:\n\nUnison asks what to do when files are different, but you can automate the process by using the following which accepts default (nonconflicting) options:\nunison -auto dirA/ dirB/\nunison -batch dirA/ dirB/ asks no questions at all, and writes to output how many files were ignored (because they conflicted).\nNote: I am no longer using Unison (I use NextCloud, which doesn't address the original use case). However, note that rsync is not designed for bidirectional sync, while unison is. unison may have its bugs (as any other piece of software) and its wrinkles. I am surprised it seems to be actively maintained now (last time I looked I think I thought it looked dead), but I'm not sure what's the state nowadays. I haven't had the need to have a two-way file synchronizer, so there may be better options, though.\nNote 2: I am now currently using Mutagen for some purposes. It still doesn't cover the question's use case completely, but it does implement two-way sync."
  },
  {
    "question": "Slashes and the rsync command I am trying to do something along the same lines as what is asked in this question: How do I synchronize in both directions?\nHowever, what I can't figure out is whether or not I should add slashes to the end of the file path. Basically I'm trying to create an alias command that syncs the contents of two directories that go by the same name but are on two different servers. What I don't want is for one directory to be copied into the other (which I am aware is a possibility depending on how the slashes at the end are done).\nWhat I have currently is:\n\n```\nalias syncDirectories1 = 'rsync -tvur name@host:/Users/me/directory/ /Users/me/directory/'\nalias syncDirectories2 = 'rsync -tvur /Users/me/directory/ name@host:/Users/me/directory/'\n\n```\n\nFor what I am trying to accomplish, should there be slashes at the end of both file paths?",
    "answer": "rsync -av /src/foo /dest\nrsync -av /src/foo/ /dest/foo\nrsync SRC DEST\nrsync SRC DEST/\n\nIt's described in the rsync(1) manpage:\n\nA trailing slash on the source changes this behavior to avoid creating an additional directory level at the destination.  You can think of a\ntrailing  /  on  a source as meaning \"copy the contents of this directory\" as opposed to \"copy the directory by name\", but in both cases the\nattributes of the containing directory are transferred to the containing directory on the destination.  In other words, each of the  following commands copies the files in the same way, including their setting of the attributes of /dest/foo:\n\nAs to the destination, I don't think it has any major consequences. There is a difference if the source is a file and destination doesn't exist \u2014 this will make a copy of SRC called DEST:\n\n, whereas, this will create directory DEST and copy the SRC file into it:"
  },
  {
    "question": "Rsync to Amazon Ec2 Instance I have an EC2 instance running and I am able to SSH into it. \nHowever, when I try to rsync, it gives me the error Permission denied (publickey). \nThe command I'm using is: \n\n```\nrsync -avL --progress -e ssh -i ~/mykeypair.pem ~/Sites/my_site/* root@ec2-XX-XXX-XXX-XXX.us-west-2.compute.amazonaws.com:/var/www/html/\n\n```\n\nI also tried\n\n```\nrsync -avz ~/Sites/mysite/* -e \"ssh -i ~/.ssh/id_rsa.pub\" root@ec2-XX-XXX-XXX-XXX.us-west-2.compute.amazonaws.com:/var/www/html/\n\n```\n\nThanks,",
    "answer": "ssh -i ~/path/mykeypair.pem \\\nubuntu@ec2-XX-XXX-XXX-XXX.us-west-2.compute.amazonaws.com\nrsync -avL --progress -e \"ssh -i /path/to/mykeypair.pem\" \\\n~/Sites/my_site/* \\\nroot@ec2-XX-XXX-XXX-XXX.us-west-2.compute.amazonaws.com:/var/www/html/\n\nI just received that same error. I had been consistently able to ssh with:\n\nBut when using the longer rsync construction, it seemed to cause errors. I ended up encasing the ssh statement in quotations and using the full path to the key. In your example:\n\nThat seemed to do the trick."
  },
  {
    "question": "rsync delete files on sending side after transfer I want to download a large amount of data from a remote machine.\nI'd like the data to be erased on the remote machine everytime a file finishes downloading.\nHow do I do that? Is there a flag for rsync to do this?",
    "answer": "rsync --remove-source-files -options /path/to/src/ /path/to/dest\nrsync --remove-source-files -options /path/to/src/ computerB:/path/to/dest\nrsync --remove-source-files -av /path/to/src/*.avi computerB:/path/to/dest\n\nYou need to pass the --remove-source-files option to the rsync command. It tells rsync to remove from the sending side the files (meaning non-directories) that are a part of the transfer and have been successfully duplicated on the receiving side. Do not pass the --delete option to rsync command as it delete extraneous files from destination directory.\nDelete source after successful transfer using rsync.\nThe syntax is:\n\nReference : http://www.cyberciti.biz/faq/linux-unix-bsd-appleosx-rsync-delete-file-after-transfer/"
  },
  {
    "question": "Recursive copy of a specific file type maintaining the file structure in Unix/Linux? I need to copy all *.jar files from a directory maintaining the folder structure of the parent directories. How can I do it in UNIX/Linux terminal?\nThe command cp -r *.jar /destination_dir is not what I am looking for.",
    "answer": "rsync is useful for local file copying as well as between machines. This will do what you want:\nrsync -avm --include='*.jar' -f 'hide,! */' . /destination_dir\nThe entire directory structure from . is copied to /destination_dir, but only the .jar files are copied. The -a ensures all permissions and times on files are unchanged. The -m will omit empty directories. -v is for verbose output.\nFor a dry run add a -n, it will tell you what it would do but not actually copy anything."
  },
  {
    "question": "Why is this rsync connection unexpectedly closed on Windows? I'm trying to use rsync on Windows 7. I installed cwRsync and tried to connect to Ubuntu 9.04.\n\n```\n$ rsync -azC --force --more-options ./ user@server:/my/path/\nrsync: connection unexpectedly closed (0 bytes received so far) [receiver] \nrsync error: error in rsync protocol data stream (code 12) at io.c(600) [receiver=3.0.5]\nrsync: connection unexpectedly closed (0 bytes received so far) [sender]\nrsync error: error in rsync protocol data stream (code 12) at io.c(610) [sender=3.0.8]\n\n```",
    "answer": "i get the solution. i've using cygwin and this is the problem the rsync command for Windows work only in windows shell and works in the windows powershell.\nA few times it has happened the same error between two linux boxes. and appears to be by incompatible versions of rsync"
  },
  {
    "question": "How to change the owner for a rsync I understand preserving the permissions for rsync.\nHowever in my case my local computer does not have the user the files need to under for the webserver. So when I rsync I need the owner and group to be apache on the webserver, but be my username on my local computer. Any suggestions?\nI wanted to clarify to explain exactly what I need done.\nMy personal computer: named 'home' with the user account 'michael'\nMy web server: named 'server' with the user account 'remote' and user account 'apache'\nCurrent situation: My website is on 'home' with the owner 'michael' and on 'server' with the owner 'apache'. 'home' needs to be using the user 'michael' and 'server' needs to be using the user 'apache'\nTask: rsync my website on 'home' to 'server' but have all the files owner by 'apache' and the group 'apache'\nProblem: rsync will preseve the permissions, owner, and group; however, I need all the files to be owner by apache. I know the not preserving the owner will put the owner of the user on 'server' but since that user is 'remote' then it uses that instead of 'apache'. I can not rsync with the user 'apache' (which would be nice), but a security risk I'm not willing to open up.\nMy only idea on how to solve: after each rsync manually chown -R and chgrp -R, but it's a huge system and this takes a long time, especially since this is going to production.\nDoes anyone know how to do this?\nCurrent command I use to rsync:\n\n```\nrsync --progress -rltpDzC --force --delete -e \"ssh -p22\" ./ remote@server.com:/website\n\n```",
    "answer": "ssh-keygen -f ~/.ssh/apache-rsync\ncommand=\"rsync --server -vlogDtprCz --delete . /website\",from=\"IP.ADDR.OF.SENDER\",no-port-forwarding,no-X11-forwarding,no-pty ssh-rsa AAABKEYPUBTEXTsVX9NjIK59wJ+fjDgTQtGwhATsfidQbO6u77dbAjTUmWCZjKAQ/fEFWZGSlqcO2yXXXXXXXXXXVd9DSS1tjE6vAQaRdnMXBggtn4M9rnePD2qlR5QOAUUwhyFPhm6U4VFhRoa3wLvoqCVtCV0cuirB6I45On96OPijOwvAuz3KIE3+W9offomzHsljUMXXXXXXXXXXMoYLywMG/GPrZ8supIDYk57waTQWymUyRohoQqFGMzuDNbq+U0JSRlvLFoVUZ5Piz+gKJwwiFwwAW2iNag/c4Mrb/BVDQAyEQ== comment@email.address\nrsync -av --delete -e 'ssh -i ~/.ssh/apache-rsync apache@server' ./ /website\n\nThere are hacks you could put together on the receiving machine to get the ownership right -- run 'chmod -R apache /website' out of cron would be an effective but pretty kludgey option -- but instead, I'd recommend securely allowing rsync-over-ssh-as-apache.\nYou'd create a dedicated ssh keypair for this:\n\nand then take ~/.ssh/apache-rsync.pub over to the webserver, where you'd put it into ~apache/.ssh/authorized_keys and carefully specify the allowed command, something like so, all on one line:\n\nand then your rsync command on your \"home\" machine would be something like\n\nThere are other ways to skin this cat, but this is the clearest and involves the fewest workarounds, to my mind.  It prevents getting a shell as apache, which is the biggest security concern, natch.  If you're really deadset against allowing ssh as apache, there are  other ways ... but this is how I've done it.\nReferences here: http://ramblings.narrabilis.com/using-rsync-with-ssh, http://www.sakana.fr/blog/2008/05/07/securing-automated-rsync-over-ssh/"
  },
  {
    "question": "How to copy a directory from local machine to remote machine I am using ssh to connect to a remote machine.\nIs there a way i can copy an entire directory from a local machine to the remote machine?\nI found this link to do it the other way round i.e copying from remote machine to local machine.",
    "answer": "scp -r /path/to/local/storage user@remote.host:/path/to/copy\n\nEasiest way is scp\n\nrsync is best for when you want to update versions where it has been previously copied.\nIf that doesn't work, rerun with -v and see what the error is."
  },
  {
    "question": "How to use Rsync to copy only specific subdirectories (same names in several directories) I have such directories structure on server 1:\n\ndata\n\ncompany1\n\nunique_folder1\nother_folder\n...\n\ncompany2\n\nunique_folder1\n...\n\n...\n\n\nAnd I want duplicate this folder structure on server 2, but copy only directories/subdirectories of unique_folder1. I.e. as result must be:\n\ndata\n\ncompany1\n\nunique_folder1\n\ncompany2\n\nunique_folder1\n\n...\n\n\nI know that rsync is very good for this.\nI've tried 'include/exclude' options without success.\nE.g. I've tried:\n\n```\nrsync -avzn --list-only --include '*/unique_folder1/**' --exclude '*' -e ssh user@server.com:/path/to/old/data/ /path/to/new/data/\n\n```\n\nBut, as result, I don't see any files/directories:\n\n```\nreceiving file list ... done\nsent 43 bytes  received 21 bytes  42.67 bytes/sec\ntotal size is 0  speedup is 0.00 (DRY RUN)\n\n```\n\nWhat's wrong? Ideas?  \n\nAdditional information:\nI have sudo access to both servers. One idea I have - is to use find command and cpio together to copy to new directory with content I need and after that use Rsync. But this is very slow, there are a lot of files, etc.",
    "answer": "rsync -avzn --list-only --include 'company1/' --include 'company1/unique_folder1/***' --exclude '*' -e ssh user@server.com:/path/to/old/data/ /path/to/new/data\n+ company1/\n+ company1/unique_folder1/***\n...\n+ companyN/\n+ companyN/unique_folder1/***\nrsync -avzn                                        \\\n--list-only                                  \\\n--include-from '/path/to/new/include.txt'    \\\n--exclude '*'                                \\\n-e ssh user@server.com:/path/to/old/data/    \\\n/path/to/new/data\n\nI've found the reason. As for me - it wasn't clear that Rsync works in this way.\nSo correct command (for company1 directory only) must be:\n\nI.e. we need include each parent company directory. And of course we cannot write manually all these company directories in the command line, so we save the list into the file and use it.\n\nFinal things we need to do:\n1.Generate include file on server 1, so its content will be (I've used ls and awk):\n\n2.Copy include.txt to server 2 and use such command:"
  },
  {
    "question": "rsync through ssh tunnel I want to rsync to a cluster node to which I usually connect passing through another system: \nSay I connect first to \n\n```\n  ssh user@bridge \n\n```\n\nand from there to \n\n```\n  ssh user@clusternode\n\n```\n\nNow I want to rsync from my workstation to clusternode. I do the following:\n\nI open a ssh tunnel \n\n```\nssh -L8000:clusternode:8000 user@bridge\n\n```\n\nI rsync from my workstation to clusternode\n\n```\nrsync -e \"ssh -p8000\" source user@localhost:destination\n\n```\n\n\nand it does not work, I get \n\n```\n ssh_exchange_identification: Connection closed by remote host\n\n```\n\nWhy does it not work? What do I have to do?\n\nI have found a lot of information here:\nhttp://toddharris.net/blog/2005/10/23/rsyncing-through-an-ssh-tunnel/\nI think to understand that my problem is the second authentication between the bridge and the destination, so I changed to method 2 that is also not very elegant, but it works. I would like to try method 3, but I don't know how to configure a rsync daemon",
    "answer": "ssh -N -L 2222:remote.example.com:22 bridge.example.com&\nrsync -auve \"ssh -p 2222\" . me@localhost:/some/path\n\nHere's what worked for me.\nI run a command in the background to tunnel to the remote host:\n\nthen I rsync to localhost like this:"
  },
  {
    "question": "Rsync on Windows: wrong permissions for created directories I'm trying to push changes to my server through ssh on windows (cygwin) using rsync.\nThe command I am using is:\n\nrsync -rvz -e ssh /cygdrive/c/myfolder/ rsyncuser@192.168.1.110:/srv/www/prj112/myfolder/\n\n/srv/www/prj112/myfolder/ is owned by rsyncuser. My problem is that eventhough with rsync the sub directories are create as they publish, each directory is assigned default permission of d--------- so rsync fails to copy any files inside it.\nHow do I fix this?",
    "answer": "none /cygdrive cygdrive user,noacl,posix=0 0 0\nrsync -rvz --chmod=ugo=rwX -e ssh source destination\n\nThe option to ignore NTFS permissions has changed in Cygwin version 1.7. This might be what's causing the problem.\nTry adding the 'noacl' flag to your Cygwin mounts in C:\\cygwin\\etc\\fstab, for example:\n\nYou can pass custom permissions via rsync using the 'chmod' option:"
  },
  {
    "question": "Upgrading rsync on OS X using Homebrew The rsync version on my OS X (10.10.3) is an old one, 2.6.9.  I've tried to upgrade it using Homebrew but I get the following error:\n\n```\nError: No available formula for rsync\n\n```\n\nThe new one is supposed to be 3.0+ and much quicker, but how do I install this without deleting the old one?",
    "answer": "brew tap homebrew/dupes\nbrew install rsync\nbrew install rsync\n\nFollow the instructions here:\n\nAnd then edit /private/etc/paths to put /usr/local/bin before /usr/bin.\nEdit:\n\nWarning: homebrew/dupes was deprecated. This tap is now empty as all its formulae were migrated.\n\nSo, only\n\nis enough."
  },
  {
    "question": "Prevent rsync from deleting destination files that match a given pattern I'm using rsync to sync files from a source to a destination:\n\n```\nrsync -av --delete source destination\n\n```\n\nI have a single directory on the destination side which is not on the source side. I'd like to prevent rsync from deleting this directory. Is there an option I can pass to rsync to prevent this directory from being deleted upon sync?",
    "answer": "rsync -avrc --delete --exclude somedir source destination\n\nYou can exclude files/directories with --exclude.  This will prevent the somedir directory from being synced/deleted:"
  },
  {
    "question": "How to automatically accept the remote key when rsyncing? I'm attempting to make a system which automatically copies files from one server to many servers. As part of that I'm using rsync and installing SSH keys, which works correctly. \nMy problem is that when it attempts to connect to a new server for the first time it will ask for a confirmation. Is there a way to automatically accept?\nExample command/output:\n\n```\nrsync -v -e ssh * root@someip:/data/\nThe authenticity of host 'someip (someip)' can't be established.\nRSA key fingerprint is somerandomrsakey.\nAre you sure you want to continue connecting (yes/no)? yes\n\n```",
    "answer": "-e \"ssh -o StrictHostKeyChecking=no\"\n\nIf they genuinely are new hosts, and you can't add the keys to known_hosts beforehand (see York.Sar's answer), then you can use this option:"
  },
  {
    "question": "What do the numbers in rsync&#39;s output mean? When I run rsync with the --progress flag, I get information about the transfers as follows.\n\n```\npath/to/file\n          16 100%    0.01kB/s    0:00:01 (xfer#10857, to-check=427700/441502)\n\n```\n\nWhat do the numbers in the second row mean? I know what some of them are, but what do the others mean (marked with ??? below)?\n\n16 ???\n100% amount of transfer completed in this file\n0.0.1kB/s speed of current file transfer\n0:00:01: time elapsed in current file transfer\n10857 count of files transferred\n427700 ???\n441502 ???",
    "answer": "100%  146.38kB/s    0:00:08  (xfer#5, to-check=169/396)\n\nWhen the file transfer finishes, rsync\nreplaces the progress line with a\nsummary line that looks like this:\n\nIn this example, the file was 1238099\nbytes long in total, the average rate\nof transfer for the whole file was\n146.38 kilobytes per second over the 8 seconds that it took to complete, it\nwas the 5th transfer of a regular file\nduring the current rsync session, and\nthere are 169 more files for the\nreceiver to check (to see if they are\nup-to-date or not) remaining out of\nthe 396 total files in the file-list.\n\nfrom http://samba.anu.edu.au/ftp/rsync/rsync.html under --progress switch"
  },
  {
    "question": "rsync multiple remote directories to local machine preserving directory paths Would I be able to use rsync as such:\n\n```\nrsync -e ssh root@remote.com:/path/to/file:/path/to/second/file/ /local/directory/\n\n```\n\nor would i have to do something else?",
    "answer": "The syntax for requesting multiple files from a remote host is done\nby specifying additional remote-host args in the same style as the\nfirst, or with the hostname omitted.  For instance, all these work:\n\nrsync -av host:file1 :file2 host:file{3,4} /dest/\nrsync -av host::modname/file{1,2} host::modname/file3 /dest/\nrsync -av host::modname/file1 ::modname/file{3,4}\nrsync -e ssh root@remote.com:/path/to/file :/path/to/second/file/ /local/directory/\n\nDirectly from the rsync man page:\n\nThis means your example should have a space added before the second path:\n\nI'd suggest you first try it with the -n or --dry-run option, so you see what will be done, before the copy (and possible deletions) are actually performed."
  },
  {
    "question": "compare contents of two directories on remote server using unix I am new to unix and need some help here.\nI have two directories present on two different server. both the directories contains the same files. Now i want to check if all files are in sync in both the directories. If files are not in sync then i want to display only name of those files.\nI am able to do it when directories are on same server. not able to figure out how to do this when directories are present on two different servers.\n\n```\neg:\nserver1 /abc/home/sample1/\nserver2 /abc/home/sample2/\n\n```\n\nhere i want only files name to display when it not in sync.\nThanks in advance",
    "answer": "rsync -n -avrc /abc/home/sample1/* server2:/abc/home/sample2/\n\nYou can use rsync with the -n flag to find out if the files are in sync, without actually doing a sync.\nFor example, from server1:\n\nThis will print the names of all files (recursive, with the -r flag) that differ between server1:/abc/home/sample1/ and server2:/abc/home/sample2/\n\nrsync used parameters explanation\n-n, --dry-run - perform a trial run with no changes made\n-a, --archive - archive mode; equals -rlptgoD (no -H,-A,-X)\n-v, --verbose - increase verbosity\n-r, --recursive - recurse into directories\n-c, --checksum - skip based on checksum, not mod-time & size"
  },
  {
    "question": "How to recursively copy directories starting with &quot;abc&quot; on Linux/Unix? I have a directory ~/plugins/ and inside there are many sub-directories. If I wanted to create a backup somewhere else of just the sub-directories starting with abc could I do that with a one line copy command? I would assume something like this would work (but it doesn't):\n\n```\n\ncp -R ~/plugins/abc* ~/destination/\n\n```\n\nI would rather use a one-line command, if possible, because I would also like to use the same syntax for rsync, and if I have to do something like\n\n```\n\nfind ~/plugins/ -type d -name \"abc*\" -exec cp -R {} ~/destination;\n\n```\n\nthen that works fine for the cp command but it would mean that I would have to run rsync once for each directory and that just doesn't seem efficient :(",
    "answer": "cp -r ~/plugins/abc* ~/destination\n\nNot sure why what you're trying didn't work (but what is the \"copy\" command?), but this works on Linux at least:"
  },
  {
    "question": "Rsync Encryption I know that rsync can enable / disable the ssh encryption protocol during the file transfer. So, if the ssh encryption protocol has been disabled, does it mean that rsync does not do any encryption at all?\nAlso, the reason why I asked the above question is we use the rsync module as part of our file transfer and there is nothing in the module that specifies that ssh encryption will be used.\nIf rsync does not use any encryption, then I can theoretically open a port on both source and destination machines and push the file from source to destination.",
    "answer": "rsync performs no encryption on its own. If you don't use ssh, nor do you tunnel the rsync traffic through stunnel or some kind of VPN, then no encryption is performed. Yes, you can save some CPU cycles this way."
  },
  {
    "question": "Bash scripting rsync: rsync: link_stat (blah) failed: No such file or directory (2) I am trying to write a simple bash script for my local (Mac OS X) machine to move files from a directory on my machine to a remote machine. This line is failing:\n\n```\nrsync --verbose  --progress --stats --compress --rsh=ssh \\\n      --recursive --times --perms --links --delete \\\n      --exclude \"*bak\" --exclude \"*~\" \\\n      /repository/* $DEV_SERVER:$REMOTE_DIR\n\n```\n\n$DEV_SERVER and $REMOTE_DIR are defined previously, and I echo them to verify they're accurate.\nThe error I'm getting is:\n\n```\nrsync: link_stat /Users/myusername/mycurrentdirectory failed: No such file or directory (2)\n\n```\n\nTo note here is that rather than using the defined directory (/repository, which is in the root of the machine), it uses my working directory. What is causing this?",
    "answer": "Check that your \\ characters have no whitespace after them at the end of the line. This will cause BASH to not interpret the line wrap correctly, giving the rsync error above."
  },
  {
    "question": "Install rsync with cygwin I would like to backup a dir in Win XP on a Solaris server via rsync. I installed cygwin but when I type rsync I get 'command not found'. How can I install rsync? And how can I install ssh. I have installed Poderosa as ssh client (a sort of putty).",
    "answer": "Run your cygwin setup.exe again, and expand the categories to find 'rsync' and 'ssh'."
  },
  {
    "question": "read input from a file and sync accordingly I have a text file which contains the list of files and directories that I want to copy (one on a line). Now I want rsync to take this input from my text file and sync it to the destination that I provide.\nI've tried playing around with \"--include-from=FILE\" and \"--file-from=FILE\" options of rsync but it is just not working\nI also tried pre-fixing \"+\" on each line in my file but still it is not working.\nI have tried coming with various filter PATTERNs as outlined in the rsync man page but it is still not working.\nCould someone provide me correct syntax for this use case. I've tried above things on Fedora 15, RHEL 6.2 and Ubuntu 10.04 and none worked. So i am definitely missing something.\nMany thanks.",
    "answer": "rsync -av `cat /path/to/file` /destination/\n\ncat /path/to/file | xargs -n 200 -J % rsync -av % /destination/\nfor f in `cat /path/to/files.txt`; do cp $f /dest/; done\n\nThere is more than one way to answer this question depending on how you want to copy these files. If your intent is to copy the file list with absolute paths, then it might look something like:\nrsync -av --files-from=/path/to/files.txt / /destination/path/\n...This would expect the paths to be relative to the source location of / and would retain the entire absolute structure under that destination.\nIf your goal is to copy all of those files in the list to the destination, without preserving any kind of path hierarchy (just a collection of files), then you could try one of the following:\n\nOr a for-loop and copy:"
  },
  {
    "question": "Comprehensive list of rsync error codes I'm writing a script that does daily snapshots of users' home directories. First I do a dry run using:\n\n```\nrsync -azvrn --out-format=\"%M %f\" source/dir dest/dir\n\n```\n\nand then the actual rsync operation (by removing the -n option).\nI'm trying to parse the output of the dry run. Specifically, I'm interested in learning the exact cause of the rsync error (if one occurred). Does anyone know of \n\nThe most common rsync errors and their codes?\nA link to a comprehensive rsync error code page?\n\nMost importantly, rsync (at least on CentOs 5) does not return an error code. Rather it displays the errors internally and returns with 0. Like thus:\n\n```\nsending incremental file list\nrsync: link_stat \"/data/users/gary/testdi\" failed: No such file or directory (2)\n\nsent 18 bytes  received 12 bytes  60.00 bytes/sec\ntotal size is 0  speedup is 0.00 (DRY RUN)\n\nrsync error: some files/attrs were not transferred (see previous errors) (code 23) at main.c(1039) [sender=3.0.6]\n\n```\n\nHas anyone had to parse rsync errors and have a suggestion on how to store the rsync return state(s)? I believe, when transferring multiple files, that errors may be raised on a per file basis and are collected at the end as shown on the last line of code above.",
    "answer": "Success\nSyntax or usage error\nProtocol incompatibility\nErrors selecting input/output files, dirs\nRequested action not supported: an attempt was made to manipulate 64-bit\nfiles on a platform that cannot support them; or an option was specified\nthat is supported by the client and not by the server.\nError starting client-server protocol\nDaemon unable to append to log-file\nError in socket I/O\nError in file I/O\nError in rsync protocol data stream\nErrors with program diagnostics\nError in IPC code\nReceived SIGUSR1 or SIGINT\nSome error returned by waitpid()\nError allocating core memory buffers\nPartial transfer due to error\nPartial transfer due to vanished source files\nThe --max-delete limit stopped deletions\nTimeout in data send/receive\nTimeout waiting for daemon connection\n\nPer the rsync \"man\" page, here are the error codes it could return and what they mean. If you're scripting it in bash, you could look at $?\n\nI've never seen a comprehensive \"most common errors\" list but I'm betting error code 1 would be at the top."
  },
  {
    "question": "Python Subprocess.Popen from a thread I'm trying to launch an 'rsync' using subprocess module and Popen inside of a thread. After I call the rsync I need to read the output as well. I'm using the communicate method to read the output. The code runs fine when I do not use a thread. It appears that when I use a thread it hangs on the communicate call. Another thing I've noticed is that when I set shell=False I get nothing back from the communicate when running in a thread.",
    "answer": "import threading\nimport subprocess\n\nclass MyClass(threading.Thread):\ndef __init__(self):\nself.stdout = None\nself.stderr = None\nthreading.Thread.__init__(self)\n\ndef run(self):\np = subprocess.Popen('rsync -av /etc/passwd /tmp'.split(),\nshell=False,\nstdout=subprocess.PIPE,\nstderr=subprocess.PIPE)\n\nself.stdout, self.stderr = p.communicate()\n\nmyclass = MyClass()\nmyclass.start()\nmyclass.join()\nprint myclass.stdout\n\nYou didn't supply any code for us to look at, but here's a sample that does something similar to what you describe:"
  },
  {
    "question": "rsync: --include-from vs. --exclude-from what is the actual difference? In the documentation, it mentions these as being files containing lists of either patterns to include or patterns to exclude.  However, that implies for inclusions, everything is considered an exclusion except where things match patterns.  So for example, an include file containing:\n\n```\n/opt/**.cfg\n\n```\n\nShould only include any file named *.cfg that exists anywhere under a directory named opt any where in the tree.  So it would match the following:\n\n```\n/opt/etc/myfile.cfg\n/some/dir/opt/myfile.cfg\n/notopt/opt/some/other/dir/myfile.cfg\n\n```\n\nI'd therefore expect it to implicitly exclude anything else.  But that doesn't seem to be the case, since I am seeing this in the itemized output:\n\n```\n*deleting   etc/rc.d/init.d/somescript\n\n```\n\nSo what is the deal with --include-from and --exclude-from?  Are they just aliases for --filter-from?",
    "answer": "--include=\"*/\" --include=\"*.cfg\" --exclude=\"*\"\n\nrsync doesn't work like that. Any file with a filename pattern that does not match any of the include or exclude patterns are considered to be included. In other words, think of the include pattern as a way of overriding exclude pattern.\nFrom the docs (emphasis mine):\n\nRsync builds an ordered list of include/exclude options as specified on the command line. Rsync checks each file and directory name against each exclude/include pattern in turn. The first matching pattern is acted on. If it is an exclude pattern, then that file is skipped. If it is an include pattern then that filename is not skipped. If no matching include/exclude pattern is found then the filename is not skipped.\n\nSo, if you want to include only specific files, you first need to include those specific files, then exclude all other files:\n\nCouple of things to note here:\n\nThe include patterns have to come before the excludes, because the first pattern that matches is the one that gets considered. If the file name matches the exclude pattern first, it gets excluded.\n\nYou need to either include all subdirectories individually, like --include=\"/opt\" --include=\"/opt/dir1\" etc. for all subdirectories, or use --include=\"*/\" to include all directories (not files). I went with the second option for brevity.\n\nIt is quirky and not very intuitive. So read the docs carefully (the \"EXCLUDE PATTERNS\" section in the link) and use the --dry-run or -n option to make sure it is going to do what you think it should do."
  },
  {
    "question": "Errors using rspec, missing libraries after installing Homebrew and uninstalling MacPorts I may have taken one step too far beyond my knowledge. I installed Homebrew and after it continued to give me warnings about having MacPorts installed I uninstalled that. But now my rspec tests don't run.\nThese are the errors I get:\n\n```\n/Users/mark/.rvm/gems/ruby-1.9.2-p180/gems/nokogiri-1.4.4/lib/nokogiri.rb:13:in `require': dlopen(/Users/mark/.rvm/gems/ruby-1.9.2-p180/gems/nokogiri-1.4.4/lib/nokogiri/nokogiri.bundle, 9): Library not loaded: /opt/local/lib/libiconv.2.dylib (LoadError)\n  Referenced from: /Users/mark/.rvm/gems/ruby-1.9.2-p180/gems/nokogiri-1.4.4/lib/nokogiri/nokogiri.bundle\n  Reason: Incompatible library version: nokogiri.bundle requires version 8.0.0 or later, but libiconv.2.dylib provides version 7.0.0 - /Users/mark/.rvm/gems/ruby-1.9.2-p180/gems/nokogiri-1.4.4/lib/nokogiri/nokogiri.bundle\n.....\n.....\n\n```\n\nI've installed libiconv through Homebrew, but that didn't fix it. It's complaining about libiconv version numbers. Is this the problem?\nWhat is going on here and what do I need to do?",
    "answer": "I got things working again for anyone interested. I removed and re-installed nokogiri gem and everything seems to be working again."
  },
  {
    "question": "rsync --delete --files-from=list / dest/ does not delete unwanted files As you can see in the title I try to sync a folder with a list of files. I hoped that this command would delete all files in dest/ that are not on the list, but it didn't.\nSo I searched a little bit and know now, that rsync can't do this.\nBut I need it, so do you know any way to do it?\nPS: The list is created by a python script, so it is imaginable that your solution uses some python code.\nEDIT, let's be concrete:\nThe list looks like this:\n\n```\n/home/max/Musik/Coldplay/Parachutes/Trouble.mp3\n/home/max/Musik/Coldplay/Parachutes/Yellow.mp3\n/home/max/Musik/Coldplay/A Rush of Blood to the Head/Warning Sign.mp3\n/home/max/Musik/Coldplay/A Rush of B-Sides to Your Head/Help Is Around the Corner.mp3\n/home/max/Musik/Coldplay/B-Sides (disc 3)/Bigger Stronger.mp3\n\n```\n\nand the command like this:\n\n```\nrsync --delete --files-from=/tmp/list / /home/max/Desktop/foobar/\n\n```\n\nThis works, but if I delete a line, it is not deleted in foobar/.\nEDIT 2:\n\n```\nrsync -r --include-from=/tmp/list --exclude=* --delete-excluded / /home/max/Desktop/foobar/\n\n```\n\nThat works neither ...",
    "answer": "rsync -r --include-from=<patternlistfile> --exclude=* --delete-excluded / dest/\nre.sub(\"([[*?])\", r\"\\\\\\1\", \"abc[def*ghi?klm\")\n/some/\n/some/dir/\n/some/dir/file1\n/some/dir/file2\n/some/other/\n/some/other/dir/\n/some/other/dir/file3\n...\nrsync -r --include-from=<patternlistfile> --exclude=* --delete-excluded /some/dir/ dest/\n/file1\n/file2\n/**/\nrsync -m -r --delete-excluded --include-from=<patternfile> --exclude=* / dest/\n/**/\n/some/dir/file1\n/some/other/dir/file3\n\nPerhaps you could do this using a list of include patterns instead, and use --delete-excluded (which does as the name suggests)? Something like:\n\nIf filenames are likely to contain wildcard characters (*, ? and [) then you may need to modify the Python to escape them:\n\nEdit: Pattern-based matching works slightly differently to --files-from in that rsync won't recurse into directories that match the exclude pattern, for reasons of efficiency. So if your files are in /some/dir and /some/other/dir then your pattern file needs to look like:\n\nAlternatively, if all files are in the same directory then you could rewrite the command slightly:\n\nand then your patterns become:\n\nEdit: Thinking about it, you could include all directories with one pattern:\n\nbut then you'd end up with the entire directory tree in dest/ which probably isn't what you want. But combining it with -m (which prunes empty directories) should solve that - so the command ends up something like:\n\nand the pattern file:"
  },
  {
    "question": "rsync exclude a directory but include a subdirectory I am trying to copy a project to my server with rsync.\nI have project specific install scripts in a subdirectory \nproject/specs/install/project1\nWhat I am trying to do is exclude everything in the project/specs directory but the project specific install directory: project/specs/install/project1.\n\n```\nrsync -avz --delete --include=specs/install/project1 \\\n    --exclude=specs/* /srv/http/projects/project/ \\\n     user@server.com:~/projects/project\n\n```\n\nBut like this the content of the specs directory gets excluded but the install/project1 directory does not get included. \nI have tried everything but i just don't seem to get this to work",
    "answer": "rsync -avz --delete --include=specs/install/project1/ \\\n--exclude=specs/* /srv/http/projects/project/ \\\nuser@server.com:~/projects/project\ncat << EOF >pattern.txt\n> + specs/install/project1/\n> - specs/*\n> EOF\nrsync -avz --delete --filter=\". pattern.txt\" \\\n/srv/http/projects/project/ \\\nuser@server.com:~/projects/project\n\nSometime it's just a detail.\nJust change your include pattern adding a trailing / at the end of include pattern and it'll work:\n\nOr, in alternative, prepare a filter file like this:\n\nThen use the --filter option:\n\nFor further info go to the FILTER RULES section in the rsync(1) manual page."
  },
  {
    "question": "StrictHostKeyChecking not Ignoring Fingerprint Validation I'm Rsync-ing with the following command:\n\n```\n# rsync -arvce ssh /tmp/rsync/file.txt user@domain:/tmp/rsync/\n\n```\n\nThis works fine, and I will have to do this for multiple places, so I want to implement the StrictHostKeyChecking option.\nAfter reading other online examples I've added the option like this (3 examples):\n\n```\n# rsync -arvce ssh -o 'StrictHostKeychecking no' /tmp/rsync/file.txt user@domain:/tmp/rsync/\n\n# rsync -arvce ssh -o 'StrictHostKeychecking=no' /tmp/rsync/file.txt user@domain:/tmp/rsync/\n\n# rsync -arvce ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no /tmp/rsync/file.txt user@domain:/tmp/rsync/\n\n```\n\nI still get prompted to validate the target server's key.\nI understand the -o StrictHostKeychecking=no options let me choose whether to bypass that step each time I open a connection.\nAm I doing it incorrectly?\nhere' some links I've read about this:\nhttp://linuxcommando.blogspot.com/2008/10/how-to-disable-ssh-host-key-checking.html\nhttp://www.thegeekstuff.com/2010/04/how-to-fix-offending-key-in-sshknown_hosts-file/\nhttp://www.cyberciti.biz/faq/linux-appleosx-howto-disable-ssh-host-key-checking/",
    "answer": "I've resolved it.\nHere's the correct (or mostly correct) way to add that option:\n\nAppears there's a finicky way to apply the option, by adding double quotes and placing the ssh inside."
  },
  {
    "question": "Do not show directories in rsync output Does anybody know if there is an rsync option, so that directories that are being traversed do not show on stdout.\nI'm syncing music libraries, and the massive amount of directories make it very hard to see which file changes are actually happening.\nI'v already tried -v and -i, but both also show directories.",
    "answer": "deleting folder1/\ndeleting folder2/\ndeleting folder3/folder4/\nrsync -av --delete remote_folder local_folder | grep -E '^deleting|[^/]$'\n\nIf you're using --delete in your rsync command, the problem with calling grep -E -v '/$' is that it will omit the information lines like:\n\nIf you're making a backup and the remote folder has been completely wiped out for X reason, it will also wipe out your local folder because you don't see the deleting lines.\nTo omit the already existing folder but keep the deleting lines at the same time, you can use this expression :"
  },
  {
    "question": "rsync --include option does not exclude other files trying to rsync files of certain extension(*.sh), but the bash script below still transfer all the files, why?\n\n```\n\nfrom=/home/xxx\nrsync -zvr --include=\"*.sh\" $from/*  root@$host:/home/tmp/\n\n```",
    "answer": "rsync -zvr --include=\"*.sh\" --exclude=\"*\" $from/*  root@$host:/home/tmp/\n\nYou need to add a --exclude all and it has to come after the --include"
  },
  {
    "question": "What is the difference between tar and zip? What is the difference between tar and zip?  What are the use cases for each?",
    "answer": "tar in itself just bundles files together (the result is called a tarball), while zip applies compression as well.\nUsually you use gzip along with tar to compress the resulting tarball, thus achieving similar results as with zip.\nFor reasonably large archives there are important differences though.  A zip archive is a collection of compressed files.  A gzipped tar is a compressed collection (of uncompressed files).  Thus a zip archive is a randomly accessible list of concatenated compressed items, and a .tar.gz is an archive that must be fully expanded before the catalog is accessible.\n\nThe caveat of a zip is that you don't get compression across files (because each file is compressed independent of the others in the archive, the compression cannot take advantage of similarities among the contents of different files); the advantage is that you can access any of the files contained within by looking at only a specific (target file dependent) section of the archive (as the \"catalog\" of the collection is separate from the collection itself).\nThe caveat of a .tar.gz is that you must decompress the whole archive to access files contained therein (as the files are within the tarball); the advantage is that the compression can take advantage of similarities among the files (as it compresses the whole tarball)."
  },
  {
    "question": "Tar archiving that takes input from a list of files I have a file that contain list of files I want to archive with tar.\nLet's call it mylist.txt\nIt contains:\n\n```\nfile1.txt\nfile2.txt\n...\nfile10.txt\n\n```\n\nIs there a way I can issue TAR command that takes mylist.txt as input?\nSomething like\n\n```\ntar -cvf allfiles.tar -[someoption?] mylist.txt\n\n```\n\nSo that it is similar as if I issue this command:\n\n```\ntar -cvf allfiles.tar file1.txt file2.txt file10.txt \n\n```",
    "answer": "tar -cvf allfiles.tar -T mylist.txt\n\nYes:"
  },
  {
    "question": "Create a tar.xz in one command I am trying to create a .tar.xz compressed archive in one command. What is the specific syntax for that?\nI have tried tar cf - file | xz file.tar.xz, but that does not work.",
    "answer": "tar cfJ <archive.tar.xz> <files>\ntar -cJf <archive.tar.xz> <files>\n\nUse the -J compression option for xz. And remember to man tar :)\n\nEdit 2015-08-10:\nIf you're passing the arguments to tar with dashes (ex: tar -cf as opposed to tar cf), then the -f option must come last, since it specifies the filename (thanks to @A-B-B for pointing that out!). In that case, the command looks like:"
  },
  {
    "question": "How do I turn off the output from tar commands on Unix? I had a look at the options, but nothing seemed obvious as a manner in which to turn off the output when uncompressing a file. The below is the code I am currently using... I just need the option to switch off the output.\n\n```\ntar -zxvf tmp.tar.gz -C ~/tmp1\n\n```",
    "answer": "tar -zxf tmp.tar.gz -C ~/tmp1\n\nJust drop the option v.\n-v is for verbose. If you don't use it then it won't display:"
  },
  {
    "question": "Are tar.gz and tgz the same thing? I created .tgz file with tar czvf file command.then I ended up with a tgz file. I want to know the difference between it and tar.gz.",
    "answer": "I think in the old package repo days, .tgz was used because files on DOS floppies could only have three letter extensions. When this limitation was removed, .tar.gz was used to be more verbose by showing both the archive type (tar) and zipper (gzip).\nThey are identical."
  },
  {
    "question": "Excluding directory when creating a .tar.gz file I have a /public_html/ folder, in that folder there's a /tmp/ folder that has like 70gb of files I don't really need.\nNow I am trying to create a .tar.gz of /public_html/ excluding /tmp/\nThis is the command I ran:\n\n```\ntar -pczf MyBackup.tar.gz /home/user/public_html/ --exclude \"/home/user/public_html/tmp/\" \n\n```\n\nThe tar is still being created, and by doing an ls -sh I can see that MyBackup.tar.gz already has about 30gb, and I know for sure that /public_html/ without /tmp/ doesn't have more than 1GB of files.\nWhat did I do wrong?",
    "answer": "tar -pczf MyBackup.tar.gz /home/user/public_html/ --exclude \"/home/user/public_html/tmp\"\n\nTry removing the last / at the end of the directory path to exclude"
  },
  {
    "question": "How to check if a Unix .tar.gz file is a valid file without uncompressing? I have found the question How to determine if data is valid tar file without a file?, but I was wondering: is there a ready made command line solution?",
    "answer": "tar -tzf my_tar.tar.gz >/dev/null\n\nWhat about just getting a listing of the tarball and throw away the output, rather than decompressing the file?\n\nEdit as per comment. Thanks Frozen Flame! This test in no way implies integrity of the data. Because it was designed as a tape archival utility most implementations of tar will allow multiple copies of the same file!"
  },
  {
    "question": "How to extract filename.tar.gz file I want to extract an archive named filename.tar.gz.\nUsing tar -xzvf filename.tar.gz doesn't extract the file. it is gives this error:\n\n```\ngzip: stdin: not in gzip format\ntar: Child returned status 1\ntar: Error exit delayed from previous errors\n\n```",
    "answer": "mv filename.tar.gz filename.tar # optional\ntar xvf filename.tar\nfile ~/Downloads/filename.tbz2\n/User/Name/Downloads/filename.tbz2: bzip2 compressed data, block size = 400k\n\nIf file filename.tar.gz gives this message: POSIX tar archive,\nthe archive is a tar, not a GZip archive.\nUnpack a tar without the z, it is for gzipped (compressed), only:\n\nOr try a generic Unpacker like unp (https://packages.qa.debian.org/u/unp.html), a script for unpacking a wide variety of archive formats.\ndetermine the file type:"
  },
  {
    "question": "How do I tar a directory without retaining the directory structure? I'm working on a backup script and want to tar up a file directory:\n\n```\ntar czf ~/backup.tgz /home/username/drupal/sites/default/files\n\n```\n\nThis tars it up, but when I untar the resulting file, it includes the full file structure: the files are in home/username/drupal/sites/default/files.\nIs there a way to exclude the parent directories, so that the resulting tar just knows about the last directory (files)?",
    "answer": "cd /home/username/drupal/sites/default/files\ntar czf ~/backup.tgz *"
  },
  {
    "question": "Find files and tar them (with spaces) Alright, so simple problem here. I'm working on a simple back up code.  It works fine except if the files have spaces in them.  This is how I'm finding files and adding them to a tar archive:\n\n```\nfind . -type f | xargs tar -czvf backup.tar.gz \n\n```\n\nThe problem is when the file has a space in the name because tar thinks that it's a folder.  Basically is there a way I can add quotes around the results from find? Or a different way to fix this?",
    "answer": "find . -type f -print0 | tar -czvf backup.tar.gz --null -T -\n\nUse this:\n\nIt will:\n\ndeal with files with spaces, newlines, leading dashes, and other funniness\nhandle an unlimited number of files\nwon't repeatedly overwrite your backup.tar.gz like using tar -c with xargs will do when you have a large number of files\n\nAlso see:\n\nGNU tar manual\nHow can I build a tar from stdin?, search for null"
  },
  {
    "question": "How to tar certain file types in all subdirectories? I want to tar and all .php and .html files in a directory and its subdirectories. If I use \ntar -cf my_archive *\nit tars all the files, which I don't want. If I use \ntar -cf my_archive *.php *.html\nit ignores subdirectories. How can I make it tar recursively but include only two types of files?",
    "answer": "find ./someDir -name \"*.php\" -o -name \"*.html\" | tar -cf my_archive -T -"
  },
  {
    "question": "reading tar file contents without untarring it, in python script I have a tar file which has number of files within it.\nI need to write a python script which will read the contents of the files and gives the count o total characters, including  total number of letters, spaces, newline characters, everything,  without untarring the tar file.",
    "answer": ">>> import  tarfile\n>>> tar = tarfile.open(\"test.tar\")\n>>> tar.getmembers()\nimport tarfile,os\nimport sys\nos.chdir(\"/tmp/foo\")\ntar = tarfile.open(\"test.tar\")\nfor member in tar.getmembers():\nf=tar.extractfile(member)\ncontent=f.read()\nprint \"%s has %d newlines\" %(member, content.count(\"\\n\"))\nprint \"%s has %d spaces\" % (member,content.count(\" \"))\nprint \"%s has %d characters\" % (member, len(content))\nsys.exit()\ntar.close()\n\nyou can use getmembers()\n\nAfter that, you can use extractfile() to extract the members as file object. Just an example\n\nWith the file object f in the above example, you can use read(), readlines() etc."
  },
  {
    "question": "How do I extract files without folder structure using tar I have a tar.gz-file with the following structure:\n\n```\nfolder1/img.gif\nfolder2/img2.gif\nfolder3/img3.gif\n\n```\n\nI want to extract the image files without the folder hierarchy so the extracted result looks like:\n\n```\n/img.gif\n/img2.gif\n/img3.gif\n\n```\n\nI need to do this with a combination of Unix and PHP. Here is what I have so far, it works to extract them to the specified directory but keeps the folder hierarchy:\n\n```\nexec('gtar --keep-newer-files -xzf images.tgz -C /home/user/public_html/images/',$ret);\n\n```",
    "answer": "--strip-components count\n(x mode only) Remove the specified number of leading path ele-\nments.  Pathnames with fewer elements will be silently skipped.\nNote that the pathname is edited after checking inclusion/exclu-\nsion patterns but before security checks.\ntar -tf tarfolder.tar\ntarfolder/\ntarfolder/file.a\ntarfolder/file.b\nls -la file.*\nls: file.*: No such file or directory\ntar -xf tarfolder.tar --strip-components 1\nls -la file.*\n-rw-r--r--  1 ericgorr  wheel  0 Jan 12 12:33 file.a\n-rw-r--r--  1 ericgorr  wheel  0 Jan 12 12:33 file.b\n\nYou can use the --strip-components option of tar.\n\nI create a tar file with a similar structure to yours:\n\nThen extracted by doing:"
  },
  {
    "question": "gzip: stdin: not in gzip format tar: Child returned status 1 tar: Error is not recoverable: exiting now I have a Bash script that creates a .tar.gz file, encrypts, and then sends it to a drive. However, I cannot open the .tar.gz file afterwards. Here is my process...\nBash script that encrypts.\n\n```\n#!/bin/sh\n\n# Tar the automysqlbackup directory\ntar -zcf \"red-backup-$(date '+%Y-%m-%d').tar.gz\" /var/lib/automysqlbackup/\n\n# Encrypt the tar file\nopenssl aes-256-cbc -a -salt -in \"red-backup-$(date '+%Y-%m-%d').tar.gz\" -out \"red-backup-$(date '+%Y-%m-%d').tar.gz.enc\" -pass 'pass:MySecretPWD'\n\n# Remove the original tar file\nrm -rf \"red-backup-$(date '+%Y-%m-%d').tar.gz\"\n\n# Upload to Google Drive\ngdrive upload --file \"red-backup-$(date '+%Y-%m-%d').tar.gz.enc\" -p \"jofhriout849uioejfoiu09\"\n\n```\n\nThen I download the file and use\n\n```\nsudo openssl aes-256-cbc -e -in red-backup-2016-09-22.tar.gz.enc -out red-backup-2016-09-22.tar.gz\n\n```\n\nI then enter the passphrase for my file twice and I now get a file called\n\n```\nred-backup-2016-09-22.tar.gz\n\n```\n\nWhen I then try\n\n```\nsudo tar -zxvf red-backup-2016-09-22.tar.gz\n\n```\n\nI get\n\n```\ngzip: stdin: not in gzip format\ntar: Child returned status 1\ntar: Error is not recoverable: exiting now\n\n```\n\nI have also tried renaming the file .tar and also tried\n\n```\nsudo tar xvf red-backup-2016-09-22.tar.gz\n\n```\n\nand\n\n```\nsudo tar xvf red-backup-2016-09-22.tar\n\ntar: This does not look like a tar archive\ntar: Skipping to next header\ntar: Exiting with failure status due to previous errors\n\n```\n\nWhere am I going wrong?",
    "answer": "This is probably because of your gzip version incompatibility.\nCheck these points first:\n\nwhich gzip\n\n/usr/bin/gzip   or /bin/gzip\nIt should be either /bin/gzip or /usr/bin/gzip. If your gzip points to some other gzip application, please try by removing that path from your PATH environment variable.\nNext is\n\ngzip -V\n\ngzip 1.3.5\n(2002-09-30)\nYour problem can be resolved with these check points."
  },
  {
    "question": "Create a .tar.bz2 file Linux On my Linux machine, I wish to create a .tar.bz2 file of a certain folder. Once I place myself in that folder (in the terminal), what do I type in the terminal command line to place the compressed folder in the home directory of my machine?\nLet's say I am in the folder /home/user/folder. In the folder \"folder\" are several files (txt, .c etc). How do I compress that folder of type .tar.bz2 and place it in my /home directory?\nIn the /home/user/folder, I've tried sudo tar -cvjSf folder.tar.bz2 but get an error:\n\ntar: Cowardly refusing to create an empty archive",
    "answer": "sudo tar -cvjSf folder.tar.bz2 folder\nsudo tar -cvjSf folder.tar.bz2 *\n\nYou are not indicating what to include in the archive.\nGo one level outside your folder and try:\n\nOr from the same folder try"
  },
  {
    "question": "Listing the content of a tar file or a directory only down to some level I wonder how to list the content of a tar file only down to some level? \nI understand tar tvf mytar.tar will list all files, but sometimes I would like to only see directories down to some level.\nSimilarly, for the command ls, how do I control the level of subdirectories that will be displayed? By default, it will only show the direct subdirectories, but not go further.",
    "answer": "tar tvf scripts.tar | awk -F/ '{if (NF<4) print }'\n\ndrwx------ glens/glens       0 2010-03-17 10:44 scripts/\n-rwxr--r-- glens/www-data 1051 2009-07-27 10:42 scripts/my2cnf.pl\n-rwxr--r-- glens/www-data  359 2009-08-14 00:01 scripts/pastebin.sh\n-rwxr--r-- glens/www-data  566 2009-07-27 10:42 scripts/critic.pl\n-rwxr-xr-x glens/glens     981 2009-12-16 09:39 scripts/wiki_sys.pl\n-rwxr-xr-x glens/glens    3072 2009-07-28 10:25 scripts/blacklist_update.pl\n-rwxr--r-- glens/www-data 18418 2009-07-27 10:42 scripts/sysinfo.pl\ntar tf scripts.tar | awk -F/ '{if (NF<3) print }'\n\nscripts/\nscripts/my2cnf.pl\nscripts/pastebin.sh\nscripts/critic.pl\nscripts/wiki_sys.pl\nscripts/blacklist_update.pl\nscripts/sysinfo.pl\n\nMake sure to note, that the number is 3+ however many levels you want, because of the / in the username/group.  If you just do\n\nit's only two more.\nYou could probably pipe the output of ls -R to this awk script, and have the same effect."
  },
  {
    "question": "Uncompress tar.gz file With the usage of wget command line I got a tar.gz file. I downloaded it in the root@raspberrypi. Is there any way to uncompress it in the /usr/src folder?",
    "answer": "tar zxvf <yourfile>.tar.gz -C /usr/src/\n/usr/src/<yourfile>\n\nUse -C option of tar:\n\nand then, the content of the tar should be in:"
  },
  {
    "question": "Check the total content size of a tar gz file How can I extract the size of the total uncompressed file data in a .tar.gz file from command line?",
    "answer": "tar tzvf archive.tar.gz | sed 's/ \\+/ /g' | cut -f3 -d' ' | sed '2,$s/^/+ /' | paste -sd' ' | bc\n\nThis will sum the total content size of the extracted files:\n\nThe output is given in bytes.\nExplanation: tar tzvf lists the files in the archive in verbose format like ls -l. sed and cut isolate the file size field. The second sed puts a + in front of every size except the first and paste concatenates them, giving a sum expression that is then evaluated by bc.\nNote that this doesn't include metadata, so the disk space taken up by the files when you extract them is going to be larger - potentially many times larger if you have a lot of very small files."
  },
  {
    "question": "How can files be added to a tarfile with Python, without adding the directory hierarchy? When I invoke add() on a tarfile object with a file path, the file is added to the tarball with directory hierarchy associated. In other words, if I unzip the tarfile the directories in the original directories hierarchy are reproduced.\nIs there a way to simply add a plain file, without directory info, so that untarring the resulting tarball produces a flat list of files?",
    "answer": "tar.addfile(tarfile.TarInfo(\"myfilename.txt\"), open(\"/path/to/filename.txt\"))\n\nYou can use tarfile.addfile(), in the TarInfo object, which is the first parameter, you can specify a name that's different from the file you're adding.\nThis piece of code should add /path/to/filename to the TAR file but will extract it as myfilename:"
  },
  {
    "question": "How can I build a tar from stdin? How can I pipe information into tar specifying the names of the file?",
    "answer": "tar cfz foo.tgz --files-from=-\n\nSomething like:\n\nBut keep in mind that this won't work for all possible filenames; you should consider the --null option and feed tar from find -print0.  (The xargs example won't quite work for large file lists because it will spawn multiple tar commands.)"
  },
  {
    "question": "How to extract a single file from tar to a different directory? I know that I can use following command to extract a single file to the current working directory (assume I have a tar file named test.tar and a file named testfile1 and testfile2 are inside it):\n\n```\n$tar xvf test.tar testfile1\n\n```\n\nAnd I can use -C option to extract files to another directory:\n\n```\n$tar xvf test.tar -C anotherDirectory/\n\n```\n\nWhen I incorporate the above two techniques together, I suppose that I can extract a single file to another directory. \n\n```\n$ tar xvf test.tar testfile1 -C anotherDirectory/\n\n```\n\nBut the result is I can only extract the testfile1 to the current working directory, rather than the anotherDirectory.\nI want to know how can I extract a single file from tar to a different directory?",
    "answer": "tar xvf test.tar -C anotherDirectory/ testfile1\n\nThe problem is that your arguments are in incorrect order. The single file argument must be last.\nE.g.\n\nshould do the trick.\nPS: You should have asked this question on superuser instead of SO"
  },
  {
    "question": "Deleting files after adding to tar archive Can GNU tar add many files to an archive, deleting each one as it is added?\nThis is useful when there is not enough disk space to hold both the entire tar archive and the original files - and therefore it is not possible to simply manually delete the files after creating an archive in the usual way.",
    "answer": "With GNU tar, use the option --remove-files."
  },
  {
    "question": "How to create tar.gz archive file in Windows? How to create tar.gz archive of my files in Windows to upload and extract in cPanel?",
    "answer": "tar.gz file is just a tar file that's been gzipped. Both tar and gzip are available for windows.\nIf you like GUIs (Graphical user interface), 7zip can pack with both tar and gzip."
  },
  {
    "question": "Native .tar extraction in Powershell I have a .tar.gz file that I need to extract. I've handled the gunzip bit with the GzipStream object from System.IO.Compression, but I couldn't find anything for dealing with tarballs in that namespace. Is there a way to deal with .tar files natively in Powershell? Note that it's only important that I be able to call any such function/method/object construction/system binary from a Powershell script; it doesn't need to actually be written in powershell. (If it matters I'm using 64-bit windows 10)\nP.S. please don't say \"use 7zip\"; that's not native",
    "answer": "So it's been eleven days since I asked this and the general consensus is: \"No, there are no native tools in a vanilla window install that can handle tar extraction 'for you'\".\nThis answer comes from Matthias R. Jensen and TessellatingHeckler, who both declined to answer outside of comments (I suspect due to not wanting to say \"no\" without an intimate knowledge of the entire Windows system architecture, which is fair).\nThere are certainly scripts and classes and programs you can install, but nothing \"native\"."
  },
  {
    "question": "How to extract tar archive from stdin? I have a large tar file I split. Is it possible to cat and untar the file using pipeline.  \nSomething like:\n\n```\ncat largefile.tgz.aa largefile.tgz.ab | tar -xz\n\n```\n\ninstead of:\n\n```\ncat largefile.tgz.aa largfile.tgz.ab > largefile.tgz\ntar -xzf largefile.tgz\n\n```\n\nI have been looking around and I can't find the answer. I wanted to see if it was possible.",
    "answer": "cat largefile.tgz.aa largefile.tgz.ab | tar zxf -\n< largefile.tgz.aa < largefile.tgz.ab tar zxf -\n<largefile.tgz.* | tar zxf -\n\nUse - as the input file:\n\nMake sure you cat them in the same order they were split.\nIf you're using zsh you can use the multios feature and avoid invoking cat:\n\nOr if they are in alphabetical order:"
  },
  {
    "question": "How to extract tar file in Mac terminal As titled. I want to use some command, like for .zip files I can say \n\nunzip myfiles.zip -d mydirectory\n\nBut is there a thing for .tar file on Mac as well?",
    "answer": "tar -xvf myfile.tar\ntar -xzvf myfile.tar.gz\ntar -xvf myfile.tar -C somedirectory\nman tar\n\nYes, you can run:\n\nFor .tar.gz, you can run:\n\nIf you want to extract to any directory other than your cwd, use -C. e.g:\n\nI suggest you read the man page for tar if you wish to do anything further:"
  },
  {
    "question": "How do I extract a tar file in Java? How do I extract a tar (or tar.gz, or tar.bz2) file in Java?",
    "answer": "InputStream is = new GZIPInputStream(new FileInputStream(file));\n\nNote: This functionality was later published through a separate project, Apache Commons Compress, as described in another answer. This answer is out of date.\n\nI haven't used a tar API directly, but tar and bzip2 are implemented in Ant; you could borrow their implementation, or possibly use Ant to do what you need.\nGzip is part of Java SE (and I'm guessing the Ant implementation follows the same model).\nGZIPInputStream is just an InputStream decorator. You can wrap, for example, a FileInputStream in a GZIPInputStream and use it in the same way you'd use any InputStream:\n\n(Note that the GZIPInputStream has its own, internal buffer, so wrapping the FileInputStream in a BufferedInputStream would probably decrease performance.)"
  },
  {
    "question": "Rename Directory Name Before tar Happens I have a directory e.g. /var/tmp/my-dir/ that I frequently compress with the following command:\n\n```\n$ cd /var/tmp/\n$ tar -zcf my-dir.tar.gz my-dir/*\n\n```\n\nLater, when I untar my-dir.tar.gz, it'll create my-dir/ in the current directory. It sounds like the my-dir directory is \"wrapped\" inside the tarball. Is there a tar option to rename my-dir to e.g. your-dir before the actual tarring happens. So that ...\n\n```\n$ tar -zxf my-dir.tar.gz\n# So that ... this creates your-dir/, instead of my-dir/\n\n```\n\nThanks.",
    "answer": "tar -zxf my-dir.tar.gz --transform s/my-dir/your-dir/\n\nWhich tar?\nGNU Tar accepts a --transform argument, to which you give a sed expression to manipulate filenames.\nFor example, to rename during unpacking:\n\nBSD tar and S tar similarly have an -s argument, taking a simple /old/new/ (not a general sed expression)."
  },
  {
    "question": "How do I untar a subdirectory into the current directory? How to I extract a subdirectory in a tarball into the current directory?\nExample, the tarball from wordpress:\n\n```\nwordpress/\nwordpress/wp-trackback.php\nwordpress/wp-config-sample.php\nwordpress/wp-settings.php\nwordpress/wp-rss2.php\nwordpress/readme.html\nwordpress/index.php\n...\n\n```\n\nHow do I extract everything under wordpress/ into the current directory?  In otherwords, it will not create a wordpress directory.\nI've tried this with no luck:\n\n```\ntar xvfz latest.tar.gz wordpress -C ./\n\n```\n\nI know I can extract it normally and move it back, but I figure there has to be a way to do it in one shot.",
    "answer": "mv wordpress/.* .\nmv wordpress/* .\nrmdir wordpress\ntar --strip-components=1 -zxvf wordpress.tgz\n\nWhy don't you untar normally, then just:\n\nBut alas, there's:"
  },
  {
    "question": "How to send a compressed archive that contains executables so that Google&#39;s attachment filter won&#39;t reject it I have a directory that I want to compress to send it by e-mail, I've tried this:\n\n```\ntar -cvf filename.tar.gz directory_to_compress/\n\n```\n\nBut when I try to send it by e-mail, Google says:\n\n```\nfilename.tar.gz contains an executable file. For security reasons, Gmail does not allow you to send this type of file.\n\n```\n\nHow to compress a directory into a tar.gz file from command line?",
    "answer": "tar -cvzf filename.tar.gz directory_to_compress/\ntar -cvzf filename.bla directory_to_compress/\n\nMost tar commands have a z option to create a gziped version.\nThough seems to me the question is how to circumvent Google. I'm not sure if renaming your output file would fool Google, but you could try. I.e.,\n\nand then send the filename.bla - contents will would be a zipped tar, so at the other end it could be retrieved as usual."
  },
  {
    "question": "How to install Go in alpine linux I am trying to install Go inside an Alpine Docker image. For that I downloaded tar file from here inside my alpine docker image, untar it using following command:\n\ntar -C /usr/local -xzf go1.10.3.linux-amd64.tar.gz\n\nexported PATH to have go binary as:\n\nexport PATH=$PATH:/usr/local/go/bin   \n\nHowever, when I say go version  then it says that sh: go: not found. I am quite new to alpine. Does anyone know, what I am missing here?\nSteps to reproduce-\n\n```\n$ docker run -it alpine sh\n$ wget https://dl.google.com/go/go1.10.3.linux-amd64.tar.gz\n$ tar -C /usr/local -xzf go1.10.3.linux-amd64.tar.gz\n$ export PATH=$PATH:/usr/local/go/bin\n$ go version\n\n```",
    "answer": "echo \"installing go version 1.10.3...\"\napk add --no-cache --virtual .build-deps bash gcc musl-dev openssl go\n\nwget -O go.tgz https://dl.google.com/go/go1.10.3.src.tar.gz\ntar -C /usr/local -xzf go.tgz\ncd /usr/local/go/src/\n\n./make.bash\nexport PATH=\"/usr/local/go/bin:$PATH\"\nexport GOPATH=/opt/go/\nexport PATH=$PATH:$GOPATH/bin\napk del .build-deps\ngo version\n\nThanks BMitch.\nI compiled the go source code and performed the below steps inside alpine image container."
  },
  {
    "question": "How to use Pigz with Tar So I am using pigz in tar with\n\n```\ntar --use-compress-program=pigz\n\n```\n\nand this works but it doesn't use all of my processors, and I'd like to make sure it's recursive (-r) and using (-9 compression level).\nI read through Utilizing multi core for tar+gzip/bzip compression/decompression but it doesn't note anywhere to add additional commands in that format, and I couldn't find anything in the man page for either program for additional swithed.\nThanks,\nCam",
    "answer": "tar cf - paths-to-archive | pigz -9 -p 32 > archive.tar.gz\n\nMark Adler's top voted answer on the SO link that you included in your question does provide a solution for specifying compression-level as well as number of processors to use:\n\nSee : https://stackoverflow.com/a/12320421"
  },
  {
    "question": "Python packaging: wheels vs tarball (tar.gz) The advantage of wheels over eggs is clear (see section why not egg? https://pypi.python.org/pypi/wheel). \nHowever, it is not entirely clear to me what is the advantage of using wheels over tar.gz. I might be missing something obvious like \"they are the same\". \nAs I see it both can be installed directly using pip (even in Windows), have similar size and when packaging require a similar effort. \nIt sounds to me like the kind of questions you might get when justifying a packaging methodology.\nEDIT:\nJust found an example where tar.gz might be better than wheels. CherryPy (https://pypi.python.org/pypi/CherryPy) provides wheels for Python 3.x only, so if you want to have a local repository to serve CherryPy for Python 2.7 and 3.x dependencies, it seems to make more sense to store the tarball. Is this correct? (just to add a couple of \"case-based\" justification to the discussion)",
    "answer": "This answered it for me (directly from the wheel PEP):\n\nPython needs a package format that is easier to install than sdist.\nPython's sdist packages are defined by and require the distutils and\nsetuptools build systems, running arbitrary code to build-and-install,\nand re-compile, code just so it can be installed into a new\nvirtualenv. This system of conflating build-install is slow, hard to\nmaintain, and hinders innovation in both build systems and installers.\nWheel attempts to remedy these problems by providing a simpler\ninterface between the build system and the installer. The wheel binary\npackage format frees installers from having to know about the build\nsystem, saves time by amortizing compile time over many installations,\nand removes the need to install a build system in the target\nenvironment.\n\nhttps://www.python.org/dev/peps/pep-0427/#rationale\nNote the tarballs we're speaking of are what are referred to as \"sdists\" above."
  },
  {
    "question": "Tarballing without Git metadata My source tree contains several directories which are using Git source control, and I need to tarball the whole tree excluding any references to the Git metadata or custom log files.\nI thought I'd have a go using a combination of find/egrep/xargs/tar, but somehow the tar file contains the .git directories and the *.log files.\nThis is what I have:\n\n```\nfind -type f . | egrep -v '\\.git|\\.log' | xargs tar rvf ~/app.tar\n\n```\n\nCan someone explain my misunderstanding here? Why is tar processing the files that find and egrep are filtering?\nI'm open to other techniques as well.",
    "answer": "tar cvf ~/app.tar --exclude .git --exclude \"*.log\" .\n\nYou will get a nasty surprise when the number of files increase to more than one xargs command: Then you will first make a tar file of the first files and then overwrite the same tar file with the rest of the files.\nGNU tar has the --exclude option which will solve this issue:"
  },
  {
    "question": "Extract tar the tar.bz2 file error I tried to extract the tar.bz2 file in Fedora 17 OS.  I used the command: \n\n```\n# tar -xvjf myfile.tar.bz2\n\n```\n\nI received this error message:\n\n```\ntar (child):bzip2: Cannot exec :Nosuch of file or directory\ntar (child): Error is not recoverable: exitng now\ntar: Child returned status 2\ntar:Error is not recoverable: exitng now\n\n```\n\nHow can I resolve this?",
    "answer": "tar -jxvf\nbunzip2 myfile.tar.bz2\ntar --help\n\nFor bz2 you need to execute like this,\n\nAlternatively, you can also execute like this\n\nFor more information you should check it,\n\nIf in doubt, run file on the archive to make sure it actually is compressed in bz2 format."
  },
  {
    "question": "Shell &#39;tar: not found in archive&#39; error when using regular expression When I use tar -xzf *.gz to extract all the .gz files in the current directory, I get Not found in archive error. However, it works fine if I extract one by one or use a for-loop like\n\n```\nfor file in `ls *.gz`; do tar -xzf $file; done\n\n```\n\nWhat is the reason for this error?",
    "answer": "tar -xf *.gz\ntar -xf abc.tar.gz def.tar.gz ghi.tar.gz\ntar -xf abc.tar.gz pqr/xyz/important.c\n\nWhen you write:\n\nthe tar command sees (for example):\n\nThis is interpreted as a request to extract def.tar.gz and ghi.tar.gz from the archive abc.tar.gz.  Since the files aren't in there, you get the warning message.\nIn other words, tar operates on a single tar file (possibly compressed) at a time (in one invocation).  It does not operate on multiple tar files.\nNote that if abc.tar.gz contains a file pqr/xyz/important.c, you can extract just the one file by specifying:\n\nThe notation you used is only a variant on this notation.\n(And yes, there can be reasons to tar a tar file.  For example, Gmail does not allow you to ship a tar file or a gzipped tar file which contains a file that is executable.  However, if you embed a gzipped tar file inside a non-compressed tar file, it does not look inside the inner file to find the executable file.  I use this when I need to ship a tar file with an executable configure script.)"
  },
  {
    "question": "How to Compress/Decompress tar.gz files in java Can anyone show me the correct way to compress and decompress tar.gzip files in java i've been searching but the most i can find is either zip or gzip(alone).",
    "answer": "final TarGZipUnArchiver ua = new TarGZipUnArchiver();\n// Logging - as @Akom noted, logging is mandatory in newer versions, so you can use a code like this to configure it:\nConsoleLoggerManager manager = new ConsoleLoggerManager();\nmanager.initialize();\nua.enableLogging(manager.getLoggerForComponent(\"bla\"));\n// -- end of logging part\nua.setSourceFile(sourceFile);\ndestDir.mkdirs();\nua.setDestDirectory(destDir);\nua.extract();\n<dependency>\n<groupId>org.codehaus.plexus</groupId>\n<artifactId>plexus-archiver</artifactId>\n<version>2.2</version>\n</dependency>\n\nMy favorite is plexus-archiver - see sources on GitHub.\nAnother option is Apache commons-compress - (see mvnrepository).\nWith plexus-utils, the code for unarchiving looks like this:\n\nSimilar *Archiver classes are there for archiving.\nWith Maven, you can use this dependency:"
  },
  {
    "question": "How can I read tar.gz file using pandas read_csv with gzip compression option? I have a very simple csv, with the following data, compressed inside the tar.gz file. I need to read that in dataframe using pandas.read_csv.  \n\n```\n   A  B\n0  1  4\n1  2  5\n2  3  6\n\nimport pandas as pd\npd.read_csv(\"sample.tar.gz\",compression='gzip')\n\n```\n\nHowever, I am getting error:\n\n```\nCParserError: Error tokenizing data. C error: Expected 1 fields in line 440, saw 2\n\n```\n\nFollowing are the set of read_csv commands and the different errors I get with them:\n\n```\npd.read_csv(\"sample.tar.gz\",compression='gzip',  engine='python')\nError: line contains NULL byte\n\npd.read_csv(\"sample.tar.gz\",compression='gzip', header=0)\nCParserError: Error tokenizing data. C error: Expected 1 fields in line 440, saw 2\n\npd.read_csv(\"sample.tar.gz\",compression='gzip', header=0, sep=\" \")\nCParserError: Error tokenizing data. C error: Expected 2 fields in line 94, saw 14    \n\npd.read_csv(\"sample.tar.gz\",compression='gzip', header=0, sep=\" \", engine='python')\nError: line contains NULL byte\n\n```\n\nWhat's going wrong here? How can I fix this?",
    "answer": "df = pd.read_csv('sample.tar.gz', compression='gzip', header=0, sep=' ', quotechar='\"', error_bad_lines=False)\n\nNote: error_bad_lines=False will ignore the offending rows."
  },
  {
    "question": "How to create a tar file that omits timestamps for its contents? Is there a way to create a .tar file that omits the values of atime/ctime/mtime for its files/directories?\nWhy do we want to do this?\nWe have a step in our build process that generates a directory of artifacts that gets packaged into a tarfile.  We expect that build step to be idempotent -- given the same inputs, it produces exactly the same files/output each time.\nIdeally, we would like also like the step to be bitwise idempotent across clean builds, so that we can use hashes of successive builds to check that nothing has changed.  But because tar files include timestamps (atime/ctime/mtime) for each entry, the tar files created by that build step are never bitwise identical to the previous run, even though the contents of every file inside the archive are bitwise identical.\nIs there a way to generate a tarfile that omits the timestamps of its entries, so that the step that generates the archive could be bitwise idempotent?  (We want to leverage other file metadata that tar preserves, such as file mode bits and symlinks.)",
    "answer": "tar --sort=name --owner=root:0 --group=root:0 --mtime='UTC 1980-02-01' ... | gzip -n\n\nTo have a truly idempotent tar, mtime is a good step but not enough.\nYou also need to set the sort order, the owner and group (together with their mapping) and a proper timezone for mtime (since otherwise you're gonna have issues as well between Mac and Linux).\nI ended up with"
  },
  {
    "question": "How to compare two tarball&#39;s content I want to tell whether two tarball files contain identical files, in terms of file name and file content, not including meta-data like date, user, group.\nHowever, There are some restrictions:\nfirst, I have no control of whether the meta-data is included when making the tar file, actually, the tar file always contains meta-data, so directly diff the two tar files doesn't work. \nSecond, since some tar files are so large that I cannot afford to untar them in to a temp directory and diff the contained files one by one. (I know if I can untar file1.tar into file1/, I can compare them by invoking 'tar -dvf file2.tar' in file/. But usually I cannot afford untar even one of them)\nAny idea how I can compare the two tar files? It would be better if it can be accomplished within SHELL scripts. Alternatively, is there any way to get each sub-file's checksum without actually untar a tarball?\nThanks,",
    "answer": "tarsum is almost what you need.  Take its output, run it through sort to get the ordering identical on each, and then compare the two with diff.  That should get you a basic implementation going, and it would be easily enough to pull those steps into the main program by modifying the Python code to do the whole job."
  },
  {
    "question": "what does -zxvf mean in tar -zxvf &lt;filename&gt;? I have seen on many websites commands like this what does the \"-zxvf\" or the similar commands stand for?\n\n```\ntar -zxvf <filename>\n tar xvf <filename>\n\n```",
    "answer": "z means (un)z\u0332ip.\nx means ex\u0332tract files from the archive.\nv means print the filenames v\u0332erbosely.\nf means the following argument is a f\u0331ilename.\n\nFor more details, see tar's man page."
  },
  {
    "question": "Opening a .tar.gz file with a single command When I download a .tar.gz file, I open it with two commands, first gunzip and then tar.\nIs it possible to open it with just one command?",
    "answer": "tar xzf file.tar.gz\n\nThe letters are:\n\nx - extract\nz - gunzip the input\nf - Read from a file, not stdin"
  },
  {
    "question": "Extract files contained in archive.tar.gz to new directory named archive I have a directory containing about 800 .tgz archives, each containing about 10 files. Effectively, I want to convert each archive into a directory of the same name. Is there a simple one line command to do this, or should I write a script?",
    "answer": "tar -xvzf archive.tar.gx -C archive_dir\nfor a in *.tar.gz\ndo\na_dir=${a%.tar.gz}\nmkdir --parents $a_dir\ntar -xvzf $a -C $a_dir\ndone\n\nUpdate since GNU tar 1.28:\nuse --one-top-level, see https://www.gnu.org/software/tar/manual/tar.html#index-one_002dtop_002dlevel_002c-summary\nOlder versions need to script this. You can specify the directory that the extract is placed in by using the tar -C option.\nThe script below assumes that the directories do not exist and must be created. If the directories do exist the script will still work - the mkdir will simply fail.\n\ne.g."
  },
  {
    "question": "How to fix &#39;tar: Failed to set default locale&#39; error? I'm trying to install a package into R, something I swore on my blood never to do, yet here I am.\nThe command supposedly goes:\n\n```\ninstall.packages('NCStats',,'http://www.rforge.net/')` \n\n```\n\nwhile I am enjoying the healthy dose of:\n\n```\nWarning: dependencies 'nortest', 'plotrix', 'sciplot', 'car', 'gplots', 'gdata', 'Hmisc', 'TeachingDemos' are not available  \ntrying URL 'http://www.rforge.net/bin/macosx/leopard/contrib/2.11/NCStats_0.1-4.tgz'  \nContent type 'application/x-gzip' length 237120 bytes (231 Kb)  \nopened URL  \n==================================================\"  \ndownloaded 231 Kb  \ntar: Failed to set default locale  \nThe downloaded packages are in\n    /var/folders/Qj/Qjps7xnxFcWdSHsJY3lo+k+++TI/-Tmp-//RtmpzNO8MM/downloaded_packages`\n\n```\n\nLe-sigh. Anybody know how I can tell tar what locale I'm in, not that I understand why it needs that or why it can't just know it already?\nI'm running OSX 10.6.4 and R 2.11.1 GUI 1.34 Leopard build 64-bit (5589).",
    "answer": "system('defaults write org.R-project.R force.LANG en_US.UTF-8')\n\nStep 1 (In R Console)\n\nStep 2: Restart R\nSource: http://cran.r-project.org/bin/macosx/RMacOSX-FAQ.html#Internationalization-of-the-R_002eapp"
  },
  {
    "question": "How to construct a TarFile object in memory from byte buffer in Python 3? Is it possible to create a TarFile object in memory using a buffer containing the tar data without having to write the TarFile to disk and open it up again? We get the bytes sent over a socket.\nSomething like this:\n\n```\nimport tarfile\nbyte_array = client.read_bytes()\ntar = tarfile.open(byte_array) # how to do this?\n# use \"tar\" as a regular TarFile object\nfor member in tar.getmembers():\n    f = tar.extractfile(member)\n    print(f)\n\n```\n\nNote: one of the reasons for doing this is that we eventually want to be able to do this with multiple threads simultaneously, so using a temp file might be overridden if two threads try to do it at the same time.\nThank you for any and all help!",
    "answer": "import tarfile, io\nbyte_array = client.read_bytes()\nfile_like_object = io.BytesIO(byte_array)\ntar = tarfile.open(fileobj=file_like_object)\n\nfor member in tar.getmembers():\nf = tar.extractfile(member)\nprint(f)\n\nBytesIO() from IO module does exactly what you need."
  },
  {
    "question": "TypeError: &#39;str&#39; does not support the buffer interface ```\nplaintext = input(\"Please enter the text you want to compress\")\nfilename = input(\"Please enter the desired filename\")\nwith gzip.open(filename + \".gz\", \"wb\") as outfile:\n    outfile.write(plaintext) \n\n```\n\nThe above python code is giving me following error:\n\n```\nTraceback (most recent call last):\n  File \"C:/Users/Ankur Gupta/Desktop/Python_works/gzip_work1.py\", line 33, in <module>\n    compress_string()\n  File \"C:/Users/Ankur Gupta/Desktop/Python_works/gzip_work1.py\", line 15, in compress_string\n    outfile.write(plaintext)\n  File \"C:\\Python32\\lib\\gzip.py\", line 312, in write\n    self.crc = zlib.crc32(data, self.crc) & 0xffffffff\nTypeError: 'str' does not support the buffer interface\n\n```",
    "answer": "plaintext = input(\"Please enter the text you want to compress\")\nfilename = input(\"Please enter the desired filename\")\nwith gzip.open(filename + \".gz\", \"wb\") as outfile:\noutfile.write(bytes(plaintext, 'UTF-8'))\nplaintext = 'Polish text: \u0105\u0107\u0119\u0142\u0144\u00f3\u015b\u017a\u017c\u0104\u0106\u0118\u0141\u0143\u00d3\u015a\u0179\u017b'\nfilename = 'foo.gz'\nwith gzip.open(filename, 'wb') as outfile:\noutfile.write(bytes(plaintext, 'UTF-8'))\nwith gzip.open(filename, 'r') as infile:\noutfile_content = infile.read().decode('UTF-8')\nprint(outfile_content)\n\nIf you use Python3x then string is not the same type as for Python 2.x, you must cast it to bytes (encode it).\n\nAlso do not use variable names like string or file while those are names of module or function.\nEDIT @Tom\nYes, non-ASCII text is also compressed/decompressed. I use Polish letters with UTF-8 encoding:"
  },
  {
    "question": "How to gzip all files in all sub-directories into one compressed file in bash Possible Duplicate:\ngzipping up a set of directories and creating a tar compressed file \n\nThis post describes how to gzip each file individually within a directory structure. However, I need to do something slightly different. I need to produce one big gzip file for all files under a certain directory. I also need to be able to specify the output filename for the compressed file (e.g., files.gz) and overwrite the old compressed file file if one already exists.",
    "answer": "tar -zcvf compressFileName.tar.gz folderToCompress\n\neverything in folderToCompress will go to compressFileName\nEdit: After review and comments I realized that people may get confused with compressFileName without an extension. If you want you can use .tar.gz extension(as suggested) with the compressFileName"
  },
  {
    "question": "Why use deflate instead of gzip for text files served by Apache? What advantages do either method offer for html, css and javascript files served by a LAMP server. Are there better alternatives?\nThe server provides information to a map application using Json, so a high volume of small files.\nSee also Is there any performance hit involved in choosing gzip over deflate for http compression?",
    "answer": "1\n+---+---+\n|CMF|FLG|   (more-->)\n+---+---+\n1   2   3\n+---+---+---+---+\n|     DICTID    |   (more-->)\n+---+---+---+---+\n\n+=====================+---+---+---+---+\n|...compressed data...|    ADLER32    |\n+=====================+---+---+---+---+\n\nWhy use deflate instead of gzip for text files served by Apache?\n\nThe simple answer is don't.\n\nRFC 2616 defines deflate as:\n\ndeflate The \"zlib\" format defined in RFC 1950 in combination with the \"deflate\" compression mechanism described in RFC 1951\n\nThe zlib format is defined in RFC 1950 as :\n\nSo, a few headers and an ADLER32 checksum\nRFC 2616 defines gzip as:\n\ngzip An encoding format produced by the file compression program\n\"gzip\" (GNU zip) as described in RFC 1952 [25]. This format is a\nLempel-Ziv coding (LZ77) with a 32 bit CRC.\n\nRFC 1952 defines the compressed data as:\n\nThe format presently uses the DEFLATE method of compression but can be easily extended to use other compression methods.\n\nCRC-32 is slower than ADLER32\n\nCompared to a cyclic redundancy check of the same length, it trades reliability for speed (preferring the latter).\n\nSo ... we have 2 compression mechanisms that use the same algorithm for compression, but a different algorithm for headers and checksum.\nNow, the underlying TCP packets are already pretty reliable, so the issue here is not Adler 32 vs CRC-32 that GZIP uses.\n\nTurns out many browsers over the years implemented an incorrect deflate algorithm. Instead of expecting the zlib header in RFC 1950 they simply expected the compressed payload. Similarly various web servers made the same mistake.\nSo, over the years browsers started implementing a fuzzy logic deflate implementation, they try for zlib header and adler checksum, if that fails they try for payload.\nThe result of having complex logic like that is that it is often broken. Verve Studio have a user contributed test section that show how bad the situation is.\nFor example: deflate works in Safari 4.0 but is broken in Safari 5.1, it also always has issues on IE.\n\nSo, best thing to do is avoid deflate altogether, the minor speed boost (due to adler 32) is not worth the risk of broken payloads."
  },
  {
    "question": "How can I tell if my server is serving GZipped content? I have a webapp on a NGinx server. I set gzip on in the conf file and now I'm trying to see if it works. YSlow says it's not, but 5 out of 6 websites that do the test say it is. How can I get a definite answer on this and why is there a difference in the results?",
    "answer": "curl http://example.com/ --silent --write-out \"%{size_download}\\n\" --output /dev/null\ncurl http://example.com/ --silent -H \"Accept-Encoding: gzip,deflate\" --write-out \"%{size_download}\\n\" --output /dev/null\n\nIt looks like one possible answer is, unsurprisingly, curl:\n\nIn the second case the client tells the server that it supports content encoding and you can see that the response was indeed shorter, compressed."
  },
  {
    "question": "How to properly handle a gzipped page when using curl? I wrote a bash script that gets output from a website using curl and does a bunch of string manipulation on the html output. The problem is when I run it against a site that is returning its output gzipped. Going to the site in a browser works fine. \nWhen I run curl by hand, I get gzipped output:\n\n```\n$ curl \"http://example.com\"\n\n```\n\nHere's the header from that particular site:\n\n```\nHTTP/1.1 200 OK\nServer: nginx\nContent-Type: text/html; charset=utf-8\nX-Powered-By: PHP/5.2.17\nLast-Modified: Sat, 03 Dec 2011 00:07:57 GMT\nETag: \"6c38e1154f32dbd9ba211db8ad189b27\"\nExpires: Sun, 19 Nov 1978 05:00:00 GMT\nCache-Control: must-revalidate\nContent-Encoding: gzip\nContent-Length: 7796\nDate: Sat, 03 Dec 2011 00:46:22 GMT\nX-Varnish: 1509870407 1509810501\nAge: 504\nVia: 1.1 varnish\nConnection: keep-alive\nX-Cache-Svr: p2137050.pubip.peer1.net\nX-Cache: HIT\nX-Cache-Hits: 425\n\n```\n\nI know the returned data is gzipped, because this returns html, as expected:\n\n```\n$ curl \"http://example.com\" | gunzip\n\n```\n\nI don't want to pipe the output through gunzip, because the script works as-is on other sites, and piping through gzip would break that functionality.\nWhat I've tried\n\nchanging the user-agent (I tried the same string my browser sends, \"Mozilla/4.0\", etc)\nman curl\ngoogle search\nsearching stackoverflow\n\nEverything came up empty\nAny ideas?",
    "answer": "curl --compressed \"http://example.com\"\ncurl -V\n...\nProtocols: ...\nFeatures: GSS-Negotiate IDN IPv6 Largefile NTLM SSL libz\n\ncurl will automatically decompress the response if you set the --compressed flag:\n\n--compressed\n(HTTP)  Request  a compressed response using one of the algorithms libcurl supports, and save the uncompressed document.  If this option is used and the server sends an unsupported encoding, curl will report an error.\n\ngzip is most likely supported, but you can check this by running curl -V and looking for libz somewhere in the \"Features\" line:\n\nNote that it's really the website in question that is at fault here. If curl did not pass an Accept-Encoding: gzip request header, the server should not have sent a compressed response."
  },
  {
    "question": "Serving gzipped CSS and JavaScript from Amazon CloudFront via S3 I've been looking for ways of making my site load faster and one way that I'd like to explore is making greater use of Cloudfront.\nBecause Cloudfront was originally not designed as a custom-origin CDN and because it didn't support gzipping, I have so far been using it to host all my images, which are referenced by their Cloudfront cname in my site code, and optimized with far-futures headers.\nCSS and javascript files, on the other hand, are hosted on my own server, because until now I was under the impression that they couldn't be served gzipped from Cloudfront, and that the gain from gzipping (about 75 per cent) outweighs that from using a CDN (about 50 per cent): Amazon S3 (and thus Cloudfront) did not support serving gzipped content in a standard manner by using the HTTP Accept-Encoding header that is sent by browsers to indicate their support for gzip compression, and so they were not able to Gzip and serve components on the fly.\nThus I was under the impression, until now, that one had to choose between two alternatives:\n\nmove all assets to the Amazon CloudFront and forget about GZipping;\nkeep components self-hosted and configure our server to detect incoming requests and perform on-the-fly GZipping as appropriate, which is what I chose to do so far.\n\nThere were workarounds to solve this issue, but essentially these didn't work. [link].\nNow, it seems Amazon Cloudfront supports custom origin, and that it is now possible to use the standard HTTP Accept-Encoding method for serving gzipped content if you are using a Custom Origin  [link].\nI haven't so far been able to implement the new feature on my server.  The blog post I linked to above, which is the only one I found detailing the change, seems to imply that you can only enable gzipping (bar workarounds, which I don't want to use), if you opt for custom origin, which I'd rather not: I find it simpler to host the coresponding fileds on my Cloudfront server, and link to them from there. Despite carefully reading the documentation, I don't know:\n\nwhether the new feature means the files should be hosted on my own domain server via custom origin, and if so, what code setup will achieve this;\nhow to configure the css and javascript headers to make sure they are served gzipped from Cloudfront.",
    "answer": "gzip -9 production.min.css\n\nUPDATE:  Amazon now supports gzip compression, so this is no longer needed.  Amazon Announcement\nOriginal answer:\nThe answer is to gzip the CSS and JavaScript files. Yes, you read that right.\n\nThis will produce production.min.css.gz. Remove the .gz, upload to S3 (or whatever origin server you're using) and explicitly set the Content-Encoding header for the file to gzip.\nIt's not on-the-fly gzipping, but you could very easily wrap it up into your build/deployment scripts. The advantages are:\n\nIt requires no CPU for Apache to gzip the content when the file is requested.\nThe files are gzipped at the highest compression level (assuming gzip -9).\nYou're serving the file from a CDN.\n\nAssuming that your CSS/JavaScript files are (a) minified and (b) large enough to justify the CPU required to decompress on the user's machine, you can get significant performance gains here.\nJust remember: If you make a change to a file that is cached in CloudFront, make sure you invalidate the cache after making this type of change."
  },
  {
    "question": "Gzip versus minify I had a somewhat lively discussion the other day about minifying Javascript and CSS versus someone who prefers using Gzip.\nI'll call this person X.\nX said that Gzip allready minifies the code, since it zips your files.\nI disagree. Zip is a lossless method of shrinking filesize. Lossless means the original must be restored perfectly, meaning info must be stored to be able to restore the spaces, the un-needed characters, commented code and everything else. That takes up more space, since more must be compressed.\nI have no method of testing, but I believe that the Gzip of this code:\n\n```\n.a1 {\n    background-color:#FFFFFF;\n    padding: 40px 40px 40px 40px;\n}\n\n```\n\nWill still be bigger than the Gzip of this code:\n\n```\n.a1{body:background-color:#FFF;padding:40px}\n\n```\n\nIs there anybody who can prove this right or wrong.And please don't come saying \"It's right because that's what I've always used\".\nI am asking for scientific proof here.",
    "answer": "-rwx------  1 xxxxxxxx mkgroup-l-d     88 Apr 30 09:17 expanded.js.gz\n\n-rwx------  1 xxxxxxxx mkgroup-l-d     81 Apr 30 09:18 minified.js.gz\n-rwxrwxrwx 1 xxxxxxxx mkgroup-l-d 73134 Apr 13 11:41 common.js\n-rwxr-xr-x 1 xxxxxxxx mkgroup-l-d 26232 Apr 30 10:39 common-min.js\n-rwxrwxrwx 1 xxxxxxxx mkgroup-l-d 12402 Apr 13 11:41 common.js.gz\n-rwxr-xr-x 1 xxxxxxxx mkgroup-l-d  5608 Apr 30 10:39 common-min.js.gz\n\nVery simple to test.  I took your js, put them in different files and ran gzip -9 on them.  Here's the result.  This was done on a WinXP machine running Cygwin and gzip 1.3.12.\n\nHere's a further test using a real-world JS example.  The source file is \"common.js\"  The original file size is 73134 bytes.  Minified, it came to 26232 bytes.\nOriginal file:\n\nMinified file:\n\nOriginal file gzipped with -9 option (same version as above):\n\nMinified file gzipped with -9 option (same version as above):\n\nAs you can see, there is a definite difference between the various methods.  The best bet is to both minify as well as gzip them."
  },
  {
    "question": "Decompressing GZip Stream from HTTPClient Response I am trying to connect to an api, that returns GZip encoded JSON, from a WCF service (WCF service to WCF service). I am using the HTTPClient to connect to the API and have been able to return the JSON object as a string. However I need to be able to store this returned data in a database and as such I figured the best way would be to return and store the JSON object in an array or byte or something along those lines. \nWhat I am having trouble with specifically is the decompressing of the GZip encoding and have been trying lots of different example but still cant get it. \nThe below code is how I am establishing my connection and getting a response, this is the code that returns a string from the API.\n\n```\npublic string getData(string foo)\n{\n    string url = \"\";\n    HttpClient client = new HttpClient();\n    HttpResponseMessage response;\n    string responseJsonContent;\n    try\n    {\n        client.DefaultRequestHeaders.Accept.Add(new MediaTypeWithQualityHeaderValue(\"application/json\"));\n        response = client.GetAsync(url + foo).Result;\n        responseJsonContent = response.Content.ReadAsStringAsync().Result;\n        return responseJsonContent;\n    }\n    catch (Exception ex)\n    {\n        System.Windows.Forms.MessageBox.Show(ex.Message);\n        return \"\";\n    }\n}\n\n```\n\nI have been following a few different examples like these StackExchange API, MSDN, and a couple on stackoverflow, but I haven't been able to get any of these to work for me. \nWhat is the best way to accomplish this, am I even on the right track? \nThanks guys.",
    "answer": "HttpClientHandler handler = new HttpClientHandler()\n{\nAutomaticDecompression = DecompressionMethods.GZip | DecompressionMethods.Deflate\n};\n\nusing (var client = new HttpClient(handler)) //see update below\n{\n// your code\n}\nprivate static HttpClient client = null;\n\nContructorMethod()\n{\nif(client == null)\n{\nHttpClientHandler handler = new HttpClientHandler()\n{\nAutomaticDecompression = DecompressionMethods.GZip | DecompressionMethods.Deflate\n};\nclient = new HttpClient(handler);\n}\n// your code\n}\nvar timeout = Policy.TimeoutAsync<HttpResponseMessage>(\nTimeSpan.FromSeconds(60));\n\nservices.AddHttpClient<XApiClient>().ConfigurePrimaryHttpMessageHandler(() => new HttpClientHandler\n{\nAutomaticDecompression = DecompressionMethods.GZip | DecompressionMethods.Deflate\n}).AddPolicyHandler(request => timeout);\n\nJust instantiate HttpClient like this:\n\nUpdate June 19, 2020:\nIt's not recommended to use  httpclient in a 'using' block as it might cause port exhaustion.\n\nIf using .Net Core 2.1+, consider using IHttpClientFactory and injecting like this in your startup code."
  },
  {
    "question": "Writing then reading in-memory bytes (BytesIO) gives a blank result I wanted to try out the python BytesIO class.\nAs an experiment I tried writing to a zip file in memory, and then reading the bytes back out of that zip file. So instead of passing in a file-object to gzip, I pass in a BytesIO object. Here is the entire script:\n\n```\nfrom io import BytesIO\nimport gzip\n\n# write bytes to zip file in memory\nmyio = BytesIO()\nwith gzip.GzipFile(fileobj=myio, mode='wb') as g:\n    g.write(b\"does it work\")\n\n# read bytes from zip file in memory\nwith gzip.GzipFile(fileobj=myio, mode='rb') as g:\n    result = g.read()\n\nprint(result)\n\n```\n\nBut it is returning an empty bytes object for result. This happens in both Python 2.7 and 3.4. What am I missing?",
    "answer": "myio.seek(0)\n\nYou need to seek back to the beginning of the file after writing the initial in memory file..."
  },
  {
    "question": "How can I decompress a gzip stream with zlib? Gzip format files (created with the gzip program, for example) use the \"deflate\" compression algorithm, which is the same compression algorithm as what zlib uses. However, when using zlib to inflate a gzip compressed file, the library returns a Z_DATA_ERROR.\nHow can I use zlib to decompress a gzip file?",
    "answer": "inflateInit2(&stream, 16+MAX_WBITS);\n\nTo decompress a gzip format file with zlib, call inflateInit2 with the windowBits parameter as 16+MAX_WBITS, like this:\n\nIf you don't do this, zlib will complain about a bad stream format. By default, zlib creates streams with a zlib header, and on inflate does not recognise the different gzip header unless you tell it so. Although this is documented starting in version 1.2.1 of the zlib.h header file, it is not in the zlib manual. From the header file:\n\nwindowBits can also be greater than 15 for optional gzip decoding. Add\n32 to windowBits to enable zlib and gzip decoding with automatic header\ndetection, or add 16 to decode only the gzip format (the zlib format will\nreturn a Z_DATA_ERROR).  If a gzip stream is being decoded, strm->adler is\na crc32 instead of an adler32."
  },
  {
    "question": "uncompress a .txt.gz file in mac? How do I uncompress a .txt.gz file in mac?\nI already tried unzip file.txt.gz but it says it cannot find zipfile directory in one of file.txt.gz or file.txt.gz.zip, and cannot find file.txt.gz.ZIP, period.",
    "answer": "gunzip file.txt.gz in Terminal."
  },
  {
    "question": "Transfer-Encoding: gzip vs. Content-Encoding: gzip What is the current state of affairs when it comes to whether to do\n\n```\nTransfer-Encoding: gzip\n\n```\n\nor a\n\n```\nContent-Encoding: gzip\n\n```\n\nwhen I want to allow clients with e.g. limited bandwidth to signal their willingness to accept a compressed response and the server have the final say whether or not to compress.\nThe latter is what e.g. Apache's mod_deflate and IIS do, if you let it take care of compression. Depending on the size of the content to be compressed, it will do the additional Transfer-Encoding: chunked.\nIt will also include a Vary: Accept-Encoding, which already hints at the problem. Content-Encoding seems to be part of the entity, so changing the Content-Encoding amounts to a change of the entity, i.e. a different Accept-Encoding header means e.g. a cache cannot use its cached version of the otherwise identical entity.\nIs there a definite answer on this that I have missed (and that's not buried inside a message in a long thread in some apache newsgroup)?\nMy current impression is:\n\nTransfer-Encoding would in fact be the right way to do what is mostly done with Content-Encoding by existing server and client implentations\nContent-Encoding, because of its semantic implications, carries a couple of issues (what should the server do to the ETag when it transparently compresses a response?)\nThe reason is chicken'n'egg: Browsers don't support it because servers don't because browsers don't\n\nSo I am assuming the right way would be a Transfer-Encoding: gzip (or, if I additionally chunk the body, it would become Transfer-Encoding: gzip, chunked). And no reason to touch Vary or ETag or any other header in that case as it's a transport-level thing.\nFor now I don't care too much about the 'hop-by-hop'-ness of Transfer-Encoding, something that others seem to be concerned about first and foremost, because proxies might uncompress and forward uncompressed to the client. However, proxies might just as well forward it as-is (compressed), if the original request has the proper Accept-Encoding header, which in case of all browsers that I know is a given.\nBtw, this issue is at least a decade old, see e.g.\nhttps://bugzilla.mozilla.org/show_bug.cgi?id=68517 .\nAny clarification on this will be appreciated. Both in terms of what is considered standards-compliant and what is considered practical. For example, HTTP client libraries only supporting transparent \"Content-Encoding\" would be an argument against practicality.",
    "answer": "Quoting Roy T. Fielding, one of the authors of RFC 2616:\n\nchanging content-encoding on the fly in an inconsistent manner\n(neither \"never\" nor \"always) makes it impossible for later requests\nregarding that content (e.g., PUT or conditional GET) to be handled\ncorrectly.  This is, of course, why performing on-the-fly\ncontent-encoding is a stupid idea, and why I added Transfer-Encoding\nto HTTP as the proper way to do on-the-fly encoding without changing\nthe resource.\n\nSource: https://issues.apache.org/bugzilla/show_bug.cgi?id=39727#c31\nIn other words: Don't do on-the-fly Content-Encoding, use Transfer-Encoding instead!\nEdit: That is, unless you want to serve gzipped content to clients that only understand Content-Encoding. Which, unfortunately, seems to be most of them. But be aware that you leave the realms of the spec and might run into issues such as the one mentioned by Fielding as well as others, e.g. when caching proxies are involved."
  },
  {
    "question": "Read a gzip file in Python When I try to run this code;\n\n```\nimport gzip\nf=gzip.open('Onlyfinnaly.log.gz','rb')\nfile_content=f.read()\nprint file_content\n\n```\n\nI get no output on the screen. How to read a gzip file properly?",
    "answer": "import gzip\ncontent = \"Lots of content here\"\nf = gzip.open('Onlyfinnaly.log.gz', 'wb')\nf.write(content)\nf.close()\nimport gzip\nf=gzip.open('Onlyfinnaly.log.gz','rb')\nfile_content=f.read()\nprint file_content\n\nTry gzipping some data through the gzip libary like this...\n\n... then run your code as posted ...\n\nThis method worked for me as for some reason the gzip library fails to read some files."
  },
  {
    "question": "How to get few lines from a .gz compressed file without uncompressing How to get the first few lines from a gziped file ?\nI tried zcat, but its throwing an error\n\n```\nzcat CONN.20111109.0057.gz|head\nCONN.20111109.0057.gz.Z: A file or directory in the path name does not exist.\n\n```",
    "answer": "gzip -cd CONN.20111109.0057.gz | head\n-c --stdout --to-stdout\nWrite output on standard output; keep original files unchanged.  If there are several input files, the output consists of a sequence of independently compressed members. To obtain better compression, concatenate all input files before compressing\nthem.\n\n-d --decompress --uncompress\nDecompress.\n\nzcat(1) can be supplied by either compress(1) or by gzip(1). On your system, it appears to be compress(1) -- it is looking for a file with a .Z extension.\nSwitch to gzip -cd in place of zcat and your command should work fine:\n\nExplanation"
  },
  {
    "question": "Android: HTTP communication should use &quot;Accept-Encoding: gzip&quot; I've a HTTP communication to a webserver requesting JSON data. I'd like compress this data stream with Content-Encoding: gzip. Is there a way I can set Accept-Encoding: gzip in my HttpClient? The search for gzip in the Android References doesn't show up anything related to HTTP, as you can see here.",
    "answer": "HttpUriRequest request = new HttpGet(url);\nrequest.addHeader(\"Accept-Encoding\", \"gzip\");\n// ...\nhttpClient.execute(request);\nInputStream instream = response.getEntity().getContent();\nHeader contentEncoding = response.getFirstHeader(\"Content-Encoding\");\nif (contentEncoding != null && contentEncoding.getValue().equalsIgnoreCase(\"gzip\")) {\ninstream = new GZIPInputStream(instream);\n}\n\nYou should use http headers to indicate a connection can accept gzip encoded data, e.g:\n\nCheck response for content encoding:"
  },
  {
    "question": "Read lines from compressed text files Is it possible to read a line from a gzip-compressed text file using Python without extracting the file completely? I have a text.gz file which is around 200 MB. When I extract it, it becomes 7.4 GB. And this is not the only file I have to read. For the total process, I have to read 10 files. Although this will be a sequential job, I think it will a smart thing to do it without extracting the whole information. How can this be done using Python? I need to read the text file line-by-line.",
    "answer": "Have you tried using gzip.GzipFile?  Arguments are similar to open."
  },
  {
    "question": "Is there any way to get curl to decompress a response without sending the Accept headers in the request? Is there any way to get curl to decompress a response without sending the Accept-encoding headers in the request?\nI'm trying to debug an issue where the order of the Accept-encoding headers may be relevant, but I also need to know what the response is.  If I just send -H 'Accept-encoding: gzip and the server gzips the response, curl won't decompress it.",
    "answer": "curl -sH 'Accept-encoding: gzip' http://example.com/ | gunzip -\n\nProbably the easiest thing to do is just use gunzip to do it:\n\nOr there's also --compressed, which curl will decompress (I believe) since it knows the response is compressed. But, not sure if that meets your needs."
  },
  {
    "question": "How can I estimate the size of my gzipped script? How can I estimate the size of my JavaScript file after it is gzipped? Are there online tools for this? Or is it similar to using winzip for example?",
    "answer": "Original Size:    90 bytes (100 bytes gzipped)\nCompiled Size:    55 bytes (68 bytes gzipped)\nSaved 38.89% off the original size (32.00% off the gzipped size)\n\nhttp://closure-compiler.appspot.com/home lets you paste in code, and it will give you compression ratios for a particular file before and after GZIP.\n\nYou can use the pretty-print and white-space only options to estimate the compression of non-minified content.\nIf you need an estimate:\n\nStart with 100 JS files that have gone through the same minification pipeline.\nFor each file, compute the ratio in sizes between gzip -c \"$f\" | wc -c and wc -c \"$f\"\nThe average of those ratios is an approximation of the compression you should expect for a similar JS file.\n\nCygwin contains command line implementations of gzip and wc for Windows."
  },
  {
    "question": "Deflate compression browser compatibility and advantages over GZIP UPDATE Feb 10 2012:\nzOompf has completed some very thorough research on this very topic here. It trumps any findings below.\n\n\nUPDATE Sept 11 2010:\nA testing platform has been created for this here\n\n\n\nHTTP 1.1 definitions of GZIP and DEFLATE (zlib) for some background information:\n\" 'Gzip' is the gzip format, and 'deflate' is the zlib format. They\nshould probably have called the second one 'zlib' instead to avoid\nconfusion with the raw deflate compressed data format. While the HTTP\n1.1 RFC 2616 correctly points to the zlib specification in RFC 1950\nfor the 'deflate' transfer encoding, there have been reports of\nservers and browsers that incorrectly produce or expect raw deflate\ndata per the deflate specification in RFC 1951, most notably\nMicrosoft products. So even though the 'deflate' transfer encoding using the\nzlib format would be the more efficient approach (and in fact exactly\nwhat the zlib format was designed for), using the 'gzip' transfer\nencoding is probably more reliable due to an unfortunate choice of\nname on the part of the HTTP 1.1 authors.\" (source: http://www.gzip.org/zlib/zlib_faq.html)\nSo, my question: if I send RAW deflate data with NO zlib wrapper (or gzip,\nfor that matter) are there any modern browsers (e.g., IE6 and up, FF,\nChrome, Safari, etc) that can NOT understand the raw deflate\ncompressed data (assuming HTTP request header \"Accept-Encoding\" contains \"deflate\")?\nDeflate data will ALWAYS be a few bytes smaller than GZIP.\nIf all these browsers can successfully decode the data, what\ndownsides are there to sending RAW deflate instead of zlib?\n\nUPDATE Sept 11 2010:\nA testing platform has been created for this here",
    "answer": "/*  Browser                       DEFLATE      ZLIB     */\nXP Internet Explorer 6        PASS         FAIL\nXP Internet Explorer 7        PASS         FAIL\nXP Internet Explorer 8        PASS         FAIL\nVista Internet Explorer 8     PASS         FAIL\nXP Firefox 3.6.*              PASS         PASS\nXP Firefox 3.5.3              PASS         PASS\nXP Firefox 3.0.14             PASS         PASS\nWin 7 Firefox 3.6.*           PASS         PASS\nVista Firefox 3.6.*           PASS         PASS\nVista Firefox 3.5.3           PASS         PASS\nXP Safari 3                   PASS         PASS\nXP Safari 4                   PASS         PASS\nXP Chrome 3.0.195.27          PASS         PASS\nXP Opera 9                    PASS         PASS\nXP Opera 10                   PASS         PASS\nXP Sea Monkey 1.1.8           PASS         PASS\nAndroid 1.6 Browser (v4)*     N/A          N/A\nOS-X Safari 4                 PASS         PASS\nOS X Chrome 7.0.517.44        PASS         PASS\nOS X Opera 10.63              PASS         PASS\niPhone 3.1 Safari             PASS         PASS\n\nUPDATE: Browsers have been dropping support for raw deflate. zOompf has completed some very thorough research on this very topic here. Unfortunately, it appears that raw deflate is NOT safe to use.\n\nCheck http://www.vervestudios.co/projects/compression-tests/results for more results.\n\nHere are the browsers that have been tested:\n\n* Android Sends HTTP request header \"Accept-Encoding: gzip\". Deflate is not permitted.\n\nI conclude that we can always send raw DEFLATE (when the HTTP request header \"Accept-Encoding\" contains \"deflate\") and the browser will be able to correctly interpret the encoded data. Can someone prove this wrong?\nnote: .NET's native implementation of DEFLATE (System.IO.Compression.DeflateStream) is raw DEFLATE. It also sucks. Please use zlib.net for all of your .NET deflating needs."
  },
  {
    "question": "htaccess - How to force the client&#39;s browser to clear the cache? For my site I have the following htaccess rules:\n\n```\n# BEGIN Gzip\n<IfModule mod_deflate.c>\nAddOutputFilterByType DEFLATE text/text text/html text/plain text/xml text/css application/x-javascript application/javascript\n</IfModule>\n# END Gzip\n\n# BEGIN EXPIRES\n<IfModule mod_expires.c>\n    ExpiresActive On\n    ExpiresDefault \"access plus 10 days\"\n    ExpiresByType text/css \"access plus 1 month\"\n    ExpiresByType text/plain \"access plus 1 month\"\n    ExpiresByType image/gif \"access plus 1 month\"\n    ExpiresByType image/png \"access plus 1 month\"\n    ExpiresByType image/jpeg \"access plus 1 month\"\n    ExpiresByType application/x-javascript \"access plus 1 month\"\n    ExpiresByType application/javascript \"access plus 1 month\"\n    ExpiresByType application/x-icon \"access plus 1 year\"\n</IfModule>\n# END EXPIRES\n\n```\n\nI've just updated my site and it looked all screwy until I cleared my cache. How can I force the client's browser to clear the cache after an update so that the user can see the changes?",
    "answer": "You can force browsers to cache something, but\nYou can't force browsers to clear their cache.\nThus the only (AMAIK) way is to use a new URL for your resources. Something like versioning."
  },
  {
    "question": "How to implement GZip compression in ASP.NET? I am trying to implement GZip compression for my asp.net page (including my CSS and JS files). I tried the following code, but it only compresses my .aspx page (found it from YSlow)\n\n```\nHttpContext context = HttpContext.Current;\ncontext.Response.Filter = new GZipStream(context.Response.Filter, CompressionMode.Compress);\nHttpContext.Current.Response.AppendHeader(\"Content-encoding\", \"gzip\");\nHttpContext.Current.Response.Cache.VaryByHeaders[\"Accept-encoding\"] = true;\n\n```\n\nThe above code is only compressing my .aspx page code (markup) not the CSS and JS files which is included as external files. \nPlease tell me how can I implement GZip compression in ASP.NET using code (because I am on shared hosting server where I don't have access to IIS Server configurations). And also in the above code I am not getting the last two lines, why they are used and what's the purpose of these lines. Please explain!",
    "answer": "For compressing JS & CSS files you actually have to handle that at the IIS level, since these files are rendered directly without the ASP.NET runtime.\nYou could make a JSX & CSSX extension mapping in IIS to the aspnet_isapi.dll and then take advantage of your zip code, but IIS is likely to do a better job of this for you.\nThe content-encoding header tells the browser that it needs to unzip the content before rendering.  Some browsers are smart enough to figure this out anyway, based on the shape of the content, but it's better to just tell it.\nThe Accept-encoding cache setting is there so that a cached version of the gzipped content won't be sent to a browser that requested only text/html."
  },
  {
    "question": "Decompress gz file using R I have used ?unzip in the past to get at contents of a zipped file using R.  This time around, I am having a hard time extracting the files from a .gz file which can be found here.\nI have tried ?gzfile and ?gzcon but have not been able to get it to work.  Any help you can provide will be greatly appreciated.",
    "answer": "untar('chadwick-0.5.3.tar.gz')\n\nIf you really want to uncompress the file, just use the untar function which does support gzip.\nE.g.:"
  },
  {
    "question": "Which compression method to use in PHP? I have a large amount of data to move using two PHP scripts: one on the client side using a command line PHP script and other behind Apache. I POST the data to the server side and use php://input stream to save it on the web-server end. To prevent from reaching any memory limits, data is separated into 500kB chunks for each POST request. All this works fine. \nNow, to save the bandwidth and speed it up, I want to compress the data before sending and decompress when received on the other end. I found 3 pairs of functions that can do the job, but I cannot decide which one to use:\n\ngzencode / gzdecode\ngzdeflate / gzinflate\ngzcompress / gzuncompress\n\nWhich pair of functions would you recommend and why?\nUPDATE: I just read zlib FAQ:\nThe gzip format (gzencode) was designed to retain the directory information about a single file, such as the name and last modification date. The zlib format (gzcompress) on the other hand was designed for in-memory and communication channel applications, and has a much more compact header and trailer and uses a faster integrity check than gzip.",
    "answer": "All of these can be used.  There are subtle differences between the three:\n\ngzencode() uses the GZIP file format, the same as the gzip command line tool.  This file format has a header containing optional metadata, DEFLATE compressed data, and footer containing a CRC32 checksum and length check.\ngzcompress() uses the ZLIB format.  It has a shorter header serving only to identify the compression format, DEFLATE compressed data, and a footer containing an ADLER32 checksum.\ngzdeflate() uses the raw DEFLATE algorithm on its own, which is the basis for both of the other formats.\n\nAll three use the same algorithm under the hood, so they won't differ in speed or efficiency.  gzencode() and gzcompress() both add a checksum, so the integrity of the archive can be verified, which can be useful over unreliable transmission and storage methods.  If everything is stored locally and you don't need any additional metadata then gzdeflate() would suffice.  For portability I'd recommend gzencode() (GZIP format) which is probably better supported than gzcompress() (ZLIB format) among other tools.\nWhen compressing very short strings the overhead of each method becomes relevant since for very short input the overhead can comprise a significant part of the output. The overhead for each method, measured by compressing an empty string, is:\n\ngzencode('') - 20 bytes\ngzcompress('') - 8 bytes\ngzdeflate('') - 2 bytes"
  },
  {
    "question": "Python 3, read/write compressed json objects from/to gzip file For Python3, I followed @Martijn Pieters's code with this:\n\n```\nimport gzip\nimport json\n\n# writing\nwith gzip.GzipFile(jsonfilename, 'w') as fout:\n    for i in range(N):\n        uid = \"whatever%i\" % i\n        dv = [1, 2, 3]\n        data = json.dumps({\n            'what': uid,\n            'where': dv})\n\n        fout.write(data + '\\n')\n\n```\n\nbut this results in an error:\n\n```\nTraceback (most recent call last):\n    ...\n  File \"C:\\Users\\Think\\my_json.py\", line 118, in write_json\n    fout.write(data + '\\n')\n  File \"C:\\Users\\Think\\Anaconda3\\lib\\gzip.py\", line 258, in write\n    data = memoryview(data)\nTypeError: memoryview: a bytes-like object is required, not 'str'\n\n```\n\nAny thoughts about what is going on?",
    "answer": "import gzip\nimport json\n\ndata = []\nfor i in range(N):\nuid = \"whatever%i\" % i\ndv = [1, 2, 3]\ndata.append({\n'what': uid,\n'where': dv\n})                                           # 1. data\n\njson_str = json.dumps(data) + \"\\n\"               # 2. string (i.e. JSON)\njson_bytes = json_str.encode('utf-8')            # 3. bytes (i.e. UTF-8)\n\nwith gzip.open(jsonfilename, 'w') as fout:       # 4. fewer bytes (i.e. gzip)\nfout.write(json_bytes)\nwith gzip.open(jsonfilename, 'r') as fin:        # 4. gzip\njson_bytes = fin.read()                      # 3. bytes (i.e. UTF-8)\n\njson_str = json_bytes.decode('utf-8')            # 2. string (i.e. JSON)\ndata = json.loads(json_str)                      # 1. data\n\nprint(data)\nwith gzip.open(jsonfilename, 'w') as fout:\nfout.write(json.dumps(data).encode('utf-8'))\nwith gzip.open(jsonfilename, 'r') as fin:\ndata = json.loads(fin.read().decode('utf-8'))\n\nYou have four steps of transformation here.\n\na Python data structure (nested dicts, lists, strings, numbers, booleans)\na Python string containing a serialized representation of that data structure (\"JSON\")\na list of bytes containing a representation of that string (\"UTF-8\")\na list of bytes containing a - shorter - representation of that previous byte list (\"gzip\")\n\nSo let's take these steps one by one.\n\nNote that adding \"\\n\" is completely superfluous here. It does not break anything, but beyond that it has no use. I've added that only because you have it in your code sample.\nReading works exactly the other way around:\n\nOf course the steps can be combined:\n\nand"
  },
  {
    "question": "zlib.error: Error -3 while decompressing: incorrect header check I have a gzip file and I am trying to read it via Python as below:\n\n```\nimport zlib\n\ndo = zlib.decompressobj(16+zlib.MAX_WBITS)\nfh = open('abc.gz', 'rb')\ncdata = fh.read()\nfh.close()\ndata = do.decompress(cdata)\n\n```\n\nit throws this error:\n\n```\nzlib.error: Error -3 while decompressing: incorrect header check\n\n```\n\nHow can I overcome it?",
    "answer": "import gzip\nf = gzip.open('/home/joe/file.txt.gz', 'rb')\nfile_content = f.read()\nf.close()\n\nUpdate: dnozay's answer explains the problem and should be the accepted answer.\n\nTry the gzip module, code below is straight from the python docs."
  },
  {
    "question": "Does python urllib2 automatically uncompress gzip data fetched from webpage? I'm using \n\n```\n data=urllib2.urlopen(url).read()\n\n```\n\nI want to know:\n\nHow can I tell if the data at a URL is gzipped?\nDoes urllib2 automatically uncompress the data if it is gzipped? Will the data always be a string?",
    "answer": "from StringIO import StringIO\nimport gzip\n\nrequest = urllib2.Request('http://example.com/')\nrequest.add_header('Accept-encoding', 'gzip')\nresponse = urllib2.urlopen(request)\nif response.info().get('Content-Encoding') == 'gzip':\nbuf = StringIO(response.read())\nf = gzip.GzipFile(fileobj=buf)\ndata = f.read()\n\nHow can I tell if the data at a URL is gzipped?\n\nThis checks if the content is gzipped and decompresses it:\n\nDoes urllib2 automatically uncompress the data if it is gzipped? Will the data always be a string?\n\nNo. The urllib2 doesn't automatically uncompress the data because the 'Accept-Encoding' header is not set by the urllib2 but by you using: request.add_header('Accept-Encoding','gzip, deflate')"
  },
  {
    "question": "Compression formats with good support for random access within archives? This is similar to a previous question, but the answers there don't satisfy my needs and my question is slightly different:\nI currently use gzip compression for some very large files which contain sorted data.  When the files are not compressed, binary search is a handy and efficient way to support seeking to a location in the sorted data.\nBut when the files are compressed, things get tricky.  I recently found out about zlib's Z_FULL_FLUSH option, which can be used during compression to insert \"sync points\" in the compressed output (inflateSync() can then begin reading from various points in the file).  This is OK, though files I already have would have to be recompressed to add this feature (and strangely gzip doesn't have an option for this, but I'm willing to write my own compression program if I must).\nIt seems from one source that even Z_FULL_FLUSH is not a perfect solution...not only is it not supported by all gzip archives, but the very idea of detecting sync points in archives may produce false positives (either by coincidence with the magic number for sync points, or due to the fact that Z_SYNC_FLUSH also produces sync points but they are not usable for random access).\nIs there a better solution?  I'd like to avoid having auxiliary files for indexing if possible, and explicit, default support for quasi-random access would be helpful (even if it's large-grained--like being able to start reading at each 10 MB interval).  Is there another compression format with better support for random reads than gzip?\nEdit: As I mentioned, I wish to do binary search in the compressed data.  I don't need to seek to a specific (uncompressed) position--only to seek with some coarse granularity within the compressed file. I just want support for something like \"Decompress the data starting roughly 50% (25%, 12.5%, etc.) of the way into this compressed file.\"",
    "answer": "I don't know of any compressed file format which would support random access to a specific location in the uncompressed data (well, except for multimedia formats), but you can brew your own.\nFor example, bzip2 compressed files are composed of independent compressed blocks of size <1MB uncompressed, which are delimited by sequences of magic bytes, so you could parse the bzip2 file, get the block boundaries and then just uncompress the right block. This would need some indexing to remember where do the blocks start.\nStill, I think the best solution would be to split your file into chunks of your choice, and then compressing it with some archiver, like zip or rar, which support random access to individual files in the archive."
  },
  {
    "question": "What &#39;Content-Type&#39; header to use when serving gzipped files? I'm serving gzipped copies of my css / javascript files. As per a tutorial, I set the content-type as application/gzip when serving these files. However, chrome doesn't seem to un-gzip these files, and for the javascript files I get a lot of 'illegal character' errors. If I view source, I still see the files as compressed, rather than uncompressed.\nMy question is, what should I set as the content type for these files, in order for the browser to interpret them correctly as gzipped css / js files, and un-gzip them? If I just set text/javascript or text/css, will the browser still interpret them correctly?\nEdit: Full response headers:\n\n```\nHTTP/1.1 200 OK\nx-amz-id-2: UIBkZT/MuFxsmn+3nVOzEO63rRY99l3traCbMExUgSdGHUrOIPtNp34h1+ujYKmt\nx-amz-request-id: 19346C9B01D8FC62\nDate: Mon, 12 May 2014 03:59:51 GMT\nContent-Encoding: gzip\nLast-Modified: Mon, 12 May 2014 02:24:54 GMT\nETag: \"561080b5e19f6daea2e74fd5a0623c79\"\nAccept-Ranges: bytes\nContent-Type: application/x-gzip\nContent-Length: 5153\nServer: AmazonS3\n\n```",
    "answer": "Content-Type: application/javascript\nContent-Encoding: gzip\n\nCompressed content in the response is indicated in the Content-Encoding.  The Content-Type should remain the same, that is, it should reflect the underlying media type that is compressed.\n\nSee sections 14.11 Content-Encoding and 3.5 Content Codings of RFC 2616 for more information."
  },
  {
    "question": "Does GZIP Compression Level Have Any Impact On Decompression I understand that GZIP is a combination of LZ77 and Huffman coding and can be configured with a level between 1-9 where 1 indicates  the  fastest  compression (less  compression) and 9 indicates the slowest compression method (best  compression).\nMy question is, does the choice of level only impact the compression process or is there an additional cost also incurred in decompression depending on the level used to compress?\nI ask because typically many web servers will GZIP responses on the fly if the client supports it, e.g. Accept-Encoding: gzip. I appreciate that when doing this on the fly a level such as 6 might be the good choice for the average case, since it gives a good balance between speed and compression. \nHowever, if I have a bunch of static assets that I can GZIP just once ahead of time - and never need to do this again - would there be any downside to using the highest but slowest compression level? I.e. is there now an additional overhead for the client that would not have been incurred had a lower compression level been used.",
    "answer": "Great question, and an underexposed issue. Your intuition is solid \u2013 for some compression algorithms, choosing the max level of compression can require more work from the decompressor when it's unpacked.\nLuckily, that's not true for gzip \u2013 there's no extra overhead for the client/browser to decompress more heavily compressed gzip files (e.g. choosing 9 for compression instead of 6, assuming the standard zlib codebase that most servers use). The best measure for this is decompression rate, which for present purposes is in units of MB/sec, while also monitoring overhead like memory and CPU. Simply going by decompression time is no good because the file is smaller at higher compression settings, and we're not controlling for that factor if we're only using a stopwatch.\n\ngzip decompression quickly gets asymptotic in terms of both time-to-decompress and memory usage once you get past level 6 compressed content. The time-to-decompress flatlines for levels 7, 8, and 9 in the test results linked by Marcus M\u00fcller, though that's coarse-grained data given in whole seconds.\nYou'll also notice in those results that the memory requirements for decompression are flat for all levels of compression at 0.1 MiB. That's almost unbelievable, just a degree of excellence in software that we rarely see. Mark Adler and colleagues deserve massive props for what they achieved. gzip is a very nice format.\n\nThe memory use gets at your question about overhead. There really is none. You don't gain much with level 9 in terms of browser decompression speed, but you don't lose anything.\nNow, check out these test results for a bit more texture. You'll see how the gzip decompression rate is slightly faster with level 9 compressed content than with lower levels (at level 9, decomp rate is about 0.9% faster than at level 6, for example). That is interesting and surprising. I wouldn't expect the rate to increase. That was just one set of test results \u2013 it may not hold for other scenarios (and the difference is quite small in any case).\nParting note: Precompressing static files is a good idea, but I don't recommend gzip at level 9. You'll get smaller files than gzip-9 by instead using zopfli or libdeflate. Zopfli is a well-established gzip compressor from Google. libdeflate is new but quite excellent. In my testing it consistently beats gzip-9, but still trails zopfli. You can also use 7-Zip to create gzip files, and it will consistently beat gzip-9. (In the foregoing, gzip-9 refers to using the canonical gzip or zlib application that Apache and nginx use)."
  },
  {
    "question": "Apply GZIP compression to a CSV in Python Pandas I am trying to write a dataframe to a gzipped csv in python pandas, using the following:\n\n```\nimport pandas as pd\nimport datetime\nimport csv\nimport gzip\n\n# Get data (with previous connection and script variables)\ndf = pd.read_sql_query(script, conn)\n\n# Create today's date, to append to file\ntodaysdatestring = str(datetime.datetime.today().strftime('%Y%m%d'))\nprint todaysdatestring\n\n# Create csv with gzip compression\ndf.to_csv('foo-%s.csv.gz' % todaysdatestring,\n      sep='|',\n      header=True,\n      index=False,\n      quoting=csv.QUOTE_ALL,\n      compression='gzip',\n      quotechar='\"',\n      doublequote=True,\n      line_terminator='\\n')\n\n```\n\nThis just creates a csv called 'foo-YYYYMMDD.csv.gz', not an actual gzip archive.\nI've also tried adding this:\n\n```\n#Turn to_csv statement into a variable\nd = df.to_csv('foo-%s.csv.gz' % todaysdatestring,\n      sep='|',\n      header=True,\n      index=False,\n      quoting=csv.QUOTE_ALL,\n      compression='gzip',\n      quotechar='\"',\n      doublequote=True,\n      line_terminator='\\n')\n\n# Write above variable to gzip\n with gzip.open('foo-%s.csv.gz' % todaysdatestring, 'wb') as output:\n   output.write(d)\n\n```\n\nWhich fails as well. Any ideas?",
    "answer": "Using df.to_csv() with the keyword argument compression='gzip' should produce a gzip archive. I tested it using same keyword arguments as you, and it worked.\nYou may need to upgrade pandas, as gzip was not implemented until version 0.17.1, but trying to use it on prior versions will not raise an error, and just produce a regular csv.  You can determine your current version of pandas by looking at the output of pd.__version__."
  },
  {
    "question": "Why can&#39;t browser send gzip request? If webserver can send gzip response, why can't browser sent gzip request?",
    "answer": "The client and server have to agree on how to communicate; part of this is whether the communication can be compressed. HTTP was designed as a request/response model, and the original creation was almost certainly envisioned to always have small requests and potentially large responses. Compression is not required to implement HTTP, there are both servers and clients that don't support it.\nHTTP compression is implemented by the client saying it can support compression, and if the server sees this in the request and it supports compression it can compress the response. To compress the request the client would have to have a \"pre-request\" that actually negotiated that the request would be made compressed OR it would have to require compression as a supported encoding for ALL requests.\n* UPDATE Feb '17 *\nIt's been 8 years, but as @Phil_1984_ notes, a 3rd possible solution would be for the client and server to negotiate compression support and then use that for subsequent requests. In fact, things like HSTS work just this way with the client caching that the server expects to only speak TLS and ignore any unencrypted links. HTTP was explicitly designed to be stateless but we've moved beyond that at this point."
  },
  {
    "question": "How can I get gzip compression in IIS7 working? I have installed Static and dynamic compression for IIS7, as well as setting the two web.config values at my application Virtual Folder level. As I understand it, I don't need to enable compression at the server, or site level anymore, and I can manage it on a per folder basis using my web.config file.\nI have two settings in my .config file that I have set to customize gzip for my app:\n\n```\n<httpCompression dynamicCompressionDisableCpuUsage=\"90\"\n    dynamicCompressionEnableCpuUsage=\"0\">\n  <scheme name=\"gzip\" dll=\"%Windir%\\system32\\inetsrv\\gzip.dll\" />\n  <dynamicTypes>\n    <remove mimeType=\"*/*\"/>\n    <add mimeType=\"*/*\" enabled=\"true\" />\n  </dynamicTypes>\n</httpCompression>\n<urlCompression doDynamicCompression=\"true\"\n    dynamicCompressionBeforeCache=\"true\" />\n\n```\n\nHowever, when I run the application, I can clearly see that gzip is not used, because my page sizes are the same. I am also using YSlow for FireFox, which also confirms that my pages are not being gziped.\nWhat am I missing here? In IIS6 it was a simple matter of specifying the file types, and setting the compression level between 0-10. I don't see the need documented to specify the file types or compression level, since the defaults seem to cover the file types, and I'm not seeing the level anywhere.",
    "answer": "There was a thread on forums.iis.net about this during the iis 7 beta.  Turned out the guy didn't have the modules installed, but it sounds like you've ruled that out from your opening sentence.\nMicrosofts key advice for him was to enable failed request tracing to find out what was going wrong.  This is possibly one of the most under-appreciated features of IIS7, but certainly one of the most powerful.\n\nOpen IIS Manager.\nGo to your site, and on the actions pane (the very far right), click 'Failed Request Tracing...' under the 'Configure' section.\nClick 'enable'.\nThen, in the features view, click 'Failed request tracing rules'. Click add, next, enter 200 for the status code, next, click finish.\n\nIf you don't see \"Failed Request Tracing\" in the actions pane, you'll need to add the feature to the server - either using the \"Add Role Services\" wizard (Health and Diagnostics\\Tracing) or through the Web Platform Installer (Products\\Server\\IIS: Tracing), and then close and re-open IIS Manager.\nNext, rerun your test. This will generate some log info for us to examine.\nLook in c:\\inetpub\\logs\\FailedReqLogFiles\\w3svcx. You will see a bunch of files named fr000xx.xml.  Open up any one of them in your browser.  (By the way, if you copy these files anywhere, make sure freb.xsl is there. Also, don't delete freb.xsl - if you do, just delete the whole directory or copy it from another location, as IIS only creates it once per folder.)\nClick the 'request details' tab and select 'complete request trace'.  Search the page for 'compress' - you should find it in several areas; once for static content, and once for dynamic content.\nIf you don't find either of them, IIS isn't configured correctly. If you do find them, you should see them followed by a compression_success and  a compression_do.  Success is self explanatory; the 'do' indicates what it did - in my case, it showed \"OriginalSize 1462784 CompressedSize 179482\"\nSince yours isn't working, hopefully you will see something different that helps you solve the problem.\nMake sure you turn this off when you're done by disabling failed request tracing in the actions pane for your website."
  },
  {
    "question": "Compressing HTTP Post Data sent from browser I want to send a compressed POST data with Javascript to a server I control.  Is there any way to let the HTTP layer deal with the compression.\nI'm sending JSON. If I set the content type to GZIP/deflate will the browser automatically compress it and then Apache with the deflate mod automatically decompress it so my application doesn't have to think about the data being compressed at all?\nI know it can work the other way around but any way to make it work this way?",
    "answer": "<Location /dav-area>\nSetInputFilter DEFLATE\n</Location>\n\nWill the browser automatically gzip-encode your data for you? The short answer is no.\nThe long answer is that some user-agents can do things like this, but you definitely can't rely on it. The apache mod_deflate docs state:\n\nsome special applications actually do support request compression, for instance some WebDAV clients.\n\nSo, no, that's not going to work. You'll need to generate the appropriate HTTP request message yourself. The appropriate header in this case is Content-Encoding: gzip and NOT Content-Type: because the content itself is application/json, you're just looking to encode the entity body of your HTTP request message for transport.\nNote that you need to also add the appropriate Content-Length: header specifying the size in bytes of the message entity body after compression -OR- send your HTTP message using Transfer-Encoding: chunked and forego the content-length specification.\nOn the receiving end, you can instruct mod_deflate to use an input filter to decompress the information:\n\nThis is a bit heavy handed if you're only receiving compressed message bodies for a couple of resources. Instead, you should probably just use the client-side script to check for the Content-Encoding: gzip header and decompress the request body manually. How to do this in say, PHP, is another question entirely. If you need details for that you should post another question."
  },
  {
    "question": "Why do real-world servers prefer gzip over deflate encoding? We already know deflate encoding is a winner over gzip with respect to speed of encoding, decoding and compression size.\nSo why do no large sites (that I can find) send it (when I use a browser that accepts it)?\nYahoo claims deflate is \"less effective\". Why?\nI maintain HTTP server software that prefers deflate, so I'd like to know if there's some really good reason not to continue doing so.",
    "answer": "There is some confusion about the naming between the specifications and the HTTP:\n\nDEFLATE as defined by RFC 1951 is a compressed data format.\nZLIB as defined by RFC 1950 is a compressed data format that uses the DEFLATE data format.\nGZIP as defined by RFC 1952 is a file format that uses the DEFLATE compressed data format.\n\nBut the HTTP uses a different naming:\n\ngzip An encoding format produced by the file compression program \"gzip\" (GNU zip) as described in RFC 1952 [25]. This format is a Lempel-Ziv coding (LZ77) with a 32 bit CRC.\n\ndeflate The \"zlib\" format defined in RFC 1950 [31] in combination with the \"deflate\" compression mechanism described in RFC 1951 [29].\n\nSo to sum up:\n\ngzip is the GZIP file format.\ndeflate is actually the ZLIB data format. (But some clients do also accept the actual DEFLATE data format for deflate.)\n\nSee also this answer on the question What's the difference between the \"gzip\" and \"deflate\" HTTP 1.1 encodings?:\n\nWhat's the difference between the \"gzip\" and \"deflate\" HTTP 1.1 encodings?\n\"gzip\" is the gzip format, and \"deflate\" is the zlib format. They should probably have called the second one \"zlib\" instead to avoid confusion with the raw deflate compressed data format. While the HTTP 1.1 RFC 2616 correctly points to the zlib specification in RFC 1950 for the \"deflate\" transfer encoding, there have been reports of servers and browsers that incorrectly produce or expect raw deflate data per the deflate specification in RFC 1951, most notably Microsoft. So even though the \"deflate\" transfer encoding using the zlib format would be the more efficient approach (and in fact exactly what the zlib format was designed for), using the \"gzip\" transfer encoding is probably more reliable due to an unfortunate choice of name on the part of the HTTP 1.1 authors."
  },
  {
    "question": "Import and insert sql.gz file into database with putty I want to insert a sql.gz file into my database with SSH. What should I do?\nFor example I have a database from telephone numbers that name is numbers.sql.gz, what is this type of file and how can I import this file into my database?",
    "answer": "C:\\> pscp.exe numbers.sql.gz user@serverhostname:/home/user\nuser@serverhostname$  gunzip numbers.sql.gz\nuser@serverhostname$  ls\n\nnumbers.sql\nuser@serverhostname$  mysql -u mysqluser -p < numbers.sql\nuser@serverhostname$  mysql -u mysqluser -p databasename < numbers.sql\n\nThe file is a gzipped (compressed) SQL file, almost certainly a plain text file with .sql as its extension. The first thing you need to do is copy the file to your database server via scp.. I think PuTTY's is pscp.exe\n\nThen SSH into your server and uncompress the file with gunzip\n\nFinally, import it into your MySQL database using the < input redirection operator:\n\nIf the numbers.sql file doesn't create a database but expects one to be present already, you will need to include the database in the command as well:\n\nIf you have the ability to connect directly to your MySQL server from outside, then you could use a local MySQL client instead of having to copy and SSH.  In that case, you would just need a utility that can decompress .gz files on Windows.  I believe 7zip does so, or you can obtain the gzip/gunzip binaries for Windows."
  },
  {
    "question": "Decode gzipped web page retrieved via cURL in PHP I'm retrieving a gzipped web page via curl, but when I output the retrieved content to the browser I just get the raw gzipped data. How can I decode the data in PHP?\nOne method I found was to write the content to a tmp file and then ...\n\n```\n$f = gzopen($filename,\"r\");\n$content = gzread($filename,250000);\ngzclose($f);\n\n```\n\n.... but man, there's got to be a better way.\nEdit: This isn't a file, but a gzipped html page returned by a web server.",
    "answer": "// Allow cURL to use gzip compression, or any other supported encoding\n// A blank string activates 'auto' mode\ncurl_setopt($ch, CURLOPT_ENCODING , '');\n// Allow cURL to use gzip compression, or any other supported encoding\ncurl_setopt($ch, CURLOPT_ENCODING , 'gzip');\n\nThe following command enables cURL's \"auto encoding\" mode, where it will announce to the server which encoding methods it supports (via the Accept-Encoding header), and then automatically decompress the response for you:\n\nIf you specifically want to force the header to be Accept-Encoding: gzip you can use this command instead:\n\nRead more in the PHP documentation: curl_setopt.\nThanks to commenters for helping improve this answer."
  },
  {
    "question": "Gzip with all cores I have a set of servers filled each with a bunch of files that can be gzipped.  The servers all have different numbers of cores.  How can I write a bash script to launch a gzip for each core and make sure the gzips are not zipping the same file?",
    "answer": "CORES=$(grep -c '^processor' /proc/cpuinfo)\nfind /source -type f -print0 | xargs -0 -n 1 -P $CORES gzip -9\n\nIf you are on Linux, you can use GNU's xargs to launch as many processes as you have cores.\n\nfind -print0 / xargs -0 protects you from whitespace in filenames\nxargs -n 1 means one gzip process per file\nxargs -P specifies the number of jobs\ngzip -9 means maximum compression"
  },
  {
    "question": "How do you create a .gz file using PHP? I would like to gzip compress a file on my server using PHP.  Does anyone have an example that would input a file and output a compressed file?",
    "answer": "/**\n* GZIPs a file on disk (appending .gz to the name)\n*\n* From http://stackoverflow.com/questions/6073397/how-do-you-create-a-gz-file-using-php\n* Based on function by Kioob at:\n* http://www.php.net/manual/en/function.gzwrite.php#34955\n*\n* @param string $source Path to file that should be compressed\n* @param integer $level GZIP compression level (default: 9)\n* @return string New filename (with .gz appended) if success, or false if operation fails\n*/\nfunction gzCompressFile($source, $level = 9){\ndest = $source . '.gz';\nmode = 'wb' . $level;\nerror = false;\nif ($fp_out = gzopen($dest, $mode)) {\nif ($fp_in = fopen($source,'rb')) {\nwhile (!feof($fp_in))\ngzwrite($fp_out, fread($fp_in, 1024 * 512));\nfclose($fp_in);\n} else {\nerror = true;\n}\ngzclose($fp_out);\n} else {\nerror = true;\n}\nif ($error)\nreturn false;\nelse\nreturn $dest;\n}\n\nThe other answers here load the entire file into memory during compression, which will cause 'out of memory' errors on large files.  The function below should be more reliable on large files as it reads and writes files in 512kb chunks.\n\nUPDATE: Gerben has posted an improved version of this function that is cleaner and uses exceptions instead of returning false on an error. See https://stackoverflow.com/a/56140427/195835"
  },
  {
    "question": "How to create a zip archive of a directory? How can I create a zip archive of a directory structure in Python?",
    "answer": "import os\nimport zipfile\n\ndef zipdir(path, ziph):\n\nfor root, dirs, files in os.walk(path):\nfor file in files:\nziph.write(os.path.join(root, file),\nos.path.relpath(os.path.join(root, file),\nos.path.join(path, '..')))\n\nwith zipfile.ZipFile('Python.zip', 'w', zipfile.ZIP_DEFLATED) as zipf:\nzipdir('tmp/', zipf)\n\nAs others have pointed out, you should use zipfile. The documentation tells you what functions are available, but doesn't really explain how you can use them to zip an entire directory. I think it's easiest to explain with some example code:"
  },
  {
    "question": "Unzipping files in Python I read through the zipfile documentation, but couldn't understand how to unzip a file, only how to zip a file. How do I unzip all the contents of a zip file into the same directory?",
    "answer": "import zipfile\nwith zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\nzip_ref.extractall(directory_to_extract_to)\n\nThat's pretty much it!"
  },
  {
    "question": "Create zip file and ignore directory structure I need to create a zip file using this command:\n\n```\nzip /dir/to/file/newZip /data/to/zip/data.txt\n\n```\n\nThis works, but the created zip file creates a directory structure mimicking the directory to the raw file. It is a lot of extra folders that I don't need. \nI didn't find an answer in a cursory glance over the man page or a Google hunt.",
    "answer": "-j\n--junk-paths\nStore just the name of a saved file (junk the path), and do  not\nstore  directory names. By default, zip will store the full path\n(relative to the current directory).\n\nYou can use -j."
  },
  {
    "question": "How can you zip or unzip from the script using ONLY Windows&#39; built-in capabilities? In Windows you can zip some files by \n\nright click \u2192 Send to \u2192 Compressed (zipped) folder\n\nAnd unzip by double clicking on the .zip file and extract the files.\nIs there a way to apply those abilities from a script (.bat file) without the need to install any third-party software?",
    "answer": "Back in 2013, that was not possible. Microsoft didn't provide any executable for this.\nSee this link for some VBS way to do this.\nhttps://superuser.com/questions/201371/create-zip-folder-from-the-command-line-windows\nFrom Windows 8 on, .NET Framework 4.5 is installed by default, with System.IO.Compression.ZipArchive and PowerShell available, one can write scripts to achieve this, see\nhttps://stackoverflow.com/a/26843122/71312"
  },
  {
    "question": "How to create a zip archive with PowerShell? Is it possible to create a zip archive using PowerShell?",
    "answer": "If you head on over to CodePlex and grab the PowerShell Community Extensions, you can use their write-zip cmdlet.\nSince\n\nCodePlex is in read-only mode in preparation for shutdown\n\nyou can go to PowerShell Gallery."
  },
  {
    "question": "Is there a zip-like function that pads to longest length? Is there a built-in function that works like zip() but that will pad the results so that the length of the resultant list is the length of the longest input rather than the shortest input?\n\n```\n>>> a = ['a1']\n>>> b = ['b1', 'b2', 'b3']\n>>> c = ['c1', 'c2']\n\n>>> zip(a, b, c)\n[('a1', 'b1', 'c1')]\n\n>>> What command goes here?\n[('a1', 'b1', 'c1'), (None, 'b2', 'c2'), (None, 'b3', None)]\n\n```",
    "answer": ">>> list(itertools.zip_longest(a, b, c))\n[('a1', 'b1', 'c1'), (None, 'b2', 'c2'), (None, 'b3', None)]\n>>> list(itertools.zip_longest(a, b, c, fillvalue='foo'))\n[('a1', 'b1', 'c1'), ('foo', 'b2', 'c2'), ('foo', 'b3', 'foo')]\n>>> map(None, a, b, c)\n[('a1', 'b1', 'c1'), (None, 'b2', 'c2'), (None, 'b3', None)]\n\nIn Python 3 you can use itertools.zip_longest\n\nYou can pad with a different value than None by using the fillvalue parameter:\n\nWith Python 2 you can either use itertools.izip_longest (Python 2.6+), or you can use map with None. It is a little known feature of map (but map changed in Python 3.x, so this only works in Python 2.x)."
  },
  {
    "question": "BitBucket - download source as ZIP I know I can get the project through git clone command, but is there any way, how to download the project through the web interface from BitBucket.org?\nIn the best way, I am looking for a way to download a project source as ZIP compress file.",
    "answer": "https://bitbucket.org/owner/repository/get/v0.1.2.tar.gz\nhttps://bitbucket.org/owner/repository/get/A0B1C2D.tar.gz\n\nFor the latest version of Bitbucket (2016+), the download link can be found in the Download menu item.\n\nPre-2016\nFirst method\nIn the Overview page of the repo, there is a link to download the project.\n\nSecond method\nGo to Downloads -> Branches -> Download the branch that you want (as .zip, .gz or .bz2).  There you'll find download links for all tags.  The links will be in the format:\n\nBy tweaking it a little bit, you can also have access to any revision by changing the tag to the commit hash:"
  },
  {
    "question": "What is a good Java library to zip/unzip files? I looked at the default Zip library that comes with the JDK and the Apache compression libs and I am unhappy with them for 3 reasons:\n\nThey are bloated and have bad API design. I have to write 50 lines of boiler plate byte array output, zip input, file out streams and close relevant streams and catch exceptions and move byte buffers on my own? Why can't I have a simple API that looks like this Zipper.unzip(InputStream zipFile, File targetDirectory, String password = null) and Zipper.zip(File targetDirectory, String password = null) that just works?\nIt seems zipping unzipping destroys file meta-data and password handling is broken.\nAlso, all the libraries I tried were 2-3x slow compared to the command line zip tools I get with UNIX?\n\nFor me (2) and (3) are minor points but I really want a good tested library with a one-line interface.",
    "answer": "import net.lingala.zip4j.exception.ZipException;\nimport net.lingala.zip4j.core.ZipFile;\n\npublic static void unzip(){\nString source = \"some/compressed/file.zip\";\nString destination = \"some/destination/folder\";\nString password = \"password\";\n\ntry {\nZipFile zipFile = new ZipFile(source);\nif (zipFile.isEncrypted()) {\nzipFile.setPassword(password);\n}\nzipFile.extractAll(destination);\n} catch (ZipException e) {\ne.printStackTrace();\n}\n}\n<dependency>\n<groupId>net.lingala.zip4j</groupId>\n<artifactId>zip4j</artifactId>\n<version>1.3.2</version>\n</dependency>\n\nI know its late and there are lots of answers but this zip4j is one of the best libraries for zipping I have used. Its simple (no boiler code) and can easily handle password protected files.\n\nThe Maven dependency is:"
  },
  {
    "question": "Mac zip compress without __MACOSX folder? When I compress files with the built in zip compressor in Mac OSX, it causes an extra folder titled \"__MACOSX\" to be created in the extracted zip.\nCan I adjust my settings to keep this folder from being created or do I need to purchase a third party compression tool?\nUPDATE: I just found a freeware app for OSX that solves my problem: \"YemuZip\"\nUPDATE 2: YemuZip is no longer freeware.",
    "answer": "zip file.zip uncompressed\n\nWhen I had this problem I've done it from command line:\n\nEDIT, after many downvotes: I was using this option for some time ago and I don't know where I learnt it, so I can't give you a better explanation. Chris Johnson's answer is correct, but I won't delete mine. As one comment says, it's more accurate to what OP is asking, as it compress without those files, instead of removing them from a compressed file. I find it easier to remember, too."
  },
  {
    "question": "Creating a ZIP archive in memory using System.IO.Compression I'm trying to create a ZIP archive with a simple demo text file using a MemoryStream as follows:\n\n```\nusing (var memoryStream = new MemoryStream())\nusing (var archive = new ZipArchive(memoryStream , ZipArchiveMode.Create))\n{\n    var demoFile = archive.CreateEntry(\"foo.txt\");\n\n    using (var entryStream = demoFile.Open())\n    using (var streamWriter = new StreamWriter(entryStream))\n    {\n        streamWriter.Write(\"Bar!\");\n    }\n\n    using (var fileStream = new FileStream(@\"C:\\Temp\\test.zip\", FileMode.Create))\n    {\n        stream.CopyTo(fileStream);\n    }\n}\n\n```\n\nIf I run this code, the archive file itself is created but foo.txt isn't.\nHowever, if I replace the MemoryStream directly with the file stream, the archive is created correctly:\n\n```\nusing (var fileStream = new FileStream(@\"C:\\Temp\\test.zip\", FileMode.Create))\nusing (var archive = new ZipArchive(fileStream, FileMode.Create))\n{\n    // ...\n}\n\n```\n\nIs it possible to use a MemoryStream to create the ZIP archive without the FileStream?",
    "answer": "using (var memoryStream = new MemoryStream())\n{\nusing (var archive = new ZipArchive(memoryStream, ZipArchiveMode.Create, true))\n{\nvar demoFile = archive.CreateEntry(\"foo.txt\");\n\nusing (var entryStream = demoFile.Open())\nusing (var streamWriter = new StreamWriter(entryStream))\n{\nstreamWriter.Write(\"Bar!\");\n}\n}\n\nusing (var fileStream = new FileStream(@\"C:\\Temp\\test.zip\", FileMode.Create))\n{\nmemoryStream.Seek(0, SeekOrigin.Begin);\nmemoryStream.CopyTo(fileStream);\n}\n}\n\nThanks to ZipArchive creates invalid ZIP file, I got:\n\nThat indicated we need to call Dispose on ZipArchive before we can use it, which as Amir suggests is likely because it writes final bytes like checksum to the archive that makes it complete. But in order not close the stream so we can re-use it after you need to pass true as the third parameter to ZipArchive."
  },
  {
    "question": "Fatal error: Class &#39;ZipArchive&#39; not found in I have a problem when I install 'Archive_Zip 0.1.1' on the Linux server, but when I try to run the script to create the zip file it gives the fatal error\n\nFatal error: Class ZipArchive not found in ...\n\nwhere I put the code\n\n```\n$zip = new ZipArchive;\nvar_dump($zip);\n$res = $zip->open($filename, ZipArchive::OVERWRITE);\nif ($res !== TRUE) {\n    echo 'Error: Unable to create zip file';\n    exit;\n}\nif (is_file($src)) {\n    $zip->addFile($src);\n} else {\n    // echo \"<br>\" . dirname(__FILE__) . $src;//'/install1';\n    if (!is_dir($src)) {\n         $zip->close();\n         @unlink($filename);\n         echo 'Error: File not found';\n         exit;\n    }\n    recurse_zip($src, $zip, $path_length);\n}\n$zip->close();\necho \"<br>file name \".$filename;\n\n```\n\nbut it doesn't find the class file.\nPlease tell me the solution. What should I do to resolve the problem?\nI also put the php.ini file in the folder where the script is, but it does not work.",
    "answer": "sudo apt update\nsudo apt install php-zip\nsudo systemctl restart apache2\n\nFor the ZipArchive class to be present, PHP needs to have the zip extension installed.\nSee this page for installation instructions (both Linux and Windows).\nOn Debian and Ubuntu, you can usually install it with:\n\nThen restart your webserver. Example:"
  },
  {
    "question": "How do I ZIP a file in C#, using no 3rd-party APIs? I'm pretty sure this is not a duplicate so bear with me for just a minute.\nHow can I programatically (C#) ZIP a file (in Windows) without using any third party libraries? I need a native windows call or something like that; I really dislike the idea of starting a process, but I will if I absolutely have to. A PInovke call would be much better.\nFailing that, let me tell you what I'm really trying to accomplish: I need the ability to let a user download a collection of documents in a single request. Any ideas on how to accomplish this?",
    "answer": "Are you using .NET 3.5?  You could use the ZipPackage class and related classes.  Its more than just zipping up a file list because it wants a MIME type for each file you add.  It might do what you want.\nI'm currently using these classes for a similar problem to archive several related files into a single file for download.  We use a file extension to associate the download file with our desktop app.  One small problem we ran into was that its not possible to just use a third-party tool like 7-zip to create the zip files because the client side code can't open it -- ZipPackage adds a hidden file describing the content type of each component file and cannot open a zip file if that content type file is missing."
  },
  {
    "question": ".rar, .zip files MIME Type I'm developing a simple php upload script, and users can upload only ZIP and RAR files.\nWhat MIME types I should use to check $_FILES[x][type]? (a complete list please)",
    "answer": ".rar    application/vnd.rar, application/x-rar-compressed, application/octet-stream\n.zip    application/zip, application/octet-stream, application/x-zip-compressed, multipart/x-zip\n<?php\n\nif (isRarOrZip($argv[1])) {\necho 'It is probably a RAR or ZIP file.';\n} else {\necho 'It is probably not a RAR or ZIP file.';\n}\n\nfunction isRarOrZip($file) {\n// get the first 7 bytes\nbytes = file_get_contents($file, FALSE, NULL, 0, 7);\next = strtolower(substr($file, - 4));\n\n// RAR magic number: Rar!\\x1A\\x07\\x00\n// http://en.wikipedia.org/wiki/RAR\nif ($ext == '.rar' and bin2hex($bytes) == '526172211a0700') {\nreturn TRUE;\n}\n\n// ZIP magic number: none, though PK\\003\\004, PK\\005\\006 (empty archive),\n// or PK\\007\\008 (spanned archive) are common.\n// http://en.wikipedia.org/wiki/ZIP_(file_format)\nif ($ext == '.zip' and substr($bytes, 0, 2) == 'PK') {\nreturn TRUE;\n}\n\nreturn FALSE;\n}\nrar.exe l somefile.zip\nsomefile.zip is not RAR archive\nrar.exe l somefile.srr\nSFX Volume somefile.srr\n\nThe answers from freedompeace, Kiyarash and Sam Vloeberghs:\n\nI would do a check on the file name too. Here is how you could check if the file is a RAR or ZIP file. I tested it by creating a quick command line application.\n\nNotice that it still won't be 100% certain, but it is probably good enough.\n\nBut even WinRAR detects non RAR files as SFX archives:"
  },
  {
    "question": "How to create full compressed tar file using Python? How can I create a .tar.gz file with compression in Python?",
    "answer": "import tarfile\nimport os.path\n\ndef make_tarfile(output_filename, source_dir):\nwith tarfile.open(output_filename, \"w:gz\") as tar:\ntar.add(source_dir, arcname=os.path.basename(source_dir))\n\nTo build a .tar.gz (aka .tgz) for an entire directory tree:\n\nThis will create a gzipped tar archive containing a single top-level folder with the same name and contents as source_dir."
  },
  {
    "question": "How to unzip a file using the command line? Which commands can be used via the command line to unzip a file?\nPreferably something built into Windows or open source/free tools.",
    "answer": "You could use :\nhttp://membrane.com/synapse/library/pkunzip.html\nor\n7zip: http://www.7-zip.org/download.html\nFree byte zip: http://www.freebyte.com/fbzip/\nor infozip: http://infozip.sourceforge.net/"
  },
  {
    "question": "How to zip a whole folder using PHP I have found here at stackoveflow some code on how to ZIP a specific file, but how about a specific folder?\n\n```\nFolder/\n  index.html\n  picture.jpg\n  important.txt\n\n```\n\ninside in My Folder, there are files. after zipping the My Folder, i also want to delete the whole content of the folder except important.txt.\nFound this here at stack",
    "answer": "// Remove any trailing slashes from the path\nrootPath = rtrim($rootPath, '\\\\/');\n\n// Get real path for our folder\nrootPath = realpath('folder-to-zip');\n\n// Initialize archive object\nzip = new ZipArchive();\nzip->open('file.zip', ZipArchive::CREATE | ZipArchive::OVERWRITE);\n\n// Create recursive directory iterator\n/** @var SplFileInfo[] $files */\nfiles = new RecursiveIteratorIterator(\nnew RecursiveDirectoryIterator($rootPath),\nRecursiveIteratorIterator::LEAVES_ONLY\n);\n\nforeach ($files as $file)\n{\n// Skip directories (they would be added automatically)\nif (!$file->isDir())\n{\n// Get real and relative path for current file\nfilePath = $file->getRealPath();\nrelativePath = substr($filePath, strlen($rootPath) + 1);\n\n// Add current file to archive\nzip->addFile($filePath, $relativePath);\n}\n}\n\n// Zip archive will be created only after closing object\nzip->close();\n// Remove any trailing slashes from the path\nrootPath = rtrim($rootPath, '\\\\/');\n\n// Get real path for our folder\nrootPath = realpath('folder-to-zip');\n\n// Initialize archive object\nzip = new ZipArchive();\nzip->open('file.zip', ZipArchive::CREATE | ZipArchive::OVERWRITE);\n\n// Initialize empty \"delete list\"\nfilesToDelete = array();\n\n// Create recursive directory iterator\n/** @var SplFileInfo[] $files */\nfiles = new RecursiveIteratorIterator(\nnew RecursiveDirectoryIterator($rootPath),\nRecursiveIteratorIterator::LEAVES_ONLY\n);\n\nforeach ($files as $file)\n{\n// Skip directories (they would be added automatically)\nif (!$file->isDir())\n{\n// Get real and relative path for current file\nfilePath = $file->getRealPath();\nrelativePath = substr($filePath, strlen($rootPath) + 1);\n\n// Add current file to archive\nzip->addFile($filePath, $relativePath);\n\n// Add current file to \"delete list\"\n// delete it later cause ZipArchive create archive only after calling close function and ZipArchive lock files until archive created)\nif ($file->getFilename() != 'important.txt')\n{\nfilesToDelete[] = $filePath;\n}\n}\n}\n\n// Zip archive will be created only after closing object\nzip->close();\n\n// Delete all files from \"delete list\"\nforeach ($filesToDelete as $file)\n{\nunlink($file);\n}\n\nZip a whole folder:\n\nZip a whole folder + delete all files except \"important.txt\":"
  },
  {
    "question": "How to create a zip file in Java I have a dynamic text file that picks content from a database according to the user's query. I have to write this content into a text file and zip it in a folder in a servlet. How should I do this?",
    "answer": "StringBuilder sb = new StringBuilder();\nsb.append(\"Test String\");\n\nFile f = new File(\"d:\\\\test.zip\");\nZipOutputStream out = new ZipOutputStream(new FileOutputStream(f));\nZipEntry e = new ZipEntry(\"mytext.txt\");\nout.putNextEntry(e);\n\nbyte[] data = sb.toString().getBytes();\nout.write(data, 0, data.length);\nout.closeEntry();\n\nout.close();\nZipEntry e = new ZipEntry(\"folderName/mytext.txt\");\n\nLook at this example:\n\nThis will create a zip in the root of D: named test.zip which will contain one single file called mytext.txt. Of course you can add more zip entries and also specify a subdirectory like this:\n\nYou can find more information about compression with Java here."
  },
  {
    "question": "Need to ZIP an entire directory using Node.js I need to zip an entire directory using Node.js. I'm currently using node-zip and each time the process runs it generates an invalid ZIP file (as you can see from this Github issue).\nIs there another, better, Node.js option that will allow me to ZIP up a directory?\nEDIT: I ended up using archiver\n\n```\nwriteZip = function(dir,name) {\nvar zip = new JSZip(),\n    code = zip.folder(dir),\n    output = zip.generate(),\n    filename = ['jsd-',name,'.zip'].join('');\n\nfs.writeFileSync(baseDir + filename, output);\nconsole.log('creating ' + filename);\n};\n\n```\n\nsample value for parameters:\n\n```\ndir = /tmp/jsd-<randomstring>/\nname = <randomstring>\n\n```\n\nUPDATE: For those asking about the implementation I used, here's a link to my downloader:",
    "answer": "var file_system = require('fs');\nvar archiver = require('archiver');\n\nvar output = file_system.createWriteStream('target.zip');\nvar archive = archiver('zip');\n\noutput.on('close', function () {\nconsole.log(archive.pointer() + ' total bytes');\nconsole.log('archiver has been finalized and the output file descriptor has closed.');\n});\n\narchive.on('error', function(err){\nthrow err;\n});\n\narchive.pipe(output);\n\n// append files from a sub-directory, putting its contents at the root of archive\narchive.directory(source_dir, false);\n\n// append files from a sub-directory and naming it `new-subdir` within the archive\narchive.directory('subdir/', 'new-subdir');\n\narchive.finalize();\n\nI ended up using archiver lib. Works great.\nExample"
  },
  {
    "question": "How to use Python&#39;s pip to download and keep the zipped files for a package? If I want to use the pip command to download a package (and its dependencies), but keep all of the zipped files that get downloaded (say, django-socialregistration.tar.gz) - is there a way to do that?\nI've tried various command-line options, but it always seems to unpack and delete the zipfile - or it gets the zipfile, but only for the original package, not the dependencies.",
    "answer": "pip download <package-name>\n\npip install --download is deprecated. Starting from version 8.0.0 you should use pip download command:"
  },
  {
    "question": "Download Returned Zip file from URL If I have a URL that, when submitted in a web browser, pops up a dialog box to save a zip file, how would I go about catching and downloading this zip file in Python?",
    "answer": "import requests\n\ndef download_url(url, save_path, chunk_size=128):\nr = requests.get(url, stream=True)\nwith open(save_path, 'wb') as fd:\nfor chunk in r.iter_content(chunk_size=chunk_size):\nfd.write(chunk)\nimport urllib.request\n\ndef download_url(url, save_path):\nwith urllib.request.urlopen(url) as dl_file:\nwith open(save_path, 'wb') as out_file:\nout_file.write(dl_file.read())\nfrom contextlib import closing\n\ndef download_url(url, save_path):\nwith closing(urllib2.urlopen(url)) as dl_file:\nwith open(save_path, 'wb') as out_file:\nout_file.write(dl_file.read())\n\nMost people recommend using requests if it is available, and the requests documentation recommends this for downloading and saving raw data from a url:\n\nSince the answer asks about downloading and saving the zip file, I haven't gone into details regarding reading the zip file. See one of the many answers below for possibilities.\nIf for some reason you don't have access to requests, you can use urllib.request instead. It may not be quite as robust as the above.\n\nFinally, if you are using Python 2 still, you can use urllib2.urlopen."
  },
  {
    "question": "Using R to download zipped data file, extract, and import data @EZGraphs on Twitter writes:\n\"Lots of online csvs are zipped. Is there a way to download, unzip the archive, and load the data to a data.frame using R? #Rstats\"\nI was also trying to do this today, but ended up just downloading the zip file manually.\nI tried something like:\n\n```\nfileName <- \"http://www.newcl.org/data/zipfiles/a1.zip\"\ncon1 <- unz(fileName, filename=\"a1.dat\", open = \"r\")\n\n```\n\nbut I feel as if I'm a long way off.\nAny thoughts?",
    "answer": "temp <- tempfile()\ndownload.file(\"http://www.newcl.org/data/zipfiles/a1.zip\",temp)\ndata <- read.table(unz(temp, \"a1.dat\"))\nunlink(temp)\n\nZip archives are actually more a 'filesystem' with content metadata etc. See help(unzip) for details.  So to do what you sketch out above you need to\n\nCreate a temp. file name (eg tempfile())\nUse download.file() to fetch the file into the temp. file\nUse unz() to extract the target file from temp. file\nRemove the temp file via unlink()\n\nwhich in code (thanks for basic example, but this is simpler) looks like\n\nCompressed (.z) or gzipped (.gz) or bzip2ed (.bz2) files are just the file and those you can read directly from a connection.  So get the data provider to use that instead :)"
  },
  {
    "question": "How do I export my project as a .zip of git repository? I was recently asked to export as a .zip file one of my projects on my Git repository.\nI have actually never had to do this in the 4 years I have been using Git.\nI would prefer an answer that is all done inside command line terminal.",
    "answer": "git archive --format=zip --output /full/path/to/zipfile.zip master"
  },
  {
    "question": "Python: Open file in zip without temporarily extracting it How can I open files in a zip archive without extracting them first?\nI'm using pygame. To save disk space, I have all the images zipped up.\nIs it possible to load a given image directly from the zip file? \nFor example:\npygame.image.load('zipFile/img_01')",
    "answer": "import zipfile\narchive = zipfile.ZipFile('images.zip', 'r')\nimgfile = archive.open('img_01.png')\n...\nimport zipfile\narchive = zipfile.ZipFile('images.zip', 'r')\nimgdata = archive.read('img_01.png')\n...\n\nVincent Povirk's answer won't work completely;\n\nYou have to change it in:\n\nFor details read the ZipFile docs here."
  },
  {
    "question": "How does one make a Zip bomb? This question about zip bombs naturally led me to the Wikipedia page on the topic. The article mentions an example of a 45.1 kb zip file that decompresses to 1.3 exabytes.\nWhat are the principles/techniques that would be used to create such a file in the first place? I don't want to actually do this, more interested in a simplified \"how-stuff-works\" explanation of the concepts involved.\nThe article mentions 9 layers of zip files, so it's not a simple case of zipping a bunch of zeros. Why 9, why 10 files in each?",
    "answer": "Citing from the Wikipedia page:\n\nOne example of a Zip bomb is the file\n45.1.zip which was 45.1 kilobytes of compressed data, containing nine\nlayers of nested zip files in sets of\n10, each bottom layer archive\ncontaining a 1.30 gigabyte file for a\ntotal of 1.30 exabytes of uncompressed\ndata.\n\nSo all you need is one single 1.3GB file full of zeroes, compress that into a ZIP file, make 10 copies, pack those into a ZIP file, and repeat this process 9 times.\nThis way, you get a file which, when uncompressed completely, produces an absurd amount of data without requiring you to start out with that amount.\nAdditionally, the nested archives make it much harder for programs like virus scanners (the main target of these \"bombs\") to be smart and refuse to unpack archives that are \"too large\", because until the last level the total amount of data is not that much, you don't \"see\" how large the files at the lowest level are until you have reached that level, and each individual file is not \"too large\" - only the huge number is problematic."
  },
  {
    "question": "Zip with list output instead of tuple What is the fastest and most elegant way of doing list of lists from two lists?\nI have\n\n```\nIn [1]: a=[1,2,3,4,5,6]\n\nIn [2]: b=[7,8,9,10,11,12]\n\nIn [3]: zip(a,b)\nOut[3]: [(1, 7), (2, 8), (3, 9), (4, 10), (5, 11), (6, 12)]\n\n```\n\nAnd I'd like to have\n\n```\nIn [3]: some_method(a,b)\nOut[3]: [[1, 7], [2, 8], [3, 9], [4, 10], [5, 11], [6, 12]]\n\n```\n\nI was thinking about using map instead of zip, but I don't know if there is some standard library method to put as a first argument.\nI can def my own function for this, and use map, my question is if there is already implemented something. No is also an answer.",
    "answer": "[list(a) for a in zip([1,2,3], [4,5,6], [7,8,9])]\n\nIf you are zipping more than 2 lists (or even only 2, for that matter), a readable way would be:\n\nThis uses a list comprehension to apply list to each element (tuple) in the list, converting them into lists."
  },
  {
    "question": "Download multiple files as a zip-file using php How can I download multiple files as a zip-file using php?",
    "answer": "files = array('readme.txt', 'test.html', 'image.gif');\nzipname = 'file.zip';\nzip = new ZipArchive;\nzip->open($zipname, ZipArchive::CREATE);\nforeach ($files as $file) {\nzip->addFile($file);\n}\nzip->close();\nheader('Content-Type: application/zip');\nheader('Content-disposition: attachment; filename='.$zipname);\nheader('Content-Length: ' . filesize($zipname));\nreadfile($zipname);\n\nYou can use the ZipArchive class to create a ZIP file and stream it to the client. Something like:\n\nand to stream it:\n\nThe second line forces the browser to present a download box to the user and prompts the name filename.zip. The third line is optional but certain (mainly older) browsers have issues in certain cases without the content size being specified."
  },
  {
    "question": "Python in-memory zip library Is there a Python library that allows manipulation of zip archives in memory, without having to use actual disk files? \nThe ZipFile library does not allow you to update the archive. The only way seems to be to extract it to a directory, make your changes, and create a new zip from that directory. I want to modify zip archives without disk access, because I'll be downloading them, making changes, and uploading them again, so I have no reason to store them. \nSomething similar to Java's ZipInputStream/ZipOutputStream would do the trick, although any interface at all that avoids disk access would be fine.",
    "answer": "class zipfile.ZipFile(file[, mode[, compression[, allowZip64]]])\n\nOpen a ZIP file, where file can be either a path to a file (a string) or a file-like object.\nfile_like_object = io.BytesIO(my_zip_data)\nzipfile_ob = zipfile.ZipFile(file_like_object)\n\nAccording to the Python docs:\n\nSo, to open the file in memory, just create a file-like object (perhaps using BytesIO)."
  },
  {
    "question": "How to update one file in a zip archive Is it possible to replace a file in a zip file without unzipping?\nThe file to update is an XML file that resides in a huge zip archive. To update this XML file, I have to unzip the archive, delete the old XML file, add the new one and then rezip. This takes a considerable amount of time. So want to be able to replace that one XML through a script. I already have the one that checks for updates on the XML I have.\nusing zip command\nSorry, I would use the zip command to do things like that but the problem is the script is actually for an android phone and zip is not a command I can use unfortunately sorry I left that out. I would have used zip definitely if i could but I only have unzip for droid and then there is tar in busybox but tar doesn't do what I need.",
    "answer": "From zip(1):\n\nWhen given the name of an existing zip archive, zip will replace identically named entries in the zip archive or add entries for new names.\n\nSo just use the zip command as you normally would to create a new .zip file containing only that one file, except the .zip filename you specify will be the existing archive."
  },
  {
    "question": "Simplest way to download and unzip files in Node.js cross-platform? Just looking for a simple solution to downloading and unzipping .zip or .tar.gz files in Node.js on any operating system.\nNot sure if this is built in or I have to use a separate library.  Any ideas?  Looking for just a couple lines of code so when the next zip file comes that I want to download in node, it's a no brainer.  Feel like this should be easy and/or built in, but I can't find anything. Thanks!",
    "answer": "var zlib = require('zlib');\n\nzlib.gunzip(gzipBuffer, function(err, result) {\nif(err) return console.error(err);\n\nconsole.log(result);\n});\nvar request = require('request'),\nzlib = require('zlib'),\nfs = require('fs'),\nout = fs.createWriteStream('out');\n\n// Fetch http://example.com/foo.gz, gunzip it and store the results in 'out'\nrequest('http://example.com/foo.gz').pipe(zlib.createGunzip()).pipe(out);\n\nNode has builtin support for gzip and deflate via the zlib module:\n\nEdit: You can even pipe the data directly through e.g. Gunzip (using request):\n\nFor tar archives, there is Isaacs' tar module, which is used by npm.\nEdit 2: Updated answer as zlib doesn't support the zip format. This will only work for gzip."
  },
  {
    "question": "I didn&#39;t find &quot;ZipFile&quot; class in the &quot;System.IO.Compression&quot; namespace I can't use \"Zipfile\" class in the name space \"System.IO.Compression\" my code is :\n\n```\nusing System;\nusing System.IO;\nusing System.IO.Compression;\n\nnamespace ConsoleApplication\n{\n    class Program\n    {\n        static void Main(string[] args)\n        {\n            string startPath = @\"c:\\example\\start\";\n            string zipPath = @\"c:\\example\\result.zip\";\n            string extractPath = @\"c:\\example\\extract\";\n\n            ZipFile.CreateFromDirectory(startPath, zipPath, CompressionLevel.Fastest,true);\n\n            ZipFile.ExtractToDirectory(zipPath, extractPath);\n        }\n    }\n}\n\n```\n\nthe error is :\nThe name 'zipfile' does not exist in the current context\nHow I can solve it ?",
    "answer": "<!-- Version here correct at time of writing, but please check for latest -->\n<PackageReference Include=\"System.IO.Compression.ZipFile\" Version=\"4.3.0\" />\n\nYou need an extra reference for this; the most convenient way to do this is via the NuGet package System.IO.Compression.ZipFile\n\nIf you are working on .NET Framework without NuGet, you need to add a dll reference to the assembly, \"System.IO.Compression.FileSystem.dll\" - and ensure you are using at least .NET 4.5 (since it doesn't exist in earlier frameworks).\nFor info, you can find the assembly and .NET version(s) from MSDN"
  },
  {
    "question": "How to download/checkout a project from Google Code in Windows? How do I download a ZIP file of an entire project from Google Code when there are no prepared downloads available? \nThis is what I see on the checkout page:\n\nCommand-line access\n  Use this command to anonymously check out the latest project source code:\n\n\n```\nsvn checkout http://myproject.googlecode.com/svn/trunk/ myproject-read-only \n\n```\n\nBut I'm working on Windows and I don't have the svn binaries ... do I need these?\nI can access individual source code file or view the Subversion HTML pages, but that just allows me to access source code files one-by-one.",
    "answer": "If you don't want to install anything but do want to download an SVN or GIT repository, then you can use this: http://downloadsvn.codeplex.com/\nI have nothing to do with this project, but I just used it now and it saved me a few minutes. Maybe it will help someone."
  },
  {
    "question": "Pairs from single list Often enough, I've found the need to process a list by pairs. I was wondering which would be the pythonic and efficient way to do it, and found this on Google:\n\n```\npairs = zip(t[::2], t[1::2])\n\n```\n\nI thought that was pythonic enough, but after a recent discussion involving idioms versus efficiency, I decided to do some tests:\n\n```\nimport time\nfrom itertools import islice, izip\n\ndef pairs_1(t):\n    return zip(t[::2], t[1::2]) \n\ndef pairs_2(t):\n    return izip(t[::2], t[1::2]) \n\ndef pairs_3(t):\n    return izip(islice(t,None,None,2), islice(t,1,None,2))\n\nA = range(10000)\nB = xrange(len(A))\n\ndef pairs_4(t):\n    # ignore value of t!\n    t = B\n    return izip(islice(t,None,None,2), islice(t,1,None,2))\n\nfor f in pairs_1, pairs_2, pairs_3, pairs_4:\n    # time the pairing\n    s = time.time()\n    for i in range(1000):\n        p = f(A)\n    t1 = time.time() - s\n\n    # time using the pairs\n    s = time.time()\n    for i in range(1000):\n        p = f(A)\n        for a, b in p:\n            pass\n    t2 = time.time() - s\n    print t1, t2, t2-t1\n\n```\n\nThese were the results on my computer:\n\n```\n1.48668909073 2.63187503815 1.14518594742\n0.105381965637 1.35109519958 1.24571323395\n0.00257992744446 1.46182489395 1.45924496651\n0.00251388549805 1.70076990128 1.69825601578\n\n```\n\nIf I'm interpreting them correctly, that should mean that the implementation of lists, list indexing, and list slicing in Python is very efficient. It's a result both comforting and unexpected.\nIs there another, \"better\" way of traversing a list in pairs?\nNote that if the list has an odd number of elements then the last one will not be in any of the pairs. \nWhich would be the right way to ensure that all elements are included?\nI added these two suggestions from the answers to the tests:\n\n```\ndef pairwise(t):\n    it = iter(t)\n    return izip(it, it)\n\ndef chunkwise(t, size=2):\n    it = iter(t)\n    return izip(*[it]*size)\n\n```\n\nThese are the results:  \n\n```\n0.00159502029419 1.25745987892 1.25586485863\n0.00222492218018 1.23795199394 1.23572707176\n\n```\n\nResults so far\nMost pythonic and very efficient:\n\n```\npairs = izip(t[::2], t[1::2])\n\n```\n\nMost efficient and very pythonic:\n\n```\npairs = izip(*[iter(t)]*2)\n\n```\n\nIt took me a moment to grok that the first answer uses two iterators while the second uses a single one.\nTo deal with sequences with an odd number of elements, the suggestion has been to augment the original sequence adding one element (None) that gets paired with the previous last element, something that can be achieved with itertools.izip_longest().\nFinally\nNote that, in Python 3.x, zip() behaves as itertools.izip(), and itertools.izip()  is gone.",
    "answer": "def pairwise(t):\nit = iter(t)\nreturn zip(it,it)\n\ndef chunkwise(t, size=2):\nit = iter(t)\nreturn zip(*[it]*size)\nfrom itertools import izip_longest\ndef blockwise(t, size=2, fillvalue=None):\nit = iter(t)\nreturn izip_longest(*[it]*size, fillvalue=fillvalue)\nfrom itertools import izip as zip\n\nMy favorite way to do it:\n\nWhen you want to pair all elements you obviously might need a fillvalue:\n\nWith Python 3, itertools.izip is now simply zip .. to work with an older Python, use"
  },
  {
    "question": "How to [recursively] Zip a directory in PHP? Directory is something like:\n\n```\nhome/\n    file1.html\n    file2.html\nAnother_Dir/\n    file8.html\n    Sub_Dir/\n        file19.html\n\n```\n\nI am using the same PHP Zip class used in PHPMyAdmin http://trac.seagullproject.org/browser/branches/0.6-bugfix/lib/other/Zip.php . I'm not sure how to zip a directory rather than just a file. Here's what I have so far:\n\n```\n$aFiles = $this->da->getDirTree($target);\n/* $aFiles is something like, path => filetime\nArray\n(\n    [home] => \n    [home/file1.html] => 1251280379\n    [home/file2.html] => 1251280377\n    etc...\n)\n\n*/\n$zip = & new Zip();\nforeach( $aFiles as $fileLocation => $time ){\n    $file = $target . \"/\" . $fileLocation;\n    if ( is_file($file) ){\n        $buffer = file_get_contents($file);\n        $zip->addFile($buffer, $fileLocation);\n    }\n}\nTHEN_SOME_PHP_CLASS::toDownloadData($zip); // this bit works ok\n\n```\n\nbut when I try to unzip the corresponding downloaded zip file I get \"operation not permitted\"\nThis error only happens when I try to unzip on my mac, when I unzip through the command line the file unzips ok. Do I need to send a specific content type on download, currently 'application/zip'",
    "answer": "function Zip($source, $destination)\n{\nif (!extension_loaded('zip') || !file_exists($source)) {\nreturn false;\n}\nzip = new ZipArchive();\nif (!$zip->open($destination, ZIPARCHIVE::CREATE)) {\nreturn false;\n}\nsource = str_replace('\\\\', '/', realpath($source));\n\nif (is_dir($source) === true)\n{\nfiles = new RecursiveIteratorIterator(new RecursiveDirectoryIterator($source), RecursiveIteratorIterator::SELF_FIRST);\n\nforeach ($files as $file)\n{\nfile = str_replace('\\\\', '/', $file);\n\n// Ignore \".\" and \"..\" folders\nif( in_array(substr($file, strrpos($file, '/')+1), array('.', '..')) )\ncontinue;\nfile = realpath($file);\n\nif (is_dir($file) === true)\n{\nzip->addEmptyDir(str_replace($source . '/', '', $file . '/'));\n}\nelse if (is_file($file) === true)\n{\nzip->addFromString(str_replace($source . '/', '', $file), file_get_contents($file));\n}\n}\n}\nelse if (is_file($source) === true)\n{\nzip->addFromString(basename($source), file_get_contents($source));\n}\n\nreturn $zip->close();\n}\nZip('/folder/to/compress/', './compressed.zip');\n\nHere is a simple function that can compress any file or directory recursively, only needs the zip extension to be loaded.\n\nCall it like this:"
  },
  {
    "question": "How to read data from a zip file without having to unzip the entire file Is there anyway in .Net (C#) to extract data from a zip file without decompressing the complete file?\nI possibly want to extract data (file) from the start of a zip file if the compression algorithm compress the file used was in a deterministic order.",
    "answer": "using (ZipFile zip = ZipFile.Read(ExistingZipFile))\n{\nZipEntry e = zip[\"MyReport.doc\"];\ne.Extract(OutputStream);\n}\nusing (ZipFile zip = ZipFile.Read(ExistingZipFile))\n{\nforeach (ZipEntry e in zip)\n{\nif (header)\n{\nSystem.Console.WriteLine(\"Zipfile: {0}\", zip.Name);\nif ((zip.Comment != null) && (zip.Comment != \"\"))\nSystem.Console.WriteLine(\"Comment: {0}\", zip.Comment);\nSystem.Console.WriteLine(\"\\n{1,-22} {2,8}  {3,5}   {4,8}  {5,3} {0}\",\n\"Filename\", \"Modified\", \"Size\", \"Ratio\", \"Packed\", \"pw?\");\nSystem.Console.WriteLine(new System.String('-', 72));\nheader = false;\n}\nSystem.Console.WriteLine(\"{1,-22} {2,8} {3,5:F0}%   {4,8}  {5,3} {0}\",\ne.FileName,\ne.LastModified.ToString(\"yyyy-MM-dd HH:mm:ss\"),\ne.UncompressedSize,\ne.CompressionRatio,\ne.CompressedSize,\n(e.UsesEncryption) ? \"Y\" : \"N\");\n\n}\n}\n\nDotNetZip is your friend here.\nAs easy as:\n\n(you can also extract to a file or other destinations).\nReading the zip file's table of contents is as easy as:\n\nEdited To Note: DotNetZip used to live at Codeplex. Codeplex has been shut down. The old archive is still available at Codeplex. It looks like the code has migrated to Github:\n\nhttps://github.com/DinoChiesa/DotNetZip. Looks to be the original author's repo.\nhttps://github.com/haf/DotNetZip.Semverd. This looks to be the currently maintained version. It's also packaged up an available via Nuget at https://www.nuget.org/packages/DotNetZip/"
  },
  {
    "question": "Zip including hidden files In Linux I can zip all(except hidden files) in current directory by doing:\n\n```\nzip 1.zip *\n\n```\n\nBut how do I include the hidden files?",
    "answer": "zip yourfile.zip sourcedir/* .*\nzip 1.zip * .[^.]*\n\nEDIT: The correct way is zip -r 1.zip .\nThe commands shown in my previous answer below are incorrect because they also include the parent directory.\n\nHave you tried this:\n\nor you in your case\n\nIt should include all hidden files also."
  },
  {
    "question": "get file list of files contained in a zip file I have a zip archive: my_zip.zip. Inside it is one txt file, the name of which I do not know. I was taking a look at Python's zipfile module ( http://docs.python.org/library/zipfile.html ), but couldn't make too much sense of what I'm trying to do.\nHow would I do the equivalent of 'double-clicking' the zip file to get the txt file and then use the txt file so I can do:\n\n```\n>>> f = open('my_txt_file.txt','r')\n>>> contents = f.read()\n\n```",
    "answer": "What you need is ZipFile.namelist() that will give you a list of all the contents of the archive, you can then do a zip.open('filename_you_discover') to get the contents of that file."
  },
  {
    "question": "How can I list the files in a zip archive without decompressing it? How can I get the equivalent of an ls of a .zip file (not gzip), without decompressing it, from the command shell? That is, how can I list the different files compressed within my .zip archive?",
    "answer": "als archive_name\n\nPerreal's answer is right, but I recommend installing atool (look for it in your distribution's package manager). Then, for any kind of archive file, bzip2, gzip, tar... you have just one command to remember :"
  },
  {
    "question": "How to eliminate absolute path in zip archive if absolute paths for files are provided? I have two files in two different directories, one is '/home/test/first/first.pdf', the other is '/home/text/second/second.pdf'. I use following code to compress them:\n\n```\nimport zipfile, StringIO\nbuffer = StringIO.StringIO()\nfirst_path = '/home/test/first/first.pdf'\nsecond_path = '/home/text/second/second.pdf'\nzip = zipfile.ZipFile(buffer, 'w')\nzip.write(first_path)\nzip.write(second_path)\nzip.close()\n\n```\n\nAfter I open the zip file that I created, I have a home folder in it, then there are two sub-folders in it, first and second, then the pdf files. I don't know how to include only two pdf files instead of having full path zipped into the zip archive. I hope I make my question clear, please help.",
    "answer": "from os.path import basename\n...\nzip.write(first_path, basename(first_path))\nzip.write(second_path, basename(second_path))\nzip.close()\n\nThe zipfile write() method supports an extra argument (arcname) which is the archive name to be stored in the zip file, so you would only need to change your code with:\n\nWhen you have some spare time reading the documentation for zipfile will be helpful."
  },
  {
    "question": "Can I zip more than two lists together in Scala? Given the following Scala List:\n\n```\nval l = List(List(\"a1\", \"b1\", \"c1\"), List(\"a2\", \"b2\", \"c2\"), List(\"a3\", \"b3\", \"c3\"))\n\n```\n\nHow can I get:\n\n```\nList((\"a1\", \"a2\", \"a3\"), (\"b1\", \"b2\", \"b3\"), (\"c1\", \"c2\", \"c3\"))\n\n```\n\nSince zip can only be used to combine two Lists, I think you would need to iterate/reduce the main List somehow. Not surprisingly, the following doesn't work:\n\n```\nscala> l reduceLeft ((a, b) => a zip b)\n<console>:6: error: type mismatch;\n found   : List[(String, String)]\n required: List[String]\n       l reduceLeft ((a, b) => a zip b)\n\n```\n\nAny suggestions one how to do this? I think I'm missing a very simple way to do it.\nUpdate: I'm looking for a solution that can take a List of N Lists with M elements each and create a List of M TupleNs.\nUpdate 2: As it turns out it is better for my specific use-case to have a list of lists, rather than a list of tuples, so I am accepting pumpkin's response. It is also the simplest, as it uses a native method.",
    "answer": "I don't believe it's possible to generate a list of tuples of arbitrary size, but the transpose function does exactly what you need if you don't mind getting a list of lists instead."
  },
  {
    "question": "Can Git treat ZIP files as directories and files inside the ZIP as blobs? The scenario\nImagine I am forced to work with some of my files always stored inside .zip files. Some of the files inside the ZIP file are small text files and change often, while others are larger but luckily rather static (e.g. images).\nIf I want to place these ZIP files inside a Git repository, each ZIP is treated as a blob, so whenever I commit the repository grows by the size of the ZIP file... even if only one small text file inside changed!\nWhy this is realistic\nMicrosoft Word 2007/2010 .docx and Excel .xlsx files are ZIP files...\nWhat I want\nIs there, by any chance, a way to tell Git to not treat ZIP files as files, but rather as directories and treat their contents as files?\nThe advantages\n\nmuch smaller repository size, i.e. quicker transfer/backup\nDisplay changes with Git to ZIP files would automagically work\n\nBut it couldn't work, you say?\nI realize that without extra metadata this would lead to some amount of ambiguity: on a git checkout Git would have to decide whether to create foo.zip/bar.txt as a file in a regular directory or a ZIP file. However, this could be solved through configuration options, I would think.\nTwo ideas how it could be done (if it doesn't exist yet)\n\nusing a library such as minizip or IO::Compress::Zip inside Git\nsomehow adding a filesystem layer such that Git actually sees ZIP files as directories to start with",
    "answer": "This doesn't exist, but it could easily exist in the current framework. Just as Git acts differently with displaying binary or ASCII files when performing a diff, it could be told to offer special treatment to certain file types through the configuration interface.\nIf you don't want to change the code base (although this is kind of a cool idea you've got), you could also script it for yourself by using pre-commit and post-checkout hooks to unzip and store the files, then return them to their .zip state on checkout. You would have to restrict actions to only those files blobs / indexes that are specified by git add.\nEither way is a bit of work -- it's just a question of whether the other Git commands are aware of what's going on and play nicely."
  },
  {
    "question": "Read Content from Files which are inside Zip file I am trying to create a simple java program which reads and extracts the content from the file(s) inside zip file. Zip file contains 3 files (txt, pdf, docx). I need to read the contents of all these files and I am using Apache Tika for this purpose. \nCan somebody help me out here to achieve the functionality.  I have tried this so far but no success\nCode Snippet\n\n```\npublic class SampleZipExtract {\n\n\n    public static void main(String[] args) {\n\n        List<String> tempString = new ArrayList<String>();\n        StringBuffer sbf = new StringBuffer();\n\n        File file = new File(\"C:\\\\Users\\\\xxx\\\\Desktop\\\\abc.zip\");\n        InputStream input;\n        try {\n\n          input = new FileInputStream(file);\n          ZipInputStream zip = new ZipInputStream(input);\n          ZipEntry entry = zip.getNextEntry();\n\n          BodyContentHandler textHandler = new BodyContentHandler();\n          Metadata metadata = new Metadata();\n\n          Parser parser = new AutoDetectParser();\n\n          while (entry!= null){\n\n                if(entry.getName().endsWith(\".txt\") || \n                           entry.getName().endsWith(\".pdf\")||\n                           entry.getName().endsWith(\".docx\")){\n              System.out.println(\"entry=\" + entry.getName() + \" \" + entry.getSize());\n                     parser.parse(input, textHandler, metadata, new ParseContext());\n                     tempString.add(textHandler.toString());\n                }\n           }\n           zip.close();\n           input.close();\n\n           for (String text : tempString) {\n           System.out.println(\"Apache Tika - Converted input string : \" + text);\n           sbf.append(text);\n           System.out.println(\"Final text from all the three files \" + sbf.toString());\n        } catch (FileNotFoundException e) {\n            // TODO Auto-generated catch block\n            e.printStackTrace();\n        } catch (IOException e) {\n            // TODO Auto-generated catch block\n            e.printStackTrace();\n        } catch (SAXException e) {\n            // TODO Auto-generated catch block\n            e.printStackTrace();\n        } catch (TikaException e) {\n            // TODO Auto-generated catch block\n            e.printStackTrace();\n        }\n    }\n}\n\n```",
    "answer": "public static void main(String[] args) throws IOException {\nZipFile zipFile = new ZipFile(\"C:/test.zip\");\n\nEnumeration<? extends ZipEntry> entries = zipFile.entries();\n\nwhile(entries.hasMoreElements()){\nZipEntry entry = entries.nextElement();\nInputStream stream = zipFile.getInputStream(entry);\n}\n}\n\nIf you're wondering how to get the file content from each ZipEntry it's actually quite simple. Here's a sample code:\n\nOnce you have the InputStream you can read it however you want."
  },
  {
    "question": "Extracting a zipfile to memory? How do I extract a zip to memory?\nMy attempt (returning None on .getvalue()):\n\n```\nfrom zipfile import ZipFile\nfrom StringIO import StringIO\n\ndef extract_zip(input_zip):\n    return StringIO(ZipFile(input_zip).extractall())\n\n```",
    "answer": "def extract_zip(input_zip):\ninput_zip=ZipFile(input_zip)\nreturn {name: input_zip.read(name) for name in input_zip.namelist()}\n\nextractall extracts to the file system, so you won't get what you want. To extract a file in memory, use the ZipFile.read() method.\nIf you really need the full content in memory, you could do something like:"
  },
  {
    "question": "Install php-zip on php 5.6 on Ubuntu I can't install php-zip on my Ubuntu VM with the last php5.6.\nCan't find find anything clear on it.\nI still get that Apache error : \n\n```\nPHP Fatal error:  Class 'ZipArchive' not found in /var/www/uta/system/library/PHPExcel/PHPExcel/Writer/...\n\n```\n\nThanks",
    "answer": "Try either\n\nsudo apt-get install php-zip or\nsudo apt-get install php5.6-zip\n\nThen, you might have to restart your web server.\n\nsudo service apache2 restart or\nsudo service nginx restart\n\nIf you are installing on centos or fedora OS then use yum in place of apt-get. example:-\nsudo yum install php-zip or\nsudo yum install php5.6-zip and\nsudo service httpd restart"
  },
  {
    "question": "Unzipping files I want to display OpenOffice files, .odt and .odp at client side using a web browser. \nThese files are zipped files. Using Ajax, I can get these files from server but these are zipped files. I have to unzip them using JavaScript, I have tried using inflate.js, http://www.onicos.com/staff/iz/amuse/javascript/expert/inflate.txt, but without success.\nHow can I do this?",
    "answer": "// In my demo, this gets attached to a click event.\n// it instantiates a ZipFile, and provides a callback that is\n// invoked when the zip is read.  This can take a few seconds on a\n// large zip file, so it's asynchronous.\nvar readFile = function(){\n(\"#status\").html(\"<br/>\");\nvar url= $(\"#urlToLoad\").val();\nvar doneReading = function(zip){\nextractEntries(zip);\n};\n\nvar zipFile = new ZipFile(url, doneReading);\n};\n\n// this function extracts the entries from an instantiated zip\nfunction extractEntries(zip){\n('#report').accordion('destroy');\n\n// clear\n(\"#report\").html('');\n\nvar extractCb = function(id) {\n// this callback is invoked with the entry name, and entry text\n// in my demo, the text is just injected into an accordion panel.\nreturn (function(entryName, entryText){\nvar content = entryText.replace(new RegExp( \"\\\\n\", \"g\" ), \"<br/>\");\n(\"#\"+id).html(content);\n(\"#status\").append(\"extract cb, entry(\" + entryName + \")  id(\" + id + \")<br/>\");\n('#report').accordion('destroy');\n('#report').accordion({collapsible:true, active:false});\n});\n}\n\n// for each entry in the zip, extract it.\nfor (var i=0; i<zip.entries.length;  i++) {\nvar entry = zip.entries[i];\n\nvar entryInfo = \"<h4><a>\" + entry.name + \"</a></h4>\\n<div>\";\n\n// contrive an id for the entry, make it unique\nvar randomId = \"id-\"+ Math.floor((Math.random() * 1000000000));\n\nentryInfo += \"<span class='inputDiv'><h4>Content:</h4><span id='\" + randomId +\n\"'></span></span></div>\\n\";\n\n// insert the info for one entry as the last child within the report div\n(\"#report\").append(entryInfo);\n\n// extract asynchronously\nentry.extract(extractCb(randomId));\n}\n}\n\nI wrote an unzipper in Javascript.  It works.\nIt relies on Andy G.P. Na's binary file reader and some RFC1951 inflate logic from notmasteryet.  I added the ZipFile class.\nworking example:\nhttp://cheeso.members.winisp.net/Unzip-Example.htm (dead link)\nThe source:\nhttp://cheeso.members.winisp.net/srcview.aspx?dir=js-unzip (dead link)\nNB: the links are dead; I'll find a new host soon.\nIncluded in the source is a ZipFile.htm demonstration page, and 3 distinct scripts, one for the zipfile class, one for the inflate class, and one for a binary file reader class. The demo also depends on jQuery and jQuery UI.  If you just download the js-zip.zip file, all of the necessary source is there.\n\nHere's what the application code looks like in Javascript:\n\nThe demo works in a couple of steps:  The readFile fn is triggered by a click, and instantiates a ZipFile object, which reads the zip file. There's an asynchronous callback for when the read completes (usually happens in less than a second for reasonably sized zips) - in this demo the callback is held in the doneReading local variable, which simply calls extractEntries, which\njust blindly unzips all the content of the provided zip file.  In a real app you would probably choose some of the entries to extract (allow the user to select, or choose one or more entries programmatically, etc).\nThe extractEntries fn iterates over all entries, and calls extract() on each one, passing a callback.  Decompression of an entry takes time, maybe 1s or more for each entry in the zipfile, which means asynchrony is appropriate. The extract callback simply adds the extracted content to an jQuery accordion on the page. If the content is binary, then it gets formatted as such (not shown).\n\nIt works, but I think that the utility is somewhat limited.\nFor one thing: It's very slow. Takes ~4 seconds to unzip the 140k AppNote.txt file from PKWare. The same uncompress can be done in less than .5s in a .NET program. EDIT: The Javascript ZipFile unpacks considerably faster than this now, in IE9 and in Chrome. It is still slower than a compiled program, but it is plenty fast for normal browser usage.\nFor another: it does not do streaming. It basically slurps in the entire contents of the zipfile into memory. In a \"real\" programming environment you could read in only the metadata of a zip file (say, 64 bytes per entry) and then read and decompress the other data as desired.  There's no way to do IO like that in javascript, as far as I know, therefore the only option is to read the entire zip into memory and do random access in it.  This means it will place unreasonable demands on system memory for large zip files. Not so much a problem for a smaller zip file.\nAlso: It doesn't handle the \"general case\" zip file - there are lots of zip options that I didn't bother to implement in the unzipper - like ZIP encryption, WinZip encryption, zip64, UTF-8 encoded filenames, and so on. (EDIT - it handles UTF-8 encoded filenames now). The ZipFile class handles the basics, though. Some of these things would not be hard to implement.  I have an AES encryption class in Javascript; that could be integrated to support encryption. Supporting Zip64 would probably useless for most users of Javascript, as it is intended to support >4gb zipfiles - don't need to extract those in a browser.\nI also did not test the case for unzipping binary content. Right now it unzips text.  If you have a zipped binary file, you'd need to edit the ZipFile class to handle it properly. I didn't figure out how to do that cleanly.   It does binary files now, too.\n\nEDIT - I updated the JS unzip library and demo.  It now does binary files, in addition to text. I've made it more resilient and more general - you can now specify the encoding to use when reading text files.  Also the demo is expanded - it shows unzipping an XLSX file in the browser, among other things.\nSo, while I think it is of limited utility and interest, it works. I guess it would work in Node.js."
  },
  {
    "question": "how do I zip a whole folder tree in unix, but only certain files? I've been stuck on a little unix command line problem.\nI have a website folder (4gb) I need to grab a copy of, but just the .php, .html, .js and .css files (which is only a couple hundred kb). \nI'm thinking ideally, there is a way to zip or tar a whole folder but only grabbing certain file extensions, while retaining subfolder structures. Is this possible and if so, how?\nI did try doing a whole zip, then going through and excluding certain files but it seemed a bit excessive.\nI'm kinda new to unix.\nAny ideas would be greatly appreciated.",
    "answer": "zip -R foo.zip '*.php' '*.html' '*.js' '*.css'\nzip -r foo.zip website_folder -i '*.php' '*.html' '*.js' '*.css'\n\nSwitch into the website folder, then run\n\nYou can also run this from outside the website folder:"
  },
  {
    "question": "Why is *.tar.gz still much more common than *.tar.xz? Whenever I see some source packages or binaries which are compressed with GZip I wonder if there are still reasons to favor gz over xz (excluding time travel to 2000), the savings of the LZMA compression algorithm are substantial and decompressions isn't magnitudes worse than gzip.",
    "answer": "\"Lowest Common Denominator\". The extra space saved is rarely worth the loss of interoperability. Many embedded Linux systems have gzip, but not xz. Many old system as well. Gnu Tar which is the industry standard supports flags -z to process through gzip, but many  systems, especially based around Busybox, don't support the -J flag for xz, meaning it requires 2-step operation (and a lot of extra diskspace for uncompressed .tar unless you use the syntax of |tar xf - - which many people don't know about.) Also, uncompressing the full filesystem of some 10MB from tar.gz on a small embedded ARM takes some 2 minutes and isn't really a problem. No clue about xz but bzip2 takes around 10-15 minutes. Definitely not worth the bandwidth saved."
  },
  {
    "question": "how to zip a folder itself using java Suppose I have the following directory structure.  \n\n```\nD:\\reports\\january\\\n\n```\n\nInside january there are suppose two excel files say A.xls and B.xls. There are many places where it has been written about how to zip files using java.util.zip. But I want to zip the january folder itself inside reports folder so that both january and january.zip will be present inside reports. (That means when I unzip the january.zip file I should get the january folder).  \nCan anyone please provide me the code to do this using java.util.zip. Please let me know whether this can be more easily done by using other libraries.  \nThanks a lot...",
    "answer": "//Import all needed packages\npackage general;\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.zip.ZipEntry;\nimport java.util.zip.ZipOutputStream;\n\npublic class ZipUtils {\n\nprivate List <String> fileList;\nprivate static final String OUTPUT_ZIP_FILE = \"Folder.zip\";\nprivate static final String SOURCE_FOLDER = \"D:\\\\Reports\"; // SourceFolder path\n\npublic ZipUtils() {\nfileList = new ArrayList < String > ();\n}\n\npublic static void main(String[] args) {\nZipUtils appZip = new ZipUtils();\nappZip.generateFileList(new File(SOURCE_FOLDER));\nappZip.zipIt(OUTPUT_ZIP_FILE);\n}\n\npublic void zipIt(String zipFile) {\nbyte[] buffer = new byte[1024];\nString source = new File(SOURCE_FOLDER).getName();\nFileOutputStream fos = null;\nZipOutputStream zos = null;\ntry {\nfos = new FileOutputStream(zipFile);\nzos = new ZipOutputStream(fos);\n\nSystem.out.println(\"Output to Zip : \" + zipFile);\nFileInputStream in = null;\n\nfor (String file: this.fileList) {\nSystem.out.println(\"File Added : \" + file);\nZipEntry ze = new ZipEntry(source + File.separator + file);\nzos.putNextEntry(ze);\ntry {\nin = new FileInputStream(SOURCE_FOLDER + File.separator + file);\nint len;\nwhile ((len = in .read(buffer)) > 0) {\nzos.write(buffer, 0, len);\n}\n} finally {\nin.close();\n}\n}\n\nzos.closeEntry();\nSystem.out.println(\"Folder successfully compressed\");\n\n} catch (IOException ex) {\nex.printStackTrace();\n} finally {\ntry {\nzos.close();\n} catch (IOException e) {\ne.printStackTrace();\n}\n}\n}\n\npublic void generateFileList(File node) {\n// add file only\nif (node.isFile()) {\nfileList.add(generateZipEntry(node.toString()));\n}\n\nif (node.isDirectory()) {\nString[] subNote = node.list();\nfor (String filename: subNote) {\ngenerateFileList(new File(node, filename));\n}\n}\n}\n\nprivate String generateZipEntry(String file) {\nreturn file.substring(SOURCE_FOLDER.length() + 1, file.length());\n}\n}\n\nIt can be easily solved by package java.util.Zip no need any extra Jar files\nJust copy the following code and run it with your IDE\n\nRefer mkyong..I changed the code for the requirement of current question"
  },
  {
    "question": "How do I attach the Android Support Library source in Eclipse? Having the source code attached to external libraries is awesome. Where do I find the source code for the v4 support package? Preferably, it would be a zip file which could be easily attached to the android-support-v4.jar in Eclipse.",
    "answer": "src=c:/apps/adt-bundle-windows-64bit/sdk/extras/android/support/v4/src\n\nI just want to add yet another method of attaching sources for the support library. It requires ADT in version 20 or later. Supposedly this method works for all JARs for which setting source/javadoc location is disabled by the container. Here's what you need to do:\n\nThe android-support-v4.jar library lies in the libs directory of your project. In that same directory create a regular Java properties file named exactly like the JAR in question, but with appended .properties extension. So, for our support library it'll be: android-support-v4.jar.properties.\nOpen created properties file and set value of property named src to the location where sources for that library can be found. Your file should have one line like:\n\nSave the file.\nClose and re-open your android project.\nTry browsing to one of the support classes. The source attachment should work now.\n\nWorked perfectly in my case.\nOne thing to note: if src is not an absolute path, it will be resolved starting in the parent directory of the JAR file. Taking support library as an example - if src=support/src, ADT will assume that the class sources are located in libs/support/src.\nShort description of this feature written by its author can be found here.\nIf anyone is interested in how exactly this .properties file is processed, I recommend reading patch set #4, esp. changes in eclipse/plugins/com.android.ide.eclipse.adt/src/com/android/ide/eclipse/adt/ internal/project/LibraryClasspathContainerInitializer.java :)\nEdit\nPlease also see a fine comment by WindRider about adding sources properly in a project with multiple referenced libraries."
  },
  {
    "question": "Unzip All Files In A Directory I have a directory of ZIP files (created on a Windows machine). I can manually unzip them using unzip filename, but how can I unzip all the ZIP files in the current folder via the shell?\nUsing Ubuntu Linux Server.",
    "answer": "This works in bash, according to this link:\n\nunzip \\*.zip"
  },
  {
    "question": "Unzip files programmatically in .net I am trying to programatically unzip a zipped file.\nI have tried using the System.IO.Compression.GZipStream class in .NET, but when my app runs (actually a unit test) I get this exception: \n\nSystem.IO.InvalidDataException: The magic number in GZip header is not correct. Make sure you are passing in a GZip stream..\n\nI now realize that a .zip file is not the same as a .gz file, and that GZip is not the same as Zip. \nHowever, since I'm able to extract the file by manually double clicking the zipped file and then clicking the \"Extract all files\"-button, I think there should be a way of doing that in code as well. \nTherefore I've tried to use Process.Start() with the path to the zipped file as input. This causes my app to open a Window showing the contents in the zipped file. That's all fine, but the app will be installed on a server with none around to click the \"Extract all files\"-button. \nSo, how do I get my app to extract the files in the zipped files?\nOr is there another way to do it? I prefer doing it in code, without downloading any third party libraries or apps; the security department ain't too fancy about that...",
    "answer": "We have used SharpZipLib successfully on many projects. I know it's a third party tool, but source code is included and could provide some insight if you chose to reinvent the wheel here."
  },
  {
    "question": "Unzip a file with php I want to unzip a file and this works fine\n\n```\nsystem('unzip File.zip');\n\n```\n\nBut I need to pass in the file name through the URL and can not get it to work, this is what I have. \n\n```\n$master = $_GET[\"master\"];\nsystem('unzip $master.zip'); \n\n```\n\nWhat am I missing? I know it has to be something small and stupid I am overlooking.\nThank you,",
    "answer": "zip = new ZipArchive;\nres = $zip->open('file.zip');\nif ($res === TRUE) {\nzip->extractTo('/myzips/extract_path/');\nzip->close();\necho 'woot!';\n} else {\necho 'doh!';\n}\n// assuming file.zip is in the same directory as the executing script.\nfile = 'file.zip';\n\n// get the absolute path to $file\npath = pathinfo(realpath($file), PATHINFO_DIRNAME);\nzip = new ZipArchive;\nres = $zip->open($file);\nif ($res === TRUE) {\n// extract it to the path we determined above\nzip->extractTo($path);\nzip->close();\necho \"WOOT! $file extracted to $path\";\n} else {\necho \"Doh! I couldn't open $file\";\n}\n\nI can only assume your code came from a tutorial somewhere online? In that case, good job trying to figure it out by yourself. On the other hand, the fact that this code could actually be published online somewhere as the correct way to unzip a file is a bit frightening.\nPHP has built-in extensions for dealing with compressed files. There should be no need to use system calls for this. ZipArchivedocs is one option.\n\nAlso, as others have commented, $HTTP_GET_VARS has been deprecated since version 4.1 ... which was a reeeeeally long time ago. Don't use it. Use the $_GET superglobal instead.\nFinally, be very careful about accepting whatever input is passed to a script via a $_GET variable.\nALWAYS SANITIZE USER INPUT.\n\nUPDATE\nAs per your comment, the best way to extract the zip file into the same directory in which it resides is to determine the hard path to the file and extract it specifically to that location. So, you could do:"
  },
  {
    "question": "Downloading and unzipping a .zip file without writing to disk I have managed to get my first python script to work which downloads a list of .ZIP files from a URL and then proceeds to extract the ZIP files and writes them to disk.\nI am now at a loss to achieve the next step. \nMy primary goal is to download and extract the zip file and pass the contents (CSV data) via a TCP stream. I would prefer not to actually write any of the zip or extracted files to disk if I could get away with it.\nHere is my current script which works but unfortunately has to write the files to disk.\n\n```\nimport urllib, urllister\nimport zipfile\nimport urllib2\nimport os\nimport time\nimport pickle\n\n# check for extraction directories existence\nif not os.path.isdir('downloaded'):\n    os.makedirs('downloaded')\n\nif not os.path.isdir('extracted'):\n    os.makedirs('extracted')\n\n# open logfile for downloaded data and save to local variable\nif os.path.isfile('downloaded.pickle'):\n    downloadedLog = pickle.load(open('downloaded.pickle'))\nelse:\n    downloadedLog = {'key':'value'}\n\n# remove entries older than 5 days (to maintain speed)\n\n# path of zip files\nzipFileURL = \"http://www.thewebserver.com/that/contains/a/directory/of/zip/files\"\n\n# retrieve list of URLs from the webservers\nusock = urllib.urlopen(zipFileURL)\nparser = urllister.URLLister()\nparser.feed(usock.read())\nusock.close()\nparser.close()\n\n# only parse urls\nfor url in parser.urls: \n    if \"PUBLIC_P5MIN\" in url:\n\n        # download the file\n        downloadURL = zipFileURL + url\n        outputFilename = \"downloaded/\" + url\n\n        # check if file already exists on disk\n        if url in downloadedLog or os.path.isfile(outputFilename):\n            print \"Skipping \" + downloadURL\n            continue\n\n        print \"Downloading \",downloadURL\n        response = urllib2.urlopen(downloadURL)\n        zippedData = response.read()\n\n        # save data to disk\n        print \"Saving to \",outputFilename\n        output = open(outputFilename,'wb')\n        output.write(zippedData)\n        output.close()\n\n        # extract the data\n        zfobj = zipfile.ZipFile(outputFilename)\n        for name in zfobj.namelist():\n            uncompressed = zfobj.read(name)\n\n            # save uncompressed data to disk\n            outputFilename = \"extracted/\" + name\n            print \"Saving extracted file to \",outputFilename\n            output = open(outputFilename,'wb')\n            output.write(uncompressed)\n            output.close()\n\n            # send data via tcp stream\n\n            # file successfully downloaded and extracted store into local log and filesystem log\n            downloadedLog[url] = time.time();\n            pickle.dump(downloadedLog, open('downloaded.pickle', \"wb\" ))\n\n```",
    "answer": "import zipfile\nfrom StringIO import StringIO\n\nzipdata = StringIO()\nzipdata.write(get_zip_data())\nmyzipfile = zipfile.ZipFile(zipdata)\nfoofile = myzipfile.open('foo.txt')\nprint foofile.read()\nmyzipfile = zipfile.ZipFile(StringIO(get_zip_data()))\nfor name in myzipfile.namelist():\n[ ... ]\nimport zipfile\nfrom io import BytesIO\n\nfilebytes = BytesIO(get_zip_data())\nmyzipfile = zipfile.ZipFile(filebytes)\nfor name in myzipfile.namelist():\n[ ... ]\n\nMy suggestion would be to use a StringIO object. They emulate files, but reside in memory. So you could do something like this:\n\nOr more simply (apologies to Vishal):\n\nIn Python 3 use BytesIO instead of StringIO:"
  },
  {
    "question": "How do you recursively unzip archives in a directory and its subdirectories from the Unix command-line? The unzip command doesn't have an option for recursively unzipping archives.\nIf I have the following directory structure and archives:\n\n```\n\n/Mother/Loving.zip\n/Scurvy/Sea Dogs.zip\n/Scurvy/Cures/Limes.zip\n\n```\n\nAnd I want to unzip all of the archives into directories with the same name as each archive:\n\n```\n\n/Mother/Loving/1.txt\n/Mother/Loving.zip\n/Scurvy/Sea Dogs/2.txt\n/Scurvy/Sea Dogs.zip\n/Scurvy/Cures/Limes/3.txt\n/Scurvy/Cures/Limes.zip\n\n```\n\nWhat command or commands would I issue?\nIt's important that this doesn't choke on filenames that have spaces in them.",
    "answer": "find . -name \"*.zip\" | while read filename; do unzip -o -d \"`dirname \"$filename\"`\" \"$filename\"; done;\nfind . -name \"*.zip\" | xargs -P 5 -I fileName sh -c 'unzip -o -d \"$(dirname \"fileName\")/$(basename -s .zip \"fileName\")\" \"fileName\"'\n\nIf you want to extract the files to the respective folder you can try this\n\nA multi-processed version for systems that can handle high I/O:"
  },
  {
    "question": "Extract files from zip without keeping the structure using python ZipFile? I try to extract all files from .zip containing subfolders in one folder.  I want all the files from subfolders extract in only one folder without keeping the original structure. At the moment, I extract all, move the files to a folder, then remove previous subfolders.  The files with same names are overwrited.\nIs it possible to do it before writing files?\nHere is a structure for example:\n\n```\nmy_zip/file1.txt\nmy_zip/dir1/file2.txt\nmy_zip/dir1/dir2/file3.txt\nmy_zip/dir3/file4.txt\n\n```\n\nAt the end I whish this:\n\n```\nmy_dir/file1.txt\nmy_dir/file2.txt\nmy_dir/file3.txt\nmy_dir/file4.txt\n\n```\n\nWhat can I add to this code ?\n\n```\nimport zipfile\nmy_dir = \"D:\\\\Download\\\\\"\nmy_zip = \"D:\\\\Download\\\\my_file.zip\"\n\nzip_file = zipfile.ZipFile(my_zip, 'r')\nfor files in zip_file.namelist():\n    zip_file.extract(files, my_dir)\nzip_file.close()\n\n```\n\nif I rename files path from zip_file.namelist(), I have this error:\n\n```\nKeyError: \"There is no item named 'file2.txt' in the archive\"\n\n```",
    "answer": "import os\nimport shutil\nimport zipfile\n\nmy_dir = r\"D:\\Download\"\nmy_zip = r\"D:\\Download\\my_file.zip\"\n\nwith zipfile.ZipFile(my_zip) as zip_file:\nfor member in zip_file.namelist():\nfilename = os.path.basename(member)\n\nif not filename:\ncontinue\n\nsource = zip_file.open(member)\ntarget = open(os.path.join(my_dir, filename), \"wb\")\nwith source, target:\nshutil.copyfileobj(source, target)\n\nThis opens file handles of members of the zip archive, extracts the filename and copies it to a target file (that's how ZipFile.extract works, without taking care of subdirectories)."
  },
  {
    "question": "Create a dedicated folder for every zip files in a directory and extract zip files If I choose a zip file and right click \"extract here\" a folder with the zip filename is created and the entire content of the zip file is extracted into it.\nHowever, I would like to convert several zip files via shell.\nBut when I do  \n\n```\nunzip filename.zip\n\n```\n\nthe folder \"filename\" is not created but all the files are extracted into the current directory.\nI have looked at the parameters but there is no such parameter.\nI also tried\n\n```\nfor zipfile in \\*.zip; do mkdir $zipfile; unzip $zipfile -d $zipfile/; done\n\n```\n\nbut the .zip extension of the 2. $zipfile and 4. $zipfile have to be removed with sed.\nIf I do\n\n```\nfor zipfile in \\*.zip; do mkdir sed 's/\\.zip//i' $zipfile; unzip $zipfile -d sed 's/\\.zip//i' $zipfile/; done \n\n```\n\nit is not working.\nHow do I replace the .zip extension of $zipfile properly?\nIs there an easier way than a shell script?",
    "answer": "for f in *.zip; do unzip -d \"${f%*.zip}\" \"$f\"; done\n\n\"extract here\" is merely a feature of whatever unzip wrapper you are using.  unzip will only extract what actually is in the archive.  There is probably no simpler way than a shell script.  But sed, awk etc. are not needed for this if you have a POSIX-compliant shell:\n\n(You MUST NOT escape the * or pathname expansion will not take place.)  Be aware that if the ZIP archive contains a directory, such as with Eclipse archives (which always contain eclipse/), you would end up with ./eclipse*/eclipse/eclipse.ini in any case.  Add echo before unzip for a dry run."
  },
  {
    "question": "How to download and unzip a zip file in memory in NodeJs? I want to download a zip file from the internet and unzip it in memory without saving to a temporary file. How can I do this?\nHere is what I tried:\n\n```\nvar url = 'http://bdn-ak.bloomberg.com/precanned/Comdty_Calendar_Spread_Option_20120428.txt.zip';\n\nvar request = require('request'), fs = require('fs'), zlib = require('zlib');\n\n  request.get(url, function(err, res, file) {\n     if(err) throw err;\n     zlib.unzip(file, function(err, txt) {\n        if(err) throw err;\n        console.log(txt.toString()); //outputs nothing\n     });\n  });\n\n```\n\n[EDIT]\nAs, suggested, I tried using the adm-zip library and I still cannot make this work:\n\n```\nvar ZipEntry = require('adm-zip/zipEntry');\nrequest.get(url, function(err, res, zipFile) {\n        if(err) throw err;\n        var zip = new ZipEntry();\n        zip.setCompressedData(new Buffer(zipFile.toString('utf-8')));\n        var text = zip.getData();\n        console.log(text.toString()); // fails\n    });\n\n```",
    "answer": "npm install adm-zip\nvar file_url = 'http://notepad-plus-plus.org/repository/7.x/7.6/npp.7.6.bin.x64.zip';\n\nvar AdmZip = require('adm-zip');\nvar http = require('http');\n\nhttp.get(file_url, function(res) {\nvar data = [], dataLen = 0;\n\nres.on('data', function(chunk) {\ndata.push(chunk);\ndataLen += chunk.length;\n\n}).on('end', function() {\nvar buf = Buffer.alloc(dataLen);\n\nfor (var i = 0, len = data.length, pos = 0; i < len; i++) {\ndata[i].copy(buf, pos);\npos += data[i].length;\n}\n\nvar zip = new AdmZip(buf);\nvar zipEntries = zip.getEntries();\nconsole.log(zipEntries.length)\n\nfor (var i = 0; i < zipEntries.length; i++) {\nif (zipEntries[i].entryName.match(/readme/))\nconsole.log(zip.readAsText(zipEntries[i]));\n}\n});\n});\nvar file_url = 'https://github.com/mihaifm/linq/releases/download/3.1.1/linq.js-3.1.1.zip';\n\nvar AdmZip = require('adm-zip');\nvar request = require('request');\n\nrequest.get({url: file_url, encoding: null}, (err, res, body) => {\nvar zip = new AdmZip(body);\nvar zipEntries = zip.getEntries();\nconsole.log(zipEntries.length);\n\nzipEntries.forEach((entry) => {\nif (entry.entryName.match(/readme/i))\nconsole.log(zip.readAsText(entry));\n});\n});\n\nYou need a library that can handle buffers. The latest version of adm-zip will do:\n\nMy solution uses the http.get method, since it returns Buffer chunks.\nCode:\n\nThe idea is to create an array of buffers and concatenate them into a new one at the end. This is due to the fact that buffers cannot be resized.\nUpdate\nThis is a simpler solution that uses the request module to obtain the response in a buffer, by setting encoding: null in the options. It also follows redirects and resolves http/https automatically.\n\nThe body of the response is a buffer that can be passed directly to AdmZip, simplifying the whole process."
  },
  {
    "question": "zcat won&#39;t unzip files properly I need to unzip a compressed file on the fly in my program.  It works when I try it on my own linux computer, but for some reason the school computers fail whenever I tell them to do it.  To unzip I'm using the following command:\n\n```\n zcat /file/address/file.tar.gz\n\n```\n\nand get the error:\n\n```\n /file/address/file.tar.gz.Z: No such file or directory\n\n```\n\nI tried looking through the documentation but couldn't find a flag to turn off this odd behavior.",
    "answer": "Your school's system still has the old \"compress\" style utilities rather than the newer GNU \"gzip\" based ones.\nYou need to use gzcat rather than zcat, assuming that it's available."
  },
  {
    "question": "How can unrar a file with python How can I extract a .zip or .rar file using Python?",
    "answer": "pip install patool\nimport patoolib\npatoolib.extract_archive(\"foo_bar.rar\", outdir=\"path here\")\n\nLate, but I wasn't satisfied with any of the answers.\n\nWorks on Windows and linux without any other libraries needed."
  },
  {
    "question": "gradle - download and unzip file from url What would be the proper gradle way of downloading and unzipping the file from url (http)?\nIf possible, I'd like to prevent re-downloading each time I run the task (in ant.get can be achieved by skipexisting: 'true').\nMy current solution would be:\n\n```\ntask foo {\n  ant.get(src: 'http://.../file.zip', dest: 'somedir', skipexisting: 'true')\n  ant.unzip(src: 'somedir' + '/file.zip', dest: 'unpackdir')\n}\n\n```\n\nstill, I'd expect ant-free solution. Any chance to achieve that?",
    "answer": "There isn't currently a Gradle API for downloading from a URL. You can implement this using Ant, Groovy, or, if you do want to benefit from Gradle's dependency resolution/caching features, by pretending it's an Ivy repository with a custom artifact URL. The unzipping can be done in the usual Gradle way (copy method or Copy task)."
  },
  {
    "question": "Simple way to unzip a .zip file using zlib Is there a simple example of how to unzip a .zip file and extract the files to a directory? I am currently using zlib, and while I understand that zlib does not directly deal with zip files, there seems to be several additional things in zlibs's \"contrib\" library. I noticed and read about \"minizip\", and after reading some documents and looking at some of the code, I do not see a simple example of how to unzip a .zip file and extract the files to a directory. \nI would like to find a platform independent way of doing so, but if that is not possible then I need to find a way for windows and mac.",
    "answer": "int main()\n{\n//Open the ZIP archive\nint err = 0;\nzip *z = zip_open(\"foo.zip\", 0, &err);\n\n//Search for the file of given name\nconst char *name = \"file.txt\";\nstruct zip_stat st;\nzip_stat_init(&st);\nzip_stat(z, name, 0, &st);\n\n//Alloc memory for its uncompressed contents\nchar *contents = new char[st.size];\n\n//Read the compressed file\nzip_file *f = zip_fopen(z, name, 0);\nzip_fread(f, contents, st.size);\nzip_fclose(f);\n\n//And close the archive\nzip_close(z);\n\n//Do something with the contents\n//delete allocated memory\ndelete[] contents;\n}\n\nzlib handles the deflate compression/decompression algorithm, but there is more than that in a ZIP file.\nYou can try libzip. It is free, portable and easy to use.\nUPDATE: Here I attach quick'n'dirty example of libzip, with all the error controls ommited:"
  },
  {
    "question": "easy way to unzip file Is there a easy way to unzip file with Go?\nright now my code is:\n\n```\nfunc Unzip(src, dest string) error {\n    r, err := zip.OpenReader(src)\n    if err != nil {\n        return err\n    }\n    defer r.Close()\n\n    for _, f := range r.File {\n        rc, err := f.Open()\n        if err != nil {\n            return err\n        }\n        defer rc.Close()\n\n        path := filepath.Join(dest, f.Name)\n        if f.FileInfo().IsDir() {\n            os.MkdirAll(path, f.Mode())\n        } else {\n            f, err := os.OpenFile(\n                path, os.O_WRONLY|os.O_CREATE|os.O_TRUNC, f.Mode())\n            if err != nil {\n                return err\n            }\n            defer f.Close()\n\n            _, err = io.Copy(f, rc)\n            if err != nil {\n                return err\n            }\n        }\n    }\n\n    return nil\n}\n\n```",
    "answer": "func Unzip(src, dest string) error {\nr, err := zip.OpenReader(src)\nif err != nil {\nreturn err\n}\ndefer func() {\nif err := r.Close(); err != nil {\npanic(err)\n}\n}()\n\nos.MkdirAll(dest, 0755)\n\n// Closure to address file descriptors issue with all the deferred .Close() methods\nextractAndWriteFile := func(f *zip.File) error {\nrc, err := f.Open()\nif err != nil {\nreturn err\n}\ndefer func() {\nif err := rc.Close(); err != nil {\npanic(err)\n}\n}()\n\npath := filepath.Join(dest, f.Name)\n\n// Check for ZipSlip (Directory traversal)\nif !strings.HasPrefix(path, filepath.Clean(dest) + string(os.PathSeparator)) {\nreturn fmt.Errorf(\"illegal file path: %s\", path)\n}\n\nif f.FileInfo().IsDir() {\nos.MkdirAll(path, f.Mode())\n} else {\nos.MkdirAll(filepath.Dir(path), f.Mode())\nf, err := os.OpenFile(path, os.O_WRONLY|os.O_CREATE|os.O_TRUNC, f.Mode())\nif err != nil {\nreturn err\n}\ndefer func() {\nif err := f.Close(); err != nil {\npanic(err)\n}\n}()\n\n_, err = io.Copy(f, rc)\nif err != nil {\nreturn err\n}\n}\nreturn nil\n}\n\nfor _, f := range r.File {\nerr := extractAndWriteFile(f)\nif err != nil {\nreturn err\n}\n}\n\nreturn nil\n}\n\nSlight rework of the OP's solution to create the containing directory dest if it doesn't exist, and to wrap the file extraction/writing in a closure to eliminate stacking of defer .Close() calls per @Nick Craig-Wood's comment:\n\nNote: Updated to include Close() error handling as well (if we're looking for best practices, may as well follow ALL of them)."
  },
  {
    "question": "How to check file size before opening? How can I check the size of a file before I load it into R?\nFor example:\nhttp://math.ucdenver.edu/RTutorial/titanic.txt\nI'd like to use the optimal command to open a file based on the file's size.",
    "answer": "file.info(\"data/ullyses.txt\")\n\nsize isdir mode               mtime               ctime               atime  uid  gid\ndata/ullyses.txt 1573151 FALSE  664 2015-06-01 15:25:55 2015-06-01 15:25:55 2015-06-01 15:25:55 1008 1008\nfile.info(\"data/ullyses.txt\")$size\n[1] 1573151\n\nUse file.info()\n\nThen extract the column called size:"
  },
  {
    "question": "Unzip all zipped files in a folder to that same folder using Python 2.7.5 I would like to write a simple script to iterate through all the files in a folder and unzip those that are zipped (.zip) to that same folder. For this project, I have a folder with nearly 100 zipped .las files and I'm hoping for an easy way to batch unzip them. I tried with following script\n\n```\nimport os, zipfile\n\nfolder = 'D:/GISData/LiDAR/SomeFolder'\nextension = \".zip\"\n\nfor item in os.listdir(folder):\n    if item.endswith(extension):\n        zipfile.ZipFile.extract(item)\n\n```\n\nHowever, when I run the script, I get the following error:\n\n```\nTraceback (most recent call last):\n  File \"D:/GISData/Tools/MO_Tools/BatchUnzip.py\", line 10, in <module>\n    extract = zipfile.ZipFile.extract(item)\nTypeError: unbound method extract() must be called with ZipFile instance as first argument (got str instance instead)\n\n```\n\nI am using the python 2.7.5 interpreter. I looked at the documentation for the zipfile module (https://docs.python.org/2/library/zipfile.html#module-zipfile) and I would like to understand what I'm doing incorrectly. \nI guess in my mind, the process would go something like this:\n\nGet folder name\nLoop through folder and find zip files\nExtract zip files to folder\n\nThanks Marcus, however, when implementing the suggestion, I get another error:\n\n```\nTraceback (most recent call last):\n  File \"D:/GISData/Tools/MO_Tools/BatchUnzip.py\", line 12, in <module>\n    zipfile.ZipFile(item).extract()\n  File \"C:\\Python27\\ArcGIS10.2\\lib\\zipfile.py\", line 752, in __init__\n    self.fp = open(file, modeDict[mode])\nIOError: [Errno 2] No such file or directory: 'JeffCity_0752.las.zip'\n\n```\n\nWhen I use print statements, I can see that the files are in there. For example:\n\n```\nfor item in os.listdir(folder):\n    if item.endswith(extension):\n        print os.path.abspath(item)\n        filename = os.path.basename(item)\n        print filename\n\n```\n\nyields:\n\n```\nD:\\GISData\\Tools\\MO_Tools\\JeffCity_0752.las.zip\nJeffCity_0752.las.zip\nD:\\GISData\\Tools\\MO_Tools\\JeffCity_0753.las.zip\nJeffCity_0753.las.zip\n\n```\n\nAs I understand the documentation, \n\n```\nzipfile.ZipFile(file[, mode[, compression[, allowZip64]]])\n\n```\n\n\nOpen a ZIP file, where file can be either a path to a file (a string) or a file-like object\n\nIt appears to me like everything is present and accounted for. I just don't understand what I'm doing wrong. \nAny suggestions?\nThank You",
    "answer": "import os, zipfile\n\ndir_name = 'C:\\\\SomeDirectory'\nextension = \".zip\"\n\nos.chdir(dir_name) # change directory from working dir to dir with files\n\nfor item in os.listdir(dir_name): # loop through items in dir\nif item.endswith(extension): # check for \".zip\" extension\nfile_name = os.path.abspath(item) # get full path of files\nzip_ref = zipfile.ZipFile(file_name) # create zipfile object\nzip_ref.extractall(dir_name) # extract file to dir\nzip_ref.close() # close file\nos.remove(file_name) # delete zipped file\nos.chdir(dir_name) # change directory from working dir to dir with files\nfile_name = dir_name + \"/\" + item\n\nBelow is the code that worked for me:\n\nLooking back at the code I had amended, the directory was getting confused with the directory of the script.\nThe following also works while not ruining the working directory. First remove the line\n\nThen assign file_name as"
  },
  {
    "question": "How to extract file from zip without maintaining directory structure in Python? I'm trying to extract a specific file from a zip archive using python.\nIn this case, extract an apk's icon from the apk itself.\nI am currently using\n\n```\nwith zipfile.ZipFile('/path/to/my_file.apk') as z:\n    # extract /res/drawable/icon.png from apk to /temp/...\n    z.extract('/res/drawable/icon.png', 'temp/')\n\n```\n\nwhich does work, in my script directory it's creating temp/res/drawable/icon.png which is temp plus the same path as the file is inside the apk.\nWhat I actually want is to end up with temp/icon.png.\nIs there any way of doing this directly with a zip command, or do I need to extract, then move the file, then remove the directories manually?",
    "answer": "import shutil\nimport zipfile\n\nwith zipfile.ZipFile('/path/to/my_file.apk') as z:\nwith z.open('/res/drawable/icon.png') as zf, open('temp/icon.png', 'wb') as f:\nshutil.copyfileobj(zf, f)\nimport zipfile\n\nwith zipfile.ZipFile('/path/to/my_file.apk') as z:\nwith open('temp/icon.png', 'wb') as f:\nf.write(z.read('/res/drawable/icon.png'))\n\nYou can use zipfile.ZipFile.open:\n\nOr use zipfile.ZipFile.read:"
  },
  {
    "question": "How to read from a zip file within zip file in Python? I have a file that I want to read that is itself zipped within a zip archive. For example, parent.zip contains child.zip, which contains child.txt. I am having trouble reading child.zip.  Can anyone correct my code?\nI assume that I need to create child.zip as a file-like object and then open it with a second instance of zipfile, but being new to python my zipfile.ZipFile(zfile.open(name)) is silly.  It raises a zipfile.BadZipfile: \"File is not a zip file\" on (independently validated) child.zip\n\n```\nimport zipfile\nwith zipfile.ZipFile(\"parent.zip\", \"r\") as zfile:\n    for name in zfile.namelist():\n        if re.search(r'\\.zip$', name) is not None:\n            # We have a zip within a zip\n            with **zipfile.ZipFile(zfile.open(name))** as zfile2:\n                    for name2 in zfile2.namelist():\n                        # Now we can extract\n                        logging.info( \"Found internal internal file: \" + name2)\n                        print \"Processing code goes here\"\n\n```",
    "answer": "from io import BytesIO\n\nzfiledata = BytesIO(zfile.read(name))\nwith zipfile.ZipFile(zfiledata) as zfile2:\nimport zipfile\nfrom io import BytesIO\n\nwith zipfile.ZipFile(\"parent.zip\", \"r\") as zfile:\nfor name in zfile.namelist():\nif re.search(r'\\.zip$', name) is not None:\n\nzfiledata = BytesIO(zfile.read(name))\nwith zipfile.ZipFile(zfiledata) as zfile2:\nfor name2 in zfile2.namelist():\n\nlogging.info( \"Found internal internal file: \" + name2)\nprint \"Processing code goes here\"\n\nWhen you use the .open() call on a ZipFile instance you indeed get an open file handle. However, to read a zip file, the ZipFile class needs a little more. It needs to be able to seek on that file, and the object returned by .open() is not seekable in your case. Only Python 3 (3.2 and up) produces a ZipExFile object that supports seeking (provided the underlying file handle for the outer zip file is seekable, and nothing is trying to write to the ZipFile object).\nThe workaround is to read the whole zip entry into memory using .read(), store it in a BytesIO object (an in-memory file that is seekable) and feed that to ZipFile:\n\nor, in the context of your example:"
  },
  {
    "question": "unzip a tar.gz file? I wish to download and open the following tar.gz file in R:\nhttp://s.wordpress.org/resources/survey/wp2011-survey.tar.gz\nIs there a command which can accomplish this?",
    "answer": "fn <- \"http://s.wordpress.org/resources/survey/wp2011-survey.tar.gz\"\ndownload.file(fn,destfile=\"tmp.tar.gz\")\nuntar(\"tmp.tar.gz\",list=TRUE)  ## check contents\nuntar(\"tmp.tar.gz\")\n\nuntar(\"tmp.tar.gz\",files=\"wp2011-survey/anon-data.csv\")\nX <- read.csv(\"wp2011-survey/anon-data.csv\")\nlibrary(archive)\nlibrary(readr)\nread_csv(archive_read(\"tmp.tar.gz\", file = 3), col_types = cols())\n\nTom Wenseleers points out that the archive package can help with this:\n\nand that archive::archive_extract(\"tmp.tar.gz\", files=\"wp2011-survey/anon-data.csv\") is quite a bit faster than the in-built base R untar (especially for large archives) It supports 'tar', 'ZIP', '7-zip', 'RAR', 'CAB', 'gzip', 'bzip2', 'compress', 'lzma' and 'xz' formats."
  },
  {
    "question": "Unpack inner zips in zip with Maven I can unpack zip file via the maven-dependency plugin, but currently I have the problem that inside that zip file other zip files are include and I need to unpack them as well. How can I do this?",
    "answer": "<plugin>\n<groupId>org.apache.maven.plugins</groupId>\n<artifactId>maven-antrun-plugin</artifactId>\n<version>1.6</version>\n<executions>\n<execution>\n<id>prepare</id>\n<phase>validate</phase>\n<configuration>\n<tasks>\n<echo message=\"prepare phase\" />\n<unzip src=\"zips/archive.zip\" dest=\"output/\" />\n<unzip src=\"output/inner.zip\" dest=\"output/\" />\n<unzip dest=\"output\">\n<fileset dir=\"archives\">\n<include name=\"prefix*.zip\" />\n</fileset>\n</unzip>\n</tasks>\n</configuration>\n<goals>\n<goal>run</goal>\n</goals>\n</execution>\n</executions>\n</plugin>\n\nYou can unzip any files using ant task runner plugin:"
  },
  {
    "question": "How to zip and unzip the files? How to zip and unzip the files which are all already in DDMS : data/data/mypackage/files/ I need a simple example for that. I've already search related to zip and unzip. But, no one example available for me. Can anyone tell some example. Advance Thanks.",
    "answer": "public static void zip(String[] files, String zipFile) throws IOException {\nBufferedInputStream origin = null;\nZipOutputStream out = new ZipOutputStream(new BufferedOutputStream(new FileOutputStream(zipFile)));\ntry {\nbyte data[] = new byte[BUFFER_SIZE];\n\nfor (int i = 0; i < files.length; i++) {\nFileInputStream fi = new FileInputStream(files[i]);\norigin = new BufferedInputStream(fi, BUFFER_SIZE);\ntry {\nZipEntry entry = new ZipEntry(files[i].substring(files[i].lastIndexOf(\"/\") + 1));\nout.putNextEntry(entry);\nint count;\nwhile ((count = origin.read(data, 0, BUFFER_SIZE)) != -1) {\nout.write(data, 0, count);\n}\n}\nfinally {\norigin.close();\n}\n}\n}\nfinally {\nout.close();\n}\n}\n\npublic static void unzip(String zipFile, String location) throws IOException {\ntry {\nFile f = new File(location);\nif(!f.isDirectory()) {\nf.mkdirs();\n}\nZipInputStream zin = new ZipInputStream(new FileInputStream(zipFile));\ntry {\nZipEntry ze = null;\nwhile ((ze = zin.getNextEntry()) != null) {\nString path = location + ze.getName();\n\nif (ze.isDirectory()) {\nFile unzipFile = new File(path);\nif(!unzipFile.isDirectory()) {\nunzipFile.mkdirs();\n}\n}\nelse {\nFileOutputStream fout = new FileOutputStream(path, false);\ntry {\nfor (int c = zin.read(); c != -1; c = zin.read()) {\nfout.write(c);\n}\nzin.closeEntry();\n}\nfinally {\nfout.close();\n}\n}\n}\n}\nfinally {\nzin.close();\n}\n}\ncatch (Exception e) {\nLog.e(TAG, \"Unzip exception\", e);\n}\n}\n\nTake a look at java.util.zip.* classes for zip functionality.  I've done some basic zip/unzip code, which I've pasted below.  Hope it helps."
  },
  {
    "question": "How to programmatically extract / unzip a .7z (7-zip) file with R I'm trying to automate the extraction of a number of files compressed with 7-zip.  I need to automate this process, because a) there are many years of data I'd like to unlock and b) I'd like to share my code with others and prevent them from repeating the process by hand.\nI have both WinRAR and 7-zip installed on my computer, and I can individually open these files easily with either program.\nI've looked around at the unzip untar and unz commands, but I don't believe any of them do what I need.\nI don't know anything about compression, but if it makes any difference: each of these files only contains one file and it's just a text file.\nI would strongly prefer a solution that does not require the user to install additional software (like WinRAR or 7-Zip) and execute a command with shell, although I acknowledge this task might be impossible with just R and CRAN packages.  I actually believe running shell.exec on these files with additional parameters might work on computers with WinRAR installed, but again, I'd like to avoid that installation if possible.  :)\nRunning the code below will load the files I am trying to extract -- the .7z files in files.data are what needs to be unlocked.\n\n```\n# create a temporary file and temporary directory, download the file, extract the file to the temporary directory\ntf <- tempfile() ; td <- tempdir()\nfile.path <- \"ftp://ftp.ibge.gov.br/Orcamentos_Familiares/Pesquisa_de_Orcamentos_Familiares_2008_2009/Microdados/Dados.zip\"\ndownload.file( file.path , tf , mode = \"wb\" )\nfiles.data <- unzip( tf , exdir = td )\n\n# how do i unzip ANY of these .7z files?\nfiles.data\n\n```\n\nThanks!!! :)",
    "answer": "If you have 7z executable in your path, you can simple use system command\nsystem('7z e -o <output_dir> <archive_name>')"
  },
  {
    "question": "unzip (zip, tar, tag.gz) files with ruby I want to unzip a lot of zip files. Is there a module or script that checks which format the zip file is and decompresses it?\nThis should work on Linux, I don't care about other OSs.",
    "answer": "require 'rubygems/package'\nrequire 'zlib'\ntar_extract = Gem::Package::TarReader.new(Zlib::GzipReader.open('Path/To/myfile.tar.gz'))\ntar_extract.rewind # The extract has to be rewinded after every iteration\ntar_extract.each do |entry|\nputs entry.full_name\nputs entry.directory?\nputs entry.file?\n\nend\ntar_extract.close\n\nTo extract files from a .tar.gz file you can use the following methods from packages distributed with Ruby:\n\nEach entry of type Gem::Package::TarReader::Entry points to a file or directory within the .tar.gz file.\nSimilar code can be used (replace Reader with Writer) to write files to a .tar.gz file."
  },
  {
    "question": "JavaScript: Decompress / inflate /unzip /ungzip strings I'm looking for JavaScript implementation of string inflating algorithms.\nI want to compress on the server side (Java), and decompress on the client side (JavaScript).\nI've found:\nunzip strings in javascript\nThat one is marked as answered with an answer for different problem. Other answers are also for something else (unzipping files in ZIP format).\nJavaScript inflate implementation (possibly FF 3.6 only)\nThis is closest to what I need. However I'd like to have some alternatives.\nSuggestions?\nThanks, Ondra\nUpdate: \nI have quite a specific use case, please don't answer \"Don't do that in JavaScript.\"\nI am writing an \"offline\" reporting tool (once generated, it's put to a static store) and deflating may save megabytes for a single report. I am constrained by other apps so I can't store it as a zip file.",
    "answer": "Take a look at this Stack Overflow question, the answers there contains references to multiple compressing engines implemented in javascript. Most of these are based on LZ77."
  },
  {
    "question": "How to speed up unzipping time in Java / Android? Unzipping files on android seems to be dreadfully slow. At first I thought this was just the emulator but it appears to be the same on the phone. I've tried different compression levels, and eventually dropped down to storage mode but it still takes ages. \nAnyway, there must be a reason! Does anyone else have this problem? My unzip method looks like this:\n\n```\npublic void unzip()\n{\ntry{\n        FileInputStream fin = new FileInputStream(zipFile);\n        ZipInputStream zin = new ZipInputStream(fin);\n        File rootfolder = new File(directory);\n        rootfolder.mkdirs();\n        ZipEntry ze = null;\n        while ((ze = zin.getNextEntry())!=null){\n\n            if(ze.isDirectory()){\n                dirChecker(ze.getName());\n            }\n            else{\n                FileOutputStream fout = new FileOutputStream(directory+ze.getName());\n\n            for(int c = zin.read();c!=-1;c=zin.read()){\n                fout.write(c);\n            }\n                //Debug.out(\"Closing streams\");\n                zin.closeEntry();\n                fout.close();\n\n        }\n    }\n    zin.close();\n}\ncatch(Exception e){\n            //Debug.out(\"Error trying to unzip file \" + zipFile);\n\n}\n    }\n\n```",
    "answer": "BufferedInputStream in = new BufferedInputStream(zin);\nBufferedOutputStream out = new BufferedOutputStream(fout);\nbyte b[] = new byte[1024];\nint n;\nwhile ((n = in.read(b,0,1024)) >= 0) {\nout.write(b,0,n);\n}\n\nI don't know if unzipping on Android is slow, but copying byte for byte in a loop is surely slowing it down even more. Try using BufferedInputStream and BufferedOutputStream - it might be a bit more complicated, but in my experience it is worth it in the end.\n\nAnd then you can write with something like that:"
  },
  {
    "question": "open failed: EBUSY (Device or resource busy)  I have a strange error in my App.\nIn my app it is possible to download a zipFile, read the content as what it is and also delete it. Its doesn't matter what exactly it is. \nProblem: Only on the Motorola Xoom (version 4.0.4) I can download the file, unzip it, I can read the data and I can delete everything.\nBut if I try to Download the file again and while it unzip the file and copy the files to SD-Card it crashes with the error EBUSY (Device or resource busy).\n\nWhy is it working only the first time?\nWhat means that error? \nWhy i get this error only on the Xoom?\n\nI can't find any solution for that. On all other devices it works fine, no errors or problems. \nLogCat:\n\n```\n07-18 12:27:46.774: E/PrepareMagTask(10057): IOException\n07-18 12:27:46.774: E/PrepareMagTask(10057): java.io.FileNotFoundException: /mnt/sdcard/Android/data/com.xxxxxx.android/files/content/23760/emag.db: open failed: EBUSY (Device or resource busy)\n07-18 12:27:46.774: E/PrepareMagTask(10057):    at libcore.io.IoBridge.open(IoBridge.java:406)\n07-18 12:27:46.774: E/PrepareMagTask(10057):    at java.io.FileOutputStream.<init>(FileOutputStream.java:88)\n07-18 12:27:46.774: E/PrepareMagTask(10057):    at java.io.FileOutputStream.<init>(FileOutputStream.java:73)\n07-18 12:27:46.774: E/PrepareMagTask(10057):    at com.xxxxx.android.util.io.ZipHelper.uncompressEntry(ZipHelper.java:35)\n07-18 12:27:46.774: E/PrepareMagTask(10057):    at com.xxxxx.android.task.PrepareMagTask.doInBackground(PrepareMagTask.java:271)\n07-18 12:27:46.774: E/PrepareMagTask(10057):    at com.xxxxx.android.task.PrepareMagTask.doInBackground(PrepareMagTask.java:1)\n07-18 12:27:46.774: E/PrepareMagTask(10057):    at android.os.AsyncTask$2.call(AsyncTask.java:264)\n07-18 12:27:46.774: E/PrepareMagTask(10057):    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:305)\n07-18 12:27:46.774: E/PrepareMagTask(10057):    at java.util.concurrent.FutureTask.run(FutureTask.java:137)\n07-18 12:27:46.774: E/PrepareMagTask(10057):    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1076)\n07-18 12:27:46.774: E/PrepareMagTask(10057):    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:569)\n07-18 12:27:46.774: E/PrepareMagTask(10057):    at java.lang.Thread.run(Thread.java:856)\n07-18 12:27:46.774: E/PrepareMagTask(10057): Caused by: libcore.io.ErrnoException: open failed: EBUSY (Device or resource busy)\n07-18 12:27:46.774: E/PrepareMagTask(10057):    at libcore.io.Posix.open(Native Method)\n07-18 12:27:46.774: E/PrepareMagTask(10057):    at libcore.io.BlockGuardOs.open(BlockGuardOs.java:110)\n07-18 12:27:46.774: E/PrepareMagTask(10057):    at libcore.io.IoBridge.open(IoBridge.java:390)\n07-18 12:27:46.774: E/PrepareMagTask(10057):    ... 11 more\n\n```\n\nIt crashes at line 35 in my ZipHelper class:\n\n```\nFileHelper.copy(zipFile.getInputStream(entry), new FileOutputStream(outputFile), modify);\n\n```\n\ngetInputStream(entry) ... and I really dont know why? \nIs there a method to wait for the device or recourse, when it is busy?\nThis is happened every time I try to unzip the file, the app tries it 5 time (Downloading -> Unzip) and it crashes every time. \nEDIT: We found out, its not only the Xoom. We also have the error with the Asus Transformer with the version 4.0.4",
    "answer": "final File to = new File(file.getAbsolutePath() + System.currentTimeMillis());\nfile.renameTo(to);\nto.delete();\n\nI have a big Answer!!\nThe Problem comes from the Android System or/and the FAT32 system.\nI can not explain how the system gets the error, it has something to do with deleting files and the FAT32 System.\nBut the solution is really easy:\nBefore you delete a Directory or File: rename it!\nCode to rename:\n\nThat's it, if you rename the folder or file before you delete it, there is no chance for the system to try to open an existing file again or an open file which you want to save again (or something like this)."
  },
  {
    "question": "Unzip Archive with Groovy is there a built-in support in Groovy to handle Zip files (the groovy way)? \nOr do i have to use Java's java.util.zip.ZipFile to process Zip files in Groovy ?",
    "answer": "AFAIK, there isn't a native way. But check out the article Powerful Groovy on how you'd add a .zip(...) method to File, which would be very close to what you're looking for. You'd just need to make an .unzip(...) method."
  },
  {
    "question": "How to unzip specific folder from a .zip with Python I am looking to unzip a particular folder from a .zip in Python:\ne.g. archive.zip contains the folders foo and bar, I want to unzip foo to a specific location, retaining it's folder structure.",
    "answer": "import zipfile\n\narchive = zipfile.ZipFile('archive.zip')\n\nfor file in archive.namelist():\nif file.startswith('foo/'):\narchive.extract(file, 'destination_path')\n\nCheck zipfile module.\nFor your case:"
  },
  {
    "question": "Unzip files in folders automatically [mac os x] I have a folder on my desktop that has around 2500 folders in it, each folder has multiple files in them that are zipped, I can unzip them by manually clicking on them, is there a way to do this automatically through terminal?",
    "answer": "find ./ -name \\*.zip -exec unzip {} \\; maybe?"
  },
  {
    "question": "How do I write a BASH script to download and unzip file on a Mac? I need to create a bash script that will work on a mac.  It needs to download a ZIP file of a site and unzip it to a specific location.\n\nDownload the ZIP file (curl -O)\nUnzip the files to a specific location (unzip filename.zip path/to/save)\nDelete the .zip file\n\nI need to make it so people can double-click the text file on their desktop and it will automatically run in terminal.\nHow do I make it so that the user can double click the icon on the desktop and it will run?  What extension does the file need?",
    "answer": "mkdir /tmp/some_tmp_dir                         && \\\ncd /tmp/some_tmp_dir                            && \\\ncurl -sS http://foo.bar/filename.zip > file.zip && \\\nunzip file.zip                                  && \\\nrm file.zip\n\nOSX uses the same GNU sh/bash as Linux\n\nthe first line #!/bin/sh is so called \"shebang\" line and is mandatory"
  },
  {
    "question": "Unzipping directory structure with python I have a zip file which contains the following directory structure:\n\n```\ndir1\\dir2\\dir3a\ndir1\\dir2\\dir3b\n\n```\n\nI'm trying to unzip it and maintain the directory structure however I get the error:\n\n```\nIOError: [Errno 2] No such file or directory: 'C:\\\\\\projects\\\\\\testFolder\\\\\\subdir\\\\\\unzip.exe'\n\n```\n\nwhere testFolder is dir1 above and subdir is dir2.\nIs there a quick way of unzipping the file and maintaining the directory structure?",
    "answer": "import os, zipfile\n\nz = zipfile.ZipFile('myfile.zip')\nfor f in z.namelist():\nif f.endswith('/'):\nos.makedirs(f)\n\nThe extract and extractall methods are great if you're on Python 2.6.  I have to use Python 2.5 for now, so I just need to create the directories if they don't exist.  You can get a listing of directories with the namelist() method.  The directories will always end with a forward slash (even on Windows) e.g.,\n\nYou probably don't want to do it exactly like that (i.e., you'd probably want to extract the contents of the zip file as you iterate over the namelist), but you get the idea."
  },
  {
    "question": "Is it possible to unzip a compressed file with multiple threads? I unzip using this :\n\n```\nunzip -q \"file.zip\" -d path\n\n```\n\nHow can I unzip faster with utilizing multiple cores and threads?\nThanks",
    "answer": "In short: No, unzipping with multiple cores is not available.\nThe decompression normally has lower CPU-intensity than the compression (where multiple cores are often involved).\nYou wouldn't have much of an advantage anyway as the read/write-operations are more of the bottlenecks during decompression."
  },
  {
    "question": "Unzip .gz file using c# How to unzip .gz file and save files in a specific folder using c#?\nThis is the first time I encounter a .gz file. I've search in how to unzip it yet It didn't work for me. It didn't unzip .gz file in a specific folder. I don't want to used any third party application.\nCan anyone gave me a sample code on how to unzip it. Then save file in a folder. Thanks.",
    "answer": "namespace zip\n{\npublic class Program\n{\npublic static void Main()\n{\nstring directoryPath = @\"c:\\users\\public\\reports\";\n\nDirectoryInfo directorySelected = new DirectoryInfo(directoryPath);\n\nforeach (FileInfo fileToCompress in directorySelected.GetFiles())\n{\nCompress(fileToCompress);\n}\n\nforeach (FileInfo fileToDecompress in directorySelected.GetFiles(\"*.gz\"))\n{\nDecompress(fileToDecompress);\n}\n}\n\npublic static void Compress(FileInfo fileToCompress)\n{\nusing (FileStream originalFileStream = fileToCompress.OpenRead())\n{\nif ((File.GetAttributes(fileToCompress.FullName) & FileAttributes.Hidden) != FileAttributes.Hidden & fileToCompress.Extension != \".gz\")\n{\nusing (FileStream compressedFileStream = File.Create(fileToCompress.FullName + \".gz\"))\n{\nusing (GZipStream compressionStream = new GZipStream(compressedFileStream, CompressionMode.Compress))\n{\noriginalFileStream.CopyTo(compressionStream);\nConsole.WriteLine(\"Compressed {0} from {1} to {2} bytes.\",\nfileToCompress.Name, fileToCompress.Length.ToString(), compressedFileStream.Length.ToString());\n}\n}\n}\n}\n}\n\npublic static void Decompress(FileInfo fileToDecompress)\n{\nusing (FileStream originalFileStream = fileToDecompress.OpenRead())\n{\nstring currentFileName = fileToDecompress.FullName;\nstring newFileName = currentFileName.Remove(currentFileName.Length - fileToDecompress.Extension.Length);\n\nusing (FileStream decompressedFileStream = File.Create(newFileName))\n{\nusing (GZipStream decompressionStream = new GZipStream(originalFileStream, CompressionMode.Decompress))\n{\ndecompressionStream.CopyTo(decompressedFileStream);\nConsole.WriteLine(\"Decompressed: {0}\", fileToDecompress.Name);\n}\n}\n}\n}\n}\n}\n\nThe following example from MSDN shows how to use the GZipStream class to compress and decompress a directory of files."
  },
  {
    "question": "Unzipping downloaded files in iOS Using ASIHTTPRequest, I downloaded a zip file containing a folder with several audio files. I tried to unzip the file with SSZipArchive and ZipArchive, which are both based on minizip.\nWhen I compile the code, I get this error: Undefined symbols for architecture i386: \"_OBJC_CLASS_$_ZipArchive\", referenced from: objc-class-ref in AppDelegate.o.\nHow do I unzip this file in iOS?",
    "answer": "NSString *filepath = [[NSBundle mainBundle] pathForResource:@\"ZipFileName\" ofType:@\"zip\"];\nZipArchive *zipArchive = [[ZipArchive alloc] init];\n[zipArchive UnzipOpenFile:filepath Password:@\"xxxxxx\"];\n[zipArchive UnzipFileTo:{pathToDirectory} overWrite:YES];\n[zipArchive UnzipCloseFile];\n[zipArchive release];\n[SSZipArchive unzipFileAtPath:path toDestination:destination];\n\nI've used ZipArchive with success in the past. It's pretty ligthweight and simple to use, supports password protection, multiple files inside a ZIP, as well as compress & decompress.\nThe basic usage is:\n\nmore examples about this package here\nI have also tried SSZipArchive in some projects.\nBelow line would unzip your zip file."
  },
  {
    "question": "How to get a list of directories in a zip? I am looking for a way to list the directories in a zip file in bash under Linux. I found out there is an application called zipinfo, which can list the paths in the zip (without any extra noise to have to parse through (zipinfo -1 foo.zip), as opposed to unzip -l foo.zip). However, this isn't good enough and I am wondering if there is a better way.",
    "answer": "unzip -l foo.zip \"*/\"\nArchive:  foo.zip\nLength      Date    Time    Name\n---------  ---------- -----   ----\n2015-09-10 20:10   work/\n2015-08-31 10:45   work/test1/\n2015-08-31 10:50   work/test1/cc/\n2015-08-31 10:45   work/test1/dd/\n2015-08-31 10:45   work/test1/aa/\n2015-08-31 10:45   work/test1/bb/\n2015-09-09 21:17   work/tmp/\n2015-08-23 18:49   work/tmp/work/\n2015-09-08 19:33   work/tmp/work/loop/\n2015-08-15 16:00   work/tmp/work/1/\n2015-08-15 16:00   work/1/\n2015-08-24 18:40   work/dir/\n2015-09-05 18:07   work/rename/\n---------                     -------\n13 files\nzipinfo -1 foo.zip \"*/\"\nwork/\nwork/test1/\nwork/test1/cc/\nwork/test1/dd/\nwork/test1/aa/\nwork/test1/bb/\nwork/tmp/\nwork/tmp/work/\nwork/tmp/work/loop/\nwork/tmp/work/1/\nwork/1/\nwork/dir/\nwork/rename/\n\nTo list only directories:\n\nOutput (e.g.):\n\nor use\n\nOutput (e.g.):"
  },
  {
    "question": "Unzip buffer with Python? I have a buffer of bytes read from a library call and I would like to unzip the content which is a single text file. \nI tried with zlib, but I get this error:\n\n```\n>>> import zlib\n>>> zlib.decompress(buffer)\nerror: Error -3 while decompressing data: incorrect header check\n\n```\n\nHowever with ZipFile it works, but I have to use a temporary file: \n\n```\nimport zipfile\nf = open('foo.zip', 'wb')\nf.write(buffer)\nf.close()\nz = ZipFile('foo.zip')\nz.extractall()\nz.close()\nwith open('foo.txt', 'r') as f:\n    uncompressed_buffer = f.read()\n\n```\n\nIs it possible to use zlib and how can I avoid using a temporary file?",
    "answer": "import zipfile\nimport io\n\nbuffer = b'PK\\x03\\x04\\n\\x00\\x00\\x00\\x00\\x00\\n\\\\\\x88Gzzo\\xed\\x03\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x07\\x00\\x1c\\x00foo.txtUT\\t\\x00\\x03$\\x14gV(\\x14gVux\\x0b\\x00\\x01\\x041\\x04\\x00\\x00\\x041\\x04\\x00\\x00hi\\nPK\\x01\\x02\\x1e\\x03\\n\\x00\\x00\\x00\\x00\\x00\\n\\\\\\x88Gzzo\\xed\\x03\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x07\\x00\\x18\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\xb4\\x81\\x00\\x00\\x00\\x00foo.txtUT\\x05\\x00\\x03$\\x14gVux\\x0b\\x00\\x01\\x041\\x04\\x00\\x00\\x041\\x04\\x00\\x00PK\\x05\\x06\\x00\\x00\\x00\\x00\\x01\\x00\\x01\\x00M\\x00\\x00\\x00D\\x00\\x00\\x00\\x00\\x00'\n\nz = zipfile.ZipFile(io.BytesIO(buffer))\n\nfoo = z.read('foo.txt')        # Reads the data from \"foo.txt\"\nfoo2 = z.read(z.infolist()[0]) # Reads the data from the first file\nz.extractall()                 # Copies foo.txt to the filesystem\n\nz.close()\n\nprint foo\nprint foo2\n\nIs it possible to use zlib\n\nNo, zlib is not designed to operate on ZIP files.\n\nand how can I avoid using a temporary file?\n\nUse io.BytesIO:"
  },
  {
    "question": "How to open gzip text files in Gvim without unzipping? How to open Gzipped text files (*.gz) in Gvim without unzipping them first ?",
    "answer": "augroup gzip\nautocmd!\nautocmd BufReadPre,FileReadPre *.gz set bin\nautocmd BufReadPost,FileReadPost   *.gz '[,']!gunzip\nautocmd BufReadPost,FileReadPost   *.gz set nobin\nautocmd BufReadPost,FileReadPost   *.gz execute \":doautocmd BufReadPost \" . expand(\"%:r\")\nautocmd BufWritePost,FileWritePost *.gz !mv <afile> <afile>:r\nautocmd BufWritePost,FileWritePost *.gz !gzip <afile>:r\nautocmd FileAppendPre      *.gz !gunzip <afile>\nautocmd FileAppendPre      *.gz !mv <afile>:r <afile>\nautocmd FileAppendPost     *.gz !mv <afile> <afile>:r\nautocmd FileAppendPost     *.gz !gzip <afile>:r\naugroup END\n\nSolution mentioned in VimDocs as answered by therefromwhere"
  },
  {
    "question": "How to quickly check if a zip file is corrupted? Does anyone have any ideas for how to pragmatically quickly check if a zip file is corrupted based on file size? Ideally the best way to check if a zip is corrupted is to do a CRC check but this can take a long time especially if there is a lot of large zip files. I would be happy just to be able to do a quick file size or header check.\nThanks in advance.",
    "answer": "Section 4.3.7 of this page says that the compressed size is 4 bytes starting from byte 18. You could try reading that and comparing it to the size to the file.\nHowever, I think it's pretty much useless for checking if the zip file is corrupted for two reasons:\n\nSome zip files contain more bytes than just the zip part. For example, self-extracting archives have an executable part yet they're still valid zip.\nThe file can be corrupted without changing its size.\n\nSo, I suggest calculating the CRC for a guaranteed method of checking for corruption."
  },
  {
    "question": "How do you unzip very large files in python? Using python 2.4 and the built-in ZipFile library, I cannot read very large zip files (greater than 1 or 2 GB) because it wants to store the entire contents of the uncompressed file in memory.  Is there another way to do this (either with a third-party library or some other hack), or must I \"shell out\" and unzip it that way (which isn't as cross-platform, obviously).",
    "answer": "import zipfile\nimport zlib\nimport os\n\nsrc = open( doc, \"rb\" )\nzf = zipfile.ZipFile( src )\nfor m in  zf.infolist():\n\nprint m.filename, m.header_offset, m.compress_size, repr(m.extra), repr(m.comment)\nsrc.seek( m.header_offset )\nsrc.read( 30 ) # Good to use struct to unpack this.\nnm= src.read( len(m.filename) )\nif len(m.extra) > 0: ex= src.read( len(m.extra) )\nif len(m.comment) > 0: cm= src.read( len(m.comment) )\n\ndecomp= zlib.decompressobj(-15)\n\nout= open( m.filename, \"wb\" )\nresult= decomp.decompress( src.read( m.compress_size ) )\nout.write( result )\nresult = decomp.flush()\nout.write( result )\n\nout.close()\n\nzf.close()\nsrc.close()\n\nHere's an outline of decompression of large files."
  },
  {
    "question": "How to unzip a zip file using scala? Basically I need to unzip a .zip file which contains a folder called modeled which in turn contains a number of excel files. \nI have had some luck in finding code that was already written (ZipArchive) which is meant to unzip the zip file, but I cannot figure out why it throws an error message when I use it. The code for ZipArchive and the error message are listed below:\n\n```\nimport java.io.{OutputStream, InputStream, File, FileOutputStream}\nimport java.util.zip.{ZipEntry, ZipFile}\nimport scala.collection.JavaConversions._\n\nobject ZipArchive {\n\n  val BUFSIZE = 4096\n  val buffer = new Array[Byte](BUFSIZE)\n\n  def unZip(source: String, targetFolder: String) = {\n    val zipFile = new ZipFile(source)\n\n    unzipAllFile(zipFile.entries.toList, getZipEntryInputStream(zipFile)_, new File(targetFolder))\n  }\n\n  def getZipEntryInputStream(zipFile: ZipFile)(entry: ZipEntry) = zipFile.getInputStream(entry)\n\n  def unzipAllFile(entryList: List[ZipEntry], inputGetter: (ZipEntry) => InputStream, targetFolder: File): Boolean = {\n\n    entryList match {\n      case entry :: entries =>\n\n        if (entry.isDirectory)\n          new File(targetFolder, entry.getName).mkdirs\n        else\n          saveFile(inputGetter(entry), new FileOutputStream(new File(targetFolder, entry.getName)))\n\n        unzipAllFile(entries, inputGetter, targetFolder)\n      case _ =>\n        true\n    }\n  }\n\n  def saveFile(fis: InputStream, fos: OutputStream) = {\n    writeToFile(bufferReader(fis)_, fos)\n    fis.close\n    fos.close\n  }\n\n  def bufferReader(fis: InputStream)(buffer: Array[Byte]) = (fis.read(buffer), buffer)\n\n  def writeToFile(reader: (Array[Byte]) => Tuple2[Int, Array[Byte]], fos: OutputStream): Boolean = {\n    val (length, data) = reader(buffer)\n    if (length >= 0) {\n      fos.write(data, 0, length)\n      writeToFile(reader, fos)\n    } else\n      true\n  }\n}\n\n```\n\nError Message:\n\n```\njava.io.FileNotFoundException: src/test/resources/oepTemp/modeled/EQ_US_2_NULL_('CA')_ALL_ELT_IL_EQ_US.xlsx (No such file or directory), took 6.406 sec\n[error]     at java.io.FileOutputStream.open(Native Method)\n[error]     at java.io.FileOutputStream.<init>(FileOutputStream.java:221)\n[error]     at java.io.FileOutputStream.<init>(FileOutputStream.java:171)\n[error]     at com.contract.testing.ZipArchive$.unzipAllFile(ZipArchive.scala:28)\n[error]     at com.contract.testing.ZipArchive$.unZip(ZipArchive.scala:15)\n[error]     at com.contract.testing.OepStepDefinitions$$anonfun$1.apply$mcZ$sp(OepStepDefinitions.scala:175)\n[error]     at com.contract.testing.OepStepDefinitions$$anonfun$1.apply(OepStepDefinitions.scala:150)\n[error]     at com.contract.testing.OepStepDefinitions$$anonfun$1.apply(OepStepDefinitions.scala:150)\n[error]     at cucumber.api.scala.ScalaDsl$StepBody$$anonfun$apply$1.applyOrElse(ScalaDsl.scala:61)\n[error]     at cucumber.api.scala.ScalaDsl$StepBody$$anonfun$apply$1.applyOrElse(ScalaDsl.scala:61)\n[error]     at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)\n[error]     at cucumber.runtime.scala.ScalaStepDefinition.execute(ScalaStepDefinition.scala:71)\n[error]     at cucumber.runtime.StepDefinitionMatch.runStep(StepDefinitionMatch.java:37)\n[error]     at cucumber.runtime.Runtime.runStep(Runtime.java:298)\n[error]     at cucumber.runtime.model.StepContainer.runStep(StepContainer.java:44)\n[error]     at cucumber.runtime.model.StepContainer.runSteps(StepContainer.java:39)\n[error]     at cucumber.runtime.model.CucumberScenario.run(CucumberScenario.java:48)\n[error]     at cucumber.runtime.junit.ExecutionUnitRunner.run(ExecutionUnitRunner.java:91)\n[error]     at cucumber.runtime.junit.FeatureRunner.runChild(FeatureRunner.java:63)\n[error]     at cucumber.runtime.junit.FeatureRunner.runChild(FeatureRunner.java:18)\n[error]     ...\n\n```\n\nSo based on the error message it looks like it's trying to find the exported excel file? This part completely throws me off. Any help would be greatly appreciated.\nI've added below how I'm calling the method, perhaps I'm doing something silly. Also I'm up for using a different way to extract my zip file if you can recommend one.\n\n```\nval tempDirectoryDir = \"src/test/resources/oepTemp/\"\nZipArchive.unZip(tempDirectoryDir + \"Sub Region Input - Output.zip\", tempDirectoryDir)\n\n```",
    "answer": "package zip\n\nimport java.io.{ IOException, FileOutputStream, FileInputStream, File }\nimport java.util.zip.{ ZipEntry, ZipInputStream }\n\n/**\n* Created by anquegi on 04/06/15.\n*/\nobject Unzip extends App {\n\nval INPUT_ZIP_FILE: String = \"src/main/resources/my-zip.zip\";\nval OUTPUT_FOLDER: String = \"src/main/resources/my-zip\";\n\ndef unZipIt(zipFile: String, outputFolder: String): Unit = {\n\nval buffer = new Array[Byte](1024)\n\ntry {\n\n//output directory\nval folder = new File(OUTPUT_FOLDER);\nif (!folder.exists()) {\nfolder.mkdir();\n}\n\n//zip file content\nval zis: ZipInputStream = new ZipInputStream(new FileInputStream(zipFile));\n//get the zipped file list entry\nvar ze: ZipEntry = zis.getNextEntry();\n\nwhile (ze != null) {\n\nval fileName = ze.getName();\nval newFile = new File(outputFolder + File.separator + fileName);\n\nSystem.out.println(\"file unzip : \" + newFile.getAbsoluteFile());\n\n//create folders\nnew File(newFile.getParent()).mkdirs();\n\nval fos = new FileOutputStream(newFile);\n\nvar len: Int = zis.read(buffer);\n\nwhile (len > 0) {\n\nfos.write(buffer, 0, len)\nlen = zis.read(buffer)\n}\n\nfos.close()\nze = zis.getNextEntry()\n}\n\nzis.closeEntry()\nzis.close()\n\n} catch {\ncase e: IOException => println(\"exception caught: \" + e.getMessage)\n}\n\n}\n\nUnzip.unZipIt(INPUT_ZIP_FILE, OUTPUT_FOLDER)\n\n}\n\nWell since were are using some utilities from java, here is a version basen on this, translated to scala, maybe this should be more functional, but it is useful"
  },
  {
    "question": "Extracting zipped files using JSZIP in javascript In my webpage, a user is supposed to upload a zipped file. Within the zipped file are 2 files: another zip file and a txt file. On my server, after receiving the zip, I want to unzip the zip file to extract the zip & txt file, then move those 2 files to a predefined folder. I have a piece of code that extracts the zip file, but the data doesn't seem correct. Firstly, it unzipped a zip and 2 txt file when there should only be 1 txt file. It created an additional 'undefined' txt file. Also, in my txt file, instead of the original data, it was replaced with the following text: '[undefined] [undefined]'. \nCan anyone help me on this? The following is my code:\n\n```\nvar JSZip = require('JSZip');\n\nfs.readFile( filePath, function(err, data){\n  if (!err){\n    var zip = new JSZip();\n    JSZip.loadAsync(data).then(function(zip){\n      object.keys(zip.files).forEach(function(filename){\n        var content = zip.files[filename];\n        var dest = path + filename;\n        fs.writeFileSync(dest, content);\n      });\n    });\n  }\n});\n\n```",
    "answer": "var JSZip = require('JSZip');\n\nfs.readFile(filePath, function(err, data) {\nif (!err) {\nvar zip = new JSZip();\nzip.loadAsync(data).then(function(contents) {\nObject.keys(contents.files).forEach(function(filename) {\nzip.file(filename).async('nodebuffer').then(function(content) {\nvar dest = path + filename;\nfs.writeFileSync(dest, content);\n});\n});\n});\n}\n});\n\nIt took a bit of digging in their documentation but they have an example that shows how to read the file contents from a ZIP.\nYou are getting the object that describes the ZIP contents but not the actual content. Here is an adjusted version:"
  },
  {
    "question": "print content of more than one file in a zip archive I have some zip files that are really large and I want to print them without extracting first. I am using zcat and zless to do that and then I redirect the output to a different application. When my zip file contains more than one text file I receive the following error:\n\n```\nzcat tweets.zip >a\ngzip: tweets.zip has more than one entry--rest ignored\n\n```\n\nHow can I do what I want with zip files that contain more than one text file?",
    "answer": "Use the -p option of unzip to pipe the output.  Multiple files are concatenated.  The -c option does the same thing, but includes the file name in front of each file."
  },
  {
    "question": "linux unzip excluding everything in the folder and underneath Hi I have to unzip a file that could have a Directory and I want to exclude everything within that directory, I tried lot of options and looked here as well, but doesn't seem to find any good solution.\nThese are the contents of the zip file: \nPlease note the depth of EXCLUDE folder is unknown, but we have to exclude everything\n\n```\n    $unzip -l patch2.zip\nArchive:  patch2.zip\n  Length      Date    Time    Name\n---------  ---------- -----   ----\n        0  2013-10-29 17:42   EXCLUDE/\n        0  2013-10-29 17:24   EXCLUDE/inner/\n        0  2013-10-29 17:24   EXCLUDE/inner/inner1.txt\n        0  2013-10-29 15:45   EXCLUDE/file.txt\n        0  2013-10-29 15:44   patch.jar\n        0  2013-10-29 15:44   system.properties\n---------                     -------\n        0                     6 files\n\n```\n\nI tried this command, which only extract the files within it, but not the folder and its contents:\n\n```\n$unzip -l patch2.zip -x EXCLUDE/*\nArchive:  patch2.zip\n  Length      Date    Time    Name\n---------  ---------- -----   ----\n        0  2013-10-29 17:42   EXCLUDE/\n        0  2013-10-29 17:24   EXCLUDE/inner/\n        0  2013-10-29 17:24   EXCLUDE/inner/inner1.txt\n        0  2013-10-29 15:44   patch.jar\n        0  2013-10-29 15:44   system.properties\n---------                     -------\n        0                     5 files\n\n```\n\nThanks for the help.",
    "answer": "unzip patch2.zip -x \"EXCLUDE/*\"\n\nYou need to quote the exclude pattern so that it is passed to unzip. Otherwise it will be expanded by the shell before being passed to unzip.\nTry:"
  },
  {
    "question": "I want to extract all .zip files in a given directory in temp using powershell I wrote the following code for extracting the .zip files to temp:\n\n```\nfunction Expand-ZIPFile($file, $destination)\n{\n    $shell = new-object -com shell.application\n    $zip = $shell.NameSpace($file)\n    foreach ($item in $zip.items()) {\n       $shell.Namespace($destination).copyhere($item)\n    }\n}\n\nExpand-ZIPFile -file \"*.zip\" -destination \"C:\\temp\\CAP\"\n\n```\n\nBut I got the following error:\n\n```\nPS C:\\Users\\v-kamoti\\Desktop\\CAP> function Expand-ZIPFile($file, $destination)\n{\n   $shell = new-object -com shell.application\n   $zip = $shell.NameSpace($file)\n   foreach ($item in $zip.items()) {\n      $shell.Namespace($destination).copyhere($item)\n   }\n}\n\nExpand-ZIPFile -file \"*.zip\" -destination \"C:\\temp\\CAP\"\nYou cannot call a method on a null-valued expression.\nAt line:5 char:19\n+  foreach($item in $zip.items())\n+                   ~~~~~~~~~~~~\n+ CategoryInfo          : InvalidOperation: (:) [], RuntimeException\n\n```",
    "answer": "Get-ChildItem 'path to folder' -Filter *.zip | Expand-Archive -DestinationPath 'path to extract' -Force\n\nrequires ps v5"
  },
  {
    "question": "What is a good way to test a file to see if its a zip file? I am looking as a new file format specification and the specification says the file can be either xml based or a zip file containing an xml file and other files.\nThe file extension is the same in both cases.  What ways could I test the file to decide if it needs decompressing or just reading?",
    "answer": "local file header signature     4 bytes  (0x04034b50)\nversion needed to extract       2 bytes\ngeneral purpose bit flag        2 bytes\ncompression method              2 bytes\nlast mod file time              2 bytes\nlast mod file date              2 bytes\ncrc-32                          4 bytes\ncompressed size                 4 bytes\nuncompressed size               4 bytes\nfile name length                2 bytes\nextra field length              2 bytes\n\nfile name (variable size)\nextra field (variable size)\n\nThe zip file format is defined by PKWARE. You can find their file specification here.\nNear the top you will find the header specification:\n\nA.  Local file header:\n\nFrom this you can see that the first 4 bytes of the header should be the file signature which should be the hex value 0x04034b50. Byte order in the file is the other way round  - PKWARE specify that \"All values are stored in little-endian byte order unless otherwise specified.\", so if you use a hex editor to view the file you will see 50 4b 03 04 as the first 4 bytes.\nYou can use this to check if your file is a zip file. If you open the file in notepad, you will notice that the first two bytes (50 and 4b) are the ASCII characters PK."
  },
  {
    "question": "Zip and Unzip File in Powershell 4 I am using Windows Server 2012 R2 (64 bit). I have powershell version 4 available in it. I am trying to zip and unzip files. When I try Write-Zip command, it throws me following error:\nWrite-Zip : The term 'Write-Zip' is not recognized as the name of a cmdlet, function, script file, or operable\nprogram. Check the spelling of the name, or if a path was included, verify that the path is correct and try again.\nWhat should I do to fix it? Do I need to install zip/winrar in the server? Or is there any other command do zip/unzip files?",
    "answer": "source = \"c:\\temp\\source\"\narchive = \"c:\\temp\\archive.zip\"\n\nAdd-Type -assembly \"system.io.compression.filesystem\"\n[io.compression.zipfile]::CreateFromDirectory($source, $archive)\n\nWrite-Zip seems to be part of http://pscx.codeplex.com/ that require a separate installation before you can use it.\nHowever, if you just want to create a Zip archive from a folder, you could just run\n\nThis utilizes the CreateFromDirectory method from the .NET Framework class ZipFile. It creates a zip archive from the files located inside the $source folder and creates an archive as defined in the $archive variable. Note, ZipFile class was introduced in .NET Framework 4.5"
  },
  {
    "question": "Check if a directory exists in a zip file with Python Initially I was thinking of using os.path.isdir but I don't think this works for zip files. Is there a way to peek into the zip file and verify that this directory exists? I would like to prevent using unzip -l \"$@\" as much as possible, but if that's the only solution then I guess I have no choice.",
    "answer": "import zipfile\n\ndef isdir(z, name):\nreturn any(x.startswith(\"%s/\" % name.rstrip(\"/\")) for x in z.namelist())\n\nf = zipfile.ZipFile(\"sample.zip\", \"r\")\nprint isdir(f, \"a\")\nprint isdir(f, \"a/b\")\nprint isdir(f, \"a/X\")\nany(x.startswith(\"%s/\" % name.rstrip(\"/\")) for x in z.namelist())\nmkdir -p a/b/c/d\ntouch a/X\nzip -r sample.zip a\nadding: a/ (stored 0%)\nadding: a/X (stored 0%)\nadding: a/b/ (stored 0%)\nadding: a/b/c/ (stored 0%)\nadding: a/b/c/d/ (stored 0%)\npython z.py\nTrue\nTrue\nFalse\n\nJust check the filename with \"/\" at the end of it.\n\nYou use this line\n\nbecause it is possible that archive contains no directory explicitly; just a path with a directory name.\nExecution result:"
  },
  {
    "question": "How to copy files from host to Docker container? I am trying to build a backup and restore solution for the Docker containers that we work with.\nI have Docker base image that I have created, ubuntu:base, and do not want have to rebuild it each time with a Docker file to add files to it.\nI want to create a script that runs from the host machine and creates a new container using the ubuntu:base Docker image and then copies files into that container.\nHow can I copy files from the host to the container?",
    "answer": "docker cp foo.txt container_id:/foo.txt\ndocker cp container_id:/foo.txt foo.txt\ndocker cp src/. container_id:/target\ndocker cp container_id:/src/. target\n\nThe cp command can be used to copy files.\nOne specific file can be copied TO the container like:\n\nOne specific file can be copied FROM the container like:\n\nFor emphasis, container_id is a container ID, not an image ID. (Use docker ps to view listing which includes container_ids.)\nMultiple files contained by the folder src can be copied into the target folder using:\n\nReference: Docker CLI docs for cp\nIn Docker versions prior to 1.8 it was only possible to copy files from a container to the host. Not from the host to a container."
  },
  {
    "question": "How do I get into a Docker container&#39;s shell? I'm getting started working with Docker. I'm using the WordPress base image and docker-compose.\nI'm trying to ssh into one of the containers to inspect the files/directories that were created during the initial build. I tried to run docker-compose run containername ls -la, but that didn't do anything. Even if it did, I'd rather have a console where I can traverse the directory structure, rather than run a single command. What is the right way to do this with Docker?",
    "answer": "docker exec -it <mycontainer> sh\nOCI runtime exec failed: exec failed: unable to start container process:\nexec: \"sh\": executable file not found in $PATH: unknown\ndocker ps\nd2d4a89aaee9        larsks/mini-httpd   \"mini_httpd -d /cont   7 days ago          Up 7 days                               web\ndocker exec -it web ip addr\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN\nlink/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\ninet 127.0.0.1/8 scope host lo\nvalid_lft forever preferred_lft forever\ninet6 ::1/128 scope host\nvalid_lft forever preferred_lft forever\n18: eth0: <BROADCAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP\nlink/ether 02:42:ac:11:00:03 brd ff:ff:ff:ff:ff:ff\ninet 172.17.0.3/16 scope global eth0\nvalid_lft forever preferred_lft forever\ninet6 fe80::42:acff:fe11:3/64 scope link\nvalid_lft forever preferred_lft forever\ndocker exec -it d2d4a89aaee9 ip addr\ndocker exec -it web sh\n/ # echo This is inside the container.\nThis is inside the container.\n/ # exit\nservices:\nweb:\nimage: docker.io/alpinelinux/darkhttpd\ndocker compose exec web sh\ndocker exec -it myproject-web-1 sh\n\ndocker attach will let you connect to your Docker container, but this isn't really the same thing as ssh.  If your container is running a webserver, for example, docker attach will probably connect you to the stdout of the web server process.  It won't necessarily give you a shell.\nThe docker exec command is probably what you are looking for; this will let you run arbitrary commands inside an existing container.  For example, to run bash inside a container:\n\nOf course, whatever command you are running must exist in the container filesystem; if your container doesn't have sh, this will fail with something like:\n\n[If your container doesn't have sh -- which is a common case for minimal images -- you may need to investigate other ways to explore the container filesystem.]\nIn the above command <mycontainer> is the name or ID of the target container.  It doesn't matter whether or not you're using docker compose; just run docker ps and use either the ID (a hexadecimal string displayed in the first column) or the name (displayed in the final column).  E.g., given:\n\nI can run:\n\nI could accomplish the same thing by running:\n\nSimilarly, I could start a shell in the container;\n\nIn commands shown in this answer, the -i and -t options (combined as -it) are necessary to get an interactive shell:\n\n-i keeps stdin connected; if you don't specify -i, the shell will simply exit.\n\n-t allocates a tty device; if you don't specify -t, you won't have a very pleasant interactive experience (there will be no shell prompt or job control, for example).\n\nIf you're specifically using docker compose, there is a convenience docker compose exec command that works very much like the docker exec command, except:\n\nIt defaults to the behavior of -i and -t\nIt allows you to refer to containers by their service name in your compose.yaml file.\n\nFor example, if you have a compose.yaml like this:\n\nThen you can run:\n\nThe equivalent docker exec command would be something like:"
  },
  {
    "question": "How to copy Docker images from one host to another without using a repository How do I transfer a Docker image from one machine to another one without using a repository, no matter private or public?\nI create my own image in VirtualBox, and when it is finished I try to deploy to other machines to have real usage.\nSince it is based on my own base image (like Red Hat Linux), it cannot be recreated from a Dockerfile. My Dockerfile isn't easily portable.\nAre there simple commands I can use? Or another solution?",
    "answer": "docker save -o <path for generated tar file> <image name>\ndocker load -i <path to image tar file>\ndocker save -o c:/myfile.tar centos:16\ndocker save -o C:\\path\\to\\file.tar repository/imagename\n\nYou will need to save the Docker image as a tar file:\n\nThen copy your image to a new system with regular file transfer tools such as cp, scp, or rsync (preferred for big files). After that you will have to load the image into Docker:\n\nYou should add filename (not just directory) with -o, for example:\n\nyour image syntax may need the repository prefix (:latest tag is default)\n\nPS: You may need to sudo all commands."
  },
  {
    "question": "How to get a Docker container&#39;s IP address from the host Is there a command I can run to get the container's IP address right from the host after a new container is created?\nBasically, once Docker creates the container, I want to roll my own code deployment and container configuration scripts.",
    "answer": "docker inspect \\\n-f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' container_name_or_id\ndocker inspect \\\n--format '{{ .NetworkSettings.IPAddress }}' container_name_or_id\n\nThis solution only works if the container is connected with a single network. The --format option of inspect comes to the rescue.\nModern Docker client syntax is:\n\nOld Docker client syntax is:\n\nThese commands will return the Docker container's IP address.\nAs mentioned in the comments: if you are on Windows, use double quotes \" instead of single quotes ' around the curly braces."
  },
  {
    "question": "Should I use Vagrant or Docker for creating an isolated environment? I use Ubuntu for development and deployment and have a need for creating an isolated environment. \nI am considering either Vagrant or Docker for this purpose. What are the pros and cons, or how do these solutions compare?",
    "answer": "If your purpose is the isolation, I think Docker is what you want.\nVagrant is a virtual machine manager. It allows you to script the virtual machine configuration as well as the provisioning. However, it is still a virtual machine depending on VirtualBox (or others) with a huge overhead. It requires you to have a hard drive file that can be huge, it takes a lot of ram, and performance may be not very good.\nDocker on the other hand uses kernel cgroup and namespacing via LXC. It means that you are using the same kernel as the host and the same file system.\nYou can use Dockerfile with the docker build command in order to handle the provisioning and configuration of your container. You have an example at docs.docker.com on how to make your Dockerfile; it is very intuitive.\nThe only reason you could want to use Vagrant is if you need to do BSD, Windows or other non-Linux development on your Ubuntu box. Otherwise, go for Docker."
  },
  {
    "question": "How to force Docker for a clean build of an image I have build a Docker image from a Docker file using the below command.\n\n```\n$ docker build -t u12_core -f u12_core .\n\n```\n\nWhen I am trying to rebuild it with the same command, it's using the build cache like:\n\n```\nStep 1 : FROM ubuntu:12.04\n ---> eb965dfb09d2\nStep 2 : MAINTAINER Pavan Gupta <pavan.gupta@gmail.com>\n ---> Using cache\n ---> 4354ccf9dcd8\nStep 3 : RUN apt-get update\n ---> Using cache\n ---> bcbca2fcf204\nStep 4 : RUN apt-get install -y openjdk-7-jdk\n ---> Using cache\n ---> 103f1a261d44\nStep 5 : RUN apt-get install -y openssh-server\n ---> Using cache\n ---> dde41f8d0904\nStep 6 : RUN apt-get install -y git-core\n ---> Using cache\n ---> 9be002f08b6a\nStep 7 : RUN apt-get install -y build-essential\n ---> Using cache\n ---> a752fd73a698\nStep 8 : RUN apt-get install -y logrotate\n ---> Using cache\n ---> 93bca09b509d\nStep 9 : RUN apt-get install -y lsb-release\n ---> Using cache\n ---> fd4d10cf18bc\nStep 10 : RUN mkdir /var/run/sshd\n ---> Using cache\n ---> 63b4ecc39ff0\nStep 11 : RUN echo 'root:root' | chpasswd\n ---> Using cache\n ---> 9532e31518a6\nStep 12 : RUN sed -i 's/PermitRootLogin without-password/PermitRootLogin yes/' /etc/ssh/sshd_config\n ---> Using cache\n ---> 47d1660bd544\nStep 13 : RUN sed 's@session\\s*required\\s*pam_loginuid.so@session optional pam_loginuid.so@g' -i /etc/pam.d/sshd\n ---> Using cache\n ---> d1f97f1c52f7\nStep 14 : RUN wget -O aerospike.tgz 'http://aerospike.com/download/server/latest/artifact/ubuntu12'\n ---> Using cache\n ---> bd7dde7a98b9\nStep 15 : RUN tar -xvf aerospike.tgz\n ---> Using cache\n ---> 54adaa09921f\nStep 16 : RUN dpkg -i aerospike-server-community-*/*.deb\n ---> Using cache\n ---> 11aba013eea5\nStep 17 : EXPOSE 22 3000 3001 3002 3003\n ---> Using cache\n ---> e33aaa78a931\nStep 18 : CMD /usr/sbin/sshd -D\n ---> Using cache\n ---> 25f5fe70fa84\nSuccessfully built 25f5fe70fa84\n\n```\n\nThe cache shows that aerospike is installed. However, I don't find it inside containers spawn from this image, so I want to rebuild this image without using the cache. How can I force Docker to rebuild a clean image without the cache?",
    "answer": "docker build --no-cache -t u12_core -f u12_core .\n\nThere's a --no-cache option:\n\nIn older versions of Docker you needed to pass --no-cache=true, but this is no longer the case."
  },
  {
    "question": "How do I pass environment variables to Docker containers? How can one access an external database from a container? Is the best way to hard code in the connection string?\n\n```\n# Dockerfile\nENV DATABASE_URL amazon:rds/connection?string\n\n```",
    "answer": "sudo docker run -d -t -i -e REDIS_NAMESPACE='staging' \\\n-e POSTGRES_ENV_POSTGRES_PASSWORD='foo' \\\n-e POSTGRES_ENV_POSTGRES_USER='bar' \\\n-e POSTGRES_ENV_DB_NAME='mysite_staging' \\\n-e POSTGRES_PORT_5432_TCP_ADDR='docker-db-1.hidden.us-east-1.rds.amazonaws.com' \\\n-e SITE_URL='staging.mysite.com' \\\n-p 80:80 \\\n--link redis:redis \\\n--name container_name dockerhub_id/image_name\nsudo PASSWORD='foo' docker run  [...] -e PASSWORD [...]\ndocker run --env-file ./env.list ubuntu bash\n\nYou can pass environment variables to your containers with the -e (alias --env) flag.\ndocker run -e xx=yy\nAn example from a startup script:\n\nOr, if you don't want to have the value on the command-line where it will be displayed by ps, etc., -e can pull in the value from the current environment if you just give it without the =:\n\nIf you have many environment variables and especially if they're meant to be secret, you can use an env-file:\n\nThe --env-file flag takes a filename as an argument and expects each line to be in the VAR=VAL format, mimicking the argument passed to --env. Comment lines need only be prefixed with #"
  },
  {
    "question": "How to remove old Docker containers This question is related to Should I be concerned about excess, non-running, Docker containers?.\nI'm wondering how to remove old containers. The docker rm 3e552code34a lets you remove a single one, but I have lots already. docker rm --help doesn't give a selection option (like all, or by image name).\nMaybe there is a directory in which these containers are stored where I can delete them easily manually?",
    "answer": "docker container prune\ndocker system prune\ndocker ps --filter \"status=exited\" | grep 'weeks ago' | awk '{print $1}' | xargs --no-run-if-empty docker rm\n\nSince Docker 1.13.x you can use Docker container prune:\n\nThis will remove all stopped containers and should work on all platforms the same way.\nThere is also a Docker system prune:\n\nwhich will clean up all unused containers, networks, images (both dangling and unreferenced), and optionally, volumes, in one command.\n\nFor older Docker versions, you can string Docker commands together with other Unix commands to get what you need. Here is an example on how to clean up old containers that are weeks old:\n\nTo give credit, where it is due, this example is from https://twitter.com/jpetazzo/status/347431091415703552."
  },
  {
    "question": "How to fix Docker: Permission denied I installed Docker on my Ubuntu machine.\nWhen I run\n\n```\nsudo docker run hello-world\n\n```\n\nit works.\nBut if I write the command without sudo\n\n```\ndocker run hello-world\n\n```\n\nit displays the following:\n\n```\ndocker: Got permission denied while trying to connect \nto the Docker daemon socket at unix:///var/run/docker.sock: \nPost http://%2Fvar%2Frun%2Fdocker.sock/v1.35/containers/create: \ndial unix /var/run/docker.sock: connect: permission denied. \nSee 'docker run --help'.\n\n```\n\nThe same happens when I try to run:\n\n```\ndocker-compose up\n\n```\n\nHow can I resolve this?",
    "answer": "sudo groupadd docker\nsudo usermod -aG docker $USER\nnewgrp docker\ndocker run hello-world\nreboot\n\nIf you want to run Docker as a non-root user, then you need to add your user to the docker group.\n\nCreate the docker group if it does not exist:\n\nAdd your user to the docker group:\n\nLog in to the new docker group (to avoid having to log out and log in again; but if not enough, try to reboot):\n\nCheck if Docker can be run without root:\n\nReboot if you still get an error:\n\nFrom the official Docker documentation \"Manage Docker as a non-root user\":\n\u26a0\ufe0f Warning\n\nThe docker group grants root-level privileges to the user. For details on how this impacts security in your system, see Docker Daemon Attack Surface."
  },
  {
    "question": "What is the difference between a Docker image and a container? When using Docker, we start with a base image. We boot it up, create changes and those changes are saved in layers forming another image.\nSo eventually I have an image for my PostgreSQL instance and an image for my web application, changes to which keep on being persisted.\nWhat is a container?",
    "answer": "An instance of an image is called a container. You have an image, which is a set of layers as you describe. If you start this image, you have a running container of this image. You can have many running containers of the same image.\nYou can see all your images with docker images whereas you can see your running containers with docker ps (and you can see all containers with docker ps -a).\nSo a running instance of an image is a container."
  },
  {
    "question": "How to remove old and unused Docker images When running Docker for a long time, there are a lot of images in system. How can I remove all unused Docker images at once safety to free up the storage?\nIn addition, I also want to remove images pulled months ago, which have the correct TAG.\nSo, I'm not asking for removing untagged images only. I'm searching for a way to remove general unused images, which includes both untagged and other images such as pulled months ago with correct TAG.",
    "answer": "docker rmi $(docker images --filter \"dangling=true\" -q --no-trunc)\nalias drmae='docker rm $(docker ps -qa --no-trunc --filter \"status=exited\")'\ndcleanup(){\ndocker rm -v $(docker ps --filter status=exited -q 2>/dev/null) 2>/dev/null\ndocker rmi $(docker images --filter dangling=true -q 2>/dev/null) 2>/dev/null\n}\n\n(see below for original answer)\n\nUpdate Sept. 2016: Docker 1.13: PR 26108 and commit 86de7c0 introduce a few new commands to help facilitate visualizing how much space the docker daemon data is taking on disk and allowing for easily cleaning up \"unneeded\" excess.\ndocker system prune will delete all dangling data (containers, networks, and images). You can remove all unused volumes with the --volumes option and remove all unused images (not just dangling) with the -a option.\nYou also have:\n\ndocker container prune\ndocker image prune\ndocker network prune\ndocker volume prune\n\nFor unused images, use docker image prune -a (for removing dangling and ununsed images).\nWarning: 'unused' means \"images not referenced by any container\": be careful before using -a.\nAs illustrated in A L's answer, docker system prune --all will remove all unused images not just dangling ones... which can be a bit too much.\nCombining docker xxx prune with the --filter option can be a great way to limit the pruning (docker SDK API 1.28 minimum, so docker 17.04+)\n\nThe currently supported filters are:\n\nuntil (<timestamp>) - only remove containers, images, and networks created before given timestamp\nlabel (label=<key>, label=<key>=<value>, label!=<key>, or label!=<key>=<value>) - only remove containers, images, networks, and volumes with (or without, in case label!=... is used) the specified labels.\n\nSee \"Prune images\" for an example.\n\nWarning: there is no \"preview\" or \"--dry-run\" option for those docker xxx prune commands.\nThis is requested with moby/moby issue 30623 since 2017, but seems tricky to be implemented (Aug. 2022)\n\nHaving a more representative overview of what will be pruned will be quite complicated, for various reasons;\n\nrace conditions (can be resolved by documenting the limitations);\nA container/image/volume/network may not be in use at the time that \"dry run\" is used, but may be in use the moment the actual prune is executed (or vice-versa), so dry run will always be an \"approximation\" of what will be pruned.\nthe more difficult part is due to how objects (containers, images, networks etc.) depend on each other.\nFor example, an image can be deleted if it no longer has references to it (no more tags, no more containers using it); this is the reason that docker system prune deletes objects in a specific order (first remove all unused containers, then remove unused images).\nIn order to replicate the same flow for \"dry-run\", it will be needed to temporarily construct representation of all objects and where they're referenced based on that (basically; duplicate all reference-counters, and then remove references from that \"shadow\" representation).\nFinally; with the work being done on integrating the containerd snapshotter (image and layer store), things may change more;\nFor example, images can now be multi-arch, and (to be discussed), \"pruning\" could remove unused variants (architectures) from an image to clean up space, which brings another dimension to calculating \"what can be removed\".\n\nOriginal answer (Sep. 2016)\nI usually do:\n\nI have an [alias for removing those dangling images: drmi]13\n\nThe dangling=true filter finds unused images\n\nThat way, any intermediate image no longer referenced by a labelled image is removed.\nI do the same first for exited processes (containers)\n\nAs haridsv points out in the comments:\n\nTechnically, you should first clean up containers before cleaning up images, as this will catch more dangling images and less errors.\n\nJess Frazelle (jfrazelle) has the bashrc function:\n\nTo remove old images, and not just \"unreferenced-dangling\" images, you can consider docker-gc:\n\nA simple Docker container and image garbage collection script.\n\nContainers that exited more than an hour ago are removed.\nImages that don't belong to any remaining container after that are removed."
  },
  {
    "question": "How can I delete all local Docker images? I recently started using Docker and never realized that I should use docker-compose down instead of ctrl-c or docker-compose stop to get rid of my experiments. I now have a large number of unneeded docker images locally. \nIs there a flag I can run to delete all the local docker images & containers?\nSomething like docker rmi --all --force --all flag does not exist but I am looking for something with similar idea.",
    "answer": "docker rm -vf $(docker ps -aq)\ndocker rmi -f $(docker images -aq)\ndocker images -a -q | % { docker image rm $_ -f }\nfor /F %i in ('docker images -a -q') do docker rmi -f %i\n\nUnix\nTo delete all containers including its volumes use,\n\nTo delete all the images,\n\nRemember, you should remove all the containers before removing all the images from which those containers were created.\nWindows - Powershell\n\nWindows - cmd.exe"
  },
  {
    "question": "Exploring Docker container&#39;s file system I've noticed with docker that I need to understand what's happening inside a container or what files exist in there. One example is downloading images from the docker index - you don't have a clue what the image contains so it's impossible to start the application.\nWhat would be ideal is to be able to ssh into them or equivalent. Is there a tool to do this, or is my conceptualisation of docker wrong in thinking I should be able to do this.",
    "answer": "docker exec -t -i mycontainer /bin/bash\ndocker ps\n\ndocker commit 12345678904b5 mysnapshot\n\ndocker run -t -i mysnapshot /bin/bash\ndocker rmi mysnapshot\ndocker run -d -p 22 mysnapshot /usr/sbin/sshd -D\n\ndocker ps\n\nHere are a couple different methods...\nA) Use docker exec (easiest)\nDocker version 1.3 or newer supports the command exec that behave similar to nsenter. This command can run new process in already running container (container must have PID 1 process running already). You can run /bin/bash to explore container state:\n\nsee Docker command line documentation\nB) Use Snapshotting\nYou can evaluate container filesystem this way:\n\nThis way, you can evaluate filesystem of the running container in the precise time moment. Container is still running, no future changes are included.\nYou can later delete snapshot using (filesystem of the running container is not affected!):\n\nC) Use ssh\nIf you need continuous access, you can install sshd to your container and run the sshd daemon:\n\nThis way, you can run your app using ssh (connect and execute what you want).\nD) Use nsenter\nUse nsenter, see Why you don't need to run SSHd in your Docker containers\n\nThe short version is: with nsenter, you can get a shell into an\nexisting container, even if that container doesn\u2019t run SSH or any kind\nof special-purpose daemon"
  },
  {
    "question": "How to deal with persistent storage (e.g. databases) in Docker How do people deal with persistent storage for your Docker containers?\nI am currently using this approach: build the image, e.g. for PostgreSQL, and then start the container with\n\n```\ndocker run --volumes-from c0dbc34fd631 -d app_name/postgres\n\n```\n\nIMHO, that has the drawback, that I must not ever (by accident) delete container \"c0dbc34fd631\".\nAnother idea would be to mount host volumes \"-v\" into the container, however, the userid within the container does not necessarily match the userid from the host, and then permissions might be messed up.\nNote: Instead of --volumes-from 'cryptic_id' you can also use --volumes-from my-data-container where my-data-container is a name you assigned to a data-only container, e.g. docker run --name my-data-container ... (see the accepted answer)",
    "answer": "docker volume create --name hello\ndocker run -d -v hello:/container/path/for/volume container_image my_command\ndocker volume ls -f dangling=true\ndocker volume rm <volume name>\ndocker volume rm $(docker volume ls -f dangling=true -q)\n\ndocker volume prune\ndocker run --volumes-from data-container some-other-container command-to-execute\nsudo docker run --rm --volumes-from DATA -v $(pwd):/backup busybox tar cvf /backup/backup.tar /data\nsudo docker run -v /data -name DATA2 busybox true\n\nsudo docker run --rm --volumes-from DATA2 -v $(pwd):/backup busybox tar xvf /backup/backup.tar\ndata/\ndata/sven.txt\n\nsudo docker run --rm --volumes-from DATA -v `pwd`:/backup busybox ls /data\nsven.txt\n\nDocker 1.9.0 and above\nUse volume API\n\nThis means that the data-only container pattern must be abandoned in favour of the new volumes.\nActually the volume API is only a better way to achieve what was the data-container pattern.\nIf you create a container with a -v volume_name:/container/fs/path Docker will automatically create a named volume for you that can:\n\nBe listed through the docker volume ls\nBe identified through the docker volume inspect volume_name\nBacked up as a normal directory\nBacked up as before through a --volumes-from connection\n\nThe new volume API adds a useful command that lets you identify dangling volumes:\n\nAnd then remove it through its name:\n\nAs @mpugach underlines in the comments, you can get rid of all the dangling volumes with a nice one-liner:\n\nDocker 1.8.x and below\nThe approach that seems to work best for production is to use a data only container.\nThe data only container is run on a barebones image and actually does nothing except exposing a data volume.\nThen you can run any other container to have access to the data container volumes:\n\nHere you can get a good picture of how to arrange the different containers.\nHere there is a good insight on how volumes work.\n\nIn this blog post there is a good description of the so-called container as volume pattern which clarifies the main point of having data only containers.\nDocker documentation has now the DEFINITIVE description of the container as volume/s pattern.\nFollowing is the backup/restore procedure for Docker 1.8.x and below.\nBACKUP:\n\n--rm: remove the container when it exits\n--volumes-from DATA: attach to the volumes shared by the DATA container\n-v $(pwd):/backup: bind mount the current directory into the container; to write the tar file to\nbusybox: a small simpler image - good for quick maintenance\ntar cvf /backup/backup.tar /data: creates an uncompressed tar file of all the files in the /data directory\n\nRESTORE:\n\nHere is a nice article from the excellent Brian Goff explaining why it is good to use the same image for a container and a data container."
  },
  {
    "question": "Where are Docker images stored on the host machine? I managed to find the containers under directory /var/lib/docker/containers, but I can't find the images.\nWhat are the directories and files under /var/lib/docker?",
    "answer": "The contents of the /var/lib/docker directory vary depending on the driver Docker is using for storage.\nBy default this will be aufs but can fall back to overlay, overlay2, btrfs, devicemapper or zfs depending on your kernel support. In most places this will be aufs but the RedHats went with devicemapper.\nYou can manually set the storage driver with the -s or --storage-driver= option to the Docker daemon.\n\n/var/lib/docker/{driver-name} will contain the driver specific storage for contents of the images.\n/var/lib/docker/graph/<id> now only contains metadata about the image, in the json and layersize files.\n\nIn the case of aufs:\n\n/var/lib/docker/aufs/diff/<id> has the file contents of the images.\n/var/lib/docker/repositories-aufs is a JSON file containing local image information. This can be viewed with the command docker images.\n\nIn the case of devicemapper:\n\n/var/lib/docker/devicemapper/devicemapper/data stores the images\n/var/lib/docker/devicemapper/devicemapper/metadata the metadata\nNote these files are thin provisioned \"sparse\" files so aren't as big as they seem."
  },
  {
    "question": "Docker Compose - How to execute multiple commands? I want to do something like this where I can run multiple commands in the following code:\n\n```\ndb:\n  image: postgres\nweb:\n  build: .\n  command: python manage.py migrate\n  command: python manage.py runserver 0.0.0.0:8000\n  volumes:\n    - .:/code\n  ports:\n    - \"8000:8000\"\n  links:\n    - db\n\n```\n\nHow could I execute multiple commands?",
    "answer": "command: bash -c \"python manage.py migrate && python manage.py runserver 0.0.0.0:8000\"\ncommand: >\nbash -c \"python manage.py migrate\n&& python manage.py runserver 0.0.0.0:8000\"\ncommand: bash -c \"\npython manage.py migrate\n&& python manage.py runserver 0.0.0.0:8000\n\"\n\nFigured it out, use bash -c.\nExample:\n\nSame example in multilines:\n\nOr:"
  },
  {
    "question": "Error &quot;The input device is not a TTY&quot; I am running the following command from my Jenkinsfile. However, I get the error \"The input device is not a TTY\".\n\n```\ndocker run -v $PWD:/foobar -it cloudfoundry/cflinuxfs2 /foobar/script.sh\n\n```\n\nIs there a way to run the script from the Jenkinsfile without doing interactive mode?\nI basically have a file called script.sh that I would like to run inside the Docker container.",
    "answer": "Remove the -it from your cli to make it non interactive and remove the TTY. If you don't need either, e.g. running your command inside of a Jenkins or cron script, you should do this.\nOr you can change it to -i if you have input piped into the docker command that doesn't come from a TTY. If you have something like xyz | docker ... or docker ... <input in your command line, do this.\nOr you can change it to -t if you want TTY support but don't have it available on the input device. Do this for apps that check for a TTY to enable color formatting of the output in your logs, or for when you later attach to the container with a proper terminal.\nOr if you need an interactive terminal and aren't running in a terminal on Linux or MacOS, use a different command line interface. PowerShell is reported to include this support on Windows.\n\nWhat is a TTY? It's a terminal interface that supports escape sequences, moving the cursor around, etc, that comes from the old days of dumb terminals attached to mainframes. Today it is provided by the Linux command terminals and ssh interfaces. See the wikipedia article for more details.\nTo see the difference of running a container with and without a TTY, run a container without one: docker run --rm -i ubuntu bash. From inside that container, install vim with apt-get update; apt-get install vim. Note the lack of a prompt. When running vim against a file, try to move the cursor around within the file."
  },
  {
    "question": "Docker how to change repository name or rename image? I'm trying to change repository name of the image:\n\n```\nREPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nserver              latest              d583c3ac45fd        26 minutes ago      685.5 MB\n\n```\n\nHence I want to change the name server to something like myname/server:\n\n```\nREPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nmyname/server       latest              d583c3ac45fd        26 minutes ago      685.5 MB\n\n```\n\nHow can I do this?",
    "answer": "docker image tag server:latest myname/server:latest\ndocker image tag d583c3ac45fd myname/server:latest\ndocker rmi server\n\nor\n\nTags are just human-readable aliases for the full image name (d583c3ac45fd...).\nSo you can have as many of them associated with the same image as you like. If you don't like the old name you can remove it after you've retagged it:\n\nThat will just remove the alias/tag. Since d583c3ac45fd has other names, the actual image won't be deleted."
  },
  {
    "question": "How to list containers in Docker There's a command to list images, docker images, but there doesn't seem to be a corresponding docker containers.\nOther than becoming root and looking into /var/lib/docker there doesn't seem a way to do that. Am I missing something? Is that something one isn't supposed to do?",
    "answer": "docker ps\ndocker ps -a\ndocker ps -l\ndocker ps -n=-1\ndocker ps -s\ndocker container ls\ndocker container ls -a\ndocker rm $(docker ps -aq)\ndocker rm -f $(docker ps -a -q)\n\nTo show only running containers run:\n\nTo show all containers run:\n\nTo show the latest created container (includes all states) run:\n\nTo show n last created containers (includes all states) run:\n\nTo display total file sizes run:\n\nThe content presented above is from docker.com.\nIn the new version of Docker, commands are updated, and some management commands are added:\n\nIt is used to list all the running containers includes all states.\n\nAnd then, if you want to clean them all,\n\nIt is used to list all the containers created irrespective of its state.\nAnd to stop all the Docker containers (force)\n\nHere the container is the management command."
  },
  {
    "question": "How to see docker image contents I did a docker pull and can list the image that's downloaded. I want to see the contents of this image. Did a search on the net but no straight answer.",
    "answer": "docker run -it image_name sh\ndocker run -it --entrypoint sh image_name\ndocker image history --no-trunc image_name > image_history\n\nIf the image contains a shell, you can run an interactive shell container using that image and explore whatever content that image has.  If sh is not available, the busybox ash shell might be.\nFor instance:\n\nOr following for images with an entrypoint\n\nOr if you want to see how the image was built, meaning the steps in its Dockerfile, you can:\n\nThe steps will be logged into the image_history file."
  },
  {
    "question": "How to include files outside of Docker&#39;s build context? How can I include files from outside of Docker's build context using the \"ADD\" command in the Docker file?\nFrom the Docker documentation:\n\nThe  path must be inside the context of the build; you cannot ADD\n  ../something/something, because the first step of a docker build is to\n  send the context directory (and subdirectories) to the docker daemon.\n\nI do not want to restructure my whole project just to accommodate Docker in this matter. I want to keep all my Docker files in the same sub-directory.\nAlso, it appears Docker does not yet (and may not ever) support symlinks: Dockerfile ADD command does not follow symlinks on host #1676. \nThe only other thing I can think of is to include a pre-build step to copy the files into the Docker build context (and configure my version control to ignore those files). Is there a better workaround for than that?",
    "answer": "docker build -f docker-files/Dockerfile .\ndocker build -f ../Dockerfile .\n\nThe best way to work around this is to specify the Dockerfile independently of the build context, using -f.\nFor instance, this command will give the ADD command access to anything in your current directory.\n\nUpdate: Docker now allows having the Dockerfile outside the build context (fixed in 18.03.0-ce). So you can also do something like"
  },
  {
    "question": "How does one remove a Docker image? I'm running Docker under Vagrant under OS X 10.8.4  (Mountain Lion), and whenever I try to delete a saved image, I get an error:\n\n```\n$ docker rmi some-image-id\n2013/07/15 hh:mm:ss unexpected JSON input\n\n```\n\nAccording to the rmi help, the proper syntax is docker rmi IMAGE [IMAGE...], and I'm not sure what to make of that.\nHow can I delete an image?\n\n```\n$ docker version\nClient version: 0.4.8\nServer version: 0.4.8\nGo version: go1.1\n\n```\n\n\u00a0\n\n```\n$docker info\nContainers: 1\nImages: 3\n\n```\n\nInterestingly, when I run docker ps, no containers show up at all. Running docker images shows four (4) base images and one (1) node image.",
    "answer": "Try docker rmi node. That should work.\nSeeing all created containers is as simple as docker ps -a.\nTo remove all existing containers (not images!) run docker rm $(docker ps -aq)"
  },
  {
    "question": "How do I edit a file after I shell to a Docker container? I successfully shelled to a Docker container using:\n\n```\ndocker exec -i -t 69f1711a205e bash\n\n```\n\nNow I need to edit file and I don't have any editors inside:\n\n```\nroot@69f1711a205e:/# nano\nbash: nano: command not found\nroot@69f1711a205e:/# pico\nbash: pico: command not found\nroot@69f1711a205e:/# vi\nbash: vi: command not found\nroot@69f1711a205e:/# vim\nbash: vim: command not found\nroot@69f1711a205e:/# emacs\nbash: emacs: command not found\nroot@69f1711a205e:/#\n\n```\n\nHow do I edit files?",
    "answer": "docker exec -it <container> bash\napt-get update\napt-get install vim\nFROM  confluent/postgres-bw:0.1\n\nRUN [\"apt-get\", \"update\"]\nRUN [\"apt-get\", \"install\", \"-y\", \"vim\"]\n\nAs in the comments, there's no default editor set - strange - the $EDITOR environment variable is empty. You can log in into a container with:\n\nAnd run:\n\nOr use the following Dockerfile:\n\nDocker images are delivered trimmed to the bare minimum - so no editor is installed with the shipped container. That's why there's a need to install it manually.\nEDIT\nI also encourage you to read my post about the topic."
  },
  {
    "question": "docker push error: denied: requested access to the resource is denied I am following this link to create my first docker Image and it went successfully and now I am trying to push this Image into my docker repository following these instructions. But whenever I am trying to push this Image into repository, I  got this type of error.\n\n```\ndenied: requested access to the resource is denied\n\n```\n\n\nNote: I have successfully login into docker",
    "answer": "docker login\nYou need to include the namespace for Docker Hub to associate it with your account.\nThe namespace is the same as your Docker Hub account name.\nYou need to rename the image to YOUR_DOCKERHUB_NAME/docker-whale.\ndocker tag firstimage YOUR_DOCKERHUB_NAME/firstimage\ndocker push YOUR_DOCKERHUB_NAME/firstimage\n\nYou may need to switch your docker repo to private before docker push.\nThanks to the answer provided by Dean Wu and this comment by ses, before pushing, remember to log out, then log in from the command line to your docker hub account\n\nAccording to the docs:\n\nSo, this means you have to tag your image before pushing:\n\nand then you should be able to push it."
  },
  {
    "question": "Run a Docker image as a container After building a Docker image from a dockerfile, I see the image was built successfully, but what do I do with it? Shouldn't i be able to run it as a container?",
    "answer": "docker images\nREPOSITORY          TAG                 ID                  CREATED             SIZE\nubuntu              12.04               8dbd9e392a96        4 months ago        131.5 MB (virtual 131.5 MB)\ndocker run -i -t ubuntu:12.04 /bin/bash\ndocker run -i -t 8dbd9e392a96 /bin/bash\n\nThe specific way to run it depends on whether you gave the image a tag/name or not.\n\nWith a name (let's use Ubuntu):\n\nWithout a name, just using the ID:\n\nPlease see Docker run reference for more information."
  },
  {
    "question": "How do I assign a port mapping to an existing Docker container? I'm not sure if I've misunderstood something here, but it seems like it's only possible to set port mappings by creating a new container from an image. Is there a way to assign a port mapping to an existing Docker container?",
    "answer": "You can change the port mapping by directly editing the hostconfig.json file at\n/var/lib/docker/containers/[hash_of_the_container]/hostconfig.json or /var/snap/docker/common/var-lib-docker/containers/[hash_of_the_container]/hostconfig.json if you installed Docker as a snap.\nYou can determine the [hash_of_the_container] via the docker inspect <container_name> command and the value of the \"Id\" field is the hash.\n\nStop the container (docker stop <container_name>).\nStop docker service (per Tacsiazuma's comment)\nChange the file.\nRestart your docker engine (to flush/clear config caches).\nStart the container (docker start <container_name>).\n\nSo you don't need to create an image with this approach. You can also change the restart flag here.\nP.S. You may visit https://docs.docker.com/engine/admin/ to learn how to correctly restart your docker engine as per your host machine. I used sudo systemctl restart docker to restart my docker engine that is running on Ubuntu 16.04.\nAdding ports: As mentioned in the comments you might also want to export the port from the (containered) machine. (For example when a new port appears in docker inspect but not in docker container ls)\nIn this case you also have to edit the config.v2.json file (next to hostconfig-file).\nThe syntax is like \"ExposedPorts\":{\"8080/tcp\":{}} (if you wish to expose port 8080 from the container)"
  },
  {
    "question": "What is the runtime performance cost of a Docker container? I'd like to comprehensively understand the run-time performance cost of a Docker container. I've found references to networking anecdotally being ~100\u00b5s slower.\nI've also found references to the run-time cost being \"negligible\" and \"close to zero\" but I'd like to know more precisely what those costs are. Ideally I'd like to know what Docker is abstracting with a performance cost and things that are abstracted without a performance cost. Networking, CPU, memory, etc.\nFurthermore, if there are abstraction costs, are there ways to get around the abstraction cost. For example, perhaps I can mount a disk directly vs. virtually in Docker.",
    "answer": "An excellent 2014 IBM research paper \u201cAn Updated Performance Comparison of Virtual Machines and Linux Containers\u201d by Felter et\u00a0al. provides a comparison between bare metal, KVM, and Docker containers. The general result is: Docker is nearly identical to native performance and faster than KVM in every category.\nThe exception to this is Docker\u2019s NAT\u200a\u2014\u2009if you use port mapping (e.g., docker run -p 8080:8080), then you can expect a minor hit in latency, as shown below. However, you can now use the host network stack (e.g., docker run --net=host) when launching a Docker container, which will perform identically to the Native column (as shown in the Redis latency results lower down).\n\nThey also ran latency tests on a few specific services, such as Redis. You can see that above 20 client threads, highest latency overhead goes Docker NAT, then KVM, then a rough tie between Docker host/native.\n\nJust because it\u2019s a really useful paper, here are some other figures. Please download it for full access.\nTaking a look at Disk I/O:\n\nNow looking at CPU overhead:\n\nNow some examples of memory (read the paper for details, memory can be extra tricky):"
  },
  {
    "question": "Docker - Ubuntu - bash: ping: command not found I've got a Docker container running Ubuntu which I did as follows:\n\n```\ndocker run -it ubuntu /bin/bash\n\n```\n\nhowever it doesn't seem to have ping. E.g.\n\n```\nbash: ping: command not found\n\n```\n\nDo I need to install that?\nSeems a pretty basic command to be missing. I tried whereis ping which doesn't report anything.",
    "answer": "apt-get update -y\napt-get install -y iputils-ping\ndocker commit -m \"Installed iputils-ping\" --author \"Your Name <name@domain.com>\" ContainerNameOrId yourrepository/imagename:tag\nFROM ubuntu\nRUN apt-get update && apt-get install -y iputils-ping\nCMD bash\n\nDocker images are pretty minimal, but you can install ping in your official ubuntu docker image via:\n\nChances are you don't need ping on your image, and just want to use it for testing purposes. Above example will help you out.\nBut if you need ping to exist on your image, you can create a Dockerfile or commit the container you ran the above commands into a new image.\nCommit:\n\nDockerfile:\n\nPlease note there are best practices on creating docker images, like clearing apt cache files afterwards and etc."
  },
  {
    "question": "How to access host port from docker container I have a docker container running jenkins. As part of the build process, I need to access a web server that is run locally on the host machine. Is there a way the host web server (which can be configured to run on a port) can be exposed to the jenkins container?\nI'm running docker natively on a Linux machine.\nUPDATE:\nIn addition to @larsks answer below, to get the IP address of the Host IP from the host machine, I do the following:\n\n```\nip addr show docker0 | grep -Po 'inet \\K[\\d.]+'\n\n```",
    "answer": "ip addr show docker0\n7: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default\nlink/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff\ninet 172.17.0.1/16 brd 172.17.255.255 scope global docker0\nvalid_lft forever preferred_lft forever\ninet6 fe80::f4d2:49ff:fedd:28a0/64 scope link\nvalid_lft forever preferred_lft forever\ndefault via 172.17.0.1 dev eth0\n172.17.0.0/16 dev eth0  src 172.17.0.4\nhostip=$(ip route show | awk '/default/ {print $3}')\necho $hostip\n\nWhen running Docker natively on Linux, you can access host services using the IP address of the docker0 interface.  From inside the container, this will be your default route.\nFor example, on my system:\n\nAnd inside a container:\n\nIt's fairly easy to extract this IP address using a simple shell\nscript:\n\nYou may need to modify the iptables rules on your host to permit\nconnections from Docker containers. Something like this will do the\ntrick:\n\nThis would permit access to any ports on the host from Docker\ncontainers.  Note that:\n\niptables rules are ordered, and this rule may or may not do the\nright thing depending on what other rules come before it.\n\nyou will only be able to access host services that are either (a)\nlistening on INADDR_ANY (aka 0.0.0.0) or that are explicitly\nlistening on the docker0 interface.\n\nIf you are using Docker on MacOS or Windows 18.03+, you can connect to the magic hostname host.docker.internal.\n\nLastly, under Linux you can run your container in the host network namespace by setting --net=host; in this case localhost on your host is the same as localhost inside the container, so containerized service will act like non-containerized services and will be accessible without any additional configuration."
  },
  {
    "question": "In a Dockerfile, How to update PATH environment variable? I have a dockerfile that download and builds GTK from source, but the following line is not updating my image's environment variable:\n\n```\nRUN PATH=\"/opt/gtk/bin:$PATH\"\nRUN export PATH\n\n```\n\nI read that that I should be using ENV to set environment values, but the following instruction doesn't seem to work either:\nENV PATH /opt/gtk/bin:$PATH\nThis is my entire Dockerfile:\n\n```\nFROM ubuntu\nRUN apt-get update\nRUN apt-get install -y golang gcc make wget git libxml2-utils libwebkit2gtk-3.0-dev libcairo2 libcairo2-dev libcairo-gobject2 shared-mime-info libgdk-pixbuf2.0-* libglib2-* libatk1.0-* libpango1.0-* xserver-xorg xvfb\n\n# Downloading GTKcd\nRUN wget http://ftp.gnome.org/pub/gnome/sources/gtk+/3.12/gtk+-3.12.2.tar.xz\nRUN tar xf gtk+-3.12.2.tar.xz\nRUN cd gtk+-3.12.2\n\n# Setting environment variables before running configure\nRUN CPPFLAGS=\"-I/opt/gtk/include\"\nRUN LDFLAGS=\"-L/opt/gtk/lib\"\nRUN PKG_CONFIG_PATH=\"/opt/gtk/lib/pkgconfig\"\nRUN export CPPFLAGS LDFLAGS PKG_CONFIG_PATH\nRUN ./configure --prefix=/opt/gtk\nRUN make\nRUN make install\n\n# running ldconfig after make install so that the newly installed libraries are found.\nRUN ldconfig\n\n# Setting the LD_LIBRARY_PATH environment variable so the systems dynamic linker can find the newly installed libraries.\nRUN LD_LIBRARY_PATH=\"/opt/gtk/lib\"\n\n# Updating PATH environment program so that utility binaries installed by the various libraries will be found.\nRUN PATH=\"/opt/gtk/bin:$PATH\"\nRUN export LD_LIBRARY_PATH PATH\n\n# Collecting garbage\nRUN rm -rf gtk+-3.12.2.tar.xz\n\n# creating go code root\nRUN mkdir gocode\nRUN mkdir gocode/src\nRUN mkdir gocode/bin\nRUN mkdir gocode/pkg\n\n# Setting the GOROOT and GOPATH enviornment variables, any commands created are automatically added to PATH\nRUN GOROOT=/usr/lib/go\nRUN GOPATH=/root/gocode\nRUN PATH=$GOPATH/bin:$PATH\nRUN export GOROOT GOPATH PATH\n\n```",
    "answer": "ENV PATH=\"$PATH:/opt/gtk/bin\"\n\nYou can use Environment Replacement in your Dockerfile as follows:\n\n(please note that when using ${PATH}, this might use the host's PATH instead of the Container)"
  },
  {
    "question": "What is the difference between &quot;expose&quot; and &quot;publish&quot; in Docker? I'm experimenting with Dockerfiles, and I think I understand most of the logic. However, I don't see the difference between \"exposing\" and \"publishing\" a port in this context.\nAll the tutorials I have seen first include the EXPOSE command in the Dockerfile:\n\n```\n...\nEXPOSE 8080\n...\n\n```\n\nThey then build an image from this Dockerfile:\n\n```\n$ docker build -t an_image - < Dockerfile\n\n```\n\nAnd then publish the same port as above when running the image:\n\n```\n$ docker run -d -p 8080 an_image\n\n```\n\nor publish all ports using\n\n```\n$ docker run -d -P an_image\n\n```\n\nWhat is the point of exposing a port in the Dockerfile, if it will be published anyway? Would there ever be a need to expose a port first, and not publish it later? Effectively, I would like to specify all the ports that I will use in the Dockerfile when creating the image, and then not bother with them again, running them simply with:\n\n```\n$ docker run -d an_image\n\n```\n\nIs this possible?",
    "answer": "Basically, you have three (four) options:\n\nNeither specify EXPOSE nor -p\nOnly specify EXPOSE\nSpecify EXPOSE and -p\nOnly specify -p which implicitly does EXPOSE\n\nIf you specify neither EXPOSE nor -p, the service in the container will only be accessible from inside the container itself.\n\nIf you EXPOSE a port, the service in the container is not accessible from outside Docker, but from inside other Docker containers. So this is good for inter-container communication.\n\nIf you EXPOSE and -p a port, the service in the container is accessible from anywhere, even outside Docker.\n\nIf you do -p, but do not EXPOSE, Docker does an implicit EXPOSE. This is because if a port is open to the public, it is automatically also open to other Docker containers. Hence -p includes EXPOSE. This is effectively same as 3).\n\nThe reason why both are separated is IMHO because:\n\nchoosing a host port depends on the host and hence does not belong to the Dockerfile (otherwise it would be depending on the host),\nand often it's enough if a service in a container is accessible from other containers.\n\nThe documentation explicitly states:\n\nThe EXPOSE instruction exposes ports for use within links.\n\nIt also points you to how to link containers (legacy feature), which basically is the inter-container communication I talked about."
  },
  {
    "question": "How can I expose more than 1 port with Docker? So I have 3 ports that should be exposed to the machine's interface. Is it possible to do this with a Docker container?",
    "answer": "docker run -p <host_port>:<container_port>\ndocker run -p <host_port1>:<container_port1> -p <host_port2>:<container_port2>\n\nTo expose just one port, this is what you need to do:\n\nTo expose multiple ports, simply provide multiple -p arguments:"
  },
  {
    "question": "How to push a docker image to a private repository I have a docker image tagged as me/my-image, and I have a private repo on the dockerhub named me-private.\nWhen I push my me/my-image, I end up always hitting the public repo.\nWhat is the exact syntax to specifically push my image to my private repo?",
    "answer": "docker tag [OPTIONS] IMAGE[:TAG] [REGISTRYHOST/][USERNAME/]NAME[:TAG]\ndocker push NAME[:TAG]\ndocker tag 518a41981a6a myRegistry.com/myImage\ndocker push myRegistry.com/myImage\n\nYou need to tag your image correctly first with your registryhost:\n\nThen docker push using that same tag.\n\nExample:"
  },
  {
    "question": "Difference between RUN and CMD in a Dockerfile I'm confused about when should I use CMD vs RUN. For example, to execute bash/shell commands (i.e. ls -la) I would always use CMD or is there a situation where I would use RUN? Trying to understand the best practices about these two similar Dockerfile directives.",
    "answer": "RUN is an image build step, the state of the container after a RUN command will be committed to the container image. A Dockerfile can have many RUN steps that layer on top of one another to build the image.\nCMD is the command the container executes by default when you launch the built image. A Dockerfile will only use the final CMD defined. The CMD can be overridden when starting a container with docker run $image $other_command.\nENTRYPOINT is also closely related to CMD and can modify the way a CMD is interpreted when a container is started from an image."
  },
  {
    "question": "How to enter in a Docker container already running with a new TTY I have a container that is running the Apache service in the foreground. I would like to be able to access the container from another shell in order to \"poke around\" inside it and examine the files. At the moment, if I attach to the container, I am left looking at the Apache daemon and cannot run any commands.\nIs it possible to attach another tty to a running container? Possibly, I can take advantage of the fact that Docker is actually just wrapping around LXC containers? I have tried sudo lxc-console -n [container-id] -t [1-4] but it appears that only one tty is made available and that is the one running the apache daemon. Perhaps there is a way to enable multiple lxc consoles during the build?\nI would rather not configure and build the container with an openssh service if possible.",
    "answer": "docker exec -it [container-id] bash\n\nWith docker 1.3, there is a new command docker exec. This allows you to enter a running container:\n\nNote: this assumes bash is installed on your container. You may run sh or whatever interactive shell is installed on the container."
  },
  {
    "question": "Communication between multiple docker-compose projects I have two separate docker-compose.yml files in two different folders:\n\n~/front/docker-compose.yml\n~/api/docker-compose.yml\n\nHow can I make sure that a container in front can send requests to a container in api?\nI know that --default-gateway option can be set using docker run for an individual container, so that a specific IP address can be assigned to this container, but it seems that this option is not available when using docker-compose.\nCurrently I end up doing a docker inspect my_api_container_id and look at the gateway in the output. It works but the problem is that this IP is randomly attributed, so I can't rely on it.\nAnother form of this question might thus be:\n\nCan I attribute a fixed IP address to a particular container using docker-compose?\n\nBut in the end what I'm looking after is:\n\nHow can two different docker-compose projects communicate with each other?",
    "answer": "services:\nfront:\n...\nnetworks:\n- some-net\nnetworks:\nsome-net:\ndriver: bridge\nservices:\napi:\n...\nnetworks:\n- front_some-net\nnetworks:\nfront_some-net:\nexternal: true\n\nYou just need to make sure that the containers you want to talk to each other are on the same network. Networks are a first-class docker construct, and not specific to compose.\n\n...\n\nNote: Your app\u2019s network is given a name based on the \u201cproject name\u201d, which is based on the name of the directory it lives in, in this case a prefix front_ was added\n\nThey can then talk to each other using the service name. From front you can do ping api and vice versa."
  },
  {
    "question": "kubectl apply vs kubectl create? What I understood by the documentation is that:\n\n\n\n```\nkubectl create\n\n```\n\nCreates a new k8s resource in the cluster\n\n\n```\nkubectl replace\n\n```\n\nUpdates a resource in the live cluster\n\n\n```\nkubectl apply\n\n```\n\nIf I want to do create + replace (Reference)\n\nMy questions are\n\nWhy are there three operations for doing the same task in a cluster?\nWhat are the use cases for these operations?\nHow do they differ from each other under the hood?",
    "answer": "Those are two different approaches:\nImperative Management\nkubectl create is what we call Imperative Management. On this approach you tell the Kubernetes API what you want to create, replace or delete, not how you want your K8s cluster world to look like.\nDeclarative Management\nkubectl apply is part of the Declarative Management approach, where changes that you may have applied to a live object (i.e. through scale) are \"maintained\" even if you apply other changes to the object.\n\nYou can read more about imperative and declarative management in the Kubernetes Object Management documentation.\n\ntl:dr\nThey do different things. If the resource exists, kubectl create will error out and kubectl apply will not error out."
  },
  {
    "question": "Kubernetes pod gets recreated when deleted I have started pods with command\n\n```\n$ kubectl run busybox \\\n--image=busybox \\\n--restart=Never \\\n--tty \\\n-i \\\n--generator=run-pod/v1\n\n```\n\nSomething went wrong, and now I can't delete this Pod.\nI tried using the methods described below but the Pod keeps being recreated.\n\n```\n$ kubectl delete pods  busybox-na3tm\npod \"busybox-na3tm\" deleted\n\n$ kubectl get pods\nNAME                                     READY     STATUS              RESTARTS   AGE\nbusybox-vlzh3                            0/1       ContainerCreating   0          14s\n\n$ kubectl delete pod busybox-vlzh3 --grace-period=0\n\n$ kubectl delete pods --all\npod \"busybox-131cq\" deleted\npod \"busybox-136x9\" deleted\npod \"busybox-13f8a\" deleted\npod \"busybox-13svg\" deleted\npod \"busybox-1465m\" deleted\npod \"busybox-14uz1\" deleted\npod \"busybox-15raj\" deleted\npod \"busybox-160to\" deleted\npod \"busybox-16191\" deleted\n\n$ kubectl get pods --all-namespaces\nNAMESPACE   NAME            READY     STATUS              RESTARTS   AGE\ndefault     busybox-c9rnx   0/1       RunContainerError   0          23s\n\n```",
    "answer": "kubectl get deployments --all-namespaces\nkubectl delete -n NAMESPACE deployment DEPLOYMENT\nkubectl get jobs\n\nkubectl get daemonsets.app --all-namespaces\n\nkubectl get daemonsets.extensions --all-namespaces\n\nYou need to delete the deployment, which should in turn delete the pods and the replica sets https://github.com/kubernetes/kubernetes/issues/24137\nTo list all deployments:\n\nThen to delete the deployment:\n\nWhere NAMESPACE is the namespace it's in, and DEPLOYMENT is the name of the deployment. If NAMESPACE is default, leave off the -n option altogether.\nIn some cases it could also be running due to a job or daemonset.\nCheck the following and run their appropriate delete command."
  },
  {
    "question": "kubectl logs - continuously ```\nkubectl logs <pod-id>\n\n```\n\ngets latest logs from my deployment - I am working on a bug and interested to know the logs at runtime - How can I get continuous stream of logs ?\nedit: corrected question at the end.",
    "answer": "kubectl logs -f <pod-id>\n\nYou can use the -f flag:\n-f, --follow=false: Specify if the logs should be streamed.\nhttps://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#logs"
  },
  {
    "question": "Kubernetes API - Get Pods on Specific Nodes Reading the Kubernets documentation it looks to be possible to select a certain range of pods based on labels. I want to select all the pods on one node but I don't want to label each pod on their corresponding node.\nAm I missing something from the documentation or is it just not possible to select by node?\nIf I do:\n\n```\nkubectl get pods \\\n--output=wide\n--namespace=$NS \\\n--server=$SERVER | head\n\n#=>\n\nNAME   READY     STATUS             RESTARTS   AGE       NODE\n\n```\n\nCan any of these headers be used as selector? If yes, how to do it with kubectl?  How to do it with the API?",
    "answer": "kubectl get pods --all-namespaces -o wide --field-selector spec.nodeName=<node>\n\nAs mentioned in the accepted answer the PR is now merged and you can get pods by node as follows:"
  },
  {
    "question": "Kubernetes: How do I delete clusters and contexts from kubectl config? kubectl config view shows contexts and clusters corresponding to clusters that I have deleted.\nHow can I remove those entries?\nThe command\n\n```\nkubectl config unset clusters\n\n```\n\nappears to delete all clusters. Is there a way to selectively delete cluster entries? What about contexts?",
    "answer": "kubectl config unset users.gke_project_zone_name\n\nkubectl config unset contexts.aws_cluster1-kubernetes\n\nkubectl config unset clusters.foobar-baz\n\nkubectl config unset takes a dot-delimited path. You can delete cluster/context/user entries by name. E.g.\n\nSide note, if you teardown your cluster using cluster/kube-down.sh (or gcloud if you use Container Engine), it will delete the associated kubeconfig entries. There is also a planned kubectl config rework for a future release to make the commands more intuitive/usable/consistent."
  },
  {
    "question": "How does kubectl port-forward create a connection? kubectl exposes commands that can be used to create a Service for an application and assigns an IP address to access it from internet.\nAs far as I understand, to access any application within Kubernetes cluster there should be a Service resource created and that should have an IP address which is accessible from an external network.\nBut in case of port-forward how does kubectl create a connection to the application without an IP address which is accessible externally?",
    "answer": "kubectl port-forward makes a specific Kubernetes API request.  That means the system running it needs access to the API server, and any traffic will get tunneled over a single HTTP connection.\nHaving this is really useful for debugging (if one specific pod is acting up you can connect to it directly; in a microservice environment you can talk to a back-end service you wouldn't otherwise expose) but it's not an alternative to setting up service objects.  When I've worked with kubectl port-forward it's been visibly slower than connecting to a pod via a service, and I've found seen the command just stop after a couple of minutes.  Again these aren't big problems for debugging, but they're not what I'd want for a production system."
  },
  {
    "question": "Listing all resources in a namespace I would like to see all resources in a namespace.\nDoing kubectl get all will, despite of the name, not list things like services and ingresses.\nIf I know the the type I can explicitly ask for that particular type, but it seems there is also no command for listing all possible types. (Especially kubectl get does for example not list custom types).\nHow to show all resources before for example deleting that namespace?",
    "answer": "kubectl api-resources --verbs=list --namespaced -o name \\\n| xargs -n 1 kubectl get --show-kind --ignore-not-found -l <label>=<value> -n <namespace>\n\nBased on this comment , the supported way to list all resources is to iterate through all the api versions listed by kubectl api-resources:\n\nkubectl api-resources enumerates the resource types available in your cluster.\nthis means you can combine it with kubectl get to actually list every instance of every resource type in a namespace:"
  },
  {
    "question": "Restart container within pod I have a pod test-1495806908-xn5jn with 2 containers. I'd like to restart one of them called container-test. Is it possible to restart a single container within a pod and how? If not, how do I restart the pod?\nThe pod was created using a deployment.yaml with:\n\n```\nkubectl create -f deployment.yaml\n\n```",
    "answer": "Is it possible to restart a single container\n\nNot through kubectl, although depending on the setup of your cluster you can \"cheat\" and docker kill the-sha-goes-here, which will cause kubelet to restart the \"failed\" container (assuming, of course, the restart policy for the Pod says that is what it should do)\n\nhow do I restart the pod\n\nThat depends on how the Pod was created, but based on the Pod name you provided, it appears to be under the oversight of a ReplicaSet, so you can just kubectl delete pod test-1495806908-xn5jn and kubernetes will create a new one in its place (the new Pod will have a different name, so do not expect kubectl get pods to return test-1495806908-xn5jn ever again)"
  },
  {
    "question": "kubectl get events only for a pod When I run kubectl -n abc-namespace describe pod my-pod-zl6m6, I get a lot of information about the pod along with the Events in the end.\nIs there a way to output just the Events of the pod either using kubectl describe or kubectl get commands?\nEdit:\nThis can now (kubernetes 1.29) be achieved via the following command -\nkubectl -n abc-namespace events --for pod/my-pod-zl6m6\nAll the answers below can be ignored as they refer to older versions of kubernetes",
    "answer": "kubectl get event --namespace abc-namespace --field-selector involvedObject.name=my-pod-zl6m6\n\nYou can use the event command of kubectl.\nTo filter for a specific pod you can use a field-selector:\n\nTo see what fields are possible you can use kubectl describe on any event."
  },
  {
    "question": "How can I update a secret on Kubernetes when it is generated from a file? I've created a secret using\n\n```\nkubectl create secret generic production-tls \\\n  --from-file=./tls.key \\\n  --from-file=./tls.crt\n\n```\n\nIf I'd like to update the values - how can I do this?",
    "answer": "kubectl create secret generic production-tls \\\n--save-config \\\n--dry-run=client \\\n--from-file=./tls.key --from-file=./tls.crt \\\n-o yaml | \\\nkubectl apply -f -\n\nThis should work:"
  },
  {
    "question": "Kubernetes sort pods by age I can sort my Kubernetes pods by name using:\n\n```\nkubectl get pods --sort-by=.metadata.name\n\n```\n\nHow can I sort them (or other resoures) by age using kubectl?",
    "answer": "Pods have status, which you can use to find out startTime.\nI guess something like kubectl get po --sort-by=.status.startTime should work.\nYou could also try:\n\nkubectl get po --sort-by='{.firstTimestamp}'.\nkubectl get pods --sort-by=.metadata.creationTimestamp Thanks @chris\n\nAlso apparently in Kubernetes 1.7 release, sort-by is broken.\nhttps://github.com/kubernetes/kubectl/issues/43\nHere's the bug report : https://github.com/kubernetes/kubernetes/issues/48602\nHere's the PR: https://github.com/kubernetes/kubernetes/pull/48659/files"
  },
  {
    "question": "How to pull environment variables with Helm charts I have my deployment.yaml file within the templates directory of Helm charts with several environment variables for the container I will be running using Helm.\nNow I want to be able to pull the environment variables locally from whatever machine the helm is ran so I can hide the secrets that way.\nHow do I pass this in and have helm grab the environment variables locally when I use Helm to run the application?\nHere is some part of my deployment.yaml file\n\n```\n...\n...\n    spec:\n      restartPolicy: Always\n      containers:\n        - name: sample-app\n          image: \"sample-app:latest\"\n          imagePullPolicy: Always\n          env:          \n            - name: \"USERNAME\"\n              value: \"app-username\"\n            - name: \"PASSWORD\"\n              value: \"28sin47dsk9ik\"\n...\n...\n\n```\n\nHow can I pull the value of USERNAME and PASSWORD from local environment variables when I run helm?\nIs this possible? If yes, then how do I do this?",
    "answer": "username: root\npassword: password\napiVersion: v1\nkind: Secret\nmetadata:\nname: {{ .Release.Name }}-auth\ndata:\npassword: {{ .Values.password | b64enc }}\nusername: {{ .Values.username | b64enc }}\n...\n...\nspec:\nrestartPolicy: Always\ncontainers:\n- name: sample-app\nimage: \"sample-app:latest\"\nimagePullPolicy: Always\nenv:\n- name: \"USERNAME\"\nvalueFrom:\nsecretKeyRef:\nkey:  username\nname: {{ .Release.Name }}-auth\n- name: \"PASSWORD\"\nvalueFrom:\nsecretKeyRef:\nkey:  password\nname: {{ .Release.Name }}-auth\n...\n...\nexport USERNAME=root-user\nhelm install --set username=$USERNAME ./mychart\nhelm install --dry-run --set username=$USERNAME --debug ./mychart\n[debug] Created tunnel using local port: '44937'\n\n[debug] SERVER: \"127.0.0.1:44937\"\n\n[debug] Original chart version: \"\"\n[debug] CHART PATH: /home/maruf/go/src/github.com/the-redback/kubernetes-yaml-drafts/helm-charts/mychart\n\nNAME:   irreverant-meerkat\nREVISION: 1\nRELEASED: Fri Apr 20 03:29:11 2018\nCHART: mychart-0.1.0\nUSER-SUPPLIED VALUES:\nusername: root-user\n\nCOMPUTED VALUES:\npassword: password\nusername: root-user\n\nHOOKS:\nMANIFEST:\n\n---\n\napiVersion: v1\nkind: Secret\nmetadata:\nname: irreverant-meerkat-auth\ndata:\npassword: password\nusername: root-user\n---\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: irreverant-meerkat\nlabels:\napp: irreverant-meerkat\nspec:\nreplicas: 1\ntemplate:\nmetadata:\nname: irreverant-meerkat\nlabels:\napp: irreverant-meerkat\nspec:\ncontainers:\n- name: irreverant-meerkat\nimage: alpine\nenv:\n- name: \"USERNAME\"\nvalueFrom:\nsecretKeyRef:\nkey:  username\nname: irreverant-meerkat-auth\n- name: \"PASSWORD\"\nvalueFrom:\nsecretKeyRef:\nkey:  password\nname: irreverant-meerkat-auth\n\nimagePullPolicy: IfNotPresent\nrestartPolicy: Always\nselector:\nmatchLabels:\napp: irreverant-meerkat\n\nYou can export the variable and use it while running helm install.\nBefore that, you have to modify your chart so that the value can be set while installation.\nSkip this part, if you already know, how to setup template fields.\n\nAs you don't want to expose the data, so it's better to have it saved as secret in kubernetes.\nFirst of all, add this two lines in your Values file, so that these two values can be set from outside.\n\nNow, add a secret.yaml file inside your template folder. and, copy this code snippet into that file.\n\nNow tweak your deployment yaml template and make changes in env section, like this\n\nIf you have modified your template correctly for --set flag,\nyou can set this using environment variable.\n\nNow use this variable while running helm install,\n\nIf you run this helm install in dry-run mode, you can verify the changes,\n\nYou can see that the data of username in secret has changed to root-user.\nI have added this example into github repo.\nThere is also some discussion in kubernetes/helm repo regarding this. You can see this issue to know about all other ways to use environment variables."
  },
  {
    "question": "Get error &quot;unknown field &quot;serviceName&quot; in io.k8s.api.networking.v1.IngressBackend&quot; when switch from v1beta1 to v1 in Kubernetes Ingress I had the below YAML for my Ingress and it worked (and continues to work):\n\n```\napiVersion: networking.k8s.io/v1beta1\nkind: Ingress\nmetadata:\n  name: test-ingress\n  namespace: test-layer\nannotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /$1\nspec:\n  rules:\n    - host: mylocalhost.com\n      http:\n        paths:\n          - path: /\n            backend:\n              serviceName: test-app\n              servicePort: 5000\n\n```\n\nHowever, it tells me that it's deprecated and I should change to using networking.k8s.io/v1. When I do that (see below) it throws an error.\n\n```\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: test-ingress\n  namespace: test-layer\nannotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /$1\nspec:\n  rules:\n    - host: mylocalhost.com\n      http:\n        paths:\n          - path: /\n            backend:\n              serviceName: test-app\n              servicePort: 5000\n\n```\n\nERROR\n\n```\nerror: error validating \"test-ingress.yaml\": \n  error validating data: [ValidationError(Ingress.spec.rules[0].http.paths[0].backend): \n    unknown field \"serviceName\" in io.k8s.api.networking.v1.IngressBackend, \n    ValidationError(Ingress.spec.rules[0].http.paths[0].backend): \n      unknown field \"servicePort\" in io.k8s.api.networking.v1.IngressBackend]; \n      if you choose to ignore these errors, turn validation off with --validate=false\n\n```\n\nOther than changing the API version, I made no other changes.\nkubectl version returns:\n\n```\nClient Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.0\", GitCommit:\"e19964183377d0ec2052d1f1fa930c4d7575bd50\", GitTreeState:\"clean\", BuildDate:\"2020-08-26T14:30:33Z\", GoVersion:\"go1.15\", Compiler:\"gc\", Platform:\"windows/amd64\"}\n\nServer Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.0\", GitCommit:\"e19964183377d0ec2052d1f1fa930c4d7575bd50\", GitTreeState:\"clean\", BuildDate:\"2020-08-26T14:23:04Z\", GoVersion:\"go1.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n\n```",
    "answer": "`Ingress` and `IngressClass` resources have graduated to `networking.k8s.io/v1`. Ingress and IngressClass types in the `extensions/v1beta1` and `networking.k8s.io/v1beta1` API versions are deprecated and will no longer be served in 1.22+. Persisted objects can be accessed via the `networking.k8s.io/v1` API. Notable changes in v1 Ingress objects (v1beta1 field names are unchanged):\n* `spec.backend` -> `spec.defaultBackend`\n* `serviceName` -> `service.name`\n* `servicePort` -> `service.port.name` (for string values)\n* `servicePort` -> `service.port.number` (for numeric values)\n* `pathType` no longer has a default value in v1; \"Exact\", \"Prefix\", or \"ImplementationSpecific\" must be specified\nOther Ingress API updates:\n* backends can now be resource or service backends\n* `path` is no longer required to be a valid regular expression\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\nname: minimal-ingress\nannotations:\nnginx.ingress.kubernetes.io/rewrite-target: /\nspec:\nrules:\n- http:\npaths:\n- path: /testpath\npathType: Prefix\nbackend:\nservice:\nname: test\nport:\nnumber: 80\n\nI think that this PR contains the change you're asking about.\n\nIf you look in the 1.19 Ingress doc, it looks like the new syntax would be:\n\nI unfortunately don't have a 1.19 cluster to test myself, but I think this is what you're running into."
  },
  {
    "question": "did you specify the right host or port? error on Kubernetes I have followed the helloword tutorial on http://kubernetes.io/docs/hellonode/.\nWhen I run:\n\n```\nkubectl run hello-node --image=gcr.io/PROJECT_ID/hello-node:v1 --port=8080\n\n```\n\nI get:\n\nThe connection to the server localhost:8080 was refused - did you specify the right host or port?\n\nWhy does the command line try to connect to the localhost?",
    "answer": "gcloud container clusters get-credentials \"CLUSTER NAME\"\n\nThe issue is that your kubeconfig is not right.\nTo auto-generate it run:\n\nThis worked for me."
  },
  {
    "question": "Tailing few lines from huge logs of kubectl logs -f kubectl logs -f pod shows all logs from the beginning and it becomes a problem when the log is huge and we have to wait for a few minutes to get the last log. Its become more worst when connecting remotely. Is there a way that we can tail the logs for the last 100 lines of logs and follow them?",
    "answer": "kubectl logs --tail=20 nginx\n\nkubectl logs --since=1h nginx\n\nIn a cluster best practices are to gather all logs in a single point through an aggregator and analyze them with a dedicated tool. For that reason in K8S, log command is quite basic.\nAnyway kubectl logs -h shows some options useful for you:\n\nSome tools with your requirements (and more) are available on github, some of which are:\n\nhttps://github.com/boz/kail\n\nhttps://github.com/stern/stern\n\nhttps://github.com/rogosprojects/klogs"
  },
  {
    "question": "Tell when Job is Complete I'm looking for a way to tell (from within a script) when a Kubernetes Job has completed. I want to then get the logs out of the containers and perform cleanup. \nWhat would be a good way to do this? Would the best way be to run kubectl describe job <job_name> and grep for 1 Succeeded or something of the sort?",
    "answer": "kubectl wait --for=condition=complete job/myjob\nkubectl wait --for=condition=complete --timeout=30s job/myjob\n\nSince version 1.11, you can do:\n\nand you can also set a timeout:"
  },
  {
    "question": "Execute bash command in pod with kubectl? How to execute a Bash command in the pod? I want to do everything with a single Bash command.\n\n```\n[root@master ~]# kubectl exec -it --namespace=\"tools\" mongo-pod --bash -c \"mongo\"\nError: unknown flag: --bash\n\n```\n\nSo, the command is simply ignored.\n\n```\n[root@master ~]# kubectl exec -it --namespace=\"tools\" mongo-pod bash -c \"mongo\"\nroot@mongo-deployment-78c87cb84-jkgxx:/# \n\n```\n\nOr so.\n\n```\n[root@master ~]# kubectl exec -it --namespace=\"tools\" mongo-pod bash mongo\nDefaulting container name to mongo.\nUse 'kubectl describe pod/mongo-deployment-78c87cb84-jkgxx -n tools' to see all of the containers in this pod.\n/usr/bin/mongo: /usr/bin/mongo: cannot execute binary file\ncommand terminated with exit code 126\n\n```\n\nIf it's just a Bash, it certainly works. But I want to jump into the mongo shell immediately.\nI found a solution, but it does not work. Tell me if this is possible now?\nExecuting multiple commands( or from a shell script) in a kubernetes pod",
    "answer": "kubectl exec -it --namespace=tools mongo-pod -- bash -c \"mongo\"\n\nThe double dash symbol \"--\" is used to separate the command you want to run inside the container from the kubectl arguments.\nSo the correct way is:\n\nYou forgot a space between \"--\" and \"bash\".\nTo execute multiple commands you may want:\n\nto create a script and mount it as a volume in your pod and execute it\n\nto launch a side container with the script and run it"
  },
  {
    "question": "How to configure kubectl with cluster information from a .conf file? I have an admin.conf file containing info about a cluster, so that the following command works fine:\n\n```\nkubectl --kubeconfig ./admin.conf get nodes\n\n```\n\nHow can I config kubectl to use the cluster, user and authentication from this file as default in one command? I only see separate set-cluster, set-credentials, set-context, use-context etc. I want to get the same output when I simply run:\n\n```\nkubectl get nodes\n\n```",
    "answer": "Here are the official documentation for how to configure kubectl\nhttps://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/\nYou have a few options, specifically to this question, you can just copy your admin.conf to ~/.kube/config"
  },
  {
    "question": "What is command to find detailed information about Kubernetes master(s) using kubectl? Let say I want to find the kubelet and apiserver version of my k8s master(s), what's the best way to do it?\nI am aware of the following commands:\n\n```\nkubectl cluster-info\n\n```\n\nwhich only shows the endpoints.\n\n```\nkubectl get nodes; kubectl describe node <node>;\n\n```\n\nwhich shows very detail information but only the nodes and not master.\nThere's also\n\n```\nkubectl version\n\n```\n\nbut that only shows the kubectl version and not the kubelet or apiserver version.\nWhat other commands can I use to identify the properties of my cluster?",
    "answer": "kubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"2\", GitVersion:\"v1.2.4\", GitCommit:\"3eed1e3be6848b877ff80a93da3785d9034d0a4f\", GitTreeState:\"clean\"}\nServer Version: version.Info{Major:\"1\", Minor:\"2\", GitVersion:\"v1.2.4\", GitCommit:\"3eed1e3be6848b877ff80a93da3785d9034d0a4f\", GitTreeState:\"clean\"}\n\nkubectl version also shows the apiserver version. For example, this is the output when I run it:\n\nThe second line (\"Server Version\") contains the apiserver version. There isn't a way to get the master's kubelet version if it isn't registered as one of the nodes (which it isn't if it isn't showing up in kubectl get nodes), but in most deployments it'll be the same version as the apiserver."
  },
  {
    "question": "kubectl how to list all contexts How do I view all the contexts I have set up with kubectl?\nIt doesn't appear there's a resource known to kubectl as 'contexts'\n\n```\n$ kubectl get contexts\nerror: the server doesn't have a resource type \"contexts\"\n\n```",
    "answer": "kubectl config get-contexts\n\nYou can view all contexts via the config command"
  },
  {
    "question": "kubectl unable to connect to server: x509: certificate signed by unknown authority i'm getting an error when running kubectl one one machine (windows)\nthe k8s cluster is running on CentOs 7 kubernetes cluster 1.7\nmaster, worker\nHere's my .kube\\config\n\n```\n\n  \napiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: REDACTED\n    server: https://10.10.12.7:6443\n  name: kubernetes\ncontexts:\n- context:\n    cluster: kubernetes\n    user: system:node:localhost.localdomain\n  name: system:node:localhost.localdomain@kubernetes\ncurrent-context: system:node:localhost.localdomain@kubernetes\nkind: Config\npreferences: {}\nusers:\n- name: system:node:localhost.localdomain\n  user:\n    client-certificate-data: REDACTED\n    client-key-data: REDACTED\n  \n\n```\n\nthe cluster is built using kubeadm with the default certificates on the pki directory\nkubectl unable to connect to server: x509: certificate signed by unknown authority",
    "answer": "kubectl proxy --address 0.0.0.0 --accept-hosts '.*'\nkubectl get nodes\nNAME                    STATUS    AGE       VERSION\ncentos-k8s2             Ready     3d        v1.7.5\nlocalhost.localdomain   Ready     3d        v1.7.5\n\nSorry I wasn't able to provide this earlier, I just realized the cause:\nSo on the master node we're running a kubectl proxy\n\nI stopped this and voila the error was gone.\nI'm now able to do\n\nI hope this helps those who stumbled upon this scenario."
  },
  {
    "question": "What&#39;s the meaning of &quot;READY=2/2&quot; output by command &quot;kubectl get pod $yourpod&quot; ```\nkubectl get pod run-sh-1816639685-xejyk \nNAME                      READY     STATUS    RESTARTS   AGE\nrun-sh-1816639685-xejyk   2/2       Running   0          26m\n\n```\n\nWhat's the meaning of READY=2/2? The same with 1/1?",
    "answer": "The \"Ready\" column shows how many containers in a pod are considered ready.\nYou can have some containers starting faster then others or having their readiness checks not yet fulfilled (or still in initial delay). In such cases there will be fewer containers ready in the pod than the total number (e.g. 1/2) hence the whole pod will not be considered ready."
  },
  {
    "question": "Get current image of kubernetes deployment How can I use kubectl or the API to retrieve the current image for containers in a pod or deployment?\nFor example, in a deployment created with the below configuration, I want to retrieve the value eu.gcr.io/test0/brain:latest.\n\n```\n   apiVersion: v1\n   kind: Deployment\n   metadata:\n     name: flags\n   spec:\n     replicas: 6\n     template:\n       metadata:\n      labels:\n        app: flags\n       spec:\n         containers:\n         - name: flags\n           image: eu.gcr.io/test0/brain:latest\n\n```",
    "answer": "kubectl get deployments -o wide\n\nFrom kubectl 1.6 the -o wide option does this, so\n\nwill show the current image in the output."
  },
  {
    "question": "Is there a way to kubectl apply all the files in a directory? I am writing an ansible playbook right now that deploys a dockerized application in kubernetes.  However, for molecularity purposes I would rather not hard code the files that need to be apply after doing kompose convert -f docker-compose.yaml --volumes hostPath  Is there a way to apply all the files in a directory?",
    "answer": "kubectl apply -f <folder>\nkubectl apply -k <folder>\n\nYou can apply all files in a folder with\n\nYou may also be interested in parameterization of your manifest files using Kustomize e.g. use more replicas in a prod-namespace than in a test-namespace. You can apply parameterized manifest files with"
  },
  {
    "question": "how to check whether RBAC is enabled, using kubectl I'm trying to install a helm package on a kubernetes cluster which allegedly has RBAC disabled.\nI'm getting a permission error mentioning clusterroles.rbac.authorization.k8s.io, which is what I'd expect if RBAC was enabled.\nIs there a way to check with kubectl whether RBAC really is disabled?\nWhat I've tried:\n\nkubectl describe nodes --all-namespaces | grep -i rbac : nothing comes up\nkubectl describe rbac --all-namespaces | grep -i rbac : nothing comes up\nkubectl config get-contexts | grep -i rbac : nothing comes up\nk get clusterroles it says \"No resources found\", not an error message. So does that mean that RBAC is enabled?\nkuebctl describe cluster isn't a thing\n\nI'm aware that maybe this is the x-y problem because it's possible the helm package I'm installing is expecting RBAC to be enabled. But still, I'd like to know how to check whether or not it is enabled/disabled.",
    "answer": "You can check this by executing the command kubectl api-versions; if RBAC is enabled you should see the API version .rbac.authorization.k8s.io/v1.\nIn AKS, the best way is to check the cluster's resource details at resources.azure.com.\nIf you can spot \"enableRBAC\": true, your cluster has RBAC enabled.\nPlease note that existing non-RBAC enabled AKS clusters cannot currently be updated for RBAC use. (thanks @DennisAmeling for the clarification)"
  },
  {
    "question": "What is the difference between kubectl apply and kubectl replace I am learning Kubernetes recently, and I am not very clear about the difference between \"kubectl apply\" and \"kubectl replace\". Is there any situation that we can only use one of them?",
    "answer": "I have written up a thorough explanation of the differences between apply, replace, and patch: Kubernetes Apply vs. Replace vs. Patch (update 2024-10-31: the Atomist blog no longer exists, use this Internet Archive link). It includes an explanation that the current top-ranked answer to this question is wrong.\nBriefly, kubectl apply uses the provided spec to create a resource if it does not exist and update, i.e., patch, it if it does.  The spec provided to apply need only contain the required parts of a spec, when creating a resource the API will use defaults for the rest and when updating a resource it will use its current values.\nThe kubectl replace completely replaces the existing resource with the one defined by the provided spec. replace wants a complete spec as input, including read-only properties supplied by the API like .metadata.resourceVersion, .spec.nodeName for pods, .spec.clusterIP for services, and .secrets for service accounts. kubectl has some internal tricks to help you get that right, but typically the use case for replace is getting a resource spec, changing a property, and then using that changed, complete spec to replace the existing resource.\nThe kubectl replace command has a --force option which actually does not use the replace, i.e., PUT, API endpoint. It forcibly deletes (DELETE) and then recreates, (POST) the resource using the provided spec."
  },
  {
    "question": "How to pass image pull secret while using &#39;kubectl run&#39; command? I am trying to use kubectl run command to pull an image from private registry and run a command from that. But I don't see an option to specify image pull secret. It looks like it is not possible to pass image secret as part for run command. \nIs there any alternate option to pull a container and run a command using kubectl? The command output should be seen on the console. Also once the command finishes the pod should die.",
    "answer": "kubectl run -i -t hello-world --rm --generator=run-pod/v1 \\\n--image=eu.gcr.io/your-registry/hello-world \\\n--image-pull-policy=\"IfNotPresent\" \\\n--overrides='{ \"spec\": { \"template\": { \"spec\": { \"imagePullSecrets\": [{\"name\": \"your-registry-secret\"}] } } } }'\n\nYou can use the overrides if you specify it right, it's an array in the end, that took me a bit to figure out, the below works on Kubernetes of at least 1.6:\n--overrides='{ \"spec\": { \"template\": { \"spec\": { \"imagePullSecrets\": [{\"name\": \"your-registry-secret\"}] } } } }'\nfor example"
  },
  {
    "question": "How to Add Users to Kubernetes (kubectl)? I've created a Kubernetes cluster on AWS with kops and can successfully administer it via kubectl from my local machine.\nI can view the current config with kubectl config view as well as directly access the stored state at ~/.kube/config, such as:\n\n```\napiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: REDACTED\n    server: https://api.{CLUSTER_NAME}\n  name: {CLUSTER_NAME}\ncontexts:\n- context:\n    cluster: {CLUSTER_NAME}\n    user: {CLUSTER_NAME}\n  name: {CLUSTER_NAME}\ncurrent-context: {CLUSTER_NAME}\nkind: Config\npreferences: {}\nusers:\n- name: {CLUSTER_NAME}\n  user:\n    client-certificate-data: REDACTED\n    client-key-data: REDACTED\n    password: REDACTED\n    username: admin\n- name: {CLUSTER_NAME}-basic-auth\n  user:\n    password: REDACTED\n    username: admin\n\n```\n\nI need to enable other users to also administer. This user guide describes how to define these on another users machine, but doesn't describe how to actually create the user's credentials within the cluster itself. How do you do this?\nAlso, is it safe to just share the cluster.certificate-authority-data?",
    "answer": "kubectl create sa alice\nsecret=$(kubectl get sa alice -o json | jq -r .secrets[].name)\nkubectl get secret $secret -o json | jq -r '.data[\"ca.crt\"]' | base64 -D > ca.crt\nuser_token=$(kubectl get secret $secret -o json | jq -r '.data[\"token\"]' | base64 -D)\nc=$(kubectl config current-context)\n\nname=$(kubectl config get-contexts $c | awk '{print $3}' | tail -n 1)\n\nendpoint=$(kubectl config view -o jsonpath=\"{.clusters[?(@.name == \\\"$name\\\")].cluster.server}\")\nbrew install kubectl\nkubectl config set-cluster cluster-staging \\\n--embed-certs=true \\\n--server=$endpoint \\\n--certificate-authority=./ca.crt\nkubectl config set-credentials alice-staging --token=$user_token\nkubectl config set-context alice-staging \\\n--cluster=cluster-staging \\\n--user=alice-staging \\\n--namespace=alice\nkubectl config use-context alice-staging\n{\n\"apiVersion\": \"abac.authorization.kubernetes.io/v1beta1\",\n\"kind\": \"Policy\",\n\"spec\": {\n\"user\": \"system:serviceaccount:default:alice\",\n\"namespace\": \"default\",\n\"resource\": \"*\",\n\"readonly\": true\n}\n}\n\nFor a full overview on Authentication, refer to the official Kubernetes docs on Authentication and Authorization\nFor users, ideally you use an Identity provider for Kubernetes (OpenID Connect).\nIf you are on GKE / ACS you integrate with respective Identity and Access Management frameworks\nIf you self-host kubernetes (which is the case when you use kops), you may use coreos/dex to integrate with LDAP / OAuth2 identity providers - a good reference is this detailed 2 part SSO for Kubernetes article.\nkops (1.10+) now has built-in authentication support which eases the integration with AWS IAM as identity provider if you're on AWS.\nfor Dex there are a few open source cli clients as follows:\n\nNordstrom/kubelogin\npusher/k8s-auth-example\n\nIf you are looking for a quick and easy (not most secure and easy to manage in the long run) way to get started, you may abuse serviceaccounts - with 2 options for specialised Policies to control access. (see below)\nNOTE since 1.6  Role Based Access Control is strongly recommended! this answer does not cover RBAC setup\nEDIT: Great, but outdated (2017-2018), guide by Bitnami on User setup with RBAC is also available.\nSteps to enable service account access are (depending on if your cluster configuration includes RBAC or ABAC policies, these accounts may have full Admin rights!):\nEDIT: Here is a bash script to automate Service Account creation - see below steps\n\nCreate service account for user Alice\n\nGet related secret\n\nGet ca.crt from secret (using OSX base64 with -D flag for decode)\n\nGet service account token from secret\n\nGet information from your kubectl config (current-context, server..)\n\nOn a fresh machine, follow these steps (given the ca.cert and $endpoint information retrieved above:\n\nInstall kubectl\n\nSet cluster (run in directory where ca.crt is stored)\n\nSet user credentials\n\nDefine the combination of alice user with the staging cluster\n\nSwitch current-context to alice-staging for the user\n\nTo control user access with policies (using ABAC), you need to create a policy file (for example):\n\nProvision this policy.json on every master node and add --authorization-mode=ABAC --authorization-policy-file=/path/to/policy.json flags to API servers\nThis would allow Alice (through her service account) read only rights to all resources in default namespace only."
  },
  {
    "question": "kubectl port forwarding timeout issue While using kubectl port-forward function I was able to succeed in port forwarding a local port to a remote port. However it seems that after a few minutes idling the connection is dropped. Not sure why that is so.\nHere is the command used to portforward:\n\n```\nkubectl --namespace somenamespace port-forward somepodname 50051:50051\n\n```\n\nError message:\n\n```\nForwarding from 127.0.0.1:50051 -> 50051\nForwarding from [::1]:50051 -> 50051\nE1125 17:18:55.723715    9940 portforward.go:178] lost connection to pod\n\n```\n\nWas hoping to be able to keep the connection up",
    "answer": "Seems there is a 5 minute timeout that can be overridden with kubelet parameters:\nhttps://github.com/kubernetes/kubernetes/issues/19231\n\nIf you want to pass something higher than 5 minutes (or unlimited) into your kubelets, you can specify the streaming-connection-idle-timeout. E.g.\n--streaming-connection-idle-timeout=4h to set it to 4 hours. Or:\n--streaming-connection-idle-timeout=0 to make it unlimited.\n(DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)"
  },
  {
    "question": "Kubectl error: the object has been modified; please apply your changes to the latest version and try again I am getting below error while trying to apply patch :\n\n```\ncore@dgoutam22-1-coreos-5760 ~ $ kubectl apply -f ads-central-configuration.yaml\nWarning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply\nError from server (Conflict): error when applying patch:\n{\"data\":{\"default\":\"{\\\"dedicated_redis_cluster\\\": {\\\"nodes\\\": [{\\\"host\\\": \\\"192.168.1.94\\\", \\\"port\\\": 6379}]}}\"},\"metadata\":{\"annotations\":{\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"apiVersion\\\":\\\"v1\\\",\\\"data\\\":{\\\"default\\\":\\\"{\\\\\\\"dedicated_redis_cluster\\\\\\\": {\\\\\\\"nodes\\\\\\\": [{\\\\\\\"host\\\\\\\": \\\\\\\"192.168.1.94\\\\\\\", \\\\\\\"port\\\\\\\": 6379}]}}\\\"},\\\"kind\\\":\\\"ConfigMap\\\",\\\"metadata\\\":{\\\"annotations\\\":{},\\\"creationTimestamp\\\":\\\"2018-06-27T07:19:13Z\\\",\\\"labels\\\":{\\\"acp-app\\\":\\\"acp-discovery-service\\\",\\\"version\\\":\\\"1\\\"},\\\"name\\\":\\\"ads-central-configuration\\\",\\\"namespace\\\":\\\"acp-system\\\",\\\"resourceVersion\\\":\\\"1109832\\\",\\\"selfLink\\\":\\\"/api/v1/namespaces/acp-system/configmaps/ads-central-configuration\\\",\\\"uid\\\":\\\"64901676-79da-11e8-bd65-fa163eaa7a28\\\"}}\\n\"},\"creationTimestamp\":\"2018-06-27T07:19:13Z\",\"resourceVersion\":\"1109832\",\"uid\":\"64901676-79da-11e8-bd65-fa163eaa7a28\"}}\nto:\n&{0xc4200bb380 0xc420356230 acp-system ads-central-configuration ads-central-configuration.yaml 0xc42000c970 4434 false}\n**for: \"ads-central-configuration.yaml\": Operation cannot be fulfilled on configmaps \"ads-central-configuration\": the object has been modified; please apply your changes to the latest version and try again**\ncore@dgoutam22-1-coreos-5760 ~ $ \n\n```",
    "answer": "---\napiVersion: \"apps/v1\"\nkind: \"Deployment\"\nmetadata:\nname: \"nginx-1\"\nnamespace: \"default\"\nlabels:\napp: \"nginx-1\"\nspec:\nreplicas: 3\nselector:\nmatchLabels:\napp: \"nginx-1\"\ntemplate:\nmetadata:\nlabels:\napp: \"nginx-1\"\nspec:\ncontainers:\n- name: \"nginx\"\nimage: \"nginx:latest\"\n---\napiVersion: \"autoscaling/v2beta1\"\nkind: \"HorizontalPodAutoscaler\"\nmetadata:\nname: \"nginx-1-hpa\"\nnamespace: \"default\"\nlabels:\napp: \"nginx-1\"\nspec:\nscaleTargetRef:\nkind: \"Deployment\"\nname: \"nginx-1\"\napiVersion: \"apps/v1\"\nminReplicas: 1\nmaxReplicas: 5\nmetrics:\n- type: \"Resource\"\nresource:\nname: \"cpu\"\ntargetAverageUtilization: 80\nWarning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply\nError from server (Conflict): error when applying patch:\n{\"metadata\":{\"annotations\":{\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"apiVersion\\\":\\\"apps/v1\\\",\\\"kind\\\":\\\"Deployment\\\",\\\"metadata\\\":{\\\"annotations\\\":{\\\"deployment.kubernetes.io/revision\\\":\\\"1\\\"},\\\"creationTimestamp\\\":\\\"2019-09-17T21:34:39Z\\\",\\\"generation\\\":1,\\\"labels\\\":{\\\"app\\\":\\\"nginx-1\\\"},\\\"name\\\":\\\"nginx-1\\\",\\\"namespace\\\":\\\"default\\\",\\\"resourceVersion\\\":\\\"218884\\\",\\\"selfLink\\\":\\\"/apis/apps/v1/namespaces/default/deployments/nginx-1\\\",\\\"uid\\\":\\\"f41c5b6f-d992-11e9-9adc-42010a80023b\\\"},\\\"spec\\\":{\\\"progressDeadlineSeconds\\\":600,\\\"replicas\\\":3,\\\"revisionHistoryLimit\\\":10,\\\"selector\\\":{\\\"matchLabels\\\":{\\\"app\\\":\\\"nginx-1\\\"}},\\\"strategy\\\":{\\\"rollingUpdate\\\":{\\\"maxSurge\\\":\\\"25%\\\",\\\"maxUnavailable\\\":\\\"25%\\\"},\\\"type\\\":\\\"RollingUpdate\\\"},\\\"template\\\":{\\\"metadata\\\":{\\\"creationTimestamp\\\":null,\\\"labels\\\":{\\\"app\\\":\\\"nginx-1\\\"}},\\\"spec\\\":{\\\"containers\\\":[{\\\"image\\\":\\\"nginx:latest\\\",\\\"imagePullPolicy\\\":\\\"Always\\\",\\\"name\\\":\\\"nginx\\\",\\\"resources\\\":{},\\\"terminationMessagePath\\\":\\\"/dev/termination-log\\\",\\\"terminationMessagePolicy\\\":\\\"File\\\"}],\\\"dnsPolicy\\\":\\\"ClusterFirst\\\",\\\"restartPolicy\\\":\\\"Always\\\",\\\"schedulerName\\\":\\\"default-scheduler\\\",\\\"securityContext\\\":{},\\\"terminationGracePeriodSeconds\\\":30}}},\\\"status\\\":{\\\"availableReplicas\\\":3,\\\"conditions\\\":[{\\\"lastTransitionTime\\\":\\\"2019-09-17T21:34:47Z\\\",\\\"lastUpdateTime\\\":\\\"2019-09-17T21:34:47Z\\\",\\\"message\\\":\\\"Deployment has minimum availability.\\\",\\\"reason\\\":\\\"MinimumReplicasAvailable\\\",\\\"status\\\":\\\"True\\\",\\\"type\\\":\\\"Available\\\"},{\\\"lastTransitionTime\\\":\\\"2019-09-17T21:34:39Z\\\",\\\"lastUpdateTime\\\":\\\"2019-09-17T21:34:47Z\\\",\\\"message\\\":\\\"ReplicaSet \\\\\\\"nginx-1-7b4bb7fbf8\\\\\\\" has successfully progressed.\\\",\\\"reason\\\":\\\"NewReplicaSetAvailable\\\",\\\"status\\\":\\\"True\\\",\\\"type\\\":\\\"Progressing\\\"}],\\\"observedGeneration\\\":1,\\\"readyReplicas\\\":3,\\\"replicas\\\":3,\\\"updatedReplicas\\\":3}}\\n\"},\"generation\":1,\"resourceVersion\":\"218884\"},\"spec\":{\"replicas\":3},\"status\":{\"availableReplicas\":3,\"observedGeneration\":1,\"readyReplicas\":3,\"replicas\":3,\"updatedReplicas\":3}}\nto:\nResource: \"apps/v1, Resource=deployments\", GroupVersionKind: \"apps/v1, Kind=Deployment\"\nName: \"nginx-1\", Namespace: \"default\"\nObject: &{map[\"apiVersion\":\"apps/v1\" \"metadata\":map[\"name\":\"nginx-1\" \"namespace\":\"default\" \"selfLink\":\"/apis/apps/v1/namespaces/default/deployments/nginx-1\" \"uid\":\"f41c5b6f-d992-11e9-9adc-42010a80023b\" \"generation\":'\\x02' \"labels\":map[\"app\":\"nginx-1\"] \"annotations\":map[\"deployment.kubernetes.io/revision\":\"1\"] \"resourceVersion\":\"219951\" \"creationTimestamp\":\"2019-09-17T21:34:39Z\"] \"spec\":map[\"replicas\":'\\x01' \"selector\":map[\"matchLabels\":map[\"app\":\"nginx-1\"]] \"template\":map[\"metadata\":map[\"labels\":map[\"app\":\"nginx-1\"] \"creationTimestamp\":<nil>] \"spec\":map[\"containers\":[map[\"imagePullPolicy\":\"Always\" \"name\":\"nginx\" \"image\":\"nginx:latest\" \"resources\":map[] \"terminationMessagePath\":\"/dev/termination-log\" \"terminationMessagePolicy\":\"File\"]] \"restartPolicy\":\"Always\" \"terminationGracePeriodSeconds\":'\\x1e' \"dnsPolicy\":\"ClusterFirst\" \"securityContext\":map[] \"schedulerName\":\"default-scheduler\"]] \"strategy\":map[\"type\":\"RollingUpdate\" \"rollingUpdate\":map[\"maxUnavailable\":\"25%\" \"maxSurge\":\"25%\"]] \"revisionHistoryLimit\":'\\n' \"progressDeadlineSeconds\":'\\u0258'] \"status\":map[\"observedGeneration\":'\\x02' \"replicas\":'\\x01' \"updatedReplicas\":'\\x01' \"readyReplicas\":'\\x01' \"availableReplicas\":'\\x01' \"conditions\":[map[\"message\":\"Deployment has minimum availability.\" \"type\":\"Available\" \"status\":\"True\" \"lastUpdateTime\":\"2019-09-17T21:34:47Z\" \"lastTransitionTime\":\"2019-09-17T21:34:47Z\" \"reason\":\"MinimumReplicasAvailable\"] map[\"lastTransitionTime\":\"2019-09-17T21:34:39Z\" \"reason\":\"NewReplicaSetAvailable\" \"message\":\"ReplicaSet \\\"nginx-1-7b4bb7fbf8\\\" has successfully progressed.\" \"type\":\"Progressing\" \"status\":\"True\" \"lastUpdateTime\":\"2019-09-17T21:34:47Z\"]]] \"kind\":\"Deployment\"]}\nfor: \"test.yaml\": Operation cannot be fulfilled on deployments.apps \"nginx-1\": the object has been modified; please apply your changes to the latest version and try again\n\nI am able to reproduce the issue in my test environment. Steps to reproduce:\n\nCreate a deployment from Kubernetes Engine > Workloads > Deploy\nInput your Application Name, Namespace, Labels\nSelect cluster or create new cluster\n\nYou are able to view the YAML file here and here is the sample:\n\nAfter deployment if you go to Kubernetes Engine > Workloads > nginx-1 (click on it)\na.) You will get Deployment details (Overview, Details, Revision history, events, YAML)\nb.) click on YAML and copy the content from YAML tab\nc.) create new YAML file and paste the content and save the file\nd.) Now if you run the command $kubectl apply -f newyamlfile.yaml, it will shows you the below error:\n\nTo solve the problem, you need to find the exact yaml file and then edit it as per your requirement, after that you can run $kubectl apply -f nginx-1.yaml\nHope this information finds you well."
  },
  {
    "question": "Install Google Cloud components error from gcloud command I am trying to install several GCP components from the gcloud command-line tool and always get the same error:\n\n```\n$ gcloud components list\n\nYour current Cloud SDK version is: 146.0.0\nThe latest available version is: 146.0.0\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                                                  Components                                                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502     Status    \u2502                         Name                         \u2502            ID            \u2502    Size   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Not Installed \u2502 App Engine Go Extensions                             \u2502 app-engine-go            \u2502  47.9 MiB \u2502\n\u2502 Not Installed \u2502 Bigtable Command Line Tool                           \u2502 cbt                      \u2502   3.8 MiB \u2502\n\u2502 Not Installed \u2502 Cloud Datalab Command Line Tool                      \u2502 datalab                  \u2502   < 1 MiB \u2502\n\u2502 Not Installed \u2502 Cloud Datastore Emulator                             \u2502 cloud-datastore-emulator \u2502  15.4 MiB \u2502\n\u2502 Not Installed \u2502 Cloud Datastore Emulator (Legacy)                    \u2502 gcd-emulator             \u2502  38.1 MiB \u2502\n\u2502 Not Installed \u2502 Cloud Pub/Sub Emulator                               \u2502 pubsub-emulator          \u2502  21.0 MiB \u2502\n\u2502 Not Installed \u2502 Google Container Registry's Docker credential helper \u2502 docker-credential-gcr    \u2502   3.3 MiB \u2502\n\u2502 Not Installed \u2502 gcloud app Java Extensions                           \u2502 app-engine-java          \u2502 128.3 MiB \u2502\n\u2502 Not Installed \u2502 gcloud app Python Extensions                         \u2502 app-engine-python        \u2502   7.2 MiB \u2502\n\u2502 Not Installed \u2502 kubectl                                              \u2502 kubectl                  \u2502  11.5 MiB \u2502\n\u2502 Installed     \u2502 BigQuery Command Line Tool                           \u2502 bq                       \u2502   < 1 MiB \u2502\n\u2502 Installed     \u2502 Cloud SDK Core Libraries                             \u2502 core                     \u2502   5.7 MiB \u2502\n\u2502 Installed     \u2502 Cloud Storage Command Line Tool                      \u2502 gsutil                   \u2502   2.8 MiB \u2502\n\u2502 Installed     \u2502 Default set of gcloud commands                       \u2502 gcloud                   \u2502           \u2502\n\u2502 Installed     \u2502 gcloud Alpha Commands                                \u2502 alpha                    \u2502   < 1 MiB \u2502\n\u2502 Installed     \u2502 gcloud Beta Commands                                 \u2502 beta                     \u2502   < 1 MiB \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n```\n\nThen attempting to install kubectl\n\n```\n$ gcloud components install kubectl\n\nYou cannot perform this action because this Cloud SDK installation is \nmanaged by an external package manager.  If you would like to get the \nlatest version, please see our main download page at:\nhttps://cloud.google.com/sdk/\nERROR: (gcloud.components.install) The component manager is disabled for this installation\n\n```\n\nAny idea why is this error raised?",
    "answer": "The Cloud SDK component manager only works if you don't install the SDK through another package manager.  If you want to use the component manager, you can install using one of these methods:\nhttps://cloud.google.com/sdk/downloads#versioned\nhttps://cloud.google.com/sdk/downloads#interactive\nAdditional packages are available in our deb and yum repos so all the same components are available, you just need to use your existing package manager to install them:\nhttps://cloud.google.com/sdk/downloads#apt-get\nhttps://cloud.google.com/sdk/downloads#yum"
  },
  {
    "question": "How to view logs of failed jobs with kubectl? I've created a Kubernetes job that has now failed. Where can I find the logs to this job?\nI'm not sure how to find the associated pod (I assume once the job fails it deletes the pod)?\nRunning kubectl describe job does not seem to show any relevant information:\n\n```\nName:           app-raiden-migration-12-19-58-21-11-2018\nNamespace:      localdev\nSelector:       controller-uid=c2fd06be-ed87-11e8-8782-080027eeb8a0\nLabels:         jobType=database-migration\nAnnotations:    kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"jobType\":\"database-migration\"},\"name\":\"app-raiden-migration-12-19-58-21-1...\nParallelism:    1\nCompletions:    1\nStart Time:     Wed, 21 Nov 2018 12:19:58 +0000\nPods Statuses:  0 Running / 0 Succeeded / 1 Failed\nPod Template:\n  Labels:  controller-uid=c2fd06be-ed87-11e8-8782-080027eeb8a0\n           job-name=app-raiden-migration-12-19-58-21-11-2018\n  Containers:\n   app:\n    Image:  pp3-raiden-app:latest\n    Port:   <none>\n    Command:\n      php\n      artisan\n      migrate\n    Environment:\n      DB_HOST:        local-mysql\n      DB_PORT:        3306\n      DB_DATABASE:    raiden\n      DB_USERNAME:    <set to the key 'username' in secret 'cloudsql-db-credentials'>  Optional: false\n      DB_PASSWORD:    <set to the key 'password' in secret 'cloudsql-db-credentials'>  Optional: false\n      LOG_CHANNEL:    stderr\n      APP_NAME:       Laravel\n      APP_KEY:        ABCDEF123ERD456EABCDEF123ERD456E\n      APP_URL:        http://192.168.99.100\n      OAUTH_PRIVATE:  <set to the key 'oauth_private.key' in secret 'laravel-oauth'>  Optional: false\n      OAUTH_PUBLIC:   <set to the key 'oauth_public.key' in secret 'laravel-oauth'>   Optional: false\n    Mounts:           <none>\n  Volumes:            <none>\nEvents:\n  Type     Reason                Age   From            Message\n  ----     ------                ----  ----            -------\n  Normal   SuccessfulCreate      2m    job-controller  Created pod: app-raiden-migration-12-19-58-21-11-2018-pwnjn\n  Warning  BackoffLimitExceeded  2m    job-controller  Job has reach the specified backoff limit\n\n```",
    "answer": "One other approach:\n\nkubectl describe job $JOB\nPod name is shown under \"Events\"\nkubectl logs $POD"
  },
  {
    "question": "Kubernetes / kubectl - &quot;A container name must be specified&quot; but seems like it is? I'm debugging log output from kubectl that states:\n\n```\nError from server (BadRequest): a container name must be specified for pod postgres-operator-49202276-bjtf4, choose one of: [apiserver postgres-operator]\n\n```\n\nOK, so that's an explanatory error message, but looking at my JSON template it ought to just create both containers specified, correct? What am I missing? (please forgive my ignorance.)\nI'm using just a standard kubectl create -f command to create the JSON file within a shell script. The JSON deployment file is as follows:\n\n```\n{\n    \"apiVersion\": \"extensions/v1beta1\",\n    \"kind\": \"Deployment\",\n    \"metadata\": {\n        \"name\": \"postgres-operator\"\n    },\n    \"spec\": {\n        \"replicas\": 1,\n        \"template\": {\n            \"metadata\": {\n                \"labels\": {\n                    \"name\": \"postgres-operator\"\n                }\n            },\n            \"spec\": {\n                \"containers\": [{\n                    \"name\": \"apiserver\",\n                    \"image\": \"$CCP_IMAGE_PREFIX/apiserver:$CO_IMAGE_TAG\",\n                    \"imagePullPolicy\": \"IfNotPresent\",\n                    \"env\": [{\n                        \"name\": \"DEBUG\",\n                        \"value\": \"true\"\n                    }],\n                    \"volumeMounts\": [{\n                        \"mountPath\": \"/config\",\n                        \"name\": \"apiserver-conf\",\n                        \"readOnly\": true\n                    }, {\n                        \"mountPath\": \"/operator-conf\",\n                        \"name\": \"operator-conf\",\n                        \"readOnly\": true\n                    }]\n                }, {\n                    \"name\": \"postgres-operator\",\n                    \"image\": \"$CCP_IMAGE_PREFIX/postgres-operator:$CO_IMAGE_TAG\",\n                    \"imagePullPolicy\": \"IfNotPresent\",\n                    \"env\": [{\n                        \"name\": \"DEBUG\",\n                        \"value\": \"true\"\n                    }, {\n                        \"name\": \"NAMESPACE\",\n                        \"valueFrom\": {\n                            \"fieldRef\": {\n                                \"fieldPath\": \"metadata.namespace\"\n                            }\n                        }\n                    }, {\n                        \"name\": \"MY_POD_NAME\",\n                        \"valueFrom\": {\n                            \"fieldRef\": {\n                                \"fieldPath\": \"metadata.name\"\n                            }\n                        }\n                    }],\n                    \"volumeMounts\": [{\n                        \"mountPath\": \"/operator-conf\",\n                        \"name\": \"operator-conf\",\n                        \"readOnly\": true\n                    }]\n                }],\n                \"volumes\": [{\n                    \"name\": \"operator-conf\",\n                    \"configMap\": {\n                        \"name\": \"operator-conf\"\n                    }\n                }, {\n                    \"name\": \"apiserver-conf\",\n                    \"configMap\": {\n                        \"name\": \"apiserver-conf\"\n                    }\n                }]\n            }\n        }\n    }\n}\n\n```",
    "answer": "kubectl logs deployment/postgres-operator -c apiserver\n\nkubectl logs deployment/postgres-operator -c postgres-operator\n\nIf a pod has more than 1 containers then you need to provide the name of the specific container.\nin your case, There is a pod (postgres-operator-49202276-bjtf4) which has 2 containers (apiserver and postgres-operator ).\nfollowing commands will provide logs for the specific containers"
  },
  {
    "question": "Kubernetes create deployment unexpected SchemaError I'm following that tutorial (https://www.baeldung.com/spring-boot-minikube)\nI want to create Kubernetes deployment in yaml file (simple-crud-dpl.yaml):\n\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: simple-crud\nspec:\n  selector:\n      matchLabels:\n        app: simple-crud\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: simple-crud\n    spec:\n      containers:\n        - name: simple-crud\n          image: simple-crud:latest\n          imagePullPolicy: Never\n          ports:\n            - containerPort: 8080\n\n```\n\nbut when I run kubectl create -f simple-crud-dpl.yaml i got:\nerror: SchemaError(io.k8s.api.autoscaling.v2beta2.MetricTarget): invalid object doesn't have additional properties\nI'm using the newest version of kubectl: \n\n```\nkubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"10\", GitVersion:\"v1.10.11\", GitCommit:\"637c7e288581ee40ab4ca210618a89a555b6e7e9\", GitTreeState:\"clean\", BuildDate:\"2018-11-26T14:38:32Z\", GoVersion:\"go1.9.3\", Compiler:\"gc\", Platform:\"darwin/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.0\", GitCommit:\"641856db18352033a0d96dbc99153fa3b27298e5\", GitTreeState:\"clean\", BuildDate:\"2019-03-25T15:45:25Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n\n```\n\nI'm also using minikube locally as it's described in tutorial. Everything is working till deployment and service. I'm not able to do it.",
    "answer": "After installing kubectl with brew you should run:\n\nrm /usr/local/bin/kubectl\nbrew link --overwrite kubernetes-cli\n\nAnd also optionally:\nbrew link --overwrite --dry-run kubernetes-cli."
  },
  {
    "question": "Can I set a default namespace in Kubernetes? Can I set the default namespace? That is:\n\n```\n$ kubectl get pods -n NAMESPACE\n\n```\n\nIt saves me having to type it in each time especially when I'm on the one namespace for most of the day.",
    "answer": "kubectl config set-context --current --namespace=NAMESPACE\n\nYes, you can set the namespace as per the docs like so:\n\nAlternatively, you can use kubectx for this."
  },
  {
    "question": "How to format the output of kubectl describe to JSON kubectl get command has this flag -o to format the output.\nIs there a similar way to format the output of the kubectl describe command?\nFor example:\n\n```\nkubectl describe -o=\"jsonpath={...}\" pods my-rc\n\n```\n\nwould print a JSON format for the list of pods in my-rc replication controller. But -o is not accepted for the describe command.",
    "answer": "kubectl get pods -l app=guestbook,tier=frontend -o name\npod/frontend-a4kjz\npod/frontend-am1ua\npod/frontend-yz2dq\n\nkubectl describe doesn't support -o or equivalent. It's meant to be human-readable rather than script-friendly. You can achieve what you described with kubectl get pods -l <selector_of_your_rc> -o <output_format>, for example:"
  },
  {
    "question": "Kubernetes POD delete with Pattern Match or Wildcard When I am using below it deletes the running POD after matching the pattern from commandline: \n\n```\nkubectl get pods -n bi-dev --no-headers=true | awk '/group-react/{print $1}' | xargs kubectl delete -n bi-dev pod\n\n```\n\nHowever when I am using this command as an alias in .bash_profile it doesn't execute . \nThis is how I defined it : \n\n```\n alias kdpgroup=\"kubectl get pods -n bi-dev --no-headers=true | awk '/group-react/{print $1}'|  kubectl delete -n bi-dev pod\"\n\n```\n\nwhen execute this as below I get below error in commandline:\n\n```\n~ $ kdpgroup\nerror: resource(s) were provided, but no name, label selector, or --all flag specified\n\n```\n\nWhen I define this in .bash_profile I get this : \n\n```\n~ $ . ./.bash_profile\n-bash: alias: }| xargs  kubectl delete -n bi-dev pod: not found\n~ $\n\n```\n\nAm I missing something to delete POD using Pattern Match or with Wilcard  ?\nthanks",
    "answer": "kubectl delete pods -l app=my-app\nkubectl delete pods -l app=my-app -n default\nkubectl delete pods -l 'app-type in (react, graphql)'\n\nAm I missing something to delete POD using Pattern Match or with Wilcard?\n\nWhen using Kubernetes it is more common to use labels and selectors. E.g. if you deployed an application, you usually set a label on the pods e.g. app=my-app and you can then get the pods with e.g. kubectl get pods -l app=my-app.\nUsing this aproach, it is easier to delete the pods you are interested in, with e.g.\n\nor with namespaces\n\nSee more on Kubernetes Labels and Selectors\nSet-based selector\n\nI have some pod's running in the name of \"superset-react\" and \"superset-graphql\" and I want to search my wildcard superset and delete both of them in one command\n\nI suggest that those pods has labels app=something-react and app=something-graphql. If you want to classify those apps, e.g. if your \"superset\" varies, you could add a label app-type=react and app-type=graphql to all those type of apps.\nThen you can delete pods for both app types with this command:"
  },
  {
    "question": "Kubernetes describe pod - Error from server (NotFound) I am trying to debug a pod with the status \"ImagePullBackOff\".\nThe pod is in the namespace minio-operator, but when I try to to describe the pod, it is apparently not found.\nWhy does that happen?\n\n```\n[psr-admin@zon-psr-2-u001 ~]$ kubectl get all -n minio-operator\nNAME                                  READY    STATUS              RESTARTS    AGE\npod/minio-operator-5dd99dd858-n6fdj   0/1      ImagepullBackoff    0           7d\n\nNAME                             READY.    UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/minio-operator   0         1            0           7d\n\nNAME                                        DESIRED   CURRENT    READY     AGE\nreplicaset.apps/minio-operator-5dd99dd858   1         1          0         7d\n[psr-admin@zon-psr-2-u001 ~]$ kubectl describe pod minio-operator-5dd99dd858-n6fdj\nError from server (NotFound): pods \"minio-operator-5dd99dd858-n6fdj\" not found\n\n```\n\nError from server (NotFound): pods \"minio-operator-5dd99dd858-n6fdj\" not found",
    "answer": "kubectl describe pod -n minio-operator <pod name>\n\nYou've not specified the namespace in your describe pod command.\nYou did kubectl get all -n minio-operator, which gets all resources in the minio-operator namespace, but your kubectl describe has no namespace, so it's looking in the default namespace for a pod that isn't there.\n\nShould work OK.\nMost resources in kubernetes are namespaced, so will require the -n <namespace> argument unless you switch namespaces."
  },
  {
    "question": "how to pass environment variable in kubectl deployment? I am setting up the kubernetes setup for django webapp.\nI am passing environment variable while creating deployment as below\n\n```\nkubectl create -f deployment.yml -l key1=value1 \n\n```\n\nI am getting error as below\n\n```\nerror: no objects passed to create\n\n```\n\nAble to create the deployment successfully, If i remove the env variable -l key1=value1 while creating deployment.\ndeployment.yaml as below\n\n```\n#Deployment\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata: \n labels: \n   service: sigma-service\n name: $key1\n\n```\n\nWhat will be the reason for causing the above error while creating deployment?",
    "answer": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: $NAME\nlabels:\napp: nginx\nspec:\nreplicas: 1\nselector:\nmatchLabels:\napp: nginx\ntemplate:\nmetadata:\nlabels:\napp: nginx\nspec:\ncontainers:\n- name: nginx\nimage: nginx:1.7.9\nports:\n- containerPort: 80\nexport NAME=my-test-nginx\nenvsubst < deployment.yaml | kubectl apply -f -\nbrew install gettext\nbrew link --force gettext\n\nI used envsubst (https://www.gnu.org/software/gettext/manual/html_node/envsubst-Invocation.html) for this. Create a deployment.yaml\n\nThen:\n\nNot sure what OS are you using to run this. On macOS, envsubst installed like:"
  },
  {
    "question": "kubectl how to rename a context I have many contexts, one for staging, one for production, and many for dev clusters. Copy and pasting the default cluster names is tedious and hard, especially over time. How can I rename them to make context switching easier?",
    "answer": "kubectl config rename-context old-name new-name\nkubectl config get-contexts\n\nRenaming contexts is easy!\n\nConfirm the change by"
  },
  {
    "question": "Kubectl error: memcache.go:265] couldn\u2019t get current server API group list: Get Everything was running smoothly with my Kubernetes clusters until today: after performing an update on my Ubuntu system, I'm now unable to establish a connection from my working environment to the kubernetes clusters.\nWhen executing the command kubectl get pods, I'm encountering the following error message:\nE0805 09:59:45.750534  234576 memcache.go:265] couldn\u2019t get current server API group list: Get \"http://localhost:3334/api?timeout=32s\": EOF\nHere are the details of my cluster setup: Kubernetes 1.27, bare-metal, Host System is Ubuntu 20.04",
    "answer": "kubectl get nodes -v=10\n\nTry\n\nand look for the errors."
  },
  {
    "question": "Define size for /dev/shm on container engine I'm running Chrome with xvfb on Debian 8. It works until I open a tab and try to load content. The process dies silently...\nFortunately, I have gotten it to run smoothly on my local docker using docker run --shm-size=1G.\nThere is a known bug in Chrome that causes it to crash when /dev/shm is too small.\nI am deploying to Container engine, and inspecting the OS specs. The host OS has a solid 7G mounted to /dev/shm, but the actual container is only allocated 64M. Chrome crashes.\nHow can I set the size of /dev/shm when using kubectl to deploy to container engine?",
    "answer": "spec:\nvolumes:\n- name: dshm\nemptyDir:\nmedium: Memory\ncontainers:\n- image: gcr.io/project/image\nvolumeMounts:\n- mountPath: /dev/shm\nname: dshm\n\nMounting an emptyDir to /dev/shm and setting the medium to Memory did the trick!"
  },
  {
    "question": "Kubectl command to list pods of a deployment in Kubernetes Is there a way to use kubectl to list only the pods belonging to a deployment?\nCurrently, I do this to get pods:\n\n```\nkubectl get pods| grep hello\n\n```\n\nBut it seems an overkill to get ALL the pods when I am interested to know only the pods for a given deployment. I use the output of this command to see the status of all pods, and then possibly exec into one of them.\nI also tried kc get -o wide deployments hellodeployment, but it does not print the Pod names.",
    "answer": "kubectl get pods -l=app=http-svc\nkubectl get pods --selector=app=http-svc\n\nkubectl get pods --selector key1=value1,key2=value2\nkubectl describe deploy/DEPLOYMENT_NAME | grep Selector\n\nThere's a label in the pod for the selector in the deployment. That's how a deployment manages its pods. For example for the label or selector app=http-svc you can do something like that this and avoid using grep and listing all the pods (this becomes useful as your number of pods becomes very large)\nHere are some examples command line:\n\nTo get the label used by your deployment:"
  },
  {
    "question": "How to create a kubectl config file for serviceaccount I have a kubernetes cluster on Azure and I created 2 namespaces and 2 service accounts because I have two teams deploying on the cluster.\nI want to give each team their own kubeconfig file for the serviceaccount I created. \nI am pretty new to Kubernetes and haven't been able to find a clear instruction on the kubernetes website. How do I create a kube config file for a serviceaccount?\nHopefully someone can help me out :), I rather not give the default kube config file to the teams.\nWith kind regards,\nBram",
    "answer": "server=https://localhost:8443\n\nname=default-token-sg96k\n\nca=$(kubectl get secret/$name -o jsonpath='{.data.ca\\.crt}')\ntoken=$(kubectl get secret/$name -o jsonpath='{.data.token}' | base64 --decode)\nnamespace=$(kubectl get secret/$name -o jsonpath='{.data.namespace}' | base64 --decode)\n\necho \"\napiVersion: v1\nkind: Config\nclusters:\n- name: default-cluster\ncluster:\ncertificate-authority-data: ${ca}\nserver: ${server}\ncontexts:\n- name: default-context\ncontext:\ncluster: default-cluster\nnamespace: default\nuser: default-user\ncurrent-context: default-context\nusers:\n- name: default-user\nuser:\ntoken: ${token}\n\" > sa.kubeconfig"
  },
  {
    "question": "Wait for kubernetes job to complete on either failure/success using command line What is the best way to wait for kubernetes job to be complete?  I noticed a lot of suggestions to use:\n\n```\nkubectl wait --for=condition=complete job/myjob\n\n```\n\nbut i think that only works if the job is successful. if it fails, i have to do something like:\n\n```\nkubectl wait --for=condition=failed job/myjob\n\n```\n\nis there a way to wait for both conditions using wait?  if not, what is the best way to wait for a job to either succeed or fail?",
    "answer": "apiVersion: batch/v1\nkind: Job\nmetadata:\nname: pi\nspec:\nparallelism: 1\ncompletions: 1\ntemplate:\nmetadata:\nname: pi\nspec:\ncontainers:\n- name: pi\nimage: perl\ncommand: [\"perl\",  \"-wle\", \"exit 0\"]\nrestartPolicy: Never\noc get job/pi -o=jsonpath='{.status}' -w &&\noc get job/pi -o=jsonpath='{.status.conditions[*].type}' | grep -i -E 'failed|complete' || echo \"Failed\"\n\njob.batch/pi created\nmap[startTime:2019-03-09T12:30:16Z active:1]Complete\napiVersion: batch/v1\nkind: Job\nmetadata:\nname: pi\nspec:\nparallelism: 1\ncompletions: 1\ntemplate:\nmetadata:\nname: pi\nspec:\ncontainers:\n- name: pi\nimage: perl\ncommand: [\"perl\",  \"-wle\", \"exit 1\"]\nrestartPolicy: Never\njob.batch \"pi\" deleted\n\noc get job/pi -o=jsonpath='{.status}' -w &&\noc get job/pi -o=jsonpath='{.status.conditions[*].type}' | grep -i -E 'failed|complete' || echo \"Failed\"\n\njob.batch/pi created\nmap[active:1 startTime:2019-03-09T12:31:05Z]Failed\n\nkubectl wait --for=condition=<condition name is waiting for a specific condition, so afaik it can not specify multiple conditions at the moment.\nMy workaround is using oc get --wait, --wait is closed the command if the target resource is updated. I will monitor status section of the job using oc get --wait until status is updated. Update of status section is meaning the Job is complete with some status conditions.\nIf the job complete successfully, then status.conditions.type is updated immediately as Complete. But if the job is failed then the job pod will be restarted automatically regardless restartPolicy is OnFailure or Never. But we can deem the job is Failed status if not to updated as Complete after first update.\nLook the my test evidence as follows.\n\nJob yaml for testing successful complete\n\nIt will show you Complete if it complete the job successfully.\n\nJob yaml for testing failed complete\n\nIt will show you Failed if the first job update is not Complete. Test if after delete the existing job resource.\n\nI hope it help you. :)"
  },
  {
    "question": "How to see logs of terminated pods I am running selenium hubs and my pods are getting terminated frequently. I would like to look at the logs of the pods which are terminated. How to do it?\n\n```\nNAME                                               READY     STATUS              RESTARTS   AGE\nchrome-75-0-0e5d3b3d-3580-49d1-bc25-3296fdb52666   0/2       Terminating         0          49s\nchrome-75-0-29bea6df-1b1a-458c-ad10-701fe44bb478   0/2       Terminating         0          23s\nchrome-75-0-8929d8c8-1f7b-4eba-96f2-918f7a0d77f5   0/2       ContainerCreating   0          7s\n\nkubectl logs chrome-75-0-8929d8c8-1f7b-4eba-96f2-918f7a0d77f5\nError from server (NotFound): pods \"chrome-75-0-8929d8c8-1f7b-4eba-96f2-918f7a0d77f5\" not found\n$ kubectl logs chrome-75-0-8929d8c8-1f7b-4eba-96f2-918f7a0d77f5 --previous\nError from server (NotFound): pods \"chrome-75-0-8929d8c8-1f7b-4eba-96f2-918f7a0d77f5\" not found\n\n```",
    "answer": "Running kubectl logs -p will fetch logs from existing resources at API level. This means that terminated pods' logs will be unavailable using this command.\nAs mentioned in other answers, the best way is to have your logs centralized via logging agents or directly pushing these logs into an external service.\nAlternatively and given the logging architecture in Kubernetes, you might be able to fetch the logs directly from the log-rotate files in the node hosting the pods. However, this option might depend on the Kubernetes implementation as log files might be deleted when the pod eviction is triggered."
  },
  {
    "question": "kubeadm init shows kubelet isn&#39;t running or healthy I am trying to run Kubernetes and trying to use sudo kubeadm init.\nSwap is off as recommended by official doc.\nThe issue is it displays the warning:\n\n```\n[kubelet-check] It seems like the kubelet isn't running or healthy.\n[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get http://localhost:10248/healthz: dial tcp 127.0.0.1:10248: connect: connection refused.\n[kubelet-check] It seems like the kubelet isn't running or healthy.\n[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get http://localhost:10248/healthz: dial tcp 127.0.0.1:10248: connect: connection refused.\n[kubelet-check] It seems like the kubelet isn't running or healthy.\n[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get http://localhost:10248/healthz: dial tcp 127.0.0.1:10248: connect: connection refused.\n[kubelet-check] It seems like the kubelet isn't running or healthy.\n[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get http://localhost:10248/healthz: dial tcp 127.0.0.1:10248: connect: connection refused.\n[kubelet-check] It seems like the kubelet isn't running or healthy.\n[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get http://localhost:10248/healthz: dial tcp 127.0.0.1:10248: connect: connection refused.\n\n\nUnfortunately, an error has occurred:\n            timed out waiting for the condition\n\nThis error is likely caused by:\n            - The kubelet is not running\n            - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)\n            - No internet connection is available so the kubelet cannot pull or find the following control plane images:\n                - k8s.gcr.io/kube-apiserver-amd64:v1.11.2\n                - k8s.gcr.io/kube-controller-manager-amd64:v1.11.2\n                - k8s.gcr.io/kube-scheduler-amd64:v1.11.2\n                - k8s.gcr.io/etcd-amd64:3.2.18\n                - You can check or miligate this in beforehand with \"kubeadm config images pull\" to make sure the images\n                  are downloaded locally and cached.\n\n        If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:\n            - 'systemctl status kubelet'\n            - 'journalctl -xeu kubelet'\n\n        Additionally, a control plane component may have crashed or exited when started by the container runtime.\n        To troubleshoot, list all containers using your preferred container runtimes CLI, e.g. docker.\n        Here is one example how you may list all Kubernetes containers running in docker:\n            - 'docker ps -a | grep kube | grep -v pause'\n            Once you have found the failing container, you can inspect its logs with:\n            - 'docker logs CONTAINERID'\ncouldn't initialize a Kubernetes cluster\n\n```\n\nThe docker version I am using is Docker version 17.03.2-ce, build f5ec1e2\nI m using Ubuntu 16.04 LTS 64bit\nThe docker images shows the following images:\n\n```\nREPOSITORY                                 TAG                 IMAGE ID            CREATED             SIZE\nk8s.gcr.io/kube-apiserver-amd64            v1.11.2             821507941e9c        3 weeks ago         187 MB\nk8s.gcr.io/kube-controller-manager-amd64   v1.11.2             38521457c799        3 weeks ago         155 MB\nk8s.gcr.io/kube-proxy-amd64                v1.11.2             46a3cd725628        3 weeks ago         97.8 MB\nk8s.gcr.io/kube-scheduler-amd64            v1.11.2             37a1403e6c1a        3 weeks ago         56.8 MB\nk8s.gcr.io/coredns                         1.1.3               b3b94275d97c        3 months ago        45.6 MB\nk8s.gcr.io/etcd-amd64                      3.2.18              b8df3b177be2        4 months ago        219 MB\nk8s.gcr.io/pause                           3.1                 da86e6ba6ca1        8 months ago        742 kB\n\n```\n\nFull logs can be found here :\nhttps://pastebin.com/T5V0taE3\nI didn't found any solution on internet.\nEDIT:\ndocker ps -a output:\n\n```\nubuntu@ubuntu-HP-Pavilion-15-Notebook-PC:~$ docker ps -a\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS\n\n```\n\njournalctl -xeu kubelet output:\n\n```\njournalctl -xeu kubelet\n-- Subject: Unit kubelet.service has finished shutting down\n-- Defined-By: systemd\n-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel\n-- \n-- Unit kubelet.service has finished shutting down.\nSep 01 10:40:05 ubuntu-HP-Pavilion-15-Notebook-PC systemd[1]: Started kubelet: T\n-- Subject: Unit kubelet.service has finished start-up\n-- Defined-By: systemd\n-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel\n-- \n-- Unit kubelet.service has finished starting up.\n-- \n-- The start-up result is done.\nSep 01 10:40:06 ubuntu-HP-Pavilion-15-Notebook-PC kubelet[9107]: Flag --cgroup-d\nSep 01 10:40:06 ubuntu-HP-Pavilion-15-Notebook-PC kubelet[9107]: Flag --cgroup-d\nSep 01 10:40:06 ubuntu-HP-Pavilion-15-Notebook-PC kubelet[9107]: I0901 10:40:06.\nSep 01 10:40:06 ubuntu-HP-Pavilion-15-Notebook-PC kubelet[9107]: I0901 10:40:06.\nSep 01 10:40:06 ubuntu-HP-Pavilion-15-Notebook-PC kubelet[9107]: I0901 10:40:06.\nSep 01 10:40:06 ubuntu-HP-Pavilion-15-Notebook-PC kubelet[9107]: I0901 10:40:06.\nSep 01 10:40:06 ubuntu-HP-Pavilion-15-Notebook-PC kubelet[9107]: F0901 10:40:06.\nSep 01 10:40:06 ubuntu-HP-Pavilion-15-Notebook-PC systemd[1]: kubelet.service: M\nSep 01 10:40:06 ubuntu-HP-Pavilion-15-Notebook-PC systemd[1]: kubelet.service: U\nSep 01 10:40:06 ubuntu-HP-Pavilion-15-Notebook-PC systemd[1]: kubelet.service: F\nlines 788-810/810 (END)\n-- Subject: Unit kubelet.service has finished shutting down\n-- Defined-By: systemd\n-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel\n-- \n-- Unit kubelet.service has finished shutting down.\nSep 01 10:40:05 ubuntu-HP-Pavilion-15-Notebook-PC systemd[1]: Started kubelet: The Kubernetes Node Agent.\n-- Subject: Unit kubelet.service has finished start-up\n-- Defined-By: systemd\n-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel\n-- \n-- Unit kubelet.service has finished starting up.\n-- \n-- The start-up result is done.\nSep 01 10:40:06 ubuntu-HP-Pavilion-15-Notebook-PC kubelet[9107]: Flag --cgroup-driver has been deprecated, This parameter should be set via the\nSep 01 10:40:06 ubuntu-HP-Pavilion-15-Notebook-PC kubelet[9107]: Flag --cgroup-driver has been deprecated, This parameter should be set via the\nSep 01 10:40:06 ubuntu-HP-Pavilion-15-Notebook-PC kubelet[9107]: I0901 10:40:06.117131    9107 server.go:408] Version: v1.11.2\nSep 01 10:40:06 ubuntu-HP-Pavilion-15-Notebook-PC kubelet[9107]: I0901 10:40:06.117406    9107 plugins.go:97] No cloud provider specified.\nSep 01 10:40:06 ubuntu-HP-Pavilion-15-Notebook-PC kubelet[9107]: I0901 10:40:06.121192    9107 certificate_store.go:131] Loading cert/key pair \nSep 01 10:40:06 ubuntu-HP-Pavilion-15-Notebook-PC kubelet[9107]: I0901 10:40:06.145720    9107 server.go:648] --cgroups-per-qos enabled, but --\nSep 01 10:40:06 ubuntu-HP-Pavilion-15-Notebook-PC kubelet[9107]: F0901 10:40:06.146074    9107 server.go:262] failed to run Kubelet: Running wi\nSep 01 10:40:06 ubuntu-HP-Pavilion-15-Notebook-PC systemd[1]: kubelet.service: Main process exited, code=exited, status=255/n/a\nSep 01 10:40:06 ubuntu-HP-Pavilion-15-Notebook-PC systemd[1]: kubelet.service: Unit entered failed state.\nSep 01 10:40:06 ubuntu-HP-Pavilion-15-Notebook-PC systemd[1]: kubelet.service: Failed with result 'exit-code'.\n~\n\n\n          PORTS               NAMES\n\n```\n\nAny help/suggestion/comment would be appreciated.",
    "answer": "sudo swapoff -a\nsudo sed -i '/ swap / s/^/#/' /etc/fstab\n\nUnfortunately, swap was still enabled.\nThe error was fixed by\n\nreboot the machine."
  },
  {
    "question": "Create kubernetes pod with volume using kubectl run I understand that you can create a pod with Deployment/Job using kubectl run.  But is it possible to create one with a volume attached to it?  I tried running this command:\n\n```\nkubectl run -i --rm --tty ubuntu --overrides='{ \"apiVersion\":\"batch/v1\", \"spec\": {\"containers\": {\"image\": \"ubuntu:14.04\", \"volumeMounts\": {\"mountPath\": \"/home/store\", \"name\":\"store\"}}, \"volumes\":{\"name\":\"store\", \"emptyDir\":{}}}}' --image=ubuntu:14.04 --restart=Never -- bash\n\n```\n\nBut the volume does not appear in the interactive bash.\nIs there a better way to create a pod with volume that you can attach to?",
    "answer": "kubectl run -i --rm --tty ubuntu --overrides='\n{\n\"apiVersion\": \"batch/v1\",\n\"spec\": {\n\"template\": {\n\"spec\": {\n\"containers\": [\n{\n\"name\": \"ubuntu\",\n\"image\": \"ubuntu:14.04\",\n\"args\": [\n\"bash\"\n],\n\"stdin\": true,\n\"stdinOnce\": true,\n\"tty\": true,\n\"volumeMounts\": [{\n\"mountPath\": \"/home/store\",\n\"name\": \"store\"\n}]\n}\n],\n\"volumes\": [{\n\"name\":\"store\",\n\"emptyDir\":{}\n}]\n}\n}\n}\n}\n'  --image=ubuntu:14.04 --restart=Never -- bash\nkubectl get job ubuntu -o json\n\nYour JSON override is specified incorrectly. Unfortunately kubectl run just ignores fields it doesn't understand.\n\nTo debug this issue I ran the command you specified, and then in another terminal ran:\n\nFrom there you can see that the actual job structure differs from your json override (you were missing the nested template/spec, and volumes, volumeMounts, and containers need to be arrays)."
  },
  {
    "question": "kubectl wait --for=condition=complete --timeout=30s I am trying to check the status of a pod using kubectl wait command through this documentation.\nFollowing is the command that i am trying\n\n```\nkubectl wait --for=condition=complete --timeout=30s -n d1 job/test-job1-oo-9j9kj\n\n```\n\nFollowing is the error that i am getting\n\n```\nKubectl error: status.conditions accessor error: Failure is of the type string, expected map[string]interface{}\n\n```\n\nand my kubectl -o json output can be accessed via this github link.\nCan someone help me to fix the issue",
    "answer": "kubectl wait --for=condition=complete pod/mypod-xxxxxxxxxx-xxxxx\nerror: .status.conditions accessor error: Failure is of the type string, expected map[string]interface{}\n\nThis totally looks like you are running kubectl wait --for=condition=complete on a Pod as described in your output rather than a Job.\nA pod doesn't have the --for=condition=complete option. Exactly, what I get when I run it on a pod:"
  },
  {
    "question": "How to view members of subject with Group kind There is a default ClusterRoleBinding named cluster-admin.\nWhen I run kubectl get clusterrolebindings cluster-admin -o yaml I get:  \n\n```\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  annotations:\n    rbac.authorization.kubernetes.io/autoupdate: \"true\"\n  creationTimestamp: 2018-06-13T12:19:26Z\n  labels:\n    kubernetes.io/bootstrapping: rbac-defaults\n  name: cluster-admin\n  resourceVersion: \"98\"\n  selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/cluster-admin\n  uid: 0361e9f2-6f04-11e8-b5dd-000c2904e34b\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:masters\n\n```\n\nIn the subjects field I have:  \n\n```\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:masters\n\n```\n\nHow can I see the members of the group system:masters ?\nI read here about groups but I don't understand how can I see who is inside the groups as the example above with system:masters.  \nI noticed that when I decoded /etc/kubernetes/pki/apiserver-kubelet-client.crt using the command: \nopenssl x509 -in apiserver-kubelet-client.crt -text -noout it contained the subject system:masters but I still didn't understand who are the users in this group:  \n\n```\nIssuer: CN=kubernetes\nValidity\n    Not Before: Jul 31 19:08:36 2018 GMT\n    Not After : Jul 31 19:08:37 2019 GMT\nSubject: O=system:masters, CN=kube-apiserver-kubelet-client\nSubject Public Key Info:\n    Public Key Algorithm: rsaEncryption\n        Public-Key: (2048 bit)\n        Modulus:\n\n```",
    "answer": "Admittedly, late to the party here.\nHave a read through the Kubernetes 'Authenticating' docs. Kubernetes does not have an in-built mechanism for defining and controlling users (as distinct from ServiceAccounts which are used to provide a cluster identity for Pods, and therefore services running on them).\nThis means that Kubernetes does not therefore have any internal DB to reference, to determine and display group membership.\nIn smaller clusters, x509 certificates are typically used to authenticate users. The API server is configured to trust a CA for the purpose, and then users are issued certificates signed by that CA. As you had noticed, if the subject contains an 'Organisation' field, that is mapped to a Kubernetes group. If you want a user to be a member of more than one group, then you specify multiple 'O' fields. (As an aside, to my mind it would have made more sense to use the 'OU' field, but that is not the case)\nIn answer to your question, it appears that in the case of a cluster where users are authenticated by certificates, your only route is to have access to the issued certs, and to check for the presence of the 'O' field in the subject. I guess in more advanced cases, Kubernetes would be integrated with a centralised tool such as AD, which could be queried natively for group membership."
  },
  {
    "question": "Easiest way to copy a single file from host to Vagrant guest? I have a use case where I occasionally want to copy a single file from my host machine to the Vagrant guest.\nI don't want to do so via traditional provisioners (Puppet / Chef) because this is often a one-off -- I just want something quick to add to my Vagrantfile.\nI don't want to share an entire directory, possibly because I want to overwrite an existing file without nuking an entire directory on the guest.\nIt also seems a bit overkill to write a shell provisioning script, and deal with potential escaping, when all I want to do is copy a file.\nSo, what's the easiest way to copy a single file from host to guest?",
    "answer": "Vagrant.configure(\"2\") do |config|\n\nconfig.vm.provision \"file\", source: \"~/.gitconfig\", destination: \".gitconfig\"\nend\n\nInstead of using a shell provisioner to copy the file, you can also use a Vagrant file provisioner.\n\nProvisioner name: \"file\"\nThe file provisioner allows you to upload a file from the host machine to the guest machine."
  },
  {
    "question": "Where does Vagrant download its .box files to? What happens to the .box file after the following command is executed?\n\n```\nvagrant box add lucid32 http://files.vagrantup.com/lucid32.box\n\n```\n\nI can't find the lucid32.box file on the file system after download has completed.",
    "answer": "As mentioned in the docs, boxes are stored at:\n\nMac OS X and Linux: ~/.vagrant.d/boxes\nWindows: C:/Users/USERNAME/.vagrant.d/boxes"
  },
  {
    "question": "Vagrant error : Failed to mount folders in Linux guest I have some issues with Vagrant shared folders, my base system is Ubuntu 13.10 desktop.\nI do not understand why I have this error is something that is not right configured ? Is a NFS issue or Virtualbox Guest Additions ? I have tried with different many boxes but the same issue.\n\n```\nFailed to mount folders in Linux guest. This is usually because\n    the \"vboxsf\" file system is not available. Please verify that\n    the guest additions are properly installed in the guest and\n    can work properly. The command attempted was:\n\n    mount -t vboxsf -o uid=`id -u vagrant`,gid=`getent group vagrant | cut -d: -f3` /vagrant /vagrant\n    mount -t vboxsf -o uid=`id -u vagrant`,gid=`id -g vagrant` /vagrant /vagrant\n\n```\n\nHere is the complete process after vagrant up :\n\n```\n$ vagrant up\nBringing machine 'default' up with 'virtualbox' provider...\n==> default: Importing base box 'u131032'...\n==> default: Matching MAC address for NAT networking...\n==> default: Setting the name of the VM: vagrant_default_1396020504136_46442\n==> default: Clearing any previously set forwarded ports...\n==> default: Clearing any previously set network interfaces...\n==> default: Preparing network interfaces based on configuration...\n    default: Adapter 1: nat\n    default: Adapter 2: hostonly\n==> default: Forwarding ports...\n    default: 22 => 2222 (adapter 1)\n==> default: Running 'pre-boot' VM customizations...\n==> default: Booting VM...\n==> default: Waiting for machine to boot. This may take a few minutes...\n    default: SSH address: 127.0.0.1:2222\n    default: SSH username: vagrant\n    default: SSH auth method: private key\n    default: Error: Connection timeout. Retrying...\n    default: Error: Remote connection disconnect. Retrying...\n    default: Error: Remote connection disconnect. Retrying...\n    default: Error: Remote connection disconnect. Retrying...\n    default: Error: Remote connection disconnect. Retrying...\n    default: Error: Remote connection disconnect. Retrying...\n    default: Error: Remote connection disconnect. Retrying...\n    default: Error: Remote connection disconnect. Retrying...\n    default: Error: Remote connection disconnect. Retrying...\n    default: Error: Remote connection disconnect. Retrying...\n    default: Error: Remote connection disconnect. Retrying...\n    default: Error: Remote connection disconnect. Retrying...\n    default: Error: Remote connection disconnect. Retrying...\n==> default: Machine booted and ready!\nGuestAdditions versions on your host (4.3.10) and guest (4.2.16) do not match.\n * Stopping VirtualBox Additions\n   ...done.\nReading package lists...\nBuilding dependency tree...\nReading state information...\nThe following packages were automatically installed and are no longer required:\n  dkms libdrm-intel1 libdrm-nouveau2 libdrm-radeon1 libfontenc1\n  libgl1-mesa-dri libglapi-mesa libice6 libllvm3.3 libpciaccess0 libpixman-1-0\n  libsm6 libtxc-dxtn-s2tc0 libxaw7 libxcomposite1 libxdamage1 libxfixes3\n  libxfont1 libxkbfile1 libxmu6 libxpm4 libxrandr2 libxrender1 libxt6\n  x11-common x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n  xserver-common xserver-xorg-core\nUse 'apt-get autoremove' to remove them.\nThe following packages will be REMOVED:\n  virtualbox-guest-dkms* virtualbox-guest-utils* virtualbox-guest-x11*\n0 upgraded, 0 newly installed, 3 to remove and 0 not upgraded.\nAfter this operation, 11.1 MB disk space will be freed.\n(Reading database ... 65615 files and directories currently installed.)\nRemoving virtualbox-guest-dkms ...\n\n-------- Uninstall Beginning --------\nModule:  virtualbox-guest\nVersion: 4.2.16\nKernel:  3.11.0-18-generic (i686)\n-------------------------------------\n\nStatus: Before uninstall, this module version was ACTIVE on this kernel.\n\nvboxguest.ko:\n - Uninstallation\n   - Deleting from: /lib/modules/3.11.0-18-generic/updates/dkms/\n - Original module\n   - No original module was found for this module on this kernel.\n   - Use the dkms install command to reinstall any previous module version.\n\n\nvboxsf.ko:\n - Uninstallation\n   - Deleting from: /lib/modules/3.11.0-18-generic/updates/dkms/\n - Original module\n   - No original module was found for this module on this kernel.\n   - Use the dkms install command to reinstall any previous module version.\n\n\nvboxvideo.ko:\n - Uninstallation\n   - Deleting from: /lib/modules/3.11.0-18-generic/updates/dkms/\n - Original module\n   - No original module was found for this module on this kernel.\n   - Use the dkms install command to reinstall any previous module version.\n\ndepmod....\n\nDKMS: uninstall completed.\n\n------------------------------\nDeleting module version: 4.2.16\ncompletely from the DKMS tree.\n------------------------------\nDone.\nRemoving virtualbox-guest-x11 ...\nPurging configuration files for virtualbox-guest-x11 ...\nRemoving virtualbox-guest-utils ...\nPurging configuration files for virtualbox-guest-utils ...\nProcessing triggers for ureadahead ...\nProcessing triggers for man-db ...\nReading package lists...\nBuilding dependency tree...\nReading state information...\ndkms is already the newest version.\ndkms set to manually installed.\nlinux-headers-3.11.0-18-generic is already the newest version.\nlinux-headers-3.11.0-18-generic set to manually installed.\nThe following packages were automatically installed and are no longer required:\n  libdrm-intel1 libdrm-nouveau2 libdrm-radeon1 libfontenc1 libgl1-mesa-dri\n  libglapi-mesa libice6 libllvm3.3 libpciaccess0 libpixman-1-0 libsm6\n  libtxc-dxtn-s2tc0 libxaw7 libxcomposite1 libxdamage1 libxfixes3 libxfont1\n  libxkbfile1 libxmu6 libxpm4 libxrandr2 libxrender1 libxt6 x11-common\n  x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils xserver-common\n  xserver-xorg-core\nUse 'apt-get autoremove' to remove them.\n0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\nCopy iso file /usr/share/virtualbox/VBoxGuestAdditions.iso into the box /tmp/VBoxGuestAdditions.iso\nmount: block device /tmp/VBoxGuestAdditions.iso is write-protected, mounting read-only\nInstalling Virtualbox Guest Additions 4.3.10 - guest version is 4.2.16\nVerifying archive integrity... All good.\nUncompressing VirtualBox 4.3.10 Guest Additions for Linux............\nVirtualBox Guest Additions installer\nCopying additional installer modules ...\nInstalling additional modules ...\nRemoving existing VirtualBox DKMS kernel modules ...done.\nRemoving existing VirtualBox non-DKMS kernel modules ...done.\nBuilding the VirtualBox Guest Additions kernel modules ...done.\nDoing non-kernel setup of the Guest Additions ...done.\nStarting the VirtualBox Guest Additions ...done.\nInstalling the Window System drivers\nCould not find the X.Org or XFree86 Window System, skipping.\nAn error occurred during installation of VirtualBox Guest Additions 4.3.10. Some functionality may not work as intended.\nIn most cases it is OK that the \"Window System drivers\" installation failed.\n==> default: Checking for guest additions in VM...\n==> default: Setting hostname...\n==> default: Configuring and enabling network interfaces...\n==> default: Exporting NFS shared folders...\n==> default: Preparing to edit /etc/exports. Administrator privileges will be required...\nnfsd running\nsudo: /usr/bin/exportfs: command not found\n==> default: Mounting NFS shared folders...\n==> default: Mounting shared folders...\n    default: /vagrant => /home/me/Documents/Work/project/vagrant\nFailed to mount folders in Linux guest. This is usually because\nthe \"vboxsf\" file system is not available. Please verify that\nthe guest additions are properly installed in the guest and\ncan work properly. The command attempted was:\n\nmount -t vboxsf -o uid=`id -u vagrant`,gid=`getent group vagrant | cut -d: -f3` /vagrant /vagrant\nmount -t vboxsf -o uid=`id -u vagrant`,gid=`id -g vagrant` /vagrant /vagrant\n\n```\n\nMy Vagrantfile configuration is :\n\n```\n# -*- mode: ruby -*-\n# vi: set ft=ruby :\n\n# Vagrantfile API/syntax version. Don't touch unless you know what you're doing!\nVAGRANTFILE_API_VERSION = \"2\"\n\npersonalization = File.expand_path(\"../Personalization\", __FILE__)\nload personalization\n\nVagrant.configure(VAGRANTFILE_API_VERSION) do |config|\n  config.vm.box = $base_box\n  config.vm.box_url = $base_box_url\n\n  config.vm.hostname = $vhost + \".dev\"\n\n  config.hostsupdater.aliases = [\"api.\" + $vhost + \".dev\", \"mysql.\" + $vhost + \".dev\"]\n  config.hostsupdater.remove_on_suspend = true\n\n  # set auto_update to ture to check the correct \n  # additions version when booting the machine\n  config.vbguest.auto_update = true\n  config.vbguest.auto_reboot = true\n\n  config.vm.network :private_network, ip: $ip\n\n  config.vm.synced_folder \"../\", \"/srv/www/vhosts/\" + $vhost + \".dev\", type: \"nfs\"\n\n  config.vm.provider :virtualbox do |v|\n    v.customize [\"modifyvm\", :id, \"--memory\", 2048]\n    v.customize [\"modifyvm\", :id, \"--cpus\", \"1\"]\n    v.customize [\"modifyvm\", :id, \"--cpuexecutioncap\", \"100\"]\n    v.customize [\"modifyvm\", :id, \"--ioapic\", \"off\"]\n    v.customize [\"modifyvm\", :id, \"--natdnshostresolver1\", \"on\"]\n  end\n\n  config.vm.provision \"shell\" do |s|\n    s.path = \"vagrant-bootstrap.sh\"\n    s.args = $vhost + \" \" + $mysql_password + \" \" + $application_database\n  end\nend\n\n```\n\nThe Personalization file is :\n\n```\n# Name of the vhost to create\n$vhost = \"project\"\n\n# Use the Ubunut 32bit or 64bit\n$base_box_url = \"http://cloud-images.ubuntu.com/vagrant/saucy/current/saucy-server-cloudimg-i386-vagrant-disk1.box\"\n\n# VM IP\n$ip = \"192.168.7.7\"\n\n# Base box name\n$base_box = \"u131032\"\n\n# MySQL\n$mysql_password = \"admin\"\n$application_database = \"project\"\n\n```\n\nThe following plugins are enabled in Vagrant:\n\n```\n$ vagrant plugin list\nvagrant-hostsupdater (0.0.11)\nvagrant-login (1.0.1, system)\nvagrant-share (1.0.1, system)\nvagrant-vbguest (0.10.0)\n\n```",
    "answer": "vagrant plugin install vagrant-vbguest\nvagrant reload\n==> default: Attempting graceful shutdown of VM...\n...\n==> default: Machine booted and ready!\nGuestAdditions 4.3.12 running --- OK.\n==> default: Checking for guest additions in VM...\n==> default: Configuring and enabling network interfaces...\n==> default: Exporting NFS shared folders...\n==> default: Preparing to edit /etc/exports. Administrator privileges will be required...\n==> default: Mounting NFS shared folders...\n==> default: VM already provisioned. Run `vagrant provision` or use `--provision` to force it\n\nThe plugin vagrant-vbguest   solved my problem:\n\nOutput:\n\nJust make sure you are running the latest version of VirtualBox"
  },
  {
    "question": "How to change Vagrant &#39;default&#39; machine name? Where does the name 'default' come from when launching a vagrant box?\n\n```\n$ vagrant up\nBringing machine 'default' up with 'virtualbox' provider...\n\n```\n\nIs there a way to set this?",
    "answer": "vagrant init precise64 http://files.vagrantup.com/precise64.box\nVagrant.configure('2') do |config|\nconfig.vm.box = \"precise64\"\nconfig.vm.box_url = \"http://files.vagrantup.com/precise64.box\"\nend\nVagrant.configure('2') do |config|\nconfig.vm.box = \"precise64\"\nconfig.vm.box_url = \"http://files.vagrantup.com/precise64.box\"\nconfig.vm.define \"foohost\"\nend\nVagrant.configure('2') do |config|\nconfig.vm.box = \"precise64\"\nconfig.vm.box_url = \"http://files.vagrantup.com/precise64.box\"\nconfig.vm.provider :virtualbox do |vb|\nvb.name = \"foohost\"\nend\nend\nVagrant.configure('2') do |config|\nconfig.vm.box = \"precise64\"\nconfig.vm.box_url = \"http://files.vagrantup.com/precise64.box\"\nconfig.vm.define \"foohost\"\nconfig.vm.provider :virtualbox do |vb|\nvb.name = \"barhost\"\nend\nend\nVagrant.configure(VAGRANTFILE_API_VERSION) do |config|\nconfig.vm.hostname = \"buzbar\"\nend\nVagrant.configure('2') do |config|\nconfig.vm.box = \"precise64\"\nconfig.vm.box_url = \"http://files.vagrantup.com/precise64.box\"\nconfig.vm.hostname = \"buzbar\"\nconfig.vm.define \"foohost\"\nconfig.vm.provider :virtualbox do |vb|\nvb.name = \"barhost\"\nend\nend\n\nI found the multiple options confusing, so I decided to test all of them to see exactly what they do.\nI'm using VirtualBox 4.2.16-r86992 and Vagrant 1.3.3.\nI created a directory called nametest and ran\n\nto generate a default Vagrantfile.  Then I opened the VirtualBox GUI so I could see what names the boxes I create would show up as.\n\nDefault Vagrantfile\n\nVirtualBox GUI Name:  \"nametest_default_1386347922\"\nComments:  The name defaults to the format DIRECTORY_default_TIMESTAMP.\nDefine VM\n\nVirtualBox GUI Name:  \"nametest_foohost_1386347922\"\nComments:  If you explicitly define a VM, the name used replaces the token 'default'. This is the name vagrant outputs on the console. Simplifying based on zook's (commenter) input\nSet Provider Name\n\nVirtualBox GUI Name:  \"foohost\"\nComments:  If you set the name attribute in a provider configuration block, that name will become the entire name displayed in the VirtualBox GUI.\nCombined Example: Define VM -and- Set Provider Name\n\nVirtualBox GUI Name:  \"barhost\"\nComments:  If you use both methods at the same time, the value assigned to name in the provider configuration block wins. Simplifying based on zook's (commenter) input\nSet hostname (BONUS)\n\nComments: This sets the hostname inside the VM. This would be the output of hostname command in the VM and also this is what's visible in the prompt like vagrant@<hostname>, here it will look like vagrant@buzbar\n\nFinal Code\n\nSo there it is.  You now know 3 different options you can set and the effects they have.  I guess it's a matter of preference at this point?  (I'm new to Vagrant, so I can't speak to best practices yet.)"
  },
  {
    "question": "Psql could not connect to server: No such file or directory, 5432 error? I'm trying to run psql on my Vagrant machine, but I get this error:\n\n```\npsql: could not connect to server: No such file or directory\n\nIs the server running locally and accepting connections on \nUnix domain socket \"/var/run/postgresql/.s.PGSQL.5432\"?\n\n```\n\nNote: Vagrant 1.9.2\nBox: ubuntu/trusty64, https://atlas.hashicorp.com/ubuntu/boxes/trusty64\nEDIT\nCommands I've used in order to install and run postgres:\n\nsudo apt-get update\nsudo apt-get install postgresql\nsudo su postgres\npsql -d postgres -U postgres",
    "answer": "I've had this same issue, related to the configuration of my pg_hba.conf file (located in /etc/postgresql/9.6/main). Please note that 9.6 is the postgresql version I am using.\nThe error itself is related to a misconfiguration of postgresql, which causes the server to crash before it starts.\nI would suggest following these instructions:\n\nCertify that postgresql service is running, using sudo service postgresql start\nRun pg_lsclusters from your terminal\nCheck what is the cluster you are running, the output should be something like:\nVersion - Cluster   Port Status Owner    Data directory\n9.6  -------  main --   5432 online postgres /var/lib/postgresql/9.6/main\n\nDisregard the '---' signs, as they are being used there only for alignment.\nThe important information are the version and the cluster. You can also check whether the server is running or not in the status column.\n\nCopy the info from the version and the cluster, and use like so:\npg_ctlcluster <version> <cluster> start, so in my case, using version 9.6 and cluster 'main', it would be pg_ctlcluster 9.6 main start\nIf something is wrong, then postgresql will generate a log, that can be accessed on /var/log/postgresql/postgresql-<version>-main.log, so in my case, the full command would be sudo nano /var/log/postgresql/postgresql-9.6-main.log.\nThe output should show what is the error.\n\n2017-07-13 16:53:04 BRT [32176-1] LOG:  invalid authentication method \"all\"\n2017-07-13 16:53:04 BRT [32176-2] CONTEXT:  line 90 of configuration file \"/etc/postgresql/9.5/main/pg_hba.conf\"\n2017-07-13 16:53:04 BRT [32176-3] FATAL:  could not load pg_hba.conf\n\nFix the errors and restart postgresql service through sudo service postgresql restart and it should be fine.\n\nI have searched a lot to find this, credit goes to this post.\nBest of luck!"
  },
  {
    "question": "Failed to open/create the internal network Vagrant on Windows10 I upgraded my Windows 10 to the last update yesterday and now, when I launch  vagrant up command, I get this error :\n\n```\n==> default: Booting VM...\n==> default: Waiting for machine to boot. This may take a few minutes...\nThe guest machine entered an invalid state while waiting for it\nto boot. Valid states are 'starting, running'. The machine is in the\n'poweroff' state. Please verify everything is configured\nproperly and try again.\n\nIf the provider you're using has a GUI that comes with it,\nit is often helpful to open that and watch the machine, since the\nGUI often has more helpful error messages than Vagrant can retrieve.\nFor example, if you're using VirtualBox, run `vagrant up` while the\nVirtualBox GUI is open.\n\nThe primary issue for this error is that the provider you're using\nis not properly configured. This is very rarely a Vagrant issue.\n\n```\n\nWhen I try with GUI I have this error :\n\n```\nFailed to open/create the internal network 'HostInterfaceNetworking-VirtualBox Host-Only Ethernet Adapter' (VERR_INTNET_FLT_IF_NOT_FOUND).\nFailed to attach the network LUN (VERR_INTNET_FLT_IF_NOT_FOUND).\n\n```\n\nI have re-installed VirtualBox 5.0.10 and the extension pack,\nreconfigured Host-Only Ethernet Adapter, but always the same error...\nAny ideas?",
    "answer": "I found a solution\n\nOpen Windows Network Connections\nRight click on VirtualBox Host only adapter that created\nChoose properties\nCheck \"VirtualBox NDIS6 Bridged Networking driver\"\ndisable and Enable the adapter"
  },
  {
    "question": "How to disable &quot;Cannot Render Console from...&quot; on Rails I'm using Ubuntu/vagrant as my development environment.\nI'm getting these messages on rails console:\n\n```\nStarted GET \"/assets/home-fcec5b5a277ac7c20cc9f45a209a3bcd.js?body=1\" for 10.0.2.2 at 2015-04-02 15:48:31 +0000\nCannot render console from 10.0.2.2! Allowed networks: 127.0.0.1, ::1, 127.0.0.0/127.255.255.255\n\n```\n\nIs it possible to disable those \"cannot render...\" messages or allow them in any way?",
    "answer": "class Application < Rails::Application\nconfig.web_console.permissions = '10.0.2.2'\nend\n\nYou need to specifically allow the 10.0.2.2 network space in the Web Console config.\nSo you'll want something like this:\n\nRead here for more information.\nAs pointed out by pguardiario, this wants to go into config/environments/development.rb rather than config/application.rb so it is only applied in your development environment."
  },
  {
    "question": "How to ssh to vagrant without actually running &quot;vagrant ssh&quot;? I would like to reproduce the way Vagrant logs in to my VM within a shell script using an ssh command, so I create an alias to my Vagrant instance.\nWhat is the command syntax to use the regular ssh command to access it?",
    "answer": "PORT=$(vagrant ssh-config | grep Port | grep -o '[0-9]\\+')\nssh -q \\\n-o UserKnownHostsFile=/dev/null \\\n-o StrictHostKeyChecking=no \\\n-i ~/.vagrant.d/insecure_private_key \\\nvagrant@localhost \\\n-p $PORT \\\n\"$@\"\nssh $(vagrant ssh-config | awk 'NR>1 {print \" -o \"$1\"=\"$2}') localhost\nHOST=name-of-my-host\nssh $(vagrant ssh-config $HOST | sed '/^[[:space:]]*$/d' |  awk 'NR>1 {print \" -o \"$1\"=\"$2}') localhost\n\nI've had to re-implement \"vagrant ssh\" because it's -c option didn't pass on arguments properly. This is basically what it does (there might be more, but it works fine this way)\n\nAs a one-liner (with thanks to kgadek):\n\nTo account for when you have more than one vagrant host, this will select the desired host, as well as cull blank lines from the config (using sed):"
  },
  {
    "question": "Vagrant ssh authentication failure The problem with ssh authentication:\n\n```\n==> default: Clearing any previously set forwarded ports...\n==> default: Clearing any previously set network interfaces...\n==> default: Preparing network interfaces based on configuration...\n    default: Adapter 1: nat\n    default: Adapter 2: bridged\n==> default: Forwarding ports...\n    default: 22 => 2222 (adapter 1)\n==> default: Booting VM...\n==> default: Waiting for machine to boot. This may take a few minutes...\n    default: SSH address: 127.0.0.1:2222\n    default: SSH username: vagrant\n    default: SSH auth method: private key\n    default: Error: Connection timeout. Retrying...\n    default: Error: Connection timeout. Retrying...\n    default: Error: Connection timeout. Retrying...\n    default: Error: Connection timeout. Retrying...\n    default: Error: Authentication failure. Retrying...\n    default: Error: Authentication failure. Retrying...\n    default: Error: Authentication failure. Retrying...\n    default: Error: Authentication failure. Retrying...\n    default: Error: Authentication failure. Retrying...\n\n```\n\nI can Ctrl+C out of the authentication loop and then successfully ssh in manually.\nI performed the following steps on the guest box:\n\nEnabled Remote Login for All Users.\nCreated the ~/.ssh directory with 0700 permissions.\nCreated the ~/.ssh/authorized_keys file with 0600 permissions.\nPasted this public key\ninto ~/.ssh/authorized_keys\n\nI've also tried using a private (hostonly) network instead of the public (bridged) network, using this line in the Vagrantfile:\nconfig.vm.network \"private_network\", ip: \"172.16.177.7\"\nI get the same output (except Adapter 2: hostonly) but then cannot ssh in manually.\nI also tried config.vm.network \"private_network\", ip: \"10.0.0.100\".\nI also tried setting config.ssh.password in the Vagrantfile. This does output SSH auth method: password but still doesn't authenticate.\nAnd I also tried rebuilding the box and rechecking all the above.\nIt looks like others have had success with this configuration, so there must be something I'm doing wrong.\nI found this thread and enabled the GUI, but that doesn't help.",
    "answer": "Make sure your first network interface is NAT. The other second network interface can be anything you want when you're building box. Don't forget the Vagrant user, as discussed in the Google thread.\nGood luck."
  },
  {
    "question": "SSH to Vagrant box in Windows? I'm using Vagrant to start a VirtualBox VM in windows.\nIn other platforms, I can just\n\n```\n$ vagrant ssh\n\n```\n\nto connect to the VM.\nHow do i connect to this Vagrant box in windows?\nThe way suggested in Vagrant documentation to use PuTTy also did not work:\nhttp://docs-v1.vagrantup.com/v1/docs/getting-started/ssh.html",
    "answer": "I use PuTTY to connect to my Vagrant boxes on Windows7.\nMake sure you\n\nconvert the %USERPROFILE%\\.vagrant.d\\insecure_private_key to .ppk using PuTTYGen\nuse the .ppk key in your PuTTY session - configured in Connection > SSH > Auth > Private key file\nuse host 127.0.0.1\nuse port 2222 instead of 22\nyou can set the default username (vagrant) under Connection > SSH > Auth > Private key for authentication"
  },
  {
    "question": "VBoxManage: error: Failed to create the host-only adapter I am running vagrant 1.4 and virtual box 4.3 on fedora 17 machine. When I do \"vagrant up\", I get this error:\n\n```\nBringing machine 'default' up with 'virtualbox' provider...                                                                            \n\n\n[default] Clearing any previously set forwarded ports...                                                                                                                \n[default] Clearing any previously set network interfaces...                                                                                                             \nThere was an error while executing `VBoxManage`, a CLI used by Vagrant                                                                                                  \nfor controlling VirtualBox. The command and stderr is shown below.                                                                                                      \n\nCommand: [\"hostonlyif\", \"create\"]                                                                                                                                       \n\nStderr: 0%...\nProgress state: NS_ERROR_FAILURE\nVBoxManage: error: Failed to create the host-only adapter\nVBoxManage: error: VBoxNetAdpCtl: Error while adding new interface: VBoxNetAdpCtl: ioctl failed for /dev/vboxnetctl: Inappropriate ioctl for devic\nVBoxManage: error: Details: code NS_ERROR_FAILURE (0x80004005), component HostNetworkInterface, interface IHostNetworkInterface\nVBoxManage: error: Context: \"int handleCreate(HandlerArg*, int, int*)\" at line 66 of file VBoxManageHostonly.cpp\n\n```",
    "answer": "sudo \"/Library/Application Support/VirtualBox/LaunchDaemons/VirtualBoxStartup.sh\" restart\n\nI had the same problem today. The reason was that I had another VM running in VirtualBox.\nSolution:\n\nOpen VirtualBox and shut down every VM running\nGo to System Preferences > Security & Privacy Then hit the \"Allow\" button to let Oracle (VirtualBox) load.\nRestart VirtualBox\n\nYou should now be able to run vagrant up or vagrant reload and have your new host configured.\n\nAs mentioned in this answer, recent versions of macOS can block VirtualBox.\n\nSolution:\nGo to System Preferences > Security & Privacy Then hit the \"Allow\" button to let Oracle (VirtualBox) load.\n(thanks to @pazhyn, @lsimonetti & @dave-beauchesne for clarifications)"
  },
  {
    "question": "How to set host_key_checking=false in ansible inventory file? I would like to use ansible-playbook command instead of 'vagrant provision'. However setting host_key_checking=false in the hosts file does not seem to work.\n\n```\n# hosts file\nvagrant ansible_ssh_private_key_file=~/.vagrant.d/insecure_private_key \nansible_ssh_user=vagrant ansible_ssh_port=2222 ansible_ssh_host=127.0.0.1 \nhost_key_checking=false\n\n```\n\nIs there a configuration variable outside of Vagrantfile that can override this value?\nAlso, how would this work if running ansible from a Kubernetes pod?",
    "answer": "ansible_ssh_common_args='-o StrictHostKeyChecking=no'\nansible_ssh_extra_args='-o StrictHostKeyChecking=no'\n[defaults]\nhost_key_checking = False\nexport ANSIBLE_HOST_KEY_CHECKING=False\n\nDue to the fact that I answered this in 2014, I have updated my answer to account for more recent versions of ansible.\nYes, you can do it at the host/inventory level (Which became possible on newer ansible versions) or global level:\ninventory:\nAdd the following.\n\nhost:\nAdd the following.\n\nhosts/inventory options will work with connection type ssh and not paramiko. Some people may strongly argue that inventory and hosts is more secure because the scope is more limited.\nglobal:\nAnsible User Guide - Host Key Checking\n\nYou can do it either in the /etc/ansible/ansible.cfg or ~/.ansible.cfg file:\n\nOr you can setup and env variable (this might not work on newer ansible versions):\n\nKubernetes:\n\nYou can also run a ansible head node as a Kubernetes pod like described here.\n\nIn this case you can use any of the global options above."
  },
  {
    "question": "How to add a downloaded .box file to Vagrant? How do I add a downloaded .box file to Vagrant's list of available boxes? The .box file is located on an external drive.\nI tried running vagrant box add my-box d:/path/to/box, but Vagrant interprets the path as a URL.",
    "answer": "vagrant box add my-box file:///d:/path/to/file.box\n\nSolution:\n\nHas to be in a URL format."
  },
  {
    "question": "How do I associate a Vagrant project directory with an existing VirtualBox VM? Somehow my Vagrant project has disassociated itself from its VirtualBox VM, so that when I vagrant up Vagrant will import the base-box and create a new virtual machine.\nIs there a way to re-associate the Vagrant project with the existing VM?\nHow does Vagrant internally associate a Vagrantfile with a VirtualBox VM directory?",
    "answer": "{\n\"active\":{\n\"default\":\"02f8b71c-75c6-4f33-a161-0f46a0665ab6\"\n}\n}\n\nWARNING: The solution below works for Vagrant 1.0.x but not Vagrant 1.1+.\nVagrant uses the \".vagrant\" file in the same directory as your \"Vagrantfile\" to track the UUID of your VM. This file will not exist if a VM does not exist. The format of the file is JSON. It looks like this if a single VM exists:\n\ndefault is the name of the default virtual machine (if you're not using multi-VM setups).\nIf your VM has somehow become disassociated, what you can do is do VBoxManage list vms which will list every VM that VirtualBox knows about by its name and UUID. Then manually create a .vagrant file in the same directory as your Vagrantfile and fill in the contents properly.\nRun vagrant status to ensure that Vagrant picked up the proper changes.\nNote: This is not officially supported by Vagrant and Vagrant may change the format of .vagrant at any time. But this is valid as of Vagrant 0.9.7 and will be valid for Vagrant 1.0."
  },
  {
    "question": "Using Laravel Homestead: &#39;no input file specified&#39; I am new to using Laravel, and Homestead, and would appreciate any help or a point in the right direction.  I have successfully been able to get to the \"You have arrived\" screen when I run \"php artisan serve\" but when I try to do the same thing via Vagrant, I get \"no input file specified\".  My Homestead.yaml file looks like this:\n\n```\nauthorize: /Users/me/.ssh/id_rsa.pub\n\nkeys:\n    - /Users/me/.ssh/id_rsa\n\nfolders:\n    - map: /Users/me/code/exampleproject\n      to: /home/vagrant/code/exampleproject\n\nsites:\n    - map: exampleproject.app\n      to: /home/vagrant/code/exampleproject/public\n\nvariables:\n    - key: APP_ENV\n      value: local\n\n```\n\nOn my computer I have the following directories:\n\n```\n/Users/me/code/Homestead\n/Users/me/code/exampleproject //this is the directory created with composer\n\n```\n\nOn my Vagrant Box I have for some reason two directories named \"code\" and \"Code\":\n\n```\n/home/vagrant/code/exampleproject \n/home/vagrant/Code\n\n```\n\nI have checked and I can see changes made to my computer exampleproject files are reflected in the vagrant box files.\nNot really sure how to figure this out!!  I would really appreciate any help possible :)",
    "answer": "vagrant up --provision\nhomestead up --provision\n\nInstead of reinstalling try\n\nor"
  },
  {
    "question": "Vagrant Not Starting Up. User that created VM doesn&#39;t match current user I was trying to start up my vagrant machine, so I navigated to the folder where my vagrantfile is, and used: \nvagrant up && vagrant ssh\nbut I got the following error message:\n\nThe VirtualBox VM was created with a user that doesn't match the\n  current user running Vagrant. VirtualBox requires that the same user\n  be used to manage the VM that was created. Please re-run Vagrant with\n  that user. This is not a Vagrant issue.\nThe UID used to create the VM was: 0 Your UID is: 501\n\nI also tried with sudo, but that didn't work either.\nDo I need to switch UID's? And how would I do this?",
    "answer": "I ran into the same problem today.\nI edited my UID by opening the file .vagrant/machines/default/virtualbox/creator_uid and changing the 501 to a 0.\nAfter I saved the file, the command vagrant up worked like a champ.\nNB: the .vagrant folder is in the same directory as your Vagrantfile, where you ran vagrant up"
  },
  {
    "question": "A VirtualBox machine with the name &#39;homestead&#39; already exists Since homestead 2.0 homestead laravel has not been working \nI don't know why 'homestead init' creates a Homestead.yaml file in mydirectory/.homestead \nand not in the project directory. Homestead up OR Vagrant up create the following message \nA VirtualBox machine with the name 'homestead' already exists.\n\n```\nBringing machine 'default' up with 'virtualbox' provider...\n==> default: Importing base box 'laravel/homestead'...\n==> default: Matching MAC address for NAT networking...\n==> default: Checking if box 'laravel/homestead' is up to date...\nA VirtualBox machine with the name 'homestead' already exists.\nPlease use another name or delete the machine with the existing\nname, and try again.\n\n```",
    "answer": "vboxmanage list vms\n\"my-vm\" {c700b8b6-b766-4638-871b-736b44b7db18}\n\nI solved by using vboxmanage to get the ID of the VM.\n\nCopy the ID of the desired VM (the c700\u2026db18 string) into the contents of ~/.vagrant/machines/default/virtualbox/id. Save the file then run vagrant up to get the vm working without having to destroy it."
  },
  {
    "question": "Vagrant reverse port forwarding? I'm working on a web services architecture. I've got some software that I need to run on the native host machine, not in Vagrant. But I'd like to run some client services on the guest.\nVagrant's config.vm.forwarded_port parameter will open a port on the host and send the data to the guest. But how can I open a port on the guest and send the data to the host? (It's still port forwarding, but in the reverse direction.)",
    "answer": "When you run vagrant ssh, it's actually using this underlying command:\nssh -p 2222 -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o LogLevel=ERROR -o IdentitiesOnly=yes -i ~/.vagrant.d/insecure_private_key vagrant@127.0.0.1\nSSH supports forwarding ports in the direction you want with the -R guestport:host:hostport option. So, if you wanted to connect to port 12345 on the guest and have it forwarded to localhost:80, you would use this command:\nssh -p 2222 -R 12345:localhost:80 -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o LogLevel=ERROR -o IdentitiesOnly=yes -i ~/.vagrant.d/insecure_private_key vagrant@127.0.0.1\nAs Eero correctly comments, you can also use the command vagrant ssh -- -R 12345:localhost:80, which has the same effect in a much more concise command."
  },
  {
    "question": "How can I kill whatever process is using port 8080 so that I can vagrant up? On MacOSX, I'm using Packer to build a Vagrant box so I need to continually bring it up and tear it down. I'm attempting to 'vagrant up', and receive the standard error because the port is in use: \n\"Vagrant cannot forward the specified ports on this VM, since they would collide with some other application that is already listening on these ports. The forwarded port to 8080 is already in use on the host machine.\"\nThe solution seems simple enough: I just need to identify the process that is holding port 8080 open and kill that process, right?. It's not that easy.\n\nIf I run the command:\n\n```\nnmap localhost -p 8080\n\n```\n\nI receive the following output:\n\n```\nPORT     STATE SERVICE\n8080/tcp open  http-proxy\n\n```\n\n\nIf I run the following command:\n\n```\ntop -o prt\n\n```\n\nThe highest port in use in 1360\n\nIf I run the following command:\n\n```\n netstat -tulpn | grep :8080\n\n```\n\nI receive: \n\n```\nnetstat: n: unknown or uninstrumented protocol\n\n```\n\n\nIf I run the following command:\n\n```\nlsof -i :8080\n\n```\n\nI receive no output\n\nIf I restart my computer, the port is now available and I can now 'vagrant up'.\nHow can I kill whatever process is using port 8080 so that I can vagrant up without restarting my computer?",
    "answer": "lsof -n -i4TCP:8080\nlsof -i -P\n\nThis might help\n\nThe PID is the second field in the output.\nOr try:"
  },
  {
    "question": "How to export a Vagrant virtual machine to transfer it I have a vagrant box up and running (configured with a LAMP stack). I need to transfer it to another PC. How can I export it?\nI guess that I can get a file (or files) that can be copied to another PC, so there I can run some command to import the vagrant box.",
    "answer": "git clone <your_repo>\nvagrant up\n\nYou have two ways to do this, I'll call it dirty way and clean way:\n1. The dirty way\nCreate a box from your current virtual environment, using vagrant package command:\nhttp://docs.vagrantup.com/v2/cli/package.html\nThen copy the box to the other pc, add it using vagrant box add and run it using vagrant up as usual.\nKeep in mind that files in your working directory (the one with the Vagrantfile) are shared when the virtual machine boots, so you need to copy it to the other pc as well.\n2. The clean way\nTheoretically it should never be necessary to do export/import with Vagrant. If you have the foresight to use provisioning for configuring the virtual environment (chef, puppet, ansible), and a version control system like git for your working directory, copying an environment would be at this point simple as running:"
  },
  {
    "question": "Is there a default password to connect to vagrant when using `homestead ssh` for the first time? I'm trying to connect to vagrant via homestead ssh:\n\n```\nvagrant@127.0.0.1's password:\n\n```\n\nBut my public key password doesn't work.\nMy Homestead.yaml looks like this:\n\n```\nauthorize: ~/.ssh/id_rsa.pub\n\nkeys:\n    - ~/.ssh/id_rsa\n\n```\n\nI'm using \"Laravel Homestead version 2.0.14\" with \"Vagrant 1.7.2\".",
    "answer": "debug1: Trying private key: /home/aaron/Documents/VMs/.vagrant/machines/default/virtualbox/private_key\n\nAfter trying a lot of passwords and becoming totally confused why my public key password is not working I found out that I have to use vagrant as password.\nMaybe this info helps someone else too - that's because I've written it down here.\nEdit:\nAccording to the Vagrant documentation, there is usually a default password for the user vagrant which is vagrant.\nRead more on here: official website\nIn recent versions however, they have moved to generating keypairs for each machine. If you would like to find out where that key is, you can run vagrant ssh -- -v. This will show the verbose output of the ssh login process. You should see a line like"
  },
  {
    "question": "Removing list of vms in vagrant cache I'm not looking for this answer, because my question is different.\nWhen I type vagrant global-status I get a list of vms, but some of those directories and VMs have been deleted along with the Vagrantfiles.\nBut when i type vagrant destroy [machineid], I get the following error, which I am looking to resolve. Please advise.\n\n```\nThe working directory for Vagrant doesn't exist! This is the\nspecified working directory:\n\n/Users/steven/projects/php/vagrant-laravel\n\n```",
    "answer": "vagrant global-status --prune\n\nYou should use the following command to remove invalid entries from the global index:"
  },
  {
    "question": "How do I add my own public key to Vagrant VM? I got a problem with adding an ssh key to a Vagrant VM. Basically the setup that I have here works fine. Once the VMs are created, I can access them via vagrant ssh, the user \"vagrant\" exists and there's an ssh key for this user in the authorized_keys file.\nWhat I'd like to do now is: to be able to connect to those VMs via ssh or use scp. So I would only need to add my public key from id_rsa.pub to the authorized_keys - just like I'd do with ssh-copy-id. \nIs there a way to tell Vagrant during the setup that my public key should be included? If not (which is likely, according to my google results), is there a way to easily append my public key during the vagrant setup?",
    "answer": "config.vm.provision \"file\", source: \"~/.ssh/id_rsa.pub\", destination: \"~/.ssh/me.pub\"\nVagrant.configure(2) do |config|\n\nconfig.vm.provision \"shell\", inline: <<-SHELL\ncat /home/vagrant/.ssh/me.pub >> /home/vagrant/.ssh/authorized_keys\nSHELL\n\nend\n\nCopying the desired public key would fall squarely into the provisioning phase. The exact answer depends on what provisioning you fancy to use (shell, Chef, Puppet etc). The most trivial would be a file provisioner for the key, something along this:\n\nWell, actually you need to append to authorized_keys. Use the the shell provisioner, like so:\n\nYou can also use a true provisioner, like Puppet. For example see Managing SSH Authorized Keys with Puppet."
  },
  {
    "question": "Vagrant&#39;s port forwarding not working I'm running into a small problem at the end of the Getting Started guide for vagrant. I'm working on a CentOS basebox that has Apache2 running (provisioning via Puppet). I've set up port forwarding for web requests using the following line in Vagrantfile:\n\n```\n config.vm.forward_port \"web\", 80, 4567\n\n```\n\nBut when I make requests to that port, they fail. The error reported by Safari is 'Safari can\u2019t open the page \u201chttp://localhost:4567/\u201d because the server unexpectedly dropped the connection.'\nI did a vagrant reload and saw \"[default] -- web: 80 => 4567 (adapter 1)\" in the scroll, so where should I begin to troubleshoot this? Thanks.",
    "answer": "I'll make this an actual answer instead of just more comments.\nFirst thing: try curl 'http://localhost:80' from within the VM.  If that doesn't work, then it's definitely not the port forwarding.\nNext: try curl -v 'http://localhost:4567/' from your host machine.  Curl might give you a better error message than Safari.\nI'd check that there are no firewalls set up restricting access to port 80.  The default Vagrant VM (Ubuntu) doesn't come with a firewall set up, but you said you're using something else, so it might be worth it to check.\nIf that's not it, try making something other than Apache listed on port 80.  Python ships with a simple HTTP server you can use -- go to the folder with index.html and run sudo python -m SimpleHTTPServer 80, then try hitting that with curl from both boxes.  If that works, then it's probably an Apache configuration issue.  I don't have enough experience with Apache to help if that's the case (I use nginx)."
  },
  {
    "question": "Vagrant was unable to mount VirtualBox shared folders Current setup: Virtualbox 5.1.20 with Vagrant 1.9.3\nPrevious working setup: Virtualbox 5.1.18 with Vagrant 1.9.3\nI installed the latest version of Virtualbox and tried to start my VM. The folder mounting has stopped working. The same folder mounting works fine in Virtualbox 5.1.18. I get the following error message:\n\nVagrant was unable to mount VirtualBox shared folders. This is usually because the filesystem \"vboxsf\" is not available. This filesystem is made available via the VirtualBox Guest Additions and kernel module. Please verify that these guest additions are properly installed in the guest. This is not a bug in Vagrant and is usually caused by a faulty Vagrant box. For context, the command attempted was:\n\n```\n mount -t vboxsf -o uid=1000,gid=1000 keys /keys \n\n```\n\nThe error output from the command was:\n\n```\nmount: wrong fs type, bad option, bad superblock on keys,\nmissing codepage or helper program, or other error\nIn some cases useful info is found in syslog - try dmesg | tail or so.\n\n```\n\n\nI have tried manually installing vagrant-vbguest plugin (via vagrant plugin install vagrant-vbguest), but that fixed nothing. Downgrading back to 5.1.18 does resolve it, but this isn't ideal moving forward.\nHow can I get folder mounting working with Virtualbox 5.1.20 and Vagrant 1.9.3?",
    "answer": "vagrant ssh\nls -lh /sbin/mount.vboxsf\nlrwxrwxrwx 1 root root 49 Apr 19 14:05 /sbin/mount.vboxsf -> /opt/VBoxGuestAdditions-5.1.20/other/mount.vboxsf\nsudo ln -sf /opt/VBoxGuestAdditions-*/lib/VBoxGuestAdditions/mount.vboxsf /sbin/mount.vboxsf\nexit\nvagrant reload\nDo you wish to continue? [yes or no]\nvagrant up\nvagrant vbguest --do install --no-cleanup\nvagrant ssh\nsudo /mnt/VBoxLinuxAdditions.run\ncd /opt\nsudo wget -c http://download.virtualbox.org/virtualbox/5.1.28/VBoxGuestAdditions_5.1.28.iso -O VBoxGuestAdditions_5.1.28.iso\nsudo mount VBoxGuestAdditions_5.1.28.iso -o loop /mnt\nsudo sh /mnt/VBoxLinuxAdditions.run\n\nUpdate\n\nFixed in VirtualBox 5.1.22.\n\n(https://www.virtualbox.org/ticket/16670)\nOriginal answer\nIn my case a symlink inside the VM was wrong.\nLog into the VM:\n\nThen:\n\nThis link is broken. When you look inside /opt/VBoxGuestAdditions-5.1.20/ you see that the subfolder \"other\" doesn't exists.\nThis line fixes that:\n\nNow logout of the VM:\n\nAnd check if it works for you:\n\nBut i can't fix the link because /sbin/mount.vboxsf does not exist inside my box!\nIf the link /sbin/mount.vboxsf does not exists in the first place, than the VBoxGuestAdditions couldn't be installed. This can happen if you have not downloaded the matching VirtualBox Extension Pack after you updated VirtualBox (v5.2.18 and up should download it automatically during Virtualbox updates). You can do that now:\n\nHalt all your running VMs.\nOpen the VirtualBox program.\nStart the download of the VirtualBox Extension Pack and install it.\nIf you are not being asked if you want to download the VirtualBox Extension Pack, open ->File ->Check for Update, to trigger the download dialog.\n\nIf there was no update for the VirtualBox Extension, than it's possible the VBoxGuestAdditions can't be installed because the installer shows a message and the vagrant plugin vbguest can not prompt an answer (this is \"fixed\" in vbguest v0.15.0). You can see that in the output of \"vagrant up\". There should be a line similar to this one:\n\nIf that is the case you can run the installer manually. To do so, start the box:\n\nThen trigger vbguest to try the installation again but leave the VBoxGuestAdditions ISO mounted afterwards:\n\nLog into the box:\n\nRan the installer manually:\n\nA last hope: DIY!\nIf nothing of the above worked, try to install the guest additions manually:"
  },
  {
    "question": "vagrant login as root by default Problem: frequently the first command I type to my boxes is su -.\nQuestion: how do I make vagrant ssh use the root user by default?\nVersion: vagrant 1.6.5",
    "answer": "config.ssh.username = 'root'\nconfig.ssh.password = 'vagrant'\nconfig.ssh.insert_key = 'true'\n==> mybox: Waiting for machine to boot. This may take a few minutes...\nmybox: SSH address: 127.0.0.1:2222\nmybox: SSH username: root\nmybox: SSH auth method: password\nmybox: Warning: Connection timeout. Retrying...\nmybox: Warning: Remote connection disconnect. Retrying...\n==> mybox: Inserting Vagrant public key within guest...\n==> mybox: Key inserted! Disconnecting and reconnecting using new SSH key...\n==> mybox: Machine booted and ready!\n\nSolution:\nAdd the following to your Vagrantfile:\n\nWhen you vagrant ssh henceforth, you will login as root and should expect the following:\n\nUpdate 23-Jun-2015:\nThis works for version 1.7.2 as well. Keying security has improved since 1.7.0; this technique overrides back to the previous method which uses a known private key. This solution is not intended to be used for a box that is accessible publicly without proper security measures done prior to publishing.\nReference:\n\nhttps://docs.vagrantup.com/v2/vagrantfile/ssh_settings.html"
  },
  {
    "question": "How do I increase the RAM and set up host-only networking in Vagrant? I would like to increase the RAM to at least 1 GB and I would like to configure \u201cHost-Only\u201d networking to use \"199.188.44.20\".\nThis is my Vagrantfile:\n\n```\n# -*- mode: ruby -*-\n# vi: set ft=ruby :\n\nVagrant::Config.run do |config|\n\n    config.vm.customize [\"modifyvm\", :id, \"--memory\", 1024]\n\n    config.vm.network :hostonly, \"199.188.44.20\"\n\n    config.vm.define :web do |web_config|\n        web_config.vm.box = \"lucid32\"\n        web_config.vm.forward_port 80, 8080\n\n        web_config.vm.provision :puppet do |puppet|\n            puppet.manifests_path = \"manifests\"\n            puppet.manifest_file = \"lucid32.pp\"\n        end\n    end\n\n    config.vm.define :web2 do |web2_config| \n        web2_config.vm.box = \"lucid32\"\n        web2_config.vm.forward_port 80, 8081\n\n        web2_config.vm.provision :puppet do |puppet|\n            puppet.manifests_path = \"manifests\"\n            puppet.manifest_file = \"myweb.pp\"\n        end\n    end\nend\n\n```\n\nHowever, when I run vagrant up I am getting:\n\nThe VM failed to remain in the \"running\" state while attempting to boot. This is normally cause by a misconfiguration or host system incompatibles. Please open the VirtualBox GUI and attempt to boot the virtual machine manually to get more informative error message\n\nAnd, I am getting a connection refuse error when I try to log in to the VM.",
    "answer": "config.vm.customize [\n\"modifyvm\", :id,\n\"--name\", \"Test_Environment\",\n\"--memory\", \"1024\"\n]\nVagrant::Config.run do |config|\nconfig.vm.network :hostonly, \"192.168.50.4\"\nend\nconfig.vm.define :web do |web_config|\nweb_config.vm.box = \"lucid32\"\nweb_config.vm.forward_port 80, 8080\nend\nweb_config.vm.provision :puppet do |puppet|\npuppet.manifests_path = \"manifests\"\npuppet.manifest_file = \"lucid32.pp\"\nend\n\nYou can modify various VM properties by adding the following configuration (see the Vagrant docs for a bit more info):\n\nYou can obtain the properties that you want to change from the documents for VirtualBox command-line options:\n\nhttp://www.virtualbox.org/manual/ch08.html#vboxmanage-modifyvm\n\nThe vagrant documentation has the section on how to change IP address:\n\nAlso you can restructure the configuration like this, ending is do with end without nesting it.\nThis is simpler."
  },
  {
    "question": "Where is Vagrant saving changes to the VM? I am just starting with Vagrant and I am having a little trouble understanding a few details. I have read through the docs but still am missing a basic concept.  When I want to start a Vagrant box I run:\nvagrant up\nThis will \"build the VM based on the box\" I understand that the boxes are stored at ~/.vagrant.d and in fact I have packaged up my own box from a base Ubuntu box.  However, when I run vagrant up and start to add files to the vm, where is the virtual hard drive for the vm stored?  For example, when I run apt-get install apache2 and the root system is modified, where is this modified? \nWhen I do a du on my current directory I do not see any changes.  I also do not see any changes in the ~/.vagrant.d directory.  However, I can do vagrant halt, restart my local machine and then run vagrant up again and the changes are persisted somewhere.  \nvagrant up also reports \n\n```\n[default] VM already created. Booting if its not already running...\n\n```\n\nCan someone tell me where the VM is created and where the changes are made?",
    "answer": "1. Start VirtualBox.\n2. Go to the VM that shows as running instance.\n3. Check out the Settings ->Storage.\n4. You can find the path to location where VMs are created and stored in your file system.\n\nVagrant imports the base box which is located at ~/.vagrant.d/boxes/, like you said.\nThis is where the base boxes are kept. It uses it to start a VM from the clean state. When importing a VM, the responsibility of where to store data files and VM state is up to VirtualBox itself. This is a configurable location but typically defaults to ~/VirtualBox\\ VMS for Mac OS X and Linux. In Windows the boxes are kept in %userprofile%\\.vagrant.d\\boxes\nIt is easy to find the place where they are getting created, regardless of what platform you happen to be using."
  },
  {
    "question": "How to use vagrant in a proxy environment? My company's network is using proxy. So when I use vagrant up, it showed me a 401 permission error.\nHow can I do some setting to use vagrant?",
    "answer": "vagrant plugin install vagrant-proxyconf\nconfig.proxy.http     = \"http://yourproxy:8080\"\nconfig.proxy.https    = \"http://yourproxy:8080\"\nconfig.proxy.no_proxy = \"localhost,127.0.0.1\"\n\nInstall proxyconf:\n\nConfigure your Vagrantfile:"
  },
  {
    "question": "What exactly do the Vagrant commands do? There is surprisingly no documentation on what the Vagrant commands do, other than references throughout the \"getting started\" tutorial.\nWhat I've worked out so far:\n\nbox - manage \"boxes\"\ndestroy - shut down the VM then delete its stored image?\ngem\nhalt - shut down the VM\ninit - prepare a directory with a new Vagrantfile\npackage - shut down the VM, then convert it to a 'package' which can be turned into a box? (Or something)\nprovision - run just the provisioning (eg, Chef, Puppet...) stage\nreload - modify the VM configuration (eg, reapply Vagrantfile), reboot the VM, reprovision\nresume - un-suspend (ie, unhibernate)\nssh - open an SSH shell connection to the VM\nssh-config\nstatus\nsuspend - hibernate the VM\nup - some or all of: copy a VM image to create a new VM, apply configuration to it, boot it\n\nDo I have these right? What are the others? I'm still a bit unclear on the exact difference between reload and destroy/up.",
    "answer": "box             manages boxes: installation, removal, etc.\nconnect         connect to a remotely shared Vagrant environment\ndestroy         stops and deletes all traces of the vagrant machine\ndocker-logs     outputs the logs from the Docker container\ndocker-run      run a one-off command in the context of a container\nglobal-status   outputs status Vagrant environments for this user\nhalt            stops the vagrant machine\nhelp            shows the help for a subcommand\ninit            initializes a new Vagrant environment by creating a Vagrantfile\nlist-commands   outputs all available Vagrant subcommands, even non-primary ones\nlogin           log in to Vagrant Cloud\npackage         packages a running vagrant environment into a box\nplugin          manages plugins: install, uninstall, update, etc.\nprovision       provisions the vagrant machine\nrdp             connects to machine via RDP\nreload          restarts vagrant machine, loads new Vagrantfile configuration\nresume          resume a suspended vagrant machine\nrsync           syncs rsync synced folders to remote machine\nrsync-auto      syncs rsync synced folders automatically when files change\nshare           share your Vagrant environment with anyone in the world\nssh             connects to machine via SSH\nssh-config      outputs OpenSSH valid configuration to connect to the machine\nstatus          outputs status of the vagrant machine\nsuspend         suspends the machine\nup              starts and provisions the vagrant environment\nversion         prints current and latest Vagrant version\n\nI'm not sure when it changed, but the current version (1.6.3) has a proper list of commands, and running vagrant list-commands gives an even more complete list:\n\nThe only commands left out from the complete list when running vagrant by itself are the docker and rsync ones. On my system, anyway.\nThat seems to be the definitive answer, now."
  },
  {
    "question": "Vagrant error: NFS is reporting that your exports file is invalid I am trying to set up the Discourse development environment using these instructions. However when I run Vagrant I get the error:\n\nNFS is reporting that your exports file is invalid. Vagrant does\n  this check before making any changes to the file. Please correct\n  the issues below and execute \"vagrant reload\":\nCan't open /etc/exports\n\nI checked, and I don't have an etc/exports folder, so I created one and ran vagrant reload. This got me:\n\n/Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/plugins/hosts/bsd/host.rb:146:in read': Is a directory - /etc/exports (Errno::EISDIR)\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/plugins/hosts/bsd/host.rb:146:innfs_prune'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/plugins/synced_folders/nfs/synced_folder.rb:68:in cleanup'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/builtin/synced_folder_cleanup.rb:24:inblock in call'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/builtin/synced_folder_cleanup.rb:22:in each_key'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/builtin/synced_folder_cleanup.rb:22:incall'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/warden.rb:34:in call'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/plugins/providers/virtualbox/action/prepare_nfs_valid_ids.rb:12:incall'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/warden.rb:34:in call'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/builtin/handle_forwarded_port_collisions.rb:118:incall'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/warden.rb:34:in call'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/plugins/providers/virtualbox/action/prepare_forwarded_port_collision_params.rb:30:incall'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/warden.rb:34:in call'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/builtin/env_set.rb:19:incall'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/warden.rb:34:in call'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/builtin/provision.rb:52:incall'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/warden.rb:34:in call'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/plugins/providers/virtualbox/action/clear_forwarded_ports.rb:13:incall'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/warden.rb:34:in call'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/plugins/providers/virtualbox/action/set_name.rb:19:incall'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/warden.rb:34:in call'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/plugins/providers/virtualbox/action/clean_machine_folder.rb:17:incall'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/warden.rb:34:in call'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/plugins/providers/virtualbox/action/check_accessible.rb:18:incall'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/warden.rb:34:in call'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/warden.rb:95:inblock in finalize_action'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/warden.rb:34:in call'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/warden.rb:34:incall'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/builder.rb:116:in call'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/runner.rb:69:inblock in run'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/util/busy.rb:19:in busy'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/runner.rb:69:inrun'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/builtin/call.rb:51:in call'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/warden.rb:34:incall'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/warden.rb:95:in block in finalize_action'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/warden.rb:34:incall'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/warden.rb:34:in call'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/builder.rb:116:incall'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/runner.rb:69:in block in run'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/util/busy.rb:19:inbusy'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/runner.rb:69:in run'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/builtin/call.rb:51:incall'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/warden.rb:34:in call'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/warden.rb:95:inblock in finalize_action'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/warden.rb:34:in call'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/warden.rb:34:incall'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/builder.rb:116:in call'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/runner.rb:69:inblock in run'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/util/busy.rb:19:in busy'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/runner.rb:69:inrun'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/builtin/call.rb:51:in call'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/warden.rb:34:incall'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/builtin/config_validate.rb:25:in call'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/warden.rb:34:incall'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/plugins/providers/virtualbox/action/check_virtualbox.rb:17:in call'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/warden.rb:34:incall'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/builtin/call.rb:57:in call'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/warden.rb:34:incall'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/plugins/providers/virtualbox/action/check_virtualbox.rb:17:in call'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/warden.rb:34:incall'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/builtin/config_validate.rb:25:in call'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/warden.rb:34:incall'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/warden.rb:95:in block in finalize_action'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/warden.rb:34:incall'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/warden.rb:34:in call'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/builder.rb:116:incall'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/runner.rb:69:in block in run'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/util/busy.rb:19:inbusy'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/runner.rb:69:in run'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/builtin/call.rb:51:incall'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/warden.rb:34:in call'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/plugins/providers/virtualbox/action/check_virtualbox.rb:17:incall'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/warden.rb:34:in call'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/builder.rb:116:incall'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/runner.rb:69:in block in run'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/util/busy.rb:19:inbusy'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/action/runner.rb:69:in run'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/machine.rb:147:inaction'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/plugins/commands/reload/command.rb:37:in block in execute'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/plugin/v2/command.rb:193:inblock in with_target_vms'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/plugin/v2/command.rb:191:in each'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/plugin/v2/command.rb:191:inwith_target_vms'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/plugins/commands/reload/command.rb:36:in execute'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/cli.rb:38:inexecute'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/lib/vagrant/environment.rb:484:in cli'\n      from /Applications/Vagrant/embedded/gems/gems/vagrant-1.4.1/bin/vagrant:127:in'\n      from /Applications/Vagrant/bin/../embedded/gems/bin/vagrant:23:in load'\n      from /Applications/Vagrant/bin/../embedded/gems/bin/vagrant:23:in'\n\nI am on a Mac (OS X 10.6.8). How do I fix this? I tried searching for the original error message, and all I found was a few repositories containing the Vagrant source code that throws it.",
    "answer": "Try using the known good versions of VirtualBox and Vagrant noted in Discourse as Your First Rails App:\n\nVagrant 1.1.2\nVirtualBox 4.2.10\n\nI was having the same issue on Mac (OS\u00a0X 10.9 (Mavericks)), but rolling back to these versions seemed to fix it for me."
  },
  {
    "question": "Vagrant for a Java project: should you compile in the VM or on the host? Here\u2019s the question: When using Vagrant for a Java project (or any compiled language project for that matter), should you compile in the VM or on the host? Also, would you want your IDE and all your development tools to be run from inside the VM as well, or on the host?\nIt seems to be not very well defined exactly how a Java IDE and the compile/deploy process work with a Vagrant VM. Generally my impression is that code is edited on the host, and run on the VM, which works great for non-compiled languages. Other answers on Stackoverflow have implied that Vagrant is less useful for compiled languages because of the extra compile step, but I still want to see what can be done.\nSome things I\u2019ve thought through already:\nWhy compile on the VM\n\nif compiling on host, java is one more piece of software to install\nif compiling on host, the java version on host must be manually kept up to date with that on the VM\nthe corresponding java version on the host might be unavailable (say, on a Mac)\n\nWhy have IDE on the VM\n\ntighter integration between environment and IDE, can use shortcuts to run the application\ncan connect debugger for java applications without remote debugging (one step run/debug)\n\nWhy compile on the host\n\nfaster compile times\nwant to keep the VM as close to what production looks like as possible\n\nWhy have IDE on the host\n\nit\u2019s the vagrant convention to edit code on the host and run it on the VM\nbetter UI performance (X forwarding and VNC are slow)\n\nWhat are your thoughts: should I run my IDE from inside the VM or the host? Should I compile from inside the VM or the host?",
    "answer": "After much thought and experimentation, I've decided on where to use Vagrant and how it integrates with the Java development workflow.\nFor JavaEE / deployed applications, configuring a web server and a database server are definitely things that have \"enough\" complexity to warrant the use of Vagrant. With two servers and the myriad ways to configure them, it's easy for configuration to get out of sync from one developer to another, bringing about the \"works on my machine\" syndrome. For this kind of software, it would work best to edit and compile the code on the host, and deploy to a Vagrant VM that mimics your production environment. The deployment folder for the web server could even be symlinked to a compile target on the host, removing the need to manually redeploy. So Vagrant could be an important part of your development lifecycle, but the cycle time for code/compile/deploy from the host and run on the VM with Java would be longer than the cycle time for code on the host and run on the VM that we see with PHP/Ruby/Node/etc.\nFor standalone Java applications (like libraries or desktop applications) the story changes a bit. In this case it makes the most sense to edit, compile, and run on the host machine, eschewing the use of Vagrant altogether. If you're using one of the big Java IDE's (Eclipse, Netbeans, IntelliJ...), you already have Java installed on the machine. At that point there is very little advantage compared to the overhead of using Vagrant, and only serves to put an extra layer of complexity in your development process. This is because by the time you are able to edit Java with an IDE you are able to run everything on the host anyway. One issue is that the version of Java required for the project may not match the version running the IDE on the host. In general (hopefully) this is not too much of a problem; as of this writing JDK6 is end-of-lifed and JDK8 is not yet released (guess where that leaves us). But if you did need to run multiple versions, you should be able to set JAVA_HOME on the host as needed. Though this does introduce extra complexity, it is less complexity than maintaining a Vagrant runtime just to work with projects using different versions of Java.\nThe interesting question is what to do with containerless web applications. Should the web server (in this case internal to the application) be run inside the VM as we did for the external web server? Or run on the host as we did for the standalone application? For containerless web applications, there is no external web server to worry about, but there is still likely a database. In this situation we can take a hybrid approach. Running a containerless web app is essentially the same as running a standalone application, so it would be effective to compile and run your code on the host machine. But with a database involved there is still enough complexity and configuration there that it makes sense to have the database server be on its own Vagrant VM.\nHopefully this gives Java developers who are interested in Vagrant some context about how to use it."
  },
  {
    "question": "How to connect to Mysql Server inside VirtualBox Vagrant? I mounted a new VirtualBox Machine with Vagrant, and inside that VM I installed Mysql Server. How can I connect to that server outside the vm? I already forward the port 3306 of the Vagrantfile , but when I try to connect to the mysql server, it`s resposts with the error: \n'reading initial communication packet'\n\n```\nERROR 2013 (HY000): Lost connection to MySQL server at 'reading initial communication packet', system error: 0\n\n```",
    "answer": "sudo service mysql restart\n\nMake sure MySQL binds to 0.0.0.0 and not 127.0.0.1 or it will not be accessible from outside the machine\nYou can ensure this by editing the /etc/mysql/my.conf file and looking for the bind-address item--you want it to look like bind-address = 0.0.0.0.  Then save this and restart MySQL:\n\nIf you are doing this on a production server, you want to be aware of the security implications, discussed here: https://serverfault.com/questions/257513/how-bad-is-setting-mysqls-bind-address-to-0-0-0-0"
  },
  {
    "question": "How to pass parameter on &#39;vagrant up&#39; and have it in the scope of Vagrantfile? I'm looking for a way to pass parameter to Chef cookbook like:\n\n```\n$ vagrant up some_parameter\n\n```\n\nAnd then use some_parameter inside one of the Chef cookbooks.",
    "answer": "MY_VAR='my value' vagrant up\n\nYou cannot pass any parameter to vagrant. The only way is to use environment variables\n\nAnd use ENV['MY_VAR'] in recipe."
  },
  {
    "question": "vagrant.d outside of the home folder I've the problem that my home directory is actually located on a remote server and with ~/.vagrant.d sitting on that server, the performance of vagrant decreases heavily (and file-server backup size increases).\nSo is there any way to move at least ~/vagrant.d/boxes out of the home directory?\nCheers.",
    "answer": "By default Vagrant uses ~/.vagrant.d. Fortunately, vagrant provides an environment variable called VAGRANT_HOME by which you can set vagrant home.\nJust do the following to change the vagrant home (this only works in the current session)\nexport VAGRANT_HOME=/path/to/vagrant\nTo make it permanent, add this to your ~/.bash_profile (for login shell).\nUpdate: VAGRANT_HOME has been added to the documentation - Environmental Variables\n\nVAGRANT_HOME can be set to change the directory where Vagrant stores global state. By default, this is set to ~/.vagrant.d. The Vagrant home directory is where things such as boxes are stored, so it can actually become quite large on disk."
  },
  {
    "question": "Pass environment variables to vagrant shell provisioner It looks like passing environment variables when calling vagrant up is simple if you're using a Ruby provisioner:\n\n```\nVAR=123 vagrant up\n\n```\n\nIn the Vagrantfile:\n\n```\nENV['VAR']\n\n```\n\nHow do I do this with the :shell provisioner? Simply doing this does not seem to work:\n\n```\n$VAR\n\n```",
    "answer": "config.vm.provision \"shell\" do |s|\ns.inline = \"VAR1 is $1 and VAR2 is $2\"\ns.args   = \"#{ENV['VAR1']} #{ENV['VAR2']}\"\nend\n\nIt's not ideal, but I got this to work for now:"
  },
  {
    "question": "How do I remove a Vagrant box from global-status, after deleting that box from the filesystem? I deleted a folder containing a running Vagrant box before realising it was still running.\nHow can I delete it from Vagrant (global-status) now?\nI already removed the Virtualbox VM.",
    "answer": "vagrant global-status --prune\n\nTo discard old boxes listed in vagrant global-status (eg. you deleted the folder containing the .vagrant dir from the filesystem) you just need to run:\n\nYou might need to remove the Virtual Machine directly from your provider (VMWare, Virtualbox, ..) control interface."
  },
  {
    "question": "execute commands as user after Vagrant provisioning There are some commands that have to be run as a normal user after the initial provisioning.  I thought I could do this using a separate shell script and the command su --login -c <command> vagrant, but it's not getting the user's path or other environment settings from .bashrc.\ne.g.:\n\n```\n#!/usr/bin/env bash\nsu --login -c \"rbenv install 2.0.0-p353\" vagrant\nsu --login -c \"rbenv global 2.0.0-p353\" vagrant\nsu --login -c \"gem update --system\" vagrant\nsu --login -c \"yes | gem update\" vagrant\nsu --login -c \"gem install rdoc\" vagrant\nsu --login -c \"gem install rails pg\" vagrant\n\n```\n\nIs there a way to do this?  Maybe it has to be done with another provisioning tool like Puppet or Chef?  I've thought of creating another shell script that sources the .bashrc, copying it to the box using a :file provisioner and executing the commands like that, but it seems sort of like a hack.\nWhat's the right way to do this?",
    "answer": "Vagrant.configure(\"2\") do |config|\nscript = <<-SCRIPT\nrbenv install 2.0.0-p353\nrbenv global 2.0.0-p353\ngem update --system\nyes | gem update\ngem install rdoc\ngem install rails pg\nSCRIPT\n\nconfig.vm.provision \"shell\", inline: $script, privileged: false\nend\n\nYou should be able to do this using the Vagrant Shell provisioner, e.g.\n\nThe key is to specify privileged: false so that it will use the default user and not root."
  },
  {
    "question": "Vagrant box could not be found or could not be accessed in the remote catalog - incompatible curl version I just downloaded Vagrant and did the settings as well as install virtual box. I just can't start my project (vagrant up). I have a vagrant file and so on. What can I do?\n\n```\n$ vagrant up\nBringing machine 'default' up with 'virtualbox' provider...\n==> default: Box 'scotch/box' could not be found. Attempting to find and install\n...\n    default: Box Provider: virtualbox\n    default: Box Version: >= 0\nThe box 'scotch/box' could not be found or\ncould not be accessed in the remote catalog. If this is a private\nbox on HashiCorp's Atlas, please verify you're logged in via\n`vagrant login`. Also, please double-check the name. The expanded\nURL and error message are shown below:\n\nURL: [\"https://atlas.hashicorp.com/scotch/box\"] \n\n```",
    "answer": "sudo rm /opt/vagrant/embedded/bin/curl\n\nThere seems to be an issue with vagrant 1.8.7 and the embedded curl version vs the mac os binary (shipped by default on mac os Sierra and others) - remove the embedded one\n\nNote: you also need to remove the embedded curl when you add a vagrant box (remotely or locally) so if you get the same error when running vagrant box add .... just remove the curl from vagrant and it will work"
  },
  {
    "question": "How to combine Vagrant with Jenkins for the perfect Continuous Integration Environment? You have a project which has got some SW requirements to run (e.g.: a specific version of Apache, a version of PHP, an instance of a MySQL database and a couple of other pieces of software).\nYou have already discovered Vagrant, so your virtual environment is all setup. You can create boxes out of your configuration files and cookbooks.\nYou have also understood the advantages of a Continuous Integration system such as Jenkins.\nNow you would like to combine these two worlds (Vagrant and Jenkins) to get the perfect Continuous Integration Environment. To be more specific, you would like not to install the SW required by your project on the machine running Jenkins, but you would like to use the virtual environment provided by Vagrant to periodically build your project on the top of it. The CI software (Jenkins) will build the Vagrant box for you and build and test your project on the top of it.\nHow would you setup your environment to achieve this?",
    "answer": "it is a good solution for build system, my suggestion:\n\nYour current jenkins works as master CI (probably started by user jenkins)\nCreate another user in same machine or another machine to work as jenkins slave mode\n\njenkins slave can be invoked from jenkins master, and it can use different user like vagrant who had permission and environment for vagrant, therefore it will not interfere the original jenkins master server\ncreate your base vagrant box, then it can be reused to speedup for your deployment\n\nMost of the installation information (packages) could be managed by puppet (or chef) to be loaded into your vm box.\n\nProbably you can take a look at veewee, which can create vagrant box on fly.\nHere is the Make CI easier with Jenkins CI and Vagrant for my guideline for this suggestion."
  },
  {
    "question": "Demand a Vagrant plugin within the Vagrantfile? Supposed execution of a Vagrantfile requires a specific Vagrant plugin to be installed. So, basically what you need to do is\n\n```\n$ vagrant plugin install foobar-plugin\n$ vagrant up\n\n```\n\nIf you skip the first step, vagrant up fails.\nIs there an option in Vagrant to make it install the plugin automatically? In other words: Is it possible to specify within a Vagrantfile which plugins to install automatically before creating and booting up the machine?",
    "answer": "Vagrant.configure(\"2\") do |config|\nconfig.vagrant.plugins = \"vagrant-some-plugin\"\n\nconfig.vagrant.plugins = [\"vagrant-some-plugin\", \"vagrant-some-other-plugin\"]\n\nconfig.vagrant.plugins = {\"vagrant-some-plugin\" => {\"version\" => \"1.0.0\"}}\nend\nvagrant up\nVagrant has detected project local plugins configured for this\nproject which are not installed.\n\nvagrant-some-plugin\nInstall local plugins (Y/N) [N]: y\nInstalling the 'vagrant-some-plugin' plugin. This can take a few minutes...\nFetching vagrant-some-plugin-1.0.0.gem\nInstalled the plugin 'vagrant-some-plugin (1.0.0)'!\n\nVagrant has completed installing local plugins for the current Vagrant\nproject directory. Please run the requested command again.\n\n2019 Update: Vagrant now has functionality to require plugins through the Vagrantfile via:\n\nIf Vagrant detects there are plugins not already installed it will prompt the user to install them itself:\n\nSee https://www.vagrantup.com/docs/vagrantfile/vagrant_settings.html"
  },
  {
    "question": "How can I create a VM in Vagrant with VirtualBox with two CPUs? On Windows 7 64 bit trying to start up a VM (Ubuntu 32 bit). I'm having trouble getting my VM to show two cores despite adding the modify vm command in my Vagrantfile. My Vagrant version is 1.2.2.\n\n```\n# -*- mode: ruby -*-\n# vi: set ft=ruby :\n\nVagrant.configure(\"2\") do |config|\n  config.vm.box = \"precise32\"\n  config.vm.box_url = \"http://files.vagrantup.com/precise32.box\"\n\n  config.vm.provider :virtualbox do |vb|\n    vb.customize [\"modifyvm\", :id, \"--memory\", \"2048\"]\n    vb.customize [\"modifyvm\", :id, \"--cpus\", \"2\"]   \n  end  \nend\n\n```\n\nWith this Vagrantfile I issue the vagrant up command. Then I issue vagrant ssh followed by lscpu which yields:\n\n```\nArchitecture:          i686\nCPU op-mode(s):        32-bit, 64-bit\nByte Order:            Little Endian\nCPU(s):                1\nOn-line CPU(s) list:   0\nThread(s) per core:    1\nCore(s) per socket:    1\nSocket(s):             1\nVendor ID:             GenuineIntel\nCPU family:            6\nModel:                 58\nStepping:              9\nCPU MHz:               2565.513\nBogoMIPS:              5131.02\nL1d cache:             32K\nL1d cache:             32K\nL2d cache:             6144K\n\n```\n\nI think CPU(s) should show 2, so my VM only has one CPU right now. How can I get 2 CPUs to show up when I run lscpu?",
    "answer": "Add vb.customize [\"modifyvm\", :id, \"--ioapic\", \"on\"] to the config.vm.provider block inside your Vagrantfile.\nLooking at the VirtualBox documentation it mentions:\n\n\"Note Enabling the I/O APIC is required for 64-bit guest operating\nsystems, especially Windows Vista; it is also required if you want to\nuse more than one virtual CPU in a virtual machine.\""
  },
  {
    "question": "Vagrant, how to specify the disk size? I want to make sure that my development environment has enough free space to install tools and other stuff. I can't find any configuration option about telling to Vagrant the minimum disk size that I want. Is this possible or I need to create my own box?",
    "answer": "vagrant plugin install vagrant-disksize\nvagrant.configure('2') do |config|\nconfig.vm.box = 'ubuntu/xenial64'\nconfig.disksize.size = '50GB'\nend\n\nI have used the vagrant plugin vagrant-disksize to resize the disk.\nIt worked. It can also help to specify the initial disk size.\nRun the following at the command line:\n\nand use the following in your Vagrantfile:"
  },
  {
    "question": "How do I delete a virtualbox machine in the GURU_MEDITATION error state? How do I delete a VirtualBox machine in the GURU_MEDITATION error state?  Is it enough just to delete the directory while VirtualBox is not running?\nEDIT: After posting, I deleted the entire directory that \"Show in File Manager\" navigates to.\nIt looks like:\n\nNote that there is no power off, and even remove is greyed out.  I believe this is the exact same as it looked even before I deleted the directory.\nEDIT 2: I tried the command line poweroff after deleting the files.  It hangs:\n\nvboxmanage controlvm wmf-vagrant_1354733432 poweroff 0%...10%...20%...\n\nEDIT 3: It also fails to unregister it from the command-line:\n\nVBoxManage unregistervm wmf-vagrant_1354733432 --delete VBoxManage:\n  error: Cannot unregister the machine 'wmf-vagrant_1354733432' while it\n  is locked VBoxManage: error: Details: code VBOX_E_INVALID_OBJECT_STATE\n  (0x80bb0007), component Machine, interface IMachine, callee\n  nsISupports Context: \"Unregister(fDelete ?\n  (CleanupMode_T)CleanupMode_DetachAllReturnHardDisksOnly :\n  (CleanupMode_T)CleanupMode_DetachAllReturnNone,\n  ComSafeArrayAsOutParam(aMedia))\" at line 160 of file\n  VBoxManageMisc.cpp",
    "answer": "Kill the VBoxHeadless process and run \"vagrant destroy\"\nDestroying vagrant and sending the kill signal with the \"killall\" command looks like:\nkillall -9 VBoxHeadless && vagrant destroy"
  },
  {
    "question": "Vagrant insecure by default? EDIT 2: TL;DR: the answer was yes in 2013, but this flaw has been fixed\nBy following the Getting Started instructions on vagrantup.com, I seem to end up with a virtual machine that is accepting SSH connections on port 2222 so that anyone can get root access to my VM and read my host working directory using the default credentials (username=password=vagrant or vagrant_insecure_private_key).\nIs this true? If yes, why is it not considered a gaping security vulnerability? What if I had copied sensitive data to the VM?\nEDIT: and for those who think anyone on the internet being able to read your sources and executing arbitrary code on your VM is not that bad, I recommend reading the \"Breaking out\" section in this blog post http://blog.ontoillogical.com/blog/2012/10/31/breaking-in-and-out-of-vagrant/\nIn a nutshell: running Vagrant \"as intended\" can also enable anyone to break into your host/development machine (e.g., by using a malicious git post-commit hook).",
    "answer": "The short answer is YES.\nWhy?\nWhen building Vagrant base boxes (manually or using tools like Veewee to automate), builders follow the vagrant base boxes specifications which defines the following:\n\nUser root and vagrant use vagrant as password\nPublic key authentication (password-less) for the user vagrant.\n\nVagrant project provides an insecure key pair for SSH Public Key Authentication so that vagrant ssh works.\nBecause everyone has access to the private key, anyone can use the private key to login to your VMs (suppose they know your IP of the host machine, port is by default 2222 as forwarding rules in place.)\nIt is NOT secure OOTB. However, you can remove the trusted key from ~vagrant/.ssh/authorized_keys and add your own, change password for vagrant and root, then it's considered relatively safe.\nUpdate\nSince Vagrant 1.2.3, by default SSH forwarded port binds to 127.0.0.1 so only local connections are allowed [GH-1785].\nIMPORTANT Update\nSince Vagrant 1.7.0 (PR #4707) Vagrant will replace the default insecure ssh keypair with randomly generated keypair on first vagrant up.\nSee in the CHANGELOG: the default insecure keypair is used, Vagrant will automatically replace it with a randomly generated keypair on first vagrant up. GH-2608"
  },
  {
    "question": "How to find the Vagrant IP? I have been developing an automated deployment using Capistrano and using Vagrant as my test virtual server.\nThe thing is, I need the IP of Vagrant to \"ssh into it\".\nI tried ifconfig and got the IP but it looks like it is not the exact vagrant IP.\nCan anybody help me to get the Vagrant IP?",
    "answer": "vagrant ssh-config > .ssh.config\nrole :web, \"default\"\nrole :app, \"default\"\nset :use_sudo, true\nset :user, 'root'\nset :run_method, :sudo\n\ndefault_run_options[:pty] = true\nssh_options[:forward_agent] = true\nssh_options[:config] = '.ssh.config'\n\nrun:\n\nand then in config/deploy.rb"
  },
  {
    "question": "How to upgrade to VirtualBox Guest Additions on VM box? I've got the latest version of VirtualBox installed on my desktop (4.3.4).\nI'm using Vagrant to run a VM based on the example 64-bit Ubuntu 12.04 LTS box at:\nhttp://files.vagrantup.com/precise64.box\nEverytime I run vagrant up, I get the following warning:\n\n```\nThe guest additions on this VM do not match the installed version of\nVirtualBox! In most cases this is fine, but in rare cases it can\ncause things such as shared folders to not work properly. If you see\nshared folder errors, please update the guest additions within the\nvirtual machine and reload your VM.\n\nGuest Additions Version: 4.2.0\nVirtualBox Version: 4.3\n\n```\n\nI've googled, but I can't find a way to upgrade to Guest Additions v4.3.  The latest version in the Ubuntu repository for precise is 4.1, and there's no download link on the official VirtualBox download page.",
    "answer": "You can check out the following plugin, it should suit your needs:\nhttps://github.com/dotless-de/vagrant-vbguest\nFor Vagrant \u2265 1.1\nvagrant plugin install vagrant-vbguest\nVagrant 1.0 and older\nvagrant gem install vagrant-vbguest"
  },
  {
    "question": "How do I exit Vim? I am stuck and cannot escape. It says:\n\n```\ntype :quit<Enter> to quit VIM\n\n```\n\nBut when I type that it simply appears in the object body.",
    "answer": "Hit the Esc key to enter \"Normal mode\". Then you can type : to enter \"Command-line mode\". A colon (:) will appear at the bottom of the screen and you can type in one of the following commands. To execute a command, press the Enter key.\n\n:q  to quit (short for :quit)\n:q! to quit without saving (short for :quit!)\n:wq to write and quit\n:wq! to write and quit, attempting to force the write if the file lacks write permission\n:x to write and quit; like :wq but writes only if modified (short for :exit)\n:qa to quit all (short for :quitall)\n:cq to quit, without saving, with a nonzero exit code to indicate failure (short for :cquit)\n\nYou can also quit Vim directly from \"Normal mode\" by typing ZZ to save and quit (same as :x) or ZQ to just quit (same as :q!). (Note that case is important here. ZZ and zz do not mean the same thing.)\nVim has extensive help - that you can access with the :help command - where you can find answers to all your questions and a tutorial for beginners."
  },
  {
    "question": "How to replace a character by a newline in Vim I'm trying to replace each , in the current file by a new line:\n\n```\n:%s/,/\\n/g \n\n```\n\nBut it inserts what looks like a ^@ instead of an actual newline. The file is not in DOS mode or anything.\nWhat should I do?\nIf you are curious, like me, check the question Why is \\r a newline for Vim? as well.",
    "answer": "echo bar > test\n(echo 'Before:'; xxd test) > output.txt\nvim test '+s/b/\\n/' '+s/a/\\r/' +wq\n(echo 'After:'; xxd test) >> output.txt\nmore output.txt\nBefore:\n0000000: 6261 720a                                bar.\nAfter:\n0000000: 000a 720a                                ..r.\n\nUse \\r instead of \\n.\nSubstituting by \\n inserts a null character into the text. To get a newline, use \\r. When searching for a newline, you\u2019d still use \\n, however. This asymmetry is due to the fact that \\n and \\r do slightly different things:\n\\n matches an end of line (newline), whereas \\r matches a carriage return. On the other hand, in substitutions \\n inserts a null character whereas \\r inserts a newline (more precisely, it\u2019s treated as the input CR). Here\u2019s a small, non-interactive example to illustrate this, using the Vim command line feature (in other words, you can copy and paste the following into a terminal to run it). xxd shows a hexdump of the resulting file.\n\nIn other words, \\n has inserted the byte 0x00 into the text; \\r has inserted the byte 0x0a."
  },
  {
    "question": "Indent multiple lines quickly in vi It should be trivial, and it might even be in the help, but I can't figure out how to navigate it. How do I indent multiple lines quickly in vi?",
    "answer": "Use the > command. To indent five lines, 5>>. To mark a block of lines and indent it, Vjj> to indent three lines (Vim only). To indent a curly-braces block, put your cursor on one of the curly braces and use >% or from anywhere inside block use >iB.\nIf you\u2019re copying blocks of text around and need to align the indent of a block in its new location, use ]p instead of just p. This aligns the pasted block with the surrounding text.\nAlso, the shiftwidth setting allows you to control how many spaces to indent."
  },
  {
    "question": "Vim clear last search highlighting After doing a search in Vim, I get all the occurrences highlighted. How can I disable that?  I now do another search for something gibberish that can't be found.\nIs there a way to just temporarily disable the highlight and then re-enable it when needed again?",
    "answer": ":noh\nset nohlsearch\nset hlsearch!\n\nnnoremap <F3> :set hlsearch!<CR>\n\nTo turn off highlighting until the next search:\n\nOr turn off highlighting completely:\n\nOr, to toggle it:"
  },
  {
    "question": "How to do case insensitive search in Vim I'd like to search for an upper case word, for example COPYRIGHT in a file. I tried performing a search like:\n\n```\n/copyright/i    # Doesn't work\n\n```\n\nbut it doesn't work.  I know that in Perl, if I give the i flag into a regex it will turn the regex into a case-insensitive regex. It seems that Vim  has its own way to indicate a case-insensitive regex.",
    "answer": "You can use the \\c escape sequence anywhere in the pattern. For example:\n/\\ccopyright or /copyright\\c or even /copyri\\cght\nTo do the inverse (case sensitive matching), use \\C (capital C) instead."
  },
  {
    "question": "How to duplicate a whole line in Vim? How do I duplicate a whole line in Vim in a similar way to Ctrl+D in IntelliJ IDEA/ Resharper or Ctrl+Alt+\u2191/\u2193 in Eclipse?",
    "answer": "First:\nyy or Y to copy the line (mnemonic: yank)\nor\ndd to delete the line (Vim copies what you deleted into a clipboard-like \"register\", like a cut operation)\nthen:\np to paste the copied or deleted text after the current line\nor\nShift + P to paste the copied or deleted text before the current line"
  },
  {
    "question": "How does the Vim &quot;write with sudo&quot; trick work? Many of you have probably seen the command that allows you to write to a file that needs root permissions, even when you forgot to open Vim with sudo:\n\n```\n:w !sudo tee %\n\n```\n\nThe thing is that I don't get what is exactly happening here.\nI have already figured this:\nw is for this\n\n```\n                                                        *:w_c* *:write_c*\n:[range]w[rite] [++opt] !{cmd}\n                        Execute {cmd} with [range] lines as standard input\n                        (note the space in front of the '!').  {cmd} is\n                        executed like with \":!{cmd}\", any '!' is replaced with\n                        the previous command |:!|.\n\n```\n\nso it passes all the lines as standard input.\nThe !sudo tee part calls tee with administrator privileges.\nFor all to make sense, the % should output the filename (as a parameter for tee), but I can't find references on the help for this behavior.\nHow can I dissect this command?",
    "answer": "+-----------+    tee     +------------+\n|           |  --------  |            |\n| ps -ax    |  --------  | grep 'foo' |\n|           |     ||     |            |\n+-----------+     ||     +------------+\n||\n+---------------+\n|               |\n| processes.txt |\n|               |\n+---------------+\n\" Allow saving of files as sudo when I forgot to start vim using sudo.\ncmap w!! w !sudo tee > /dev/null %\n\nIn :w !sudo tee %...\n% means \"the current file\"\nAs eugene y pointed out, % does indeed mean \"the current file name\", which is passed to tee so that it knows which file to overwrite.\n(In substitution commands, it's slightly different; as :help :% shows, it's equal to 1,$ (the entire file) (thanks to @Orafu for pointing out that this does not evaluate to the filename). For example, :%s/foo/bar means \"in the current file, replace occurrences of foo with bar.\" If you highlight some text before typing :s, you'll see that the highlighted lines take the place of % as your substitution range.)\n:w isn't updating your file\nOne confusing part of this trick is that you might think :w is modifying your file, but it isn't. If you opened and modified file1.txt, then ran :w file2.txt, it would be a \"save as\"; file1.txt wouldn't be modified, but the current buffer contents would be sent to file2.txt.\nInstead of file2.txt, you can substitute a shell command to receive the buffer contents. For instance, :w !cat will just display the contents.\nIf Vim wasn't run with sudo access, its :w can't modify a protected file, but if it passes the buffer contents to the shell, a command in the shell can be run with sudo. In this case, we use tee.\nUnderstanding tee\nAs for tee, picture the tee command as a T-shaped pipe in a normal bash piping situation: it directs output to specified file(s) and also sends it to standard output, which can be captured by the next piped command.\nFor example, in ps -ax | tee processes.txt | grep 'foo', the list of processes will be written to a text file and passed along to grep.\n\n(Diagram created with Asciiflow.)\nSee the tee man page for more info.\nTee as a hack\nIn the situation your question describes, using tee is a hack because we're ignoring half of what it does. sudo tee writes to our file and also sends the buffer contents to standard output, but we ignore standard output. We don't need to pass anything to another piped command in this case; we're just using tee as an alternate way of writing a file and so that we can call it with sudo.\nMaking this trick easy\nYou can add this to your .vimrc to make this trick easy-to-use: just type :w!!.\n\nThe > /dev/null part explicitly throws away the standard output, since, as I said, we don't need to pass anything to another piped command."
  },
  {
    "question": "What&#39;s a quick way to comment/uncomment lines in Vim? I have a Ruby code file open in vi, there are lines commented out with #:\n\n```\nclass Search < ActiveRecord::Migration\n  def self.up\n    # create_table :searches do |t|\n    #   t.integer :user_id\n    #   t.string :name\n    #   t.string :all_of\n    #   t.string :any_of\n    #   t.string :none_of\n    #   t.string :exact_phrase\n    # \n    #   t.timestamps\n    # end\n  end\n\n  def self.down\n    # drop_table :searches\n  end\nend\n\n```\n\nSay I want to uncomment all the lines in the first def ... end section. What's an efficient way to do that in Vim?\nIn general, I'm looking for an easy and fluid way to comment and uncomment lines. Here I'm dealing with Ruby code, but it could be JavaScript (//) or Haml (-#).",
    "answer": "I use the NERD Commenter script. It lets you easily comment, uncomment or toggle comments in your code.\nAs mentioned in the comments:\n\nfor anyone who is confused by the usage, default leader is \"\\\" so 10\\cc will comment ten lines and 10\\cu will uncomment those ten lines"
  },
  {
    "question": "Turning off auto indent when pasting text into vim I am making the effort to learn Vim.\nWhen I paste code into my document from the clipboard, I get extra spaces at the start of each new line:\n\n```\nline\n  line\n    line\n\n```\n\nI know you can turn off auto indent but I can't get it to work because I have some other settings conflicting or something (which look pretty obvious in my .vimrc but don't seem to matter when I take them out). \nHow do I turn off auto indenting when I paste code but still have vim auto indent when I am writing code?  Here is my .vimrc file:\n\n```\nset expandtab  \nset tabstop=2  \nset shiftwidth=2  \nset autoindent  \nset smartindent  \nset bg=dark  \nset nowrap  \n\n```",
    "answer": ":set paste\n:set nopaste\nset pastetoggle=<F3>\n\nUpdate: Better answer here: https://stackoverflow.com/a/38258720/62202\nTo turn off autoindent when you paste code, there's a special \"paste\" mode.\nType\n\nThen paste your code.  Note that the text in the tooltip now says -- INSERT (paste) --.\nAfter you pasted your code, turn off the paste-mode, so that auto-indenting when you type works correctly again.\n\nHowever, I always found that cumbersome.  That's why I map <F3> such that it can switch between paste and nopaste modes while editing the text!  I add this to .vimrc"
  },
  {
    "question": "What is the difference between the remap, noremap, nnoremap and vnoremap mapping commands in Vim? What is the difference between the remap, noremap, nnoremap and vnoremap mapping commands in Vim?",
    "answer": ":map j gg           (moves the cursor to the first line)\n:map Q j            (moves the cursor to the first line)\n:noremap W j        (moves the cursor down one line)\n:help :map\n:help :noremap\n:help recursive_mapping\n:help :map-modes\n\nremap is an option that makes mappings work recursively. By default, it is on and I'd recommend you leave it that way. The rest are mapping commands, described below:\n:map and :noremap are recursive and non-recursive versions of the various mapping commands. For example, if we run:\n\nThen:\n\nj will be mapped to gg.\nQ will also be mapped to gg, because j will be expanded for the recursive mapping.\nW will be mapped to j (and not to gg) because j will not be expanded for the non-recursive mapping.\n\nNow remember that Vim is a modal editor. It has a normal mode, visual mode and other modes.\nFor each of these sets of mappings, there is a mapping that works in normal, visual, select and operator modes (:map and :noremap), one that works in normal mode (:nmap and :nnoremap), one in visual mode (:xmap and :xnoremap) and so on.\nFor more guidance on this, see:"
  },
  {
    "question": "How do I move to end of line in Vim? I know how to generally move around in command mode, specifically, jumping to lines, etc. But what is the command to jump to the end of the line that I am currently on?",
    "answer": "Just the $ (dollar sign) key.  You can use A to move to the end of the line and switch to editing mode (Append).  To jump to the last non-blank character, you can press g then _ keys.\nThe opposite of A is I (Insert mode at beginning of line), as an aside.  Pressing just the ^ will place your cursor at the first non-white-space character of the line."
  },
  {
    "question": "Tab key == 4 spaces and auto-indent after curly braces in Vim How do I make vi-Vim never use tabs (converting spaces to tabs, bad!), makes the tab key == 4 spaces, and automatically indent code after curly brace blocks like Emacs does?\nAlso, how do I save these settings so I never have to input them again?\nI've seen other questions related to this, but it always seems to be a little off from what I want.",
    "answer": "filetype plugin indent on\n\" show existing tab with 4 spaces width\nset tabstop=4\n\" when indenting with '>', use 4 spaces width\nset shiftwidth=4\n\" On pressing tab, insert 4 spaces\nset expandtab\nset smartindent\nset tabstop=4\nset shiftwidth=4\nset expandtab\n:help smartindent\n:help auto-setting\n:set all\n:help C-indenting\n\nAs has been pointed out in a couple of other answers, the preferred method now is NOT to use smartindent, but instead use the following (in your .vimrc):\n\nIn your [.vimrc:][1] file:\n\nThe help files take a bit of time to get used to, but the more you read, the better Vim gets:\n\nEven better, you can embed these settings in your source for portability:\n\nTo see your current settings:\n\nAs graywh points out in the comments, smartindent has been replaced by cindent which \"Works more cleverly\", although still mainly for languages with C-like syntax:"
  },
  {
    "question": "Redefine tab as 4 spaces My current setting assumes 8 spaces; how could I redefine it?",
    "answer": "set tabstop=4\nset softtabstop=0 noexpandtab\nset shiftwidth=4\nset shiftwidth=4 smarttab\nset expandtab\nset tabstop=8 softtabstop=0\n\nIt depends on what you mean.\n\nDo you want tab characters in your file to appear 4 character cells wide?\nOr do you want the Tab key to generate an indent consisting of 4 space characters?\n\nDepending on which behavior you need, one of the following sets of settings should work:\nIf you want tab characters in your file to appear 4 character cells wide:\n\nIf your code requires use of actual tab characters these settings prevent unintentional insertion of spaces (these are the defaults, but you may want to set them defensively):\n\nIf you also want to use tab characters for indentation, you should also set shiftwidth to be the same as tabstop:\n\nTo make any of these settings permanent add them to your vimrc file.\nIf you want pressing the Tab key to indent with 4 space characters:\nFirst, tell vim to use 4-space indents, and to intelligently use the Tab key for indentation instead of for inserting tab characters (when at the beginning of a line):\n\nIf you'd also like vim to only use space characters, never tab characters:\n\nFinally, I also recommend setting tab stops to be different from the indentation width, in order to reduce the chance of tab characters masquerading as proper indents:\n\nTo make any of these settings permanent add them to your vimrc.\nMore Details\nIn case you need to make adjustments, or would simply like to understand what these options all mean, here's a breakdown of what each option means, along with links their the full documentation.\n\ntabstop\nThe width of a hard tabstop measured in \"spaces\" \u2014 effectively the (maximum) width of an actual tab character.\nshiftwidth\nThe size of an \"indent\". It's measured in spaces, so if your codebase indents with tab characters then you want shiftwidth to equal tabstop. This is also used by commands like =, > and <.\nexpandtab\nEnabling this will make the Tab key (in insert mode) insert spaces instead of tab characters. This also affects the behavior of the :retab command.\nIf you have this enabled you can enter a literal tab character by typing ctrlV followed by Tab. (mnemonic \"verbatim tab\")\nsmarttab\nEnabling this will cause the Tab key (in insert mode) go to the next indent (as set by 'shiftwidth') instead of the next tab stop (or soft tabstop), but only when the cursor is at the beginning of a line (i.e. the only preceding characters are whitespace). Spaces and/or tabs will be used depending on the values of other options.\nsofttabstop\nSetting this to a non-zero value other than tabstop will make the tab key (in insert mode) insert a combination of spaces (and possibly tabs) to simulate tab stops at this width. Codebases where setting this to something other than 0 is desired are extremely rare.\n\nFor further details on any of these you can also use :help 'optionname' in vim (e.g. :help 'tabstop')."
  },
  {
    "question": "What is the &lt;leader&gt; in a .vimrc file? I see <leader> in many .vimrc files, and I am wondering what does it mean? \nWhat is it used for? \nJust a general overview of the purpose and usage would be great.",
    "answer": "To define a mapping which uses the \"mapleader\" variable, the special string\n\"<Leader>\" can be used.  It is replaced with the string value of \"mapleader\".\nIf \"mapleader\" is not set or empty, a backslash is used instead.\nExample:\n:map <Leader>A  oanother line <Esc>\nWorks like:\n:map \\A  oanother line <Esc>\nBut after:\n:let mapleader = \",\"\nIt works like:\n:map ,A  oanother line <Esc>\n\nNote that the value of \"mapleader\" is used at the moment the mapping is\ndefined.  Changing \"mapleader\" after that has no effect for already defined\nmappings.\n\nThe <Leader> key is mapped to \\ by default.  So if you have a map of <Leader>t, you can execute it by default with \\+t.  For more detail or re-assigning it using the mapleader variable, see\n:help leader"
  },
  {
    "question": "How to effectively work with multiple files in Vim I've started using Vim to develop Perl scripts and am starting to find it very powerful. \nOne thing I like is to be able to open multiple files at once with:\n\n```\nvi main.pl maintenance.pl\n\n```\n\nand then hop between them with:\n\n```\n:n\n:prev\n\n```\n\nand see which file are open with:\n\n```\n:args\n\n```\n\nAnd to add a file, I can say: \n\n```\n:n test.pl\n\n```\n\nwhich I expect would then be added to my list of files, but instead it wipes out my current file list and when I type :args I only have test.pl open.\nSo how can I add and remove files in my args list?",
    "answer": "Why not use tabs (introduced in Vim 7)?\nYou can switch between tabs with :tabn and :tabp,\nWith :tabe <filepath> you can add a new tab; and with a regular :q or :wq you close a tab.\nIf you map :tabn and :tabp to your F7/F8 keys you can easily switch between files.\nIf there are not that many files or you don't have Vim 7 you can also split your screen in multiple files: :sp <filepath>. Then you can switch between splitscreens with Ctrl+W and then an arrow key in the direction you want to move (or instead of arrow keys, w for next and W for previous splitscreen)"
  },
  {
    "question": "How to copy to clipboard in Vim? Is it possible to copy to clipboard directly from Vim? yy only copies stuff to Vim's internal buffer. I want to copy to the OS's clipboard. Is there any such command in Vim or you can only yank stuff within Vim?",
    "answer": "The * register will do this. In Windows, + and * are equivalent. In unix there is a subtle difference between + and *:\n\nUnder Windows, the * and + registers\nare equivalent. For X11 systems,\nthough, they differ. For X11 systems,\n* is the selection, and + is the cut buffer (like clipboard).\nhttp://vim.wikia.com/wiki/Accessing_the_system_clipboard\n\n* is probably what you want most of the time, so I use * because it functions as I expect it to in both environments.\nIn Linux distros you have to install vim-gtk (aka gvim) first to gain clipboard functionality. This is because non-gtk vim is typically compiled without X11 support. This is to allow it to run on console only machines (often servers).\nAnd for those confused about how to use registers when yanking or putting, you merely write \" then the name of the register. So for copying something to the clipboard register you type \"*y and then to put you type \"*p (credit: Kyle Mathews)"
  },
  {
    "question": "How can I make Vim paste from (and copy to) the system&#39;s clipboard? Unlike other editors, Vim stores copied text in its own clipboard. So, it's very hard for me to copy some text from a webpage and paste it into the current working file. It so happens I have to either open gedit or type it manually.\nCan I make Vim paste from (and copy to) the system's clipboard?",
    "answer": "TL;DR\nTry using \"*yy or \"+yy to copy a line to your system's clipboard.\nFull answer\nBe aware that copying/pasting from the system clipboard will not work if :echo has('clipboard') returns 0. In this case, Vim is not compiled with the +clipboard feature and you'll have to install a different version or recompile it. Some Linux distros supply a minimal Vim installation by default, but if you install the vim-gtk or vim-gtk3 package you can get the extra features nonetheless.\nThe \"* and \"+ registers are for the system's clipboard (:help registers). Depending on your system, they may do different things. For instance, on systems that don't use X11 like OS X or Windows, the \"* register is used to read from and write to the system clipboard. On X11 systems, both registers can be used. See :help x11-selection for more details, but basically the \"* is analogous to X11's _PRIMARY_ selection (which usually copies things you select with the mouse and pastes with the middle mouse button) and \"+ is analogous to X11's _CLIPBOARD_ selection (which is the clipboard proper).\nIf all that went over your head, try using \"*yy or \"+yy to copy a line to your system's clipboard. Assuming you have the appropriate compile options, one or the other should work.\nYou might like to remap this to something more convenient for you. For example, you could put vnoremap <C-c> \"*y in your ~/.vimrc so that you can visually select and press Ctrl+c to yank to your system's clipboard.\nYou also may want to have a look at the 'clipboard' option described in :help cb. In this case, you can :set clipboard=unnamed or :set clipboard=unnamedplus to make all yanking/deleting operations automatically copy to the system clipboard. This could be an inconvenience in some cases where you are storing something else in the clipboard as it will overwrite it.\nTo paste you can use \"+p or \"*p (again, depending on your system and/or desired selection) or you can map these to something else. I type them explicitly, but I often find myself in insert mode. If you're in insert mode you can still paste them with proper indentation by using <C-r><C-p>* or <C-r><C-p>+. See :help i_CTRL-R_CTRL-P.\nVim's paste option (:help paste) is also worth mentioning: This puts Vim into a special \"paste mode\" that disables several other options, allowing you to easily paste into Vim using your terminal emulator's or multiplexer's familiar paste shortcut. (Simply type :set paste to enable it, paste your content and then type :set nopaste to disable it.) Alternatively, you can use the pastetoggle option to set a keycode that toggles the mode (:help pastetoggle).\nI recommend using registers instead of these options, but if they are still too scary, this can be a convenient workaround while you're perfecting your Vim chops.\nSee :help clipboard for more detailed information."
  },
  {
    "question": "How do I use vim registers? I only know of one instance using registers is via CtrlR* whereby I paste text from a clipboard. \nWhat are other uses of registers? How to use them? \nEverything you know about VI registers (let's focus on vi 7.2) -- share with us.",
    "answer": "\"kyy\n\"Kyy\n\"kp\n\"+p\n\"*p\n:reg\n\nRegisters in Vim let you run actions or commands on text stored within them.  To access a register, you type \"a before a command, where a is the name of a register.  If you want to copy the current line into register k, you can type\n\nOr you can append to a register by using a capital letter\n\nYou can then move through the document and paste it elsewhere using\n\nTo paste from system clipboard on Linux\n\nTo paste from system clipboard on Windows (or from \"mouse highlight\" clipboard on Linux)\n\nTo access all currently defined registers type"
  },
  {
    "question": "What is your most productive shortcut with Vim? I've heard a lot about Vim, both pros and cons.\nIt really seems you should be (as a developer) faster with Vim than with any other editor.\nI'm using Vim to do some basic stuff and I'm at best 10 times less productive with Vim.\nThe only two things you should care about when you talk about speed (you may not care enough about them, but you should) are:\n\nUsing alternatively left and right\nhands is the fastest way to use the\nkeyboard. \nNever touching the mouse is the\nsecond way to be as fast as possible.\nIt takes ages for you to move your hand,\ngrab the mouse, move it, and bring it\nback to the keyboard (and you often have\nto look at the keyboard to be sure you\nreturned your hand properly to the right place)\n\nHere are two examples demonstrating why I'm far less productive with Vim.\nCopy/Cut & paste. I do it all the time. With all the contemporary editors you press Shift  with the left hand, and you move the cursor with your right hand to select text. Then Ctrl+C copies, you move the cursor and Ctrl+V pastes.\nWith Vim it's horrible:\n\nyy to copy one line (you almost never want the whole line!)\n[number xx]yy to copy xx lines into the buffer. But you never know exactly if you've selected what you wanted. I often have to do [number xx]dd then u to undo!\n\nAnother example? Search & replace.\n\nIn PSPad: Ctrl+f then type what you want you search for, then press Enter.\nIn Vim: /, then type what you want to search for, then if there are some special characters put \\ before each special character, then press Enter.\n\nAnd everything with Vim is like that: it seems I don't know how to handle it the right way.\nNB : I've already read the Vim cheat sheet :)\nMy question is:\nWhat is the way you use Vim that makes you more productive than with a contemporary editor?",
    "answer": "Your problem with Vim is that  you don't grok vi.\nYou mention cutting with yy and complain that you almost never want to cut whole lines.  In fact programmers, editing source code, very often want to work on whole lines, ranges of lines and blocks of code.  However, yy is only one of many way to yank text into the anonymous copy buffer (or \"register\" as it's called in vi).\nThe \"Zen\" of vi is that you're speaking a language.  The initial y is a verb.  The statement yy is a synonym for y_. The y is doubled up to make it easier to type, since it is such a common operation.\nThis can also be expressed as dd P (delete the current line and paste a copy back into place; leaving a copy in the anonymous register as a side effect).  The y and d \"verbs\" take any movement as their \"subject.\"  Thus yW is \"yank from here (the cursor) to the end of the current/next (big) word\" and y'a is \"yank from here to the line containing the mark named 'a'.\"\nIf you only understand basic up, down, left, and right cursor movements then vi will be no more productive than a copy of \"notepad\" for you.  (Okay, you'll still have syntax highlighting and the ability to handle files larger than a piddling ~45KB or so; but work with me here).\nvi has 26 \"marks\" and 26 \"registers.\"  A mark is set to any cursor location using the m command.  Each mark is designated by a single lower case letter.  Thus ma sets the 'a' mark to the current location, and mz sets the 'z' mark.  You can move to the line containing a mark using the ' (single quote) command.  Thus 'a moves to the beginning of the line containing the 'a' mark.  You can move to the precise location of any mark using the ` (backquote) command.  Thus  `z will move directly to the exact location of the 'z' mark.\nBecause these are \"movements\" they can also be used as subjects for other \"statements.\"\nSo, one way to cut an arbitrary selection of text would be to drop a mark (I usually use 'a' as my \"first\" mark, 'z' as my next mark, 'b' as another, and 'e' as yet another (I don't recall ever having interactively used more than four marks in 15 years of using vi; one creates one's own conventions regarding how marks and registers are used by macros that don't disturb one's interactive context).  Then we go to the other end of our desired text; we can start at either end, it doesn't matter.  Then we can simply use d`a to cut or y`a to copy.  Thus the whole process has a 5 keystrokes overhead (six if we started in \"insert\" mode and needed to Esc out command mode).  Once we've cut or copied then pasting in a copy is a single keystroke: p.\nI say that this is one way to cut or copy text.  However, it is only one of many.  Frequently we can more succinctly describe the range of text without moving our cursor around and dropping a mark.  For example if I'm in a paragraph of text I can use { and } movements to the beginning or end of the paragraph respectively.  So, to move a paragraph of text I cut it using { d} (3 keystrokes).  (If I happen to already be on the first or last line of the paragraph I can then simply use d} or d{ respectively.\nThe notion of \"paragraph\" defaults to something which is usually intuitively reasonable.  Thus it often works for code as well as prose.\nFrequently we know some pattern (regular expression) that marks one end or the other of the text in which we're interested.  Searching forwards or backwards are movements in vi.  Thus they can also be used as \"subjects\" in our \"statements.\"  So I can use d/foo to cut from the current line to the next line containing the string \"foo\" and y?bar to copy from the current line to the most recent (previous) line containing \"bar.\"  If I don't want whole lines I can still use the search movements (as statements of their own), drop my mark(s) and use the  `x commands as described previously.\nIn addition to \"verbs\" and \"subjects\" vi also has \"objects\" (in the grammatical sense of the term).  So far I've only described the use of the anonymous register.  However, I can use any of the 26 \"named\" registers by prefixing the \"object\" reference with \" (the double quote modifier).  Thus if I use \"add I'm cutting the current line into the 'a' register and if I use \"by/foo then I'm yanking a copy of the text from here to the next line containing \"foo\" into the 'b' register.  To paste from a register I simply prefix the paste with the same modifier sequence: \"ap pastes a copy of the 'a' register's contents into the text after the cursor and \"bP pastes a copy from 'b' to before the current line.\nThis notion of \"prefixes\" also adds the analogs of grammatical \"adjectives\" and \"adverbs'  to our text manipulation \"language.\"  Most commands (verbs) and movement (verbs or objects, depending on context) can also take numeric prefixes. Thus 3J means \"join the next three lines\" and d5} means \"delete from the current line through the end of the fifth paragraph down from here.\"\nThis is all intermediate level vi.  None of it is Vim specific and there are far more advanced tricks in vi if you're ready to learn them.  If you were to master just these intermediate concepts then you'd probably find that you rarely need to write any macros because the text manipulation language is sufficiently concise and expressive to do most things easily enough using the editor's \"native\" language.\n\nA sampling of more advanced tricks:\nThere are a number of : commands, most notably the :% s/foo/bar/g global substitution technique.  (That's not advanced but other : commands can be).  The whole : set of commands was historically inherited by vi's previous incarnations as the ed (line editor) and later the ex (extended line editor) utilities.  In fact vi is so named because it's the visual interface to ex.\n: commands normally operate over lines of text.  ed and ex were written in an era when terminal screens were uncommon and many terminals were \"teletype\" (TTY) devices.  So it was common to work from printed copies of the text, using commands through an extremely terse interface (common connection speeds were 110 baud, or, roughly, 11 characters per second -- which is slower than a fast typist; lags were common on multi-user interactive sessions; additionally there was often some motivation to conserve paper).\nSo the syntax of most : commands includes an address or range of addresses (line number) followed by a command.  Naturally one could use literal line numbers: :127,215 s/foo/bar to change the first occurrence of \"foo\" into \"bar\" on each line between 127 and 215.  One could also use some abbreviations such as . or $ for current and last lines respectively.  One could also use relative prefixes + and - to refer to offsets after or before the curent line, respectively.  Thus: :.,$j meaning \"from the current line to the last line, join them all into one line\".  :% is synonymous with :1,$ (all the lines).\nThe :... g and :... v commands bear some explanation as they are incredibly powerful.  :... g is a prefix for \"globally\" applying a subsequent command to all lines which match a pattern (regular expression) while :... v applies such a command to all lines which do NOT match the given pattern (\"v\" from \"conVerse\").  As with other ex commands these can be prefixed by addressing/range references.  Thus :.,+21g/foo/d means \"delete any lines containing the string \"foo\" from the current one through the next 21 lines\" while :.,$v/bar/d means \"from here to the end of the file, delete any lines which DON'T contain the string \"bar.\"\nIt's interesting that the common Unix command grep was actually inspired by this ex command (and is named after the way in which it was documented).  The ex command :g/re/p (grep) was the way they documented how to \"globally\" \"print\" lines containing a \"regular expression\" (re).  When ed and ex were used, the :p command was one of the first that anyone learned and often the first one used when editing any file.  It was how you printed the current contents (usually just one page full at a time using :.,+25p or some such).\nNote that :% g/.../d or (its reVerse/conVerse counterpart: :% v/.../d are the most common usage patterns.  However there are  couple of other ex commands which are worth remembering:\nWe can use m to move lines around, and j to join lines.  For example if you have a list and you want to separate all the stuff matching (or conversely NOT matching some pattern) without deleting them, then you can use something like: :% g/foo/m$ ... and all the \"foo\" lines will have been moved to the end of the file.  (Note the other tip about using the end of your file as a scratch space).  This will have preserved the relative order of all the \"foo\" lines while having extracted them from the rest of the list.  (This would be equivalent to doing something like: 1G!GGmap!Ggrep foo<ENTER>1G:1,'a g/foo'/d (copy the file to its own tail, filter the tail through grep, and delete all the stuff from the head).\nTo join lines usually I can find a pattern for all the lines which need to be joined to their predecessor (all the lines which start with \"^   \" rather than \"^ * \" in some bullet list, for example).  For that case I'd use: :% g/^   /-1j (for every matching line, go up one line and join them). (BTW: for bullet lists trying to search for the bullet lines and join to the next doesn't work for a couple reasons ... it can join one bullet line to another, and it won't join any bullet line to all of its continuations; it'll only work pairwise on the matches).\nAlmost needless to mention you can use our old friend s (substitute) with the g and v (global/converse-global) commands.  Usually you don't need to do so.  However, consider some case where you want to perform a substitution only on lines matching some other pattern.  Often you can use a complicated pattern with captures and use back references to preserve the portions of the lines that you DON'T want to change.  However, it will often be easier to separate the match from the substitution: :% g/foo/s/bar/zzz/g -- for every line containing \"foo\" substitute all \"bar\" with \"zzz.\"  (Something like :% s/\\(.*foo.*\\)bar\\(.*\\)/\\1zzz\\2/g would only work for the cases those instances of \"bar\" which were PRECEDED by \"foo\" on the same line; it's ungainly enough already, and would have to be mangled further to catch all the cases where \"bar\" preceded \"foo\")\nThe point is that there are more than just p, s, and d lines in the ex command set.\nThe : addresses can also refer to marks.  Thus you can use: :'a,'bg/foo/j to join any line containing the string foo to its subsequent line, if it lies between the lines between the 'a' and 'b' marks.  (Yes, all of the preceding ex command examples can be limited to subsets of the file's lines by prefixing with these sorts of addressing expressions).\nThat's pretty obscure (I've only used something like that a few times in the last 15 years). However, I'll freely admit that I've often done things iteratively and interactively that could probably have been done more efficiently if I'd taken the time to think out the correct incantation.\nAnother very useful vi or ex command is :r to read in the contents of another file.  Thus: :r foo inserts the contents of the file named \"foo\" at the current line.\nMore powerful is the :r! command.  This reads the results of a command.  It's the same as suspending the vi session, running a command, redirecting its output to a temporary file, resuming your vi session, and reading in the contents from the temp. file.\nEven more powerful are the ! (bang) and :... ! (ex bang) commands.  These also execute external commands and read the results into the current text. However, they also filter selections of our text through the command!  This we can sort all the lines in our file using 1G!Gsort (G is the vi \"goto\" command; it defaults to going to the last line of the file, but can be prefixed by a line number, such as 1, the first line).  This is equivalent to the ex variant :1,$!sort.  Writers often use ! with the Unix fmt or fold utilities for reformating or \"word wrapping\" selections of text.  A very common macro is {!}fmt (reformat the current paragraph).  Programmers sometimes use it to run their code, or just portions of it, through indent or other code reformatting tools.\nUsing the :r! and ! commands means that any external utility or filter can be treated as an extension of our editor.  I have occasionally used these with scripts that pulled data from a database, or with wget or lynx commands that pulled data off a website, or ssh commands that pulled data from remote systems.\nAnother useful ex command is :so (short for :source).  This reads the contents of a file as a series of commands.  When you start vi it normally, implicitly, performs a :source on ~/.exinitrc file (and Vim usually does this on ~/.vimrc, naturally enough).  The use of this is that you can change your editor profile on the fly by simply sourcing in a new set of macros, abbreviations, and editor settings.  If you're sneaky you can even use this as a trick for storing sequences of ex editing commands to apply to files on demand.\nFor example I have a seven line file (36 characters) which runs a file through wc, and inserts a C-style comment at the top of the file containing that word count data.  I can apply that \"macro\" to a file by using a command like: vim +'so mymacro.ex' ./mytarget\n(The + command line option to vi and Vim is normally used to start the editing session at a given line number.  However it's a little known fact that one can follow the + by any valid ex command/expression, such as a \"source\" command as I've done here; for a simple example I have scripts which invoke: vi +'/foo/d|wq!' ~/.ssh/known_hosts to remove an entry from my SSH known hosts file non-interactively while I'm re-imaging a set of servers).\nUsually it's far easier to write such \"macros\" using Perl, AWK, sed (which is, in fact, like grep a utility inspired by the ed command).\nThe @ command is probably the most obscure vi command.  In occasionally teaching advanced systems administration courses for close to a decade I've met very few people who've ever used it.  @ executes the contents of a register as if it were a vi or ex command.\nExample: I often use: :r!locate ... to find some file on my system and read its name into my document.  From there I delete any extraneous hits, leaving only the full path to the file I'm interested in.  Rather than laboriously Tab-ing through each component of the path (or worse, if I happen to be stuck on a machine without Tab completion support in its copy of vi) I just use:\n\n0i:r (to turn the current line into a valid :r command),\n\"cdd (to delete the line into the \"c\" register) and\n@c execute that command.\n\nThat's only 10 keystrokes (and the expression \"cdd @c is effectively a finger macro for me, so I can type it almost as quickly as any common six letter word).\n\nA sobering thought\nI've only scratched to surface of vi's power and none of what I've described here is even part of the \"improvements\" for which vim is named!  All of what I've described here should work on any old copy of vi from 20 or 30 years ago.\nThere are people who have used considerably more of vi's power than I ever will."
  },
  {
    "question": "What is Vim recording and how can it be disabled? I keep seeing the recording message at the bottom of my gVim 7.2 window.\nWhat is it and how do I turn it off?",
    "answer": "You start recording by q<letter>\nand you can end it by typing q again.\nRecording is a really useful feature of Vim.\nIt records everything you type. You can then replay it simply by typing @<letter>.\nRecord search, movement, replacement...\nOne of the best feature of Vim IMHO."
  },
  {
    "question": "How to move screen without moving cursor in Vim? I recently discovered Ctrl+E and Ctrl+Y shortcuts for Vim that respectively move the screen up and down with a one line step, without moving the cursor.\nDo you know any command that leaves the cursor where it is but moves the screen so that the line which has the cursor becomes the first line? (having a command for the last line would be a nice bonus).\nI can achieve this by manually pressing Ctrl+E (or Ctrl+Y) the proper number of times, but having a command that somehow does this directly would be nice.\nAny ideas?",
    "answer": "zz - move current line to the middle\nof the screen (warning: ZZ is save and exit, so watch out for caps lock)\nzt - move current line\nto the top of the screen\nzb - move\ncurrent line to the bottom of the\nscreen"
  },
  {
    "question": "Convert DOS/Windows line endings to Linux line endings in Vim If I open files I created in Windows, the lines all end with ^M.\nHow do I delete these characters all at once?",
    "answer": "dos2unix is a commandline utility that will do this.\nIn Vim, :%s/^M//g will if you use Ctrl-v Ctrl-m to input the ^M (On Windows, use Ctrl-q Ctrl-m instead).\nVim may not show the ^M characters if the file is loaded with a dos (Windows) file format. In this case you can :set ff=unix and then save the file with :w or :w! and Vim will reformat the file as it saves it for you.\nThere is documentation on the fileformat setting, and the Vim wiki has a comprehensive page on line ending conversions.\nAlternately, if you move files back and forth a lot, you might not want to convert them, but rather to do :set ff=dos, so Vim will know it's a DOS file and use DOS conventions for line endings."
  },
  {
    "question": "Copy all the lines to clipboard Is there any way to copy all lines from open file to clipboard in VI editor. I tried yG but it's not using clipboard to store those lines. \nSo is it possible?",
    "answer": "You should yank the text to the * or + registers:\ngg\"*yG\nExplanation:\n\ngg to get the cursor to the first character of the file\n\"*y to start a yank command to the register * from the first line, until...\nG to go the end of the file"
  },
  {
    "question": "Vim delete blank lines What command can I run to remove blank lines in Vim?",
    "answer": ":g/^$/d\n\n:g will execute a command on lines which match a regex. The regex is 'blank line' and the command is :d (delete)"
  },
  {
    "question": "Differences between Emacs and Vim What practical, objective differences are there between Emacs and Vim? For example, what can be done using one but not the other (or done more easily with one than the other)? What should I know in order to choose one to learn?",
    "answer": "(the text below is my opinion, it should not be taken as fact or an insult)\nWith Emacs you are expected to have it open 24/7 and live inside the program, almost everything you do can be done from there. You write your own extensions, use it for note-taking, organization, games, programming, shell access, file access, listening to music, web browsing. It takes weeks and weeks till you will be happy with it and then you will learn new stuff all the time. You will be annoyed when you don't have access to it and constantly change your config. You won't be able to use other peoples emacs versions easily and it won't just be installed. It uses Lisp, which is great. You can make it into anything you want it to be. (anything, at all)\nWith Vim, it's almost always pre-installed. It's fast. You open up a file do a quick edit and then quit. You can work with the basic setup if you are on someone else's machine. It's not quite so editable, but it's still far better than most text editors. It recognizes that most of the time you are reading/editing not typing and makes that portion faster. You don't suffer from emacs pinkie. It's not so infuriating. It's easier to learn.\nEven though I use Emacs all day every day (and love it) unless you intend to spend a lot of time in the program you choose I would pick vim"
  },
  {
    "question": "How do I do redo (i.e. &quot;undo undo&quot;) in Vim? In Vim, I did too much undo. How do I undo this (that is, redo)?",
    "answer": "Ctrl+r"
  },
  {
    "question": "How do I fix the indentation of an entire file in Vi? In Vim, what is the command to correct the indentation of all the lines?\nOften times I'll copy and paste code into a remote terminal and have the whole thing messed up.  I want to fix this in one fell swoop.",
    "answer": "=, the indent command can take motions. So, gg to get the start of the file, = to indent, G to the end of the file, gg=G."
  },
  {
    "question": "How to paste yanked text into the Vim command line I'd like to paste yanked text into Vim's command line. Is it possible?",
    "answer": "Yes. Hit Ctrl-R then \". If you have literal control characters in what you have yanked, use Ctrl-R, Ctrl-O, \".\nHere is an explanation of what you can do with registers. What you can do with registers is extraordinary, and once you know how to use them you cannot live without them.\nRegisters are basically storage locations for strings. Vim has many registers that work in different ways:\n\n0 (yank register: when you use y in normal mode, without specifying a register, yanked text goes there and also to the default register),\n1 to 9 (shifting delete registers, when you use commands such as c or d, what has been deleted goes to register 1, what was in register 1 goes to register 2, etc.),\n\" (default register, also known as unnamed register. This is where the \" comes in Ctrl-R, \"),\na to z for your own use (capitalized A to Z are for appending to corresponding registers).\n_ (acts like /dev/null (Unix) or NUL (Windows), you can write to it but it's discarded and when you read from it, it is always empty),\n- (small delete register),\n/ (search pattern register, updated when you look for text with /, ?, * or # for instance; you can also write to it to dynamically change the search pattern),\n: (stores last VimL typed command via Q or :, readonly),\n+ and * (system clipboard registers, you can write to them to set the clipboard and read the clipboard contents from them)\n\nSee :help registers for the full reference.\nYou can, at any moment, use :registers to display the contents of all registers. Synonyms and shorthands for this command are :display, :reg and :di.\nIn Insert or Command-line mode, Ctrl-R plus a register name, inserts the contents of this register. If you want to insert them literally (no auto-indenting, no conversion of control characters like 0x08 to backspace, etc), you can use Ctrl-R, Ctrl-O, register name.\nSee :help i_CTRL-R and following paragraphs for more reference.\nBut you can also do the following (and I probably forgot many uses for registers).\n\nIn normal mode, hit \":p. The last command you used in vim is pasted into your buffer.\nLet's decompose: \" is a Normal mode command that lets you select what register is to be used during the next yank, delete or paste operation. So \": selects the colon register (storing last command). Then p is a command you already know, it pastes the contents of the register.\ncf. :help \", :help quote_:\n\nYou're editing a VimL file (for instance your .vimrc) and would like to execute a couple of consecutive lines right now: yj:@\"Enter.\nHere, yj yanks current and next line (this is because j is a linewise motion but this is out of scope of this answer) into the default register (also known as the unnamed register). Then the :@ Ex command plays Ex commands stored in the register given as argument, and \" is how you refer to the unnamed register. Also see the top of this answer, which is related.\nDo not confuse \" used here (which is a register name) with the \" from the previous example, which was a Normal-mode command.\ncf. :help :@ and :help quote_quote\n\nInsert the last search pattern into your file in Insert mode, or into the command line, with Ctrl-R, /.\ncf. :help quote_/, help i_CTRL-R\nCorollary: Keep your search pattern but add an alternative: / Ctrl-R, / \\|alternative.\n\nYou've selected two words in the middle of a line in visual mode, yanked them with y, they are in the unnamed register. Now you want to open a new line just below where you are, with those two words: :pu. This is shorthand for :put \".\u00a0The :put command, like many Ex commands, works only linewise.\ncf. :help :put\nYou could also have done: :call setreg('\"', @\", 'V') then p. The setreg function sets the register of which the name is given as first argument (as a string), initializes it with the contents of the second argument (and you can use registers as variables with the name @x where x is the register name in VimL), and turns it into the mode specified in the third argument, V for linewise, nothing for characterwise and literal ^V for blockwise.\ncf. :help setreg(). The reverse functions are getreg() and getregtype().\n\nIf you have recorded a macro with qa...q, then :echo @a will tell you what you have typed, and @a will replay the macro (probably you knew that one, very useful in order to avoid repetitive tasks)\ncf. :help q, help @\nCorollary from the previous example: If you have 8go in the clipboard, then @+ will play the clipboard contents as a macro, and thus go to the 8th byte of your file. Actually this will work with almost every register. If your last inserted string was dd in Insert mode, then @. will (because the . register contains the last inserted string) delete a line. (Vim documentation is wrong in this regard, since it states that the registers #, %, : and . will only work with p, P, :put and Ctrl-R).\ncf. :help @\nDon't confuse :@ (command that plays Vim commands from a register) and @ (normal-mode command that plays normal-mode commands from a register).\nNotable exception is @:. The command register does not contain the initial colon neither does it contain the final carriage return. However in Normal mode, @: will do what you expect, interpreting the register as an Ex command, not trying to play it in Normal mode. So if your last command was :e, the register contains e but @: will reload the file, not go to end of word.\ncf. :help @:\n\nShow what you will be doing in Normal mode before running it: @='dd' Enter. As soon as you hit the = key, Vim switches to expression evaluation: as you enter an expression and hit Enter, Vim computes it, and the result acts as a register content. Of course the register = is read-only, and one-shot. Each time you start using it, you will have to enter a new expression.\ncf. :help quote_=\nCorollary: If you are editing a command, and you realize that you should need to insert into your command line some line from your current buffer: don't press Esc! Use Ctrl-R =getline(58) Enter. After that you will be back to command line editing, but it has inserted the contents of the 58th line.\n\nDefine a search pattern manually: :let @/ = 'foo'\ncf. :help :let\nNote that doing that, you needn't to escape / in the pattern. However you need to double all single quotes of course.\n\nCopy all lines beginning with foo, and afterwards all lines containing bar to clipboard, chain these commands: qaq (resets the a register storing an empty macro inside it), :g/^foo/y A, :g/bar/y A, :let @+ = @a.\nUsing a capital register name makes the register work in append mode\nBetter, if Q has not been remapped by mswin.vim, start Ex mode with Q, chain those \u201ccolon commands\u201d which are actually better called \u201cEx commands\u201d, and go back to Normal mode by typing visual.\ncf. :help :g, :help :y, :help Q\n\nDouble-space your file: :g/^/put _.\u00a0This puts the contents of the black hole register (empty when reading, but writable, behaving like /dev/null) linewise, after each line (because every line has a beginning!).\n\nAdd a line containing foo before each line: :g/^/-put ='foo'. This is a clever use of the expression register. Here, - is a synonym for .-1 (cf. :help :range). Since :put puts the text after the line, you have to explicitly tell it to act on the previous one.\n\nCopy the entire buffer to the system clipboard: :%y+.\ncf. :help :range (for the % part) and :help :y.\n\nIf you have misrecorded a macro, you can type :let @a=' Ctrl-R =replace(@a,\"'\",\"''\",'g') Enter ' and edit it. This will modify the contents of the macro stored in register a, and it's shown here how you can use the expression register to do that. Another, simpler, way of modifying a macro is to paste it in a buffer (\"ap), edit it, and put it again into the register, by selecting it and \"ay.\n\nIf you did dddd, you might do uu in order to undo. With p you could get the last deleted line. But actually you can also recover up to 9 deletes with the registers @1 through @9.\nEven better, if you do \"1P, then . in Normal mode will play \"2P, and so on.\ncf. :help . and :help quote_number\n\nIf you want to insert the current date in Insert mode: Ctrl-R=strftime('%y%m%d')Enter.\ncf. :help strftime()\n\nOnce again, what can be confusing:\n\n:@ is a command-line command that interprets the contents of a register as vimscript and sources it\n\n@  in normal mode command that interprets the contents of a register as normal-mode keystrokes (except when you use : register, that contains last played command without the initial colon: in this case it replays the command as if you also re-typed the colon and the final return key).\n\n\"  in normal mode command that helps you select a register for yank, paste, delete, correct, etc.\n\n\"  is also a valid register name (the default, or unnamed, register) and therefore can be passed as an arguments for commands that expect register names"
  },
  {
    "question": "To switch from vertical split to horizontal split fast in Vim How can you switch your current windows from horizontal split to vertical split and vice versa in Vim?\nI did that a moment ago by accident but I cannot find the key again.",
    "answer": "Vim mailing list says (re-formatted for better readability):\n\nTo change two vertically split\nwindows to horizonally split\nCtrl-w t Ctrl-w K\nHorizontally to vertically:\nCtrl-w t Ctrl-w H\nExplanations:\nCtrl-w t     makes the first (topleft) window current\nCtrl-w K     moves the current window to full-width at the very top\nCtrl-w H     moves the current window to full-height at far left\n\nNote that the t is lowercase, and the K and H are uppercase.\nAlso, with only two windows, it seems like you can drop the Ctrl-w t part because if you're already in one of only two windows, what's the point of making it current?"
  },
  {
    "question": "&quot;Find next&quot; in Vim To search forward in Vim for cake, I'd type /cake, but the cursor jumps to the first match when I press return. Is there a Vim command analogous to \"find next\"?",
    "answer": "It is n for next and N for previous.\nAnd if you use reverse search with ? (for example, ?cake) instead of /, it is the other way round.\nIf it is installed on your system, you should try to run vimtutor command from your terminal, which will start a tutorial of the basic Vim commands.\nRob Wells advice about * and # is also very pertinent."
  },
  {
    "question": "What does the ^M character mean in Vim? I keep getting the ^M character in my .vimrc and it breaks my\nconfiguration.",
    "answer": ":%s/^M//g\n\nUnix uses 0xA for a newline character. Windows uses a combination of two characters: 0xD 0xA. 0xD is the carriage return character. ^M happens to be the way vim displays 0xD (0x0D = 13, M is the 13th letter in the English alphabet).\nYou can remove all the ^M characters by running the following:\n\nWhere ^M is entered by holding down Ctrl and typing v followed by m, and then releasing Ctrl. This is sometimes abbreviated as ^V^M, but note that you must enter it as described in the previous sentence, rather than typing it out literally.\nThis expression will replace all occurrences of ^M with the empty string (i.e. nothing). I use this to get rid of ^M in files copied from Windows to Unix (Solaris, Linux, OSX)."
  },
  {
    "question": "Changing case in Vim Is there a command in Vim that changes the case of the selected text?",
    "answer": "Visually select the text, then U for uppercase or u for lowercase. To swap all casing in a visual selection, press ~ (tilde).\nWithout using a visual selection, gU<motion> will make the characters in motion uppercase, or use gu<motion> for lowercase.\nFor more of these, see section 3 in Vim's change.txt help file."
  },
  {
    "question": "Find and replace strings in vim on multiple lines I can do :%s/<search_string>/<replace_string>/g for replacing a string across a file, or :s/<search_string>/<replace_string>/ to replace in current line.\nHow can I select and replace words from selective lines in vim?\nExample: replace text from lines 6-10, 14-18 but not from 11-13.",
    "answer": ":6,10s/<search_string>/<replace_string>/g | 14,18&&\n:for range in split('6,10 14,18')| exe range 's/<search_string>/<replace_string>/g' | endfor\n\nThe :&& command repeats the last substitution with the same flags. You can supply the additional range(s) to it (and concatenate as many as you like):\n\nIf you have many ranges though, I'd rather use a loop:"
  },
  {
    "question": "Where is my .vimrc file? I have been using Vim, and I would really like to save my settings. The problem I am having is that I cannot find my .vimrc file, and it is not in the standard /home/user/.vimrc location. How might I find this file?",
    "answer": "You need to create it. In most installations I've used it hasn't been created by default.\nYou usually create it as ~/.vimrc."
  },
  {
    "question": "How do I close a single buffer (out of many) in Vim? I open several files in Vim by, for example, running\n\n```\nvim a/*.php\n\n```\n\nwhich opens 23 files.\nI then make my edit and run the following twice\n\n```\n:q\n\n```\n\nwhich closes all my buffers.\nHow can you close only one buffer in Vim?",
    "answer": "More from manuals:\n:bd\n\nUnload buffer [N] (default: current\nbuffer) and delete it from\nthe buffer list.  If the buffer was changed, this fails,\nunless when [!] is specified, in which case changes are\nlost.\nThe file remains unaffected.\n\nIf you know what you\u2019re doing, you can also use\u00a0:bw\n:bw\n\nLike |:bdelete|, but really delete the\nbuffer.\n\nA word of caution: \u201cthe w\u00a0in\u00a0bw does not stand for write but for wipeout!\u201d"
  },
  {
    "question": "How can I make my match non greedy in vim? I have a big HTML file that has lots of markup that looks like this:\n\n```\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\">\n  <span style=\"font-size: small; font-family: Times New Roman;\">stuff here</span>\n</p>\n\n```\n\nI'm trying to do a Vim search-and-replace to get rid of all class=\"\" and style=\"\" but I'm having trouble making the match ungreedy.\nMy first attempt was this\n\n```\n%s/style=\".*?\"//g\n\n```\n\nbut Vim doesn't seem to like the ?.  Unfortunately removing the ? makes the match too greedy.\nHow can I make my match ungreedy?",
    "answer": "Instead of .* use .\\{-}.\n%s/style=\".\\{-}\"//g\nAlso, see :help non-greedy"
  },
  {
    "question": "In Vim is there a way to delete without putting text in the register? Using Vim I often want to replace a block of code with a block that I just yanked.\nBut when I delete the block of code that is to be replaced, that block itself goes into the register which erases the block I just yanked. So I've got in the habit of yanking, then inserting, then deleting what I didn't want, but with large blocks of code this gets messy trying to keep the inserted block and the block to delete separate.\nSo what is the slickest and quickest way to replace text in Vim?\n\nis there a way to delete text without putting it into the register?\nis there a way to say e.g. \"replace next word\" or \"replace up to next paragraph\"\nor is the best way to somehow use the multi-register feature?",
    "answer": "\"_d\n\nTo delete something without saving it in a register, you can use the \"black hole register\":\n\nOf course you could also use any of the other registers that don't hold anything you are interested in."
  },
  {
    "question": "How to get the list of all installed color schemes in Vim? Is there a way to get a list of all installed color schemes in Vim? That would make very easy to select one without looking at the .vim directory.",
    "answer": "Type\n:colorscheme then Space followed by TAB.\nor as Peter said,\n:colorscheme then Space followed by CTRLd\nThe short version of the command is :colo so you can use it in the two previous commands, instead of using the \"long form\"."
  },
  {
    "question": "Move cursor to end of file in vim When I want the cursor to go to the end of the file (i.e. the end of the last line) in Vim, I have to type six keystrokes:\n<ESC>G$a - which translates to ESC + Shiftg + Shift4 + a on my keyboard layout. \nHow can I do this more efficiently?\nSince I regularly work on many different machines, I cannot always change .vimrc, so I'm looking for answers without having to edit or create that file.",
    "answer": "No need to explicitly go to the end of line before doing a, use A;\nAppend text at the end of line [count] times\n<ESC>GA"
  },
  {
    "question": "Convert ^M (Windows) line breaks to normal line breaks Vim shows ^M on every line ending.\nHow do I replace this with a normal line break in a file opened in Vim?",
    "answer": "This is the only thing that worked for me:\n\n:e ++ff=dos\n\nFound it at: http://vim.wikia.com/wiki/File_format"
  },
  {
    "question": "How to insert text at beginning of a multi-line selection in vi/Vim In Vim, how do I  insert characters at the beginning of each line in a selection?\nFor instance,  I want to comment out a block of code by prepending // at the beginning of each line assuming my language's comment system doesn't allow block commenting like /* */.  How would I do this?",
    "answer": "Press Esc to enter 'command mode'\nUse Ctrl+V to enter visual block mode (try Ctrl+Q, if V doesn't work)\nMove Up/Down to select the columns of text in the lines you want to\ncomment.\nThen hit Shift+i and type the text you want to insert (or type x to delete)\nThen hit Esc, wait 1 second and the inserted text will appear on every line.\n\nFor further information and reading, check out \"Inserting text in multiple lines\" in the Vim Tips Wiki."
  },
  {
    "question": "Vim: insert the same characters across multiple lines Sometimes I want to edit a certain visual block of text across multiple lines.\nFor example, I would take a text that looks like this:\n\n```\nname\ncomment\nphone\nemail\n\n```\n\nAnd make it look like this\n\n```\nvendor_name\nvendor_comment\nvendor_phone\nvendor_email\n\n```\n\nCurrently the way I would do it now is...\n\nSelect all 4 row lines of a block by pressing V and then j four times.\nIndent with >.\nGo back one letter with h.\nGo to block visual mode with Ctrlv.\nSelect down four rows by pressing j four times. At this point you have selected a 4x1 visual blocks of whitespace (four rows and one column).\nPress C. Notice this pretty much indented to the left by one column.\nType out a \" vendor_\" without the quote. Notice the extra space we had to put back.\nPress Esc. This is one of the very few times I use Esc to get out of insert mode. Ctrlc would only edit the first line.\nRepeat step 1.\nIndent the other way with <.\n\nI don't need to indent if there is at least one column of whitespace before the words. I wouldn't need the whitespace if I didn't have to clear the visual block with c.\nBut if I have to clear, then is there a way to do what I performed above without creating the needed whitespace with indentation?\nAlso why does editing multiple lines at once only work by exiting out of insert mode with Esc over Ctrlc?\n\nHere is a more complicated example:\n\n```\nname    = models.CharField( max_length = 135 )\ncomment = models.TextField( blank = True )\nphone   = models.CharField( max_length = 135, blank = True )\nemail   = models.EmailField( blank = True )\n\n```\n\nto\n\n```\nname    = models.whatever.CharField( max_length = 135 )\ncomment = models.whatever.TextField( blank = True )\nphone   = models.whatever.CharField( max_length = 135, blank = True )\nemail   = models.whatever.EmailField( blank = True )\n\n```\n\nIn this example I would perform the vertical visual block over the ., and then reinsert it back during insert mode, i.e., type .whatever.. Hopefully now you can see the drawback to this method. I am limited to only selecting a column of text that are all the same in a vertical position.",
    "answer": "Move the cursor to the n in name.\nEnter visual block mode (Ctrlv).\nPress j three times (or 3j) to jump down by 3 lines; G (capital g) to jump to the last line\nPress I (capital i).\nType in vendor_. Note: It will only update the screen in the first line - until Esc is pressed (6.), at which point all lines will be updated.\nPress Esc.\n\nAn uppercase I must be used rather than a lowercase i, because the lowercase i is interpreted as the start of a text object, which is rather useful on its own, e.g. for selecting inside a tag block (it):"
  },
  {
    "question": "Renaming the current file in Vim How should I rename my current file in Vim?\nFor example:\n\nI am editing person.html_erb_spec.rb\nI would like it renamed to person.haml_spec.rb\nI would like to continue editing person.haml_spec.rb\n\nHow would I go about doing this, elegantly?",
    "answer": "There's a little plugin that lets you do this."
  },
  {
    "question": "How to comment in Vim&#39;s config files: &quot;.vimrc&quot;? How do I add a comment in Vim's configuration files, like .vimrc?",
    "answer": "\"   this is how a comment looks like in ~/.vimrc\n'\"' at the start of a line causes the whole line to be ignored.  '\"'\nafter a command causes the rest of the line to be ignored.  This can be used\nto add comments. ...\n\nPut a double quote to the left of the text you want to comment:\n\n(You don't need a closing quote.)\nThis comment syntax is documented at :help comment, part of docs on Ex command-lines, which begins:\n\n(Note, per :help vimrc, that \"Each line in a vimrc file is executed as an Ex command line.\")"
  },
  {
    "question": "Vim and Ctags tips and tricks I have just installed Ctags (to help with C++ development) with my Vim (or rather gVim), and would like to find out your favorite commands, macros, shortcuts, tips that go along with it...\nShare your best arsenal. What other Vim add-ons you would recommend for C++ on Vim development?\nEDIT What other add-on you would use in conjunction with Ctags?\nEDIT2 What version of gVim you use with tags? Does it make a difference? \nEDIT3 How do you enhance your programming experience for both big and small projects?",
    "answer": "Ctrl+] - go to definition\nCtrl+T - Jump back from the definition.\nCtrl+W Ctrl+] - Open the definition in a horizontal split\nAdd these lines in vimrc\nmap <C-\\> :tab split<CR>:exec(\"tag \".expand(\"<cword>\"))<CR>\nmap <A-]> :vsp <CR>:exec(\"tag \".expand(\"<cword>\"))<CR>\nCtrl+\\ - Open the definition in a new tab\nAlt+] - Open the definition in a vertical split\nAfter the tags are generated. You can use the following keys to tag into and tag out of functions:\nCtrl+Left MouseClick - Go to definition\nCtrl+Right MouseClick - Jump back from definition"
  },
  {
    "question": "Move entire line up and down in Vim In Notepad++, I can use Ctrl + Shift + Up / Down to move the current line up and down. Is there a similar command to this in Vim?  I have looked through endless guides, but have found nothing.\nIf there isn't, how could I bind the action to that key combination?\nEdit: Mykola's answer works for all lines, apart from those at the beginning and end of the buffer. Moving the first line up or the bottom line down deletes the line, and when moving the bottom line up it jumps two spaces initially, like a pawn! Can anyone offer any refinements?",
    "answer": "noremap <c-s-up> :call feedkeys( line('.')==1 ? '' : 'ddkP' )<CR>\nnoremap <c-s-down> ddp\nfunction! s:swap_lines(n1, n2)\nlet line1 = getline(a:n1)\nlet line2 = getline(a:n2)\ncall setline(a:n1, line2)\ncall setline(a:n2, line1)\nendfunction\n\nfunction! s:swap_up()\nlet n = line('.')\nif n == 1\nreturn\nendif\n\ncall s:swap_lines(n, n - 1)\nexec n - 1\nendfunction\n\nfunction! s:swap_down()\nlet n = line('.')\nif n == line('$')\nreturn\nendif\n\ncall s:swap_lines(n, n + 1)\nexec n + 1\nendfunction\n\nnoremap <silent> <c-s-up> :call <SID>swap_up()<CR>\nnoremap <silent> <c-s-down> :call <SID>swap_down()<CR>\n\nPut the following to your .vimrc to do the job\n\nDisappearing of the line looks like a Vim bug. I put a hack to avoid it. Probably there is some more accurate solution.\nUpdate\nThere are a lot of unexplained difficulties with just using Vim combinations. These are line missing and extra line jumping.\nSo here is the scripting solution which can be placed either inside .vimrc or ~/.vim/plugin/swap_lines.vim"
  },
  {
    "question": "Replace tabs with spaces in vim I would like to convert tab to spaces in gVim. I added the following line to my _vimrc:\n\n```\nset tabstop=2\n\n```\n\nIt works to stop at two spaces but it still looks like one tab key is inserted (I tried to use the h key to count spaces afterwards).\nI'm not sure what should I do to make gVim convert tabs to spaces?",
    "answer": "set tabstop=2 shiftwidth=2 expandtab\n:retab\n\nIIRC, something like:\n\nshould do the trick. If you already have tabs, then follow it up with a nice global RE to replace them with double spaces.\nIf you already have tabs you want to replace,"
  },
  {
    "question": "What are the benefits of learning Vim? As a programmer I spend a lot of hours at the keyboard and I've been doing it for the last 12 years, more or less. If there's something I've never gotten used to during all this time, it's these annoying and almost unconscious constant micro-interruptions I experience while coding, due to some of the most common code editing tasks. Things like a simple copy & paste from a different line (or even the same line), or moving 1 or 2 lines up or down from the current position require too much typing or involve the use of the arrow keys ...and it gets worse when I want to move further - I end up using the mouse. Now imagine this same scenario but on a laptop.\nI've always considered learning VIM but the amount of time needed to master it has always made me want to step back.\nI'd like to hear from people who have learnt it and if it ends up being one of those things you cannot live without.\nAt work, I use VS2008, C# and R#, which together make editing code a lot faster and easier than ever, but even so I think I could enjoy not having to use the mouse at all.\nAnd not even the arrow keys.",
    "answer": "I've been using vi and vim also for some 20 years, and I'm still learning new things.\nDavid Rayner's Best of Vim Tips site is an excellent list, though it's probably more useful once you have some familiarity with vim.\nI also want to mention the ViEmu site which has some great info on vi/vim tips and especially the article Why, oh WHY, do those nutheads use vi? (archived version)"
  },
  {
    "question": "Paste in insert mode? Is it possible to paste in insert mode in Vim?",
    "answer": "While in insert mode hit CTRL-R {register}\nExamples:\n\nCTRL-R * will insert in the contents of the clipboard\nCTRL-R \" (the unnamed register) inserts the last delete or yank.\n\nTo find this in vim's help type :h i_ctrl-r"
  },
  {
    "question": "How can I reload .emacs after changing it? How can I get Emacs to reload all my definitions that I have updated in .emacs without restarting Emacs?",
    "answer": "You can use the command load-file (M-x load-file, and then press Return twice to accept the default filename, which is the current file being edited).\nYou can also just move the point to the end of any sexp and press C-x, C-e to execute just that sexp. Usually it's not necessary to reload the whole file if you're just changing a line or two."
  },
  {
    "question": "How to set the font size in Emacs? I also want to save the font size in my .emacs file.",
    "answer": "(set-face-attribute 'default nil :height 100)\n\nThe value is in 1/10pt, so 100 will give you 10pt, etc."
  },
  {
    "question": "Why should I use an IDE? In another question, Mark speaks highly of IDEs, saying \"some people still just dont know \"why\" they should use one...\".  As someone who uses vim for programming, and works in an environment where most/all of my colleagues use either vim or emacs for all of their work, what are the advantages of IDEs?  Why should I use one?\nI'm sure this is a charged issue for some people, and I'm not interested in starting a flame war, so please only reply with the reasons you believe an IDE-based approach is superior.  I'm not interested in hearing about why I shouldn't use an IDE; I already don't use one.  I'm interested in hearing from \"the other side of the fence\", so to speak.\nIf you think that IDEs may be suitable for some types of work but not others, I'm also interested to hear why.",
    "answer": "It really depends on what language you're using, but in C# and Java I find IDEs beneficial for:\n\nQuickly navigating to a type without needing to worry about namespace, project etc\nNavigating to members by treating them as hyperlinks\nAutocompletion when you can't remember the names of all members by heart\nAutomatic code generation\nRefactoring (massive one)\nOrganise imports (automatically adding appropriate imports in Java, using directives in C#)\nWarning-as-you-type (i.e. some errors don't even require a compile cycle)\nHovering over something to see the docs\nKeeping a view of files, errors/warnings/console/unit tests etc and source code all on the screen at the same time in a useful way\nEase of running unit tests from the same window\nIntegrated debugging\nIntegrated source control\nNavigating to where a compile-time error or run-time exception occurred directly from the error details.\nEtc!\n\nAll of these save time. They're things I could do manually, but with more pain: I'd rather be coding."
  },
  {
    "question": "How do I control how Emacs makes backup files? Emacs puts backup files named foo~ everywhere and I don't like having to remember to delete them.  Also, if I edit a file that has a hard link somewhere else in the file system, the hard link points to the backup when I'm done editing, and that's confusing and awful.  How can I either eliminate these backup files, or have them go somewhere other than the same directory?",
    "answer": "(setq backup-directory-alist `((\".\" . \"~/.saves\")))\n(setq backup-by-copying t)\n(setq delete-old-versions t\nkept-new-versions 6\nkept-old-versions 2\nversion-control t)\n(setq make-backup-files nil)\n\nIf you've ever been saved by an Emacs backup file, you\nprobably want more of them, not less of them.  It is annoying\nthat they go in the same directory as the file you're editing,\nbut that is easy to change.  You can make all backup files go\ninto a directory by putting something like the following in your\n.emacs.\n\nThere are a number of arcane details associated with how Emacs\nmight create your backup files.  Should it rename the original\nand write out the edited buffer?  What if the original is linked?\nIn general, the safest but slowest bet is to always make backups\nby copying.\n\nIf that's too slow for some reason you might also have a look at\nbackup-by-copying-when-linked.\nSince your backups are all in their own place now, you might want\nmore of them, rather than less of them.  Have a look at the Emacs\ndocumentation for these variables (with C-h v).\n\nFinally, if you absolutely must have no backup files:\n\nIt makes me sick to think of it though."
  },
  {
    "question": "How can I replace a character with a newline in Emacs? I am trying to replace a character - say ; - with a new line using replace-string and/or replace-regexp in Emacs.\nI have tried the following commands:\n\nM-x replace-string RET ; RET \\n\nThis will replace ; with two characters: \\n.\n\nM-x replace-regex RET ; RET \\n\nThis results in the following error (shown in the minibuffer):\n\nInvalid use of `' in replacement text.\n\n\n\nWhat's wrong with using replace-string for this task? Is there another way to do it?",
    "answer": "M-x replace-string RET ; RET C-q C-j.\n\nC-q for quoted-insert,\n\nC-j is a newline."
  },
  {
    "question": "How do you &#39;redo&#39; changes after &#39;undo&#39; with Emacs? This article says that \"Emacs has redo because you can reverse direction while undoing, thereby undoing the undo\".\nWhat does this mean? How can a user 'redo' with Emacs?",
    "answer": "Short version: by undoing the undo. If you undo, and then do a non-editing command such as C-f, then the next undo will undo the undo, resulting in a redo.\nLonger version:\nYou can think of undo as operating on a stack of operations. If you perform some command (even a navigation command such as C-f) after a sequence of undo operations, all the undos are pushed on to the operation stack.  So the next undo undoes the last command.  Suppose you do have an operation sequence that looks like this:\n\nInsert \"foo\"\nInsert \"bar\"\nInsert \"I love spam\"\n\nNow, you undo.  It undoes the last action, resulting in the following list:\n\nInsert \"foo\"\nInsert \"bar\"\n\nIf you do something other than undo at this point - say, C-f, the operation stack looks like this:\n\nInsert \"foo\"\nInsert \"bar\"\nInsert \"I love spam\"\nUndo insert \"I love spam\"\n\nNow, when you undo, the first thing that is undone is the undo.  Resulting in your original stack (and document state):\n\nInsert \"foo\"\nInsert \"bar\"\nInsert \"I love spam\"\n\nIf you do a modifying command to break the undo sequence, that command is added after the undo and is thus the first thing to be undone afterwards.  Suppose you backspaced over \"bar\" instead of hitting C-f.  Then you would have had\n\nInsert \"foo\"\nInsert \"bar\"\nInsert \"I love spam\"\nUndo insert \"I love spam\"\nDelete \"bar\"\n\nThis adding/re-adding happens ad infinitum.  It takes a little getting used to, but it really does give Emacs a highly flexible and powerful undo/redo mechanism."
  },
  {
    "question": "How do I rename an open file in Emacs? Is there a way to rename an open file in Emacs? While I'm viewing it? Something like save-as, but the original one should go away.",
    "answer": ";; source: http://steve.yegge.googlepages.com/my-dot-emacs-file\n(defun rename-file-and-buffer (new-name)\n\"Renames both current buffer and file it's visiting to NEW-NAME.\"\n(interactive \"sNew name: \")\n(let ((name (buffer-name))\n(filename (buffer-file-name)))\n(if (not filename)\n(message \"Buffer '%s' is not visiting a file!\" name)\n(if (get-buffer new-name)\n(message \"A buffer named '%s' already exists!\" new-name)\n(progn\n(rename-file filename new-name 1)\n(rename-buffer new-name)\n(set-visited-file-name new-name)\n(set-buffer-modified-p nil))))))\n\nTry this function from Steve Yegge's .emacs:\n\nTake a look at that page, there's another really useful related function there, called \"move-buffer-file\"."
  },
  {
    "question": "Using Emacs to recursively find and replace in text files not already open As a follow-up to this question, it's trying to find out how to do something like this which should be easy, that especially stops me from getting more used to using Emacs and instead starting up the editor I'm already familiar with. I use the example here fairly often in editing multiple files.\nIn Ultraedit I'd do Alt+s then p to display a dialog box with the options: Find (includes using regular expressions across multiple lines), Replace with, In Files/Types, Directory, Match Case, Match Whole Word Only, List Changed Files and Search Sub Directories. Usually I'll first use the mouse to click-drag select the text that I want to replace.\nUsing only Emacs itself (on Windows XP), without calling any external utility, how to replace all foo\\nbar with bar\\nbaz in *.c and *.h files in some folder and all folders beneath it. Maybe Emacs is not the best tool to do this with, but how can it be done easily with a minimal command?",
    "answer": "M-x find-name-dired: you will be prompted for a root directory and a filename pattern.\nPress t to \"toggle mark\" for all files found.\nPress Q for \"Query-Replace in Files...\": you will be prompted for query/substitution regexps.\nProceed as with query-replace-regexp: SPACE to replace and move to next match, n to skip a match, etc.\nPress C-x s to save buffers. (You can then press y for yes, n for no, or ! for yes for all)"
  },
  {
    "question": "Step-by-step debugging with IPython From what I have read, there are two ways to debug code in Python:\n\nWith a traditional debugger such as pdb or ipdb. This supports commands such as c for continue, n for step-over, s for step-into etc.), but you don't have direct access to an IPython shell which can be extremely useful for object inspection.\n\nUsing  IPython by embedding an IPython shell in your code. You can do from IPython import embed, and then use embed() in your code. When your program/script hits an embed() statement, you are dropped into an IPython shell. This allows the full inspection of objects and testing of Python code using all the IPython goodies. However, when using embed() you can't step-by-step through the code anymore with handy keyboard shortcuts.\n\n\nIs there any way to combine the best of both worlds? I.e.\n\nBe able to  step-by-step through your code with handy pdb/ipdb keyboard shortcuts.\nAt any such step (e.g. on a given statement), have access to a full-fledged IPython shell.\n\nIPython debugging as in MATLAB:\nAn example of this type of \"enhanced debugging\" can be found  in MATLAB, where the user always has full access to the MATLAB engine/shell, and she can still step-by-step through her code, define conditional breakpoints, etc. From what I have discussed with other users, this is the debugging feature that people miss the most when moving from MATLAB to IPython.\nIPython debugging in Emacs and other editors:\nI don't want to make the question too specific, but I work mostly in Emacs, so I wonder if there is any way to bring this functionality into it. Ideally, Emacs (or the editor) would allow the programmer to set breakpoints anywhere on the code and communicate with the interpreter or debugger to have it stop in the location of your choice, and bring to a full IPython interpreter on that location.",
    "answer": "You can use IPython's %pdb magic. Just call %pdb in IPython and when an error occurs, you're automatically dropped to ipdb. While you don't have the stepping immediately, you're in ipdb afterwards.\nThis makes debugging individual functions easy, as you can just load a file with %load and then run a function. You could force an error with an assert at the right position.\n%pdb is a line magic. Call it as %pdb on, %pdb 1, %pdb off or %pdb 0. If called without argument it works as a toggle."
  },
  {
    "question": "What are these ^M&#39;s that keep showing up in my files in emacs? I think it may have to do with TextMate, but we work in a small team and are having some issues with full-file conflicts of nearly identical files in git \u2013 in one branch the files have a ^M appended to each line.\nWhat is this mysterious ^M character supposed to do, and where could it be coming from?\nOur developers use emacs on Windows/Mac, TextMate on Mac, coda on Mac, and occasionally the wp-admin text editor.\nDid anybody ever have this issue stemming from one of those?",
    "answer": "git config --global core.autocrlf true\n\nIn git-config, set core.autocrlf to true to make git automatically convert line endings correctly for your platform, e.g. run this command for a global setting:"
  },
  {
    "question": "How to have Emacs auto-refresh all buffers when files have changed on disk? I have a non-emacs global search and replace function that causes my disk files to become more up-to-date than my emacs buffers (en masse).  Is there any way to tell emacs to refresh all the buffers from disk in one fell swoop, instead of having to do each one individually by reloading the file?\nThanks!\nD",
    "answer": "(global-auto-revert-mode t) in your .emacs."
  },
  {
    "question": "How to change size of split screen emacs windows? I have emacs split horizontally - on top I'm editing Perl code, the bottom is the shell. By default emacs makes the two windows equal in size, but I'd like the shell buffer smaller (maybe half the size?). I was wondering how I could do that.",
    "answer": "(defun halve-other-window-height ()\n\"Expand current window to use half of the other window's lines.\"\n(interactive)\n(enlarge-window (/ (window-height (next-window)) 2)))\n\n(global-set-key (kbd \"C-c v\") 'halve-other-window-height)\n\nWith the mouse, you can drag the window sizes around.\nClick anywhere on the mode line that is not otherwise 'active' (the buffer name is safe, or any unused area to the right hand side), and you can drag up or down.\nSide-to-side dragging requires a very precise click on the spot where the two mode lines join.\nC-x\u00a0- (shrink-window-if-larger-than-buffer) will shrink a window to fit its content.\nC-x\u00a0+ (balance-windows) will make windows the same heights and widths.\nC-x\u00a0^ (enlarge-window) increases the height by 1 line, or the prefix arg value. A negative arg shrinks the window. e.g. C--\u00a0C-1\u00a0C-6\u00a0C-x\u00a0^ shrinks by 16 rows, as does C-u\u00a0-\u00a01\u00a06\u00a0C-x\u00a0^.\n(There is no default binding for shrink-window.)\nC-x\u00a0} (enlarge-window-horizontally) does likewise, horizontally.\nC-x\u00a0{ (shrink-window-horizontally) is also bound by default.\nFollowing one of these commands with repeat (C-x\u00a0z to initiate, and just z for continued repetition) makes it pretty easy to get to the exact size you want.\nIf you regularly want to do this with a specific value, you could record a keyboard macro to do it, or use something like\n(global-set-key (kbd \"C-c v\") (kbd \"C-u - 1 6 C-x ^\"))\nOr this:\n(global-set-key (kbd \"C-c v\") (kbd \"C-x o C-x 2 C-x 0 C-u - 1 C-x o\"))\nWhich is a smidgen hacky, so this would be better:\n\nTangentially, I also love winner-mode which lets you repeatedly 'undo' any changes to window configurations with C-c\u00a0left (whether the change is the size/number/arrangement of the windows, or just which buffer is displayed). C-c\u00a0right returns you to the most recent configuration. Set it globally with (winner-mode 1)"
  },
  {
    "question": "Open a file with su/sudo inside Emacs Suppose I want to open a file in an existing Emacs session using su or sudo, without dropping down to a shell and doing sudoedit or sudo emacs. One way to do this is\n\n```\nC-x C-f /sudo::/path/to/file\n\n```\n\nbut this requires an expensive round-trip through SSH. Is there a more direct way?\n[EDIT] @JBB is right. I want to be able to invoke su/sudo to save as well as open. It would be OK (but not ideal) to re-authorize when saving. What I'm looking for is variations of find-file and save-buffer that can be \"piped\" through su/sudo.",
    "answer": "The nice thing about Tramp is that you only pay for that round-trip to SSH when you open the first file.  Sudo then caches your credentials, and Emacs saves a handle, so that subsequent sudo-opened files take much less time.\nI haven't found the extra time it takes to save burdening, either.  It's fast enough, IMO."
  },
  {
    "question": "Set 4 Space Indent in Emacs in Text Mode I've been unsuccessful in getting Emacs to switch from 8 space tabs to 4 space tabs when pressing the TAB in buffers with the major mode text-mode. I've added the following to my .emacs:\n\n```\n(setq-default indent-tabs-mode nil)\n(setq-default tab-width 4)\n\n;;; And I have tried\n(setq indent-tabs-mode nil)\n(setq tab-width 4)\n\n```\n\nNo matter how I change my .emacs file (or my buffer's local variables) the TAB button always does the same thing.\n\nIf there is no text above, indent 8 spaces\nIf there is text on the previous line, indent to the beginning of the second word\n\nAs much as I love Emacs this is getting annoying. Is there a way to make Emacs to at least indent 4 space when there's not text in the previous line?",
    "answer": "(customize-variable (quote tab-stop-list))\n(custom-set-variables\n;; custom-set-variables was added by Custom.\n;; If you edit it by hand, you could mess it up, so be careful.\n;; Your init file should contain only one such instance.\n;; If there is more than one, they won't work right.\n'(tab-stop-list (quote (4 8 12 16 20 24 28 32 36 40 44 48 52 56 60 64 68 72 76 80 84 88 92 96 100 104 108 112 116 120))))\n\nDo not confuse variable tab-width with variable tab-stop-list.\nThe former is used for the display of literal TAB characters.\nThe latter controls what characters are inserted when you press the TAB character in certain modes.\n\n-- GNU Emacs Manual\n\nor add tab-stop-list entry to custom-set-variables in .emacs file:\n\nAnother way to edit the tab behavior is with with M-x edit-tab-stops.\nSee the GNU Emacs Manual on Tab Stops for more information on edit-tab-stops."
  },
  {
    "question": "Updating packages in Emacs I have the following setup for packages (not sure if there is a better recommended one):\n\n```\n(require 'package)\n(setq package-archives '((\"ELPA\" . \"http://tromey.com/elpa/\") \n                          (\"gnu\" . \"http://elpa.gnu.org/packages/\")\n                          (\"marmalade\" . \"http://marmalade-repo.org/packages/\")))\n\n; Apparently needed for the package auto-complete (why?)\n(add-to-list 'package-archives\n             '(\"melpa\" . \"http://melpa.milkbox.net/packages/\") t)\n\n(package-initialize)\n(setq url-http-attempt-keepalives nil)\n\n```\n\nI have three questions related to the installation and updating of packages.\nQ1. Is there a way to update the list of  available packages (and most recent versions) and update a specific package?\nQ.2 What is the difference between the following package sources?:\n\nELPA, \nGNU\nmarmalade \nmelpa\n\nQ.3 Does it matter the order in which they are added to package-archives?",
    "answer": "(when (not package-archive-contents)\n(package-refresh-contents))\n(setq jpk-packages\n'(\nac-dabbrev\n...\nyasnippet\n))\n\n(package-initialize)\n(add-to-list 'package-archives\n'(\"melpa\" . \"http://melpa.org/packages/\"))\n(add-to-list 'package-archives\n'(\"org\" . \"http://orgmode.org/elpa/\"))\n\n;; install any packages in jpk-packages, if they are not installed already\n(let ((refreshed nil))\n(when (not package-archive-contents)\n(package-refresh-contents)\n(setq refreshed t))\n(dolist (pkg jpk-packages)\n(when (and (not (package-installed-p pkg))\n(assoc pkg package-archive-contents))\n(unless refreshed\n(package-refresh-contents)\n(setq refreshed t))\n(package-install pkg))))\n\n(defun package-list-unaccounted-packages ()\n\"Like `package-list-packages', but shows only the packages that\nare installed and are not in `jpk-packages'.  Useful for\ncleaning out unwanted packages.\"\n(interactive)\n(package-show-package-list\n(remove-if-not (lambda (x) (and (not (memq x jpk-packages))\n(not (package-built-in-p x))\n(package-installed-p x)))\n(mapcar 'car package-archive-contents))))\n\nIn order to automatically update the list of packages, only if there is no package list already, use the following:\n\nIn order to update all installed packages, type package-list-packages, which will take you to the *Packages* buffer (and also update the list of packages), and then type U x.\npackage-refresh-contents unconditionally tries to download a package list from all repos you've added to package-archives; package-archive-contents is non nil if you have already downloaded the package list.\n\nELPA is the original.  I don't think it's really maintained anymore, but I'm not sure.  I don't use it.\nGNU is \"official\".  It's maintained along with Emacs, which means things should always work but updates and new packages don't come very often.\nMarmalade is basically a website where you can upload a complete package, and it will be added to the marmalade repo.  You don't just submit a link to the package's upstream, and it doesn't quite automate the creation of the package completely.  I think this is the Right Thing, because you don't necessarily want to track upstream.  Unfortunately, it has been unmaintained for a while, but someone recently took it over so it should be back and better at some point.\nMelpa takes a URL to e.g. the EmacsWiki lisp area or a github repo, and builds a package automatically from it.  Thus it is usually at most a day behind whatever it is tracking.  Although it tracks upstream, I've never had a problem in practice, and this is where most of my packages are from.  There is also Melpa Stable, which is like Melpa but grabs tagged revisions of the upstream repo instead of the latest revision.  Melpa stable has fewer packages than Melpa.  Probably Melpa should be preferred over Melpa Stable.\nOrg mode has its own package.el repo (http://orgmode.org/elpa/).\nAll of the package repos work the same, you just add them to your package-archives.\nHere's a more in-depth blog post about this subject, which I mostly agree with.\n\nI'm not sure, but I think if a package is duplicated in different repos, the order the repos appear in in package-archives determines precedence.  I don't know if higher precedence is at the beginning or end of the list.\n\nUpdate: In Emacs 25, there is a variable package-archive-priorities that you can use to prioritize your package repos (e.g. prefer ELPA over MELPA).\n\nHere is the relevant section of my init.el, if you're interested:"
  },
  {
    "question": "Using Emacs as an IDE Currently my workflow with Emacs when I am coding in C or C++ involves three windows.  The largest on the right contains the file I am working with.  The left is split into two, the bottom being a shell which I use to type in compile or make commands, and the top is often some sort of documentation or README file that I want to consult while I am working.  Now I know there are some pretty expert Emacs users out there, and I am curious what other Emacs functionally is useful if the intention is to use it as a complete IDE.  Specifically, most IDEs usually fulfill these functions is some form or another:\n\nSource code editor\nCompiler\nDebugging\nDocumentation Lookup\nVersion Control\nOO features like class lookup and object inspector \n\nFor a few of these, it's pretty obvious how Emacs can fit these functions, but what about the rest?  Also, if a specific language must be focused on, I'd say it should be C++. \nEdit: One user pointed out that I should have been more specific when I said 'what about the rest'.  Mostly I was curious about efficient version control, as well as documentation lookup.  For example, in SLIME it is fairly easy to do a quick hyperspec lookup on a Lisp function.  Is there a quick way to look up something in C++ STL documentation (if I forgot the exact syntax of hash_map, for example)?",
    "answer": "You'll have to be specific as to what you mean by \"the rest\".  Except for the object inspector (that I\"m aware of), emacs does all the above quite easily:\n\neditor (obvious)\ncompiler - just run M-x compile and enter your compile command.  From there on, you can just M-x compile and use the default.  Emacs will capture C/C++ compiler errors (works best with GCC) and help you navigate to lines with warnings or errors.\nDebugging - similarly, when you want to debug, type M-x gdb and it will create a gdb buffer with special bindings\nDocumentation Lookup - emacs has excellent CScope bindings for code navigation.  For other documentation: Emacs also has a manpage reader, and for everything else, there's the web and books.\nversion control - there are lots of Emacs bindings for various VCS backends (CVS, SCCS, RCS, SVN, GIT all come to mind)\n\nEdit: I realize my answer about documentation lookup really pertained to code navigation.  Here's some more to-the-point info:\n\nLooking up manpages, info manuals, and Elisp documentation from within emacs\nLooking up Python documentation from within Emacs.\n\nGoogle searching will no doubt reveal further examples.\nAs the second link shows, looking up functions (and whatever) in other documentation can be done, even if not supported out of the box."
  },
  {
    "question": "Who originally invented this type of syntax: -*- coding: utf-8 -*- Python recognizes the following as instruction which defines file's encoding:\n\n```\n# -*- coding: utf-8 -*-\n\n```\n\nI definitely saw this kind of instructions before (-*- var: value -*-), so I assume Python did not invent them and is not the only one that uses such instructions.\nWhere does this syntax come from? Is there full specification, e.g. can the value include spaces, special symbols, newlines, even -*- itself?\nWhat other software recognizes such metadata syntax?\nMy program will be writing plain text files and I'd like to include some metadata in them using this format.",
    "answer": "This way of specifying the encoding of a Python file comes from PEP 0263 - Defining Python Source Code Encodings.\nIt is also recognized by GNU Emacs (see Python Language Reference, 2.1.4 Encoding declarations), though I don't know if it was the first program to use that syntax."
  },
  {
    "question": "How do I duplicate a whole line in Emacs? I saw this same question for VIM and it has been something that I myself wanted to know how to do for Emacs. In ReSharper I use CTRL-D for this action. What is the least number of commands to perform this in Emacs?",
    "answer": "C-a C-SPACE C-n M-w C-y\nC-a C-k C-k C-y C-y\n(global-set-key \"\\C-c\\C-d\" \"\\C-a\\C- \\C-n\\M-w\\C-y\")\n\nI use\n\nwhich breaks down to\n\nC-a: move cursor to start of line\nC-SPACE: begin a selection (\"set mark\")\nC-n: move cursor to next line\nM-w: copy region\nC-y: paste (\"yank\")\n\nThe aforementioned\n\namounts to the same thing (TMTOWTDI)\n\nC-a: move cursor to start of line\nC-k: cut (\"kill\") the line\nC-k: cut the newline\nC-y: paste (\"yank\") (we're back at square one)\nC-y: paste again (now we've got two copies of the line)\n\nThese are both embarrassingly verbose compared to C-d in your editor, but in Emacs there's always a customization. C-d is bound to delete-char by default, so how about C-c C-d? Just add the following to your .emacs:\n\n(@Nathan's elisp version is probably preferable, because it won't break if any of the key bindings are changed.)\nBeware: some Emacs modes may reclaim C-c C-d to do something else."
  },
  {
    "question": "Re-open *scratch* buffer in Emacs? If I accidentally closed the scratch buffer in Emacs, how do I create a new scratch buffer?",
    "answer": "GNU Emacs default bindings:\n\nC-xb *scratch* RET\n\nor, more verbosely\n\nM-x switch-to-buffer *scratch* RET\n\nThe *scratch* buffer is the buffer selected upon startup, and has the major mode Lisp Interaction.  Note: the mode for the *scratch* buffer is controlled by the variable initial-major-mode.\nIn general you can create as many \"scratch\" buffers as you want, and name them however you choose.\n\nC-xb NAME RET\n\nswitches to a buffer NAME, creating it if it doesn't exist. A new buffer is not associated with a file on disk until you use C-xC-w (or M-x write-file RET) to choose a file where it should be saved.\n\nM-x text-mode RET\n\nchanges the current buffer's major mode to Text mode.  To find all the modes available (that is, without requiring any new packages), you can get a list by typing:\n\nM-x apropos-command -mode$ RET"
  },
  {
    "question": "Emacs - Multiple columns one buffer I'm trying to edit some assembly code which tends to be formatted in long but thin listings.  I'd like to be able to use some of the acres of horizontal space I have and see more code on-screen at one time.  Is there a method for getting Emacs (or indeed another editor) to show me multiple columns all pointing to the same buffer?\nC-x 3 (emacs) and :vsplit (vim) are great for multiple separate views into the code, but I'd like it to flow from one column to the other (like text in a newspaper).",
    "answer": "See follow-mode.\nExcerpt:\n\nFollow mode is a minor mode that makes two windows, both showing the same buffer, scroll as a single tall \u201cvirtual window.\u201d To use Follow mode, go to a frame with just one window, split it into two side-by-side windows using C-x 3, and then type M-x follow-mode. From then on, you can edit the buffer in either of the two windows, or scroll either one; the other window follows it.\n\nIn Follow mode, if you move point outside the portion visible in one window and into the portion visible in the other window, that selects the other window\u2014again, treating the two as if they were parts of one large window."
  },
  {
    "question": "How to run multiple shells on Emacs I am using Emacs 23.3.1 on windows 7. I know that I can run shell from emacs using M-x shell. I would like to have multiple shell windows in the same time, but typing M-x shell a second time just opens me the same shell window.\nIs there a way to have different shell windows?",
    "answer": "C-u M-x shell will do it.\nIt will prompt for a name for the new shell, just hit return for the default (which will be something like *shell*<2>.\nAlso works with eshell.\nAnother trick, if you use eshell: just as M-x eshell takes you back to *eshell* (rather than starting a new eshell), if you use a numeric prefix argument it will take you to that eshell buffer. For instance, C-3M-xeshell will take you to *eshell*<3>. Sadly if you use shell (rather than eshell), this trick doesn't seem to work (in my Emacs 24.0.50.1 at least.)"
  },
  {
    "question": "Is there a (repeat-last-command) in Emacs? Frequently, I've dug into apropos and docs looking for something like the following only to give up to get back to the task at hand:\n(repeat-last-command)\ndo the last C- or M- command I just executed (to be rebound to a fn key)\nor sometimes the related: \n(describe-last-function)\nwhat keystroke did I just mistakenly issue, the effect of which I'd like to add to my bag of tricks. describe-key is close, but requires knowing what I typed. \nAm I simply asking too much from my trusty sidekick?",
    "answer": "(defun describe-last-function()\n(interactive)\n(describe-function last-command))\n\nwith regards to 'describe-last-function':\nThere's a variable last-command which is set to a symbol representative of the last thing you did. So this elisp snippet - (describe-function last-command) - ought to bring up the documentation for the thing that immediately happened.\nSo you could make a trivial working describe-last-function like so\n\nPut that elisp in .emacs or equivalent, and you'll have a M-x describe-last-function.\nIf you've banged on a few keys or done something that modified last-command since the thing you're interested in, the command-history function might be of interest. You can get that by M-x command-history"
  },
  {
    "question": "How do I create a directory from within Emacs? How exactly can I create a new directory using Emacs? What commands do I use? (If possible, please provide an example)",
    "answer": "M-x make-directory RET dir/to/create RET\nM-! mkdir -p dir/parent{1,2}/node RET\n+\nC-x d *.py RET ; shows python source files in the CWD in `Dired` mode\n+ test RET     ; create `test` directory in the CWD\nM-x make-directory RET RET\n\nto create the directory dir/to/create, type:\n\nto create directories dir/parent1/node and dir/parent2/node, type:\n\nIt assumes that Emacs's inferior shell is bash/zsh or other compatible shell.\nor in a Dired mode\n\nIt doesn't create nonexistent parent directories.\nExample:\n\nCWD stands for Current Working Directory.\nor just create a new file with non-existing parent directories using C-x C-f and type:\n\nEmacs asks to create the parent directories automatically while saving a new file in recent Emacs versions. For older version, see How to make Emacs create intermediate dirs - when saving a file?"
  },
  {
    "question": "How to open Emacs inside Bash I'm using Ubuntu 11.10 (Oneiric Ocelot). When I type the command \"emacs\" in the terminal, it opens Emacs as a separate window. How can I open it inside the terminal, like the nano editor?",
    "answer": "Emacs takes many launch options. The one that you are looking for is\nemacs -nw. This will open Emacs inside the terminal disregarding the DISPLAY environment variable even if it is set.\nThe long form of this flag is emacs --no-window-system.\nMore information about Emacs launch options can be found in the manual."
  },
  {
    "question": "How to achieve code folding effects in Emacs? What's the best way to achieve something like code folding, or the type of cycling that org-mode uses.  What would be the best solution in elisp to create this type of behavior?\nEDIT:\nI'm sorry I was not clear.  I want to program something in elisp that does things very similar to code folding, or actually most like org-mode with the hierarchy that can be expanded.  I am wondering the best way to achieve this effect.  I think I have heard emacs overlays are a good solution, but I don't know.\nAs far as folding I just use the builtin set-selective-display\nEDIT NUMBER 2:\nThanks for the answers but I think I am asking the wrong question so let me try to be more clear on what I am trying to do.  I would like to create the following\nWhen you put your point on a function and call this elisp function it will put the function definition from wherever it is (I am thinking of just using find-tag for this) and unfold it in the current buffer.  The idea is if you have to jump to a different buffer to read the function definition I feel like it's a context switch to another file.  So I would like it to behave like code folding only it pulls in code external from the buffer.  This presents some problems as it can not actually paste the code into the buffer or if someone saves it will save the pulled in code.  So I am wondering if there is a way to create an area inside a buffer that is also not part of the buffer.  I think that makes sense.",
    "answer": "Folding is generally unnecessary with emacs, as it has tools that explicitly implement the actions people do manually when folding code.\nMost people have good success with simple incremental searches.  See \"foo\" mentioned somewhere? Type C-sfoo, find the definition, press enter, read it, and then press C-x C-x to go back to where you were.  Simple and very useful.\nMost modes support imenu.  M-ximenu will let you jump to a function definition (etc.) by name.  You can also bind it to a mouse click to get a menu of functions (or add it to the menubar; see the Info page for more detail).  It provides data for which-function-mode, which will let you see which function you are currently inside in the modeline.  (Why are your functions this long, though?)\nThere is also speedbar, which displays the imenu information (and other things) graphically.\nIf you want to get an overview of your file, try M-xoccur\".  Given a regex, it will create a new buffer with each match in the current buffer.  You can search for \"(defun\" to get an overview of the functions the current file implements.  Clicking on the result will move you to that position in the file.\nSo anyway, think about what you really want to achieve, and Emacs probably implements that. Don't fight with imperfect tools that make you fold and unfold things constantly."
  },
  {
    "question": "How can I delete the current line in Emacs? What is the emacs equivalent of vi's dd? I want to delete the current line. Tried CTRL + k but it only deletes from current position.",
    "answer": "C-a # Go to beginning of line\nC-k # Kill line from current point\nC-S-backspace   # Ctrl-Shift-Backspace\n(global-set-key \"\\C-cd\" 'kill-whole-line)     # Sets `C-c d` to `M-x kill-whole-line`\nC-u 5 C-S-backspace    # deletes 5 whole lines\nM-5 C-S-backspace      # deletes 5 whole lines\n\nC-u C-S-backspace      # delete 4 whole lines. C-u without a number defaults to 4\n\nC-u -5 C-S-backspace   # deletes previous 5 whole lines\nM--5 C-S-backspace     # deletes previous 5 whole lines\nC-S-backspace         # delete 1 whole line\nC-x z                 # repeat last command\nz                     # repeat last command again.\n\nThere is also\n\nwhich invokes M-x kill-whole-line.\nIf you'd like to set a different global key binding, you'd put this in ~/.emacs:\n\nIf you want to delete a number of whole lines, you can prefix the command with a number:\n\nSometimes I also find C-x z helpful:"
  },
  {
    "question": "How do I byte-compile everything in my .emacs.d directory? I have decided to check out Emacs, and I liked it very much. Now, I'm using the Emacs Starter Kit, which sort of provides better defaults and some nice customizations to default install of Emacs.\nI have customized it a little, added some stuff like yasnippet, color-themes, unbound, and other stuff. I've set up a github repository where I keep all of the customizations so I can access them from multiple places or in case something goes bad and I lose my .emacs.d directory.\nAll of this is very nice, but there is a problem: Emacs takes about 1-2 seconds to load. AFAIK I can compile individual .el files with M-x byte-compile-file to .elc, and it works. But there are a lot of .el files, and I wonder if there is a way to compile them all with a simple command or something, to speed up the loading of Emacs. My Emacs is not always open, and I open and close it quite frequently, especially after I've set it up as a default editor for edit command in Total Commander to get used to it faster (yeah, windows xp here).\nMy Emacs version is 22.3. And yes, the default Emacs installation without any customizations fires up instantly.\nI am not sure which version is preferred when loading, the .el or compiled .elc one by the way O.o\nSo, is there an elisp command or Emacs command line switch to make Emacs byte-compile everything in .emacs.d directory?",
    "answer": "C-u 0 M-x byte-recompile-directory\nwill compile all the .el files in the directory and in all subdirectories below.\nThe C-u 0 part is to make it not ask about every .el file that does not have a .elc counterpart."
  },
  {
    "question": "Emacs bulk indent for Python Working with Python in Emacs if I want to add a try/except to a block of code, I often find that I am having to indent the whole block, line by line.  In Emacs, how do you indent the whole block at once.\nI am not an experienced Emacs user, but just find it is the best tool for working through ssh.  I am using Emacs on the command line(Ubuntu), not as a gui, if that makes any difference.",
    "answer": "If you are programming Python using Emacs, then you should probably be using python-mode. With python-mode, after marking the block of code,\nC-c > or C-c C-l  shifts the region 4 spaces to the right\nC-c < or C-c C-r  shifts the region 4 spaces to the left\nIf you need to shift code by two levels of indention, or some arbitary amount you can prefix the command with an argument:\nC-u 8 C-c >     shifts the region 8 spaces to the right\nC-u 8 C-c <     shifts the region 8 spaces to the left\nAnother alternative is to use M-x indent-rigidly which is bound to C-x TAB:\nC-u 8 C-x TAB   shifts the region 8 spaces to the right\nC-u -8 C-x TAB  shifts the region 8 spaces to the left\nAlso useful are the rectangle commands that operate on rectangles of text instead of lines of text.\nFor example, after marking a rectangular region,\nC-x r o inserts blank space to fill the rectangular region (effectively shifting code to the right)\nC-x r k kills  the rectangular region (effectively shifting code to the left)\nC-x r t prompts for a string to replace the rectangle with.  Entering C-u 8 <space> will then enter 8 spaces.\nPS. With Ubuntu, to make python-mode the default mode for all .py files, simply install the python-mode package."
  },
  {
    "question": "Inline code in org-mode Markdown allows for embedded code. How can this be done in org-mode?\nI know about source-code blocks:\n\n```\n#+begin_example\nblah-blah\n#+end_example\n\n```\n\nBut what I want is something like this (obviously, with the right syntax, which I do not know):\n\n```\nThis is `embeded code`.\n\n```\n\nCan this be done in org-mode? Not possible to find that in the documentation ...",
    "answer": "This is =verbatim text= or ~code~.\n\nYou can enclose the text within = or ~ signs to have it typeset in monospaced font and export it verbatim (which means it is not processed for org-specific syntax):\n\nYou'll find all information about org-mode markup elements in the relevant section of the manual."
  },
  {
    "question": "How to change the indentation width in emacs javascript mode I'd like to use 2 spaces for my indents instead of 4. I can change the default behavior of C mode using: \n\n```\n(setq c-basic-offset 2)\n\n```\n\nHow do I change this in javascript mode?",
    "answer": "(setq js-indent-level 2)\n\njs-indent-level can be used in the default javascript-mode, which is included by default starting in emacs 23.2.\n\nshould do what you're looking for. If you're using an older version of emacs, you may be in java-mode.  I think this mode responds to c-basic-offset, however I may be wrong."
  },
  {
    "question": "How to automatically install Emacs packages by specifying a list of package names? I am using package to manage my Emacs extensions. In order to synchronize my Emacs settings on different computers, I'd like a way to specify a list of package names in .emacs file and then package could automatically search and install the packages, so that I don't need to install them manually by calling M-x package-list-packages. How to do that?",
    "answer": "(defun ensure-package-installed (&rest packages)\n\"Assure every package is installed, ask for installation if it\u2019s not.\n\nReturn a list of installed packages or nil for every skipped package.\"\n(mapcar\n(lambda (package)\n;; (package-installed-p 'evil)\n(if (package-installed-p package)\nnil\n(if (y-or-n-p (format \"Package %s is missing. Install it? \" package))\n(package-install package)\npackage)))\npackages))\n\n;; make sure to have downloaded archive description.\n;; Or use package-archive-contents as suggested by Nicolas Dudebout\n(or (file-exists-p package-user-dir)\n(package-refresh-contents))\n\n(ensure-package-installed 'iedit 'magit) ;  --> (nil nil) if iedit and magit are already installed\n\n;; activate installed packages\n(package-initialize)\n\nBased on comments by Profpatsch and answers below:"
  },
  {
    "question": "How do I change read/write mode for a file using Emacs? If a file is set to read only mode, how do I change it to write mode and vice versa from within Emacs?",
    "answer": "M-x read-only-mode\n\nin very old versions of Emacs, the command was:\n\nM-x toggle-read-only\n\nOn my Windows box, that amounts to Alt-x to bring up the meta prompt and typing \"read-only-mode\" to call the correct elisp function.\nIf you are using the default keyboard bindings,\n\nC-x C-q\n\n(which you read aloud as \"Control-X Control-Q\") will have the same effect.  Remember, however, given that emacs is essentially infinitely re-configurable, your mileage may vary.\nFollowing up from the commentary: you should note that the writeable status of the buffer does not change the writeable permission of the file.  If you try to write out to a read only file, you'll see a confirmation message.  However, if you own the file, you can write out your changes without changing the permissions on the file.\nThis is very convenient if you'd like to make a quick change to a file without having to go through the multiple steps of add write permission, write out changes, remove write permission.  I tend to forget that last step, leaving potentially critical files open for accidental changes later on."
  },
  {
    "question": "Open file via SSH and Sudo with Emacs I want to open a file inside Emacs which is located on a remote server, with sudo powers on the server. I can open local files with sudo via Tramp like this:\n\n```\nC-x C-f /sudo::/home/user/file\n\n```\n\nBut I want to use sudo on the server:\n\n```\nC-x C-f /sudo::user@server/home/user/file\n\n```\n\nBut this gives me sudo powers on my local machine, it asks for my sudo password on the local machine. Is there a way to use sudo on the server?\nBTW: Emacs is not installed on the server",
    "answer": "As of Emacs 24.3, an analog of the old multi: syntax has been layered on top of the modern tramp-default-proxies-alist approach, meaning that you can once again perform multi-hops without any prior configuration. For details, see:\nC-hig (tramp)Ad-hoc multi-hops RET\nWith the new syntax, each 'hop' is separated by |. The example in the manual is:\nC-xC-f /ssh:bird@bastion|ssh:you@remotehost:/path RET\nWhich connects firstly as bird@bastion, and from there to you@remotehost:/path\n/su: or /sudo: on remote hosts\nYou can also use this syntax to sudo/su to root (or of course any other user) on a remote host:\nC-xC-f /ssh:you@remotehost|sudo:remotehost:/path/to/file RET\nImportant: be sure to specify the hostname explicitly: sudo:remotehost: rather than sudo:: (see below).\nAs this still uses the proxy mechanism underneath, tramp-default-proxies-alist should now include the value (\"remotehost\" \"root\" \"/ssh:you@remotehost:\")\nMeaning that the proxy /ssh:you@remotehost: is going to be used whenever you request a file as root@remotehost.\nroot is the default user for these methods, but you can of course also change to a non-root user with:\nC-xC-f /ssh:you@remotehost|sudo:them@remotehost:/path/to/file RET\nAlways specify the remote hostname explicitly\nYou are probably used to using sudo:: or su:: and omitting the hostname. If you are staying on the localhost then this is still fine, but if you are hopping to a remote server then you must specify the hostname for every hop -- even if it is the same as for the previous hop. Always use sudo:hostname: or su:hostname: with remote hosts.\nThe trap here is that sudo:: does actually appear to work -- however when you do that the HOST for the dynamic proxy entry will be the hostname you originated from rather than the host you connected to. This will not only look confusing (as the wrong host will be displayed in the file paths), but it will also mean that any subsequent attempt to use sudo:: on your localhost will instead be proxied to the remote server! (and the proxy would also presumably be clobbered if you did the same thing on a second server, causing further issues).\nIn short, don't use :: when you multi-hop!\nEmacs 27+\nStarting from Emacs 27.1 (or Tramp 2.4.2, if using the GNU ELPA package) the :: case works intuitively, such that /ssh:you@remotehost|sudo:: will re-use remotehost rather than your own local host, and so you won't end up with a bad proxy entry.\nIn addition, the likes of /ssh:you@remotehost|sudo:localhost: are detected and flagged as user errors.\nIf you are liable to use a mixture of Emacs versions including versions earlier than 27 (or you are advising someone else who may be using an older version), then it would be safest to continue to treat :: as unsafe when multi-hopping, to avoid potential mishap.  (I.e. specifying the correct remote host explicitly will remain the safest approach if the Tramp version is unknown.)"
  },
  {
    "question": "Emacs Ruby autocomplete almost working I've been updating my emacs config with the use of Rsense to allow for an autocomplete drop down box to appear whilst typing code.  This works well in most files except I've found it doesn't allow me to select an answer from the table when I'm editing some code in my ruby on rails project.\nHere is my setup:\nhttps://github.com/map7/simple_emacs\nI'm using this under Ubuntu 10.04.\nFor simple ruby script files it works great. I can open up a new file and type.\n\n```\n\"test\".up...\n\n```\n\nJust as I type the 'p' character in up a list of options appear and I can go up and down the list with arrow keys and select one (eg: upcase) with the enter key.\nWhat doesn't work is when I do the exact same test but within a rails project's base directory.\nUpdate:\nFound that the problem is with (require 'rails), so it's something in the emacs-rails plugin that the autocomplete doesn't like.\nUpdate: \nIt's within emacs-rails -> rails-project.el. If I comment this macro out then autocomplete works, otherwise it doesn't:\n\n```\n(defmacro* rails-project:with-root ((root) &body body)\n  \"If you use `rails-project:root' or functions related on it\nseveral times in a block of code, you can optimize your code by\nusing this macro. Also, blocks of code will be executed only if\nrails-root exist.\n (rails-project:with-root (root)\n    (foo root)\n    (bar (rails-core:file \\\"some/path\\\")))\n \"\n `(let ((,root (rails-project:root)))\n    (when ,root\n      (flet ((rails-project:root () ,root))\n        ,@body))))\n\n```\n\nCan someone explain why this breaks autocomplete?",
    "answer": "Here's a thought: The macro binds a flet function (rails-project:root) one time to the value that (rails-project:root) has just before the body executes. (That's how it claims a performance increase: Apparently the outer (rails-project:root) is expensive, so calling once and caching the value seems like a good idea.)\nUnfortunately, if there is code inside the body that has a side effect meant intentionally to change the value that (rails-project:root) returns, it's going to have no effect.  That change will be invisible even to other code called within the body because Emacs lisp has dynamic binding of flet names."
  },
  {
    "question": "emacs/elisp: What is the hash (pound, number sign, octothorp) symbol used for? What does this do? \n\n```\n(add-hook 'compilation-mode-hook #'my-setup-compile-mode)\n\n```\n\n...and is it different than\n\n```\n(add-hook 'compilation-mode-hook 'my-setup-compile-mode)\n\n```",
    "answer": "(eq 'my-add #'my-add)\n\nThere is no difference:\n\nyields t\nThe # can be used in front of a lambda expression indicating to the byte-compiler that the following expression can be byte compiled, see the docs for Anonymous Functions.  But there's nothing to compile in the case of a symbol.\nIn general, it is used in the printed representation along with the left angle bracket (<) to indicate that the object printed is a description (but cannot be read).  For example:\n\nIt is also used in constructs by the reader to represent circular structures.  See the docs for Read Syntax for Circular Objects.\nAnd then you have its use for denoting the base for integers, e.g. #x2c -> 44.\nPlus more I'm sure."
  },
  {
    "question": "How to copy text from Emacs to another application on Linux When I cut (kill) text in Emacs 22.1.1 (in its own window on X, in KDE, on Kubuntu), I can't paste (yank) it in any other application.",
    "answer": "(setq x-select-enable-clipboard t)\n\nInsert the following into your .emacs file:"
  },
  {
    "question": "Given an emacs command name, how would you find key-bindings ? (and vice versa) If I know an emacs command name, says, \"goto-line\"; what if I want to query whether if there are any key-sequences bound to this command ?\nAnd vice versa, given a key sequence, how can I find its command name ?",
    "answer": "C-h w command-name\nC-h k key-sequence\nC-h f function-name\nC-h v variable-name\nC-h ?\n\nTo just find key bindings for a command, you can use emacs help's \"where-is\" feature\n\nIf multiple bindings are set for the command they will all be listed.\nFor the inverse, given a key sequence, you can type\n\nTo get the command that would run.\nYou can get detailed information about a command, also any non-interactive function defined, by typing\n\nWhich will give you detailed information about a function, including any key bindings for it, and\n\nwill give you information about any (bound) variable. Key-maps are kept in variables, however the key codes are stored in a raw format.  Try C-h v isearch-mode-map for an example.\nFor more help on getting help, you can type"
  },
  {
    "question": "Where can I find my .emacs file for Emacs running on Windows? I tried looking for the .emacs file for my Windows installation for Emacs, but I could not find it. Does it have the same filename under Windows as in Unix?\nDo I have to create it myself? If so, under what specific directory does it go?",
    "answer": "Copy and pasted from the Emacs FAQ, http://www.gnu.org/software/emacs/windows/:\nWhere do I put my init file?\nOn Windows, the .emacs file may be called _emacs for backward compatibility with DOS and FAT filesystems where filenames could not start with a dot. Some users prefer to continue using such a name, because Windows\u00a0Explorer cannot create a file with a name starting with a dot, even though the filesystem and most other programs can handle it. In Emacs\u00a022 and later, the init file may also be called .emacs.d/init.el. Many of the other files that are created by Lisp packages are now stored in the .emacs.d directory too, so this keeps all your Emacs related files in one place.\nAll the files mentioned above should go in your HOME directory. The HOME directory is determined by following the steps below:\n\nIf the environment variable HOME is set, use the directory it indicates.\nIf the registry entry HKCU\\SOFTWARE\\GNU\\Emacs\\HOME is set, use the directory it indicates.\nIf the registry entry HKLM\\SOFTWARE\\GNU\\Emacs\\HOME is set, use the directory it indicates. Not recommended, as it results in users sharing the same HOME directory.\nIf C:\\.emacs exists, then use C:/. This is for backward compatibility, as previous versions defaulted to C:/ if HOME was not set.\nUse the user's AppData directory, usually a directory called Application Data under the user's profile directory, the location of which varies according to Windows version and whether the computer is part of a domain.\n\nWithin Emacs, ~ at the beginning of a file name is expanded to your HOME directory, so you can always find your .emacs file with C-x C-f ~/.emacs.\nThere's further information at HOME and Startup Directories on MS-Windows."
  },
  {
    "question": "Changing the default folder in Emacs I am fairly new to Emacs and I have been trying to figure out how to change the default folder for C-x C-f on start-up. For instance when I first load Emacs and hit C-x C-f its default folder is C:\\emacs\\emacs-21.3\\bin, but I would rather it be the desktop. I believe there is some way to customize the .emacs file to do this, but I am still unsure what that is.\nUpdate: There are three solutions to the problem that I found to work, however I believe solution 3 is Windows only.\n\nSolution 1: Add (cd \"C:/Users/Name/Desktop\") to the .emacs file\nSolution 2: Add (setq default-directory \"C:/Documents and Settings/USER_NAME/Desktop/\") to the .emacs file\nSolution 3: Right click the Emacs short cut, hit properties and change the start in field to the desired directory.",
    "answer": "You didn't say so, but it sounds like you're starting Emacs from a Windows shortcut.\nThe directory that you see with c-x c-f is the cwd, in Emacs terms, the default-directory (a variable).\nWhen you start Emacs using an MS Windows shortcut, the default-directory is initially the folder (directory) specified in the \"Start In\" field of the shortcut properties.  Right click  the shortcut, select Properties, and type the path to your desktop in the Start In field.\nIf you're using Emacs from the command line, default-directory starts as the directory where you started Emacs (the cwd).\nThis approach is better than editing your .emacs file, since it will allow you to have more than one shortcuts with more than one starting directory, and it lets you have the normal command line behavior of Emacs if you need it.\nCWD = current working directory = PWD = present working directory. It makes a lot more sense at the command line than in a GUI."
  },
  {
    "question": "How do you list the active minor modes in emacs? How do you list the active minor modes in emacs?",
    "answer": "C-h m or M-x describe-mode shows all the active minor modes (and major mode) and a brief description of each."
  },
  {
    "question": "The function to show current file&#39;s full path in mini buffer I need to get the full path of the file that I'm editing with emacs.\n\nIs there a function for that?\nIf not, what would be the elisp function for getting that?\nHow can I copy the result (path name) to a clipboard so that I can reuse it?\n\nI'm using Mac OS X and Aqumacs. \n\n```\n(setq filepath (get-fullpath-current-file)) ???\n(copy-to-clipboard 'filepath) ???\n\n```\n\nADDED\n\n```\n\n(defun show-file-name ()\n  \"Show the full path file name in the minibuffer.\"\n  (interactive)\n  (message (buffer-file-name))\n  (kill-new (file-truename buffer-file-name))\n)\n(global-set-key \"\\C-cz\" 'show-file-name)\n\n```\n\nCombining the two answers that I got, I could get what I want. Thanks for the answers. And some more questions.\n\nWhat's for (file-truename)?\nCan I copy the path name to System(OS)'s clipboard, not the kill ring so that I can use the info with the other apps?",
    "answer": "(setq frame-title-format\n(list (format \"%s %%S: %%j \" (system-name))\n'(buffer-file-name \"%f\" (dired-directory dired-directory \"%b\"))))\n(defun show-file-name ()\n\"Show the full path file name in the minibuffer.\"\n(interactive)\n(message (buffer-file-name)))\n\n(global-set-key [C-f1] 'show-file-name) ; Or any other key you want\n\nIt's the built-in function buffer-file-name that gives you the full path of your file.\nThe best thing to do is to have your emacs window to always show your system-name and the full path of the buffer you're currently editing :\n\nYou can also do something like this :"
  },
  {
    "question": "How to load program reading stdin and taking parameters in gdb? Question cribbed from here:\n\nI have a program that takes input from\nstdin and also takes some parameters\nfrom command line. It looks like this:\n\n```\ncat input.txt > myprogram -path \"/home/user/work\"\n\n```\n\nI try to debug the code with gdb\ninside emacs, by M-x gdb, I try to\nload the program with the command:\n\n```\ngdb cat input.txt > myprogram -path \"/home/user/work\"\n\n```\n\nHowever, gdb does not like it.\n\nUnfortunately I don't understand the solution and am not sure what to do beyond compiling with the -g option and running the command M-x gdb.",
    "answer": "% gdb myprogram\ngdb> run params ... < input.txt\n\nIf you were doing it from a shell you'd do it like this:\n\nThis seems to work within emacs too."
  },
  {
    "question": "Emacs Interactive-Haskell repl non-response if either the cabal or working directory set to project directory I'm running into odd behavior with the Interactive-Haskell repl in emacs. When I source a file, the emacs mini buffer shows a series of interactive prompts:\n\n```\nStart a new project named 'myproject'?\nCabal dir (guessed from myproject.cabal):\nBuild target (empty for default):\nSet current directory:\n\n```\n\nIf I leave either the cabal directory or the current directory to the default value of the project directory, the repl is unresponsive. I get a lambda prompt in the interactive Haskell window, I can type, but when I press enter, it never evaluates. The cursor stays on the same line as if I did not press enter at all. \nIf I change both of those directories to some path other than my project directory, then the repl behaves normally and evaluates expressions as expected.",
    "answer": "This is a known bug. and a good catch!\nThe pull request https://github.com/haskell/haskell-mode/pull/1116/files solves it."
  },
  {
    "question": "How to swap the buffers in 2 windows emacs I am using emacs I find that sometimes I have 2 files separated into 2 windows.\nFor example:\nI open 1 file using C-x C-f file1.c RET\nand I split the frame into two windows: C-x 3\nI then open another file C-x C-f file2.c RET\nSo I have 2 files:\nwindow 1 (left) file1.c\nwindow 2 (right) file2.c\nI am wondering if there is any key combination to swap the files over? Normally I like to work on the left window when I have 2 window. I know I can easily do C-x oto move the cursor to the right window. \nHowever, I am just wondering if I can swap the files so that file2.c is in the left window and file1.c is in the right window?",
    "answer": "I use buffer-move for this. Now if you are working on the buffer on the left side, calling 'buf-move-right' will swap it with the one on the right. I guess this is what you want."
  },
  {
    "question": "Permanently summing a column in an Org-mode table In an Emacs Org-mode table, when you have a column full of integers I know you can do C-c + followed by C-y to paste the sum of the values in the column. I want to know the formula to place in the last row to always sum the whole column.\nI've tried everything. The docs show you how to sum two columns together but not one.",
    "answer": "|---+---|\n|   | 1 |\n|   | 2 |\n|   | 3 |\n|---+---|\n|   | 6 |\n| ^ | x |\n|---+---|\n\nAssign a field-name using the ^ mark:\n\nSee The Org Manual, Section-3.5.9 Advanced Features."
  },
  {
    "question": "View a file&#39;s history in Magit? View the change history of a file using Git versioning talks about other ways of viewing history of a file in Git.\nCan it be done in Emacs Magit?",
    "answer": "Since magit 2.1: magit-log-buffer-file (as per comment below)\nBefore magit 2.1: magit-file-log is what you are looking for. It will show you all commits for the file in the current buffer in the standard magit log view."
  },
  {
    "question": "The difference between setq and setq-default in Emacs Lisp I have a question about Emacs Lisp. What is the difference between setq and setq-default?\nTutorials say setq takes effect in the local buffer while setq-default affects all buffers.  \nFor example, if I wrote (setq a-var a-vars-value) in init.el, I found after starting Emacs and opening a new buffer, the a-var is also there and its value is a-vars-value. I thought it was not supposed to be there. It seems there is no difference between setq and setq-default.  \nIs there something wrong with my understanding?\nFor example:  \n\nI wrote (setq hello 123) in the init.el file, and I run emacs abuffer in the shell, then I input \"hello C-x C-e\", it shows \"123\". The same happens when I run this in all new buffers.  \nI wrote (setq tab-width 4) in the init.el file. When I run tab-width C-x C-e, it shows \"8\" (Current mode is 'Text'). However, when I use (setq-default tab-width 4), it show \"4\". I can't explain this phenomenon.",
    "answer": "Some variables in Emacs are \"buffer-local\", meaning that each buffer is allowed to have a separate value for that variable that overrides the global default. tab-width is a good example of a buffer-local variable.\nIf a variable is buffer-local, then setq sets its local value in the current buffer and setq-default sets the global default value.\nIf a variable is not buffer-local, then setq and setq-default do the same thing.\nIn your case 2, (setq tab-width 4) set the buffer-local value of tab-width to 4 in the current buffer, leaving the global default value of tab-width still at 8, so when you evaluated tab-width in a different buffer that had no local value, you saw that 8. Then, when you set the default value to 4, that buffer picked it up, since it still had no local value."
  },
  {
    "question": "How do I set the size of Emacs&#39; window? I'm trying to detect the size of the screen I'm starting emacs on, and adjust the size and position the window it is starting in (I guess that's the frame in emacs-speak) accordingly. I'm trying to set up my .emacs so that I always get a \"reasonably-big\" window with it's top-left corner near the top-left of my screen.\nI guess this is a big ask for the general case, so to narrow things down a bit I'm most interested in GNU Emacs 22 on Windows and (Debian) Linux.",
    "answer": "(defun set-frame-size-according-to-resolution ()\n(interactive)\n(if window-system\n(progn\n;; use 120 char wide window for largeish displays\n;; and smaller 80 column windows for smaller displays\n;; pick whatever numbers make sense for you\n(if (> (x-display-pixel-width) 1280)\n(add-to-list 'default-frame-alist (cons 'width 120))\n(add-to-list 'default-frame-alist (cons 'width 80)))\n;; for the height, subtract a couple hundred pixels\n;; from the screen height (for panels, menubars and\n;; whatnot), then divide by the height of a char to\n;; get the height we want\n(add-to-list 'default-frame-alist\n(cons 'height (/ (- (x-display-pixel-height) 200)\n(frame-char-height)))))))\n\n(set-frame-size-according-to-resolution)\n\nIf you want to change the size according to resolution you can do something like this (adjusting the preferred width and resolutions according to your specific needs):\n\nNote that window-system is deprecated in newer versions of emacs. A suitable replacement is (display-graphic-p). See this answer to the question How to detect that emacs is in terminal-mode? for a little more background."
  },
  {
    "question": "jump to line X in nano editor Does the Nano minimal text editor have a keyboard shortcut feature to jump to a specified line?\nVim provides several analogs.",
    "answer": "nano +10 file.txt\n\nIn the nano editor\nCtrl+_\nOn opening a file"
  },
  {
    "question": "nano error: Error opening terminal: xterm-256color After the installation of OSX Lion, I tried to:\n\n```\nnano /etc/apt/sources.list\n\n```\n\nBut I get this error:\n\n```\nError opening terminal: xterm-256color\n\n```\n\nIf I try to switch terminal.app preferences to open terminal windows in \"xterm color\" instead of xterm-256color everything works fine.\nWhat's happening?",
    "answer": "export TERM=xterm\n\nOn Red Hat this worked for me:\n\nfurther info here: https://web.archive.org/web/20220331072532/http://www.cloudfarm.it/fix-error-opening-terminal-xterm-256color-unknown-terminal-type/"
  },
  {
    "question": "How Do I Clear The Credentials In AWS Configure? I have deleted the AWS credentials in sudo nano ~/.aws/config. But, the credentials are still in aws configure. Is there a way to reset aws configure with clear state?",
    "answer": "just remove ~/.aws/credentials along with ~/.aws/config\nEDIT: Note path references user home directory and specific to each user in system."
  },
  {
    "question": "Unix: what modifier key does &quot;M-&quot; refer to (e.g. M-C) I'm trying to do a case-sensitive search in Nano.  The help menu at the bottom of the screen says I can toggle case-sensitivity by typing M-C.\n\n```\n^G Get Help         ^Y First Line       ^T Go To Line       ^W Beg of Par       M-J FullJstify      M-B Backwards\n^C Cancel           ^V Last Line        ^R Replace          ^O End of Par       M-C Case Sens       M-R Regexp\n\n```\n\nI'm not sure how to do this.  Does M- refer to a modifier key that should be held while I type C?  Or does M- mean I should press some key or key combination before hitting C?",
    "answer": "M refers to the meta key, which is not present on most keyboards. (Its use in software is for primarily historical reasons.) Usually, the meta key is emulated by another key on your keyboard. On Windows and Linux, it is usually the Alt key. On Mac OS X, that key (aka Option) already has other uses, and so Escape is used instead."
  },
  {
    "question": "Move whole line up/down shortcut in nano (analogue to intelliJ or Visual Studio) How to move a line of text up/down in Nano linux command line editor? \nIs there any analogue way to do that as in IntelliJ Idea:\n\nPlace the caret at the line to be moved.\nDo one of the following:\n\nOn the main menu, choose Code | Move Line Up or Code | Move Line Down.\nPress Shift+Alt+Up or Shift+Alt+Down.",
    "answer": "You can use Ctrl+K to cut a line, move to destination position and press Ctrl+U to paste it."
  },
  {
    "question": "How to commit a change in git when git commit opens Nano? I'm new to git and I'm trying to commit my first changes on a Windows machine. However, when I type in $git commit it takes me to a different screen than any online tutorials show or than what is mentioned in Pro Git. No online searching yields an explanation of how to use this screen, either.\nThe screen looks like this: \nI have tried every key on the keyboard, but nothing seems to actually do the commit. Also there are all these little help options in green at the bottom of the screen that seem to be giving commands, but when I press the buttons they show it just types them into the commit message. What are those help options and how do I use them?\nEveryone else seems to be using something called \"vim\" which I believe I chose not to install when installing Git because I thought the Windows console was fine.\nSo how do I actually commit and what are those green commands at the bottom of the screen? Thanks!",
    "answer": "That screen is just a text editor (called Nano) and those options at the bottom, represent commands, typically it's the ctrl key + the letter for the command.\nTo make the commit you should write your commit message, then press ctrl+o to save your message, and then ctrl+x to exit the editor.\nTo avoid that screen you could do something like git commit -m 'your commit message', the -m indicates that your commit message goes in the command."
  },
  {
    "question": "nano editor on OS X - How to type M-U for undo? I am using nano 2.5.3 on OS X Yosemite, and I see commands at the bottom such as:\n\n```\nM-U Undo\nM-E Redo\n\n```\n\nSo far, I have not been able to figure out which key or keys that M is referring to. What would be M on OS X?",
    "answer": "There is no \"UNDO\" in OSX GNU nano 2.0.6.\n\nMeta (M-) symbol is the Esc key in OSX. E.g. to cut text with M-T, enter Esc+ T.\nThe Main nano help text says:\n\nEscape-key sequences are notated with the Meta (M-) symbol and can be entered using either the Esc, Alt, or Meta key depending on your keyboard setup.\n\nDisplay the help text with all key combinations while in nano by entering ctrl+G.\nHide the help text with the same key combination.\n\nUseful for navigating quickly:\n\nctrl+ Y / ctrl+ V -> Go one screenful up / down\nEsc+ S -> Enable soft line wrapping\nEsc+ M -> Enable Mouse support\nGo to a line number\n\nEsc+ N -> Enable line numbering\n\nEsc+ _ -> go to a line\nYou'll have to type 3 keys for that: Esc+ Shift + -\n\nFYI: In debian's GNU nano 5.4, enter Esc+U on an OSX keyboard to \"UNDO\"."
  },
  {
    "question": "How to install nano on boot2docker I'm running the boot2docker VM in windows and need to modify the init scripts within the VM.  Currently my only option is VI, and it's a pain.  I'd just like to use nano, but it's not available.\nIt's running:\n\n```\ndocker@boot2docker:/c$ uname -a\nLinux boot2docker 3.18.5-tinycore64 #1 SMP Sun Feb 1 06:02:30 UTC 2015 x86_64 GNU/Linux\n\n```\n\nThere is no apt-get or any package manager.  How can I just get nano added so I can edit files easily?\nEDIT:\n@John-Petrone 's answer works, but after it's installed, running nano gives the error:\n\n```\nError opening terminal: cygwin.\n\n```\n\nIn case anyone else has that problem, the issue is $TERM was set to cygwin.  Not sure why.  I just:\n\n```\nTERM=xterm-color\n\n```\n\nAnd then nano worked! ( if set to any other available terms in /usr/share/terminfo/x funky characters showed up )",
    "answer": "tce-load -wi nano\n\nBoot2Docker is based on Tiny Linux which uses tce-load to install packages. The list of packages in the repository can be found here:\nhttp://distro.ibiblio.org/tinycorelinux/tcz_2x.html\nYou'll see that the nano packages is nano.tcz , so you'll run something like:\n\nThis article: http://lowtek.ca/roo/2015/modify-boot2docker-iso/ should also be helpful along with this one: http://www.gerrelt.nl/RaspberryPi/wordpress/tutorial-unpluggable-squeezelite-player-on-tinycore/#Search_and_install_NANO_extension"
  },
  {
    "question": "How to open multiple files and switch between them in nano text editor? In emacs, you can open new files with Ctrl+X,  Ctrl+F\nAnd go to the previous file with Ctrl+B, and go to the next file with Ctrl+N.\nHow to do this in nano text editor? While nano is launched, and then open new files, and how to switch between the opened files in the nano text editor?",
    "answer": "Put \"set multibuffer\" into ~/.nanorc or start nano with -F. Toggle this inside nano with alt-F. You can now read files into their own buffer with ctrl-R.\nUse alt-comma and alt-period to go forward and backward in the file list."
  },
  {
    "question": "Nano syntax highlighting in Mac OS X 10.7 (Lion)? How to enable syntax highlighting for nano in Mac OS X 10.7 (Lion)?\nAccording to what I found so far on Google is that it has got to do with /.nanorc file. I have no idea how to get it or make it?\nWhen I try to find nano in my terminal this is what I get:\n\n```\nNotra:~ Sukhvir$ whereis nano\n/usr/bin/nano\n\n```\n\nAccording to what I found on Internet this is the file I need to edit:\n\n```\n~/.nanorc\n\n```\n\nBut how do I get to it/how to open it/if I don't have it then how to make it?\nI am a bit new to programming folks, so step-by-step instructions will be highly appreciated.\nI need it for C mainly.\nAccording to what I found online, I have to paste this into the .nanorc file:\n\n```\ninclude \"/usr/share/nano/nanorc.nanorc\"\ninclude \"/usr/share/nano/c.nanorc\"\n\n```\n\nHowever this will not work because there is no such directory as /usr/share/nano.\nI also just did ls /usr/share/ and according to the results there is no nano in that directory. Is this a Mac OS X 10.7 (Lion) issue or an issue on my Mac?",
    "answer": "Here are some steps to help you out.\n\nCreate a new directory in /usr/local/share/ called 'nano' like this:\n\nmkdir /usr/local/share/nano\n\nNow, using nano, make a nano resource file for your C syntax like this:\n\nnano /usr/local/share/nano/c.nanorc`\n\nNow put your C code highlighting in this file and save it. Here is a link to some possible C syntax highlighting:\n\nhttp://code.google.com/p/nanosyntax/source/browse/trunk/syntax-nanorc/c.nanorc\n\nSave that file and now open your user\u2019s nano resource file by typing:\n\nnano ~/.nanorc\n\nIn this file, add a reference to the c.nanorc file you just made like this:\n\ninclude \"/usr/local/share/nano/c.nanorc\"\n\nSave your user resource file.\n\nNow, when you open up C files, you should see syntax highlighting. You can add additional syntax highlighting for different types of files using the same method. Just add more lines to your ~/.nanorc file.\nNote that depending on your user permissions, you may have to precede some of the above commands with sudo and then enter your root password."
  },
  {
    "question": "Regex-based matching and sustitution with nano? I am aware of nano's search and replace functionality, but is it capable of using regular expressions for matching and substitution (particularly substitutions that use a part of the match)? If so, can you provide some examples of the syntax used (both for matching and replacing)?\nI cut my teeth on Perl-style regular expressions, but I've found that text editors will sometimes come up with their own syntax.",
    "answer": "set regexp\n\nYou need to add, or un-comment, the following entry in your global nanorc file (on my machine, it was /etc/nanorc):\n\nThen fire up a new terminal and press CTRL + / and do your replacements which should now be regex-aware.\nEDIT\n\nSearch for conf->(\\S+):\n\nReplace with \\1_conf\n\nPress a to replace all occurrences:\n\nEnd result:"
  },
  {
    "question": "Python - Saving a File being edited in GNU Nano 2.2.4 I'm very new to programming and playing around with a Raspberry Pi and following tutorials on Youtube.\nI have opened a file in GNU Nano 2.2.6 e.g: nano my_File.py and changed some of the data.\nI'm struggling on how to overwrite the file (or save it) because when i run it in a new window it uses the original data...\nThanks.",
    "answer": "If you press Ctrl-X, for exit, it will ask you whether you want to save the file.\nCtrl-O is for saving file without exiting the editor.\nCtrl-G is for help on key combinations."
  },
  {
    "question": "Error opening terminal: xterm-256color When I try to nano something on my server I'm getting this error \"Error opening terminal: xterm-256color.\" that I've never seen before, and on top of that nano used to work fine last week.  Even when I use vi, the file will open but I can't manipulate using normal vi commands.  When I press i to activate insert mode and then try to navigate the document, it just types an A or B or something.\nThe only thing that changed is I lost my private key and had to generate a new one, so I had to upload the new public key to my server. Anyone know why this is happening?\nThe server is running Ubuntu 8.04 Hardy.\n\nSOLUTION\nOpen Terminal, select from the menu Terminal > Preferences > Settings > Advanced and under Emulation select \"Declare terminal as: xterm-color not xterm-256color\"\nReference: http://ricochen.wordpress.com/2011/07/23/mac-os-x-lion-terminal-color-remote-access-problem-fix/",
    "answer": "Probably due to a Lion upgrade/install. Did you do that recently @Gih?\nPossible duplicate (with fix) at\nnano error: Error opening terminal: xterm-256color\nEDIT:\nEasiest fix (takes 10 seconds)...from Michael:\n\nThere is a solution much easier:\nhttp://ricochen.wordpress.com/2011/07/23/mac-os-x-lion-terminal-color-remote-access-problem-fix/"
  },
  {
    "question": "Nano insert newline in search and replace Basically, I need to doublespace a part of text. I know I can do it with:\n\n```\nsed G\n\n```\n\nbut it would be great if I could do it directly in Nano.  \nI tried to search for the end of line ($) and then replace it with things  like \\n or [:newline:] but it seems that RegExp is recognized only for search and not for replace -- it always inserts the literal expression.  \nHowever, this question suggests it could be possible. Though I can't figure out how. And yes, I have:\n\n```\nset regexp\n\n```\n\nin my .nanorc",
    "answer": "DEC   HEX\n(\\t)    9  0x09   Horizontal Tab\n(\\n)   10  0x0a   New Line\n(\\r)   13  0x0d   Carriage Return\n( )   32  0x20   Space\n\nset regexp in your config file will:\n\nDo extended regular expression searches by default.\n\nYou can also activate it interactively with Meta + R*\nBut that's it! Search using RegEx, but as replacement Nano accepts only strings, except referenced captured groups \\1 to \\9 .\nNano's RegEx flavour is a POSIX implementation, and for what would interest you it does not compute [:newline:] but accepts \\s, [:cntrl:] (Control characters) and [:space:] (All whitespace characters, including line breaks) also negated POSIX classes: [^[:graph:]]\nAnswer\nIt can't be done, nothing works, tried:\n\nWhitespace Display Toggle Meta + P - no visible change\nVerbatim Input Meta + Shift + V then pressing Enter would translate into a visible ^M encoding (but works for a Tab input)\n\"Special Functions\" Esc + Esc then a 3 digit sequence - control characters not accepted (but accepts visible chars 013 would actually interpret it like the user pressed Enter)\nUnicode Input Ctrl + Shift + U - control characters not accepted (but accepts visible chars)\n\nASCII codes for some control characters:\n\nNano commands:\n\nSearch Ctrl + W\nFind Next Meta + W*\nToggle replace while searching Ctrl + R\nSearch & Replace Ctrl + \\\n\n*Meta is usually the Alt key\n\nPS: \"sometimes failure is the answer, or someone should prove me wrong\"\nPPS: please"
  },
  {
    "question": "Console editor: something between VIM and nano? I used vim for a long time, but switched to Sublime Text last year for most of my programming work.\nNow, every time I have to make use of a console editor (mostly over ssh), I feel extremely uncomfortable with vim. The shortcuts and commands have slowly left my memory, my once carefully curated vimrc is gathering dust, and I just can't use the editor effectively without re-learning everything. \nOn the other hand, nano is just dumb. Great for opening a file, adding a flag and closing it, but way too primitive for anything else.\nIs there something in between I can use? I can settle for not-as-easy-as-nano-nor-as-powerful-as-vim.",
    "answer": "sudo apt-get install mc\nsudo yum install mc\nbrew install mc\n\nYou can use mcedit. It is much more user friendly than other editors - it even supports mouse over ssh (even in Putty console from Windows).\nSimply install package Midnight Commander as follows:\n\nUbuntu/Debian:\n\nFedora/Redhat:\n\nMacOS:\n\nand it will make mcedit available.\nIn local console it even supports Shift+Del, Shift+Ins, Ctrl+Ins shortcuts that we use in Windows.\nIt also has nice ability to copy blocks into arbitrary files, effectively giving you unlimited number of clipboards."
  },
  {
    "question": "What is the benefit of using things such as emacs, vim, and nano over any other IDE or text editor? From what Ive seen. emacs etc.. run in terminal. Is there any benefit to this? It seems that it would be more of a hassle to write and organize things. I'm not trying to be subjective I literally know nothing of emacs, vim, nano etc.. and would like to know more, maybe use one of them.",
    "answer": "Your question is a tough one. Even if they do run in terminal, it's not their primary advantage. I'm talking about Emacs and Vim right now. To be short, they've been around for at least 20 years (Vim) or more (Emacs), they have a pretty active community, they're scriptable so you can do pretty much anything with them if you know how and they're extremely powerful.\nThey have a pretty big learning curve, so you'll probably end up fumbling around in them for weeks, if not months, before becoming proficient. The primary motivation for learning them is productivity. You can do some pretty amazing things in a minimal amount of keystrokes compared to, let's say, Notepad.\nTry one. If you like it, stick with it for some time, endure the pain and then you'll see the benefits. It's almost like going to the gym."
  },
  {
    "question": "Git commit -a opens GNU nano 2.2.6 How should I change it to open Vim instead? I  am trying to add a commit message to my changes using \n\n```\ngit commit -a \n\n```\n\nOR just plain \n\n```\ngit commit\n\n```\n\nthis somehow opens GNU Nano 2.2.6 editor and I am not at all comfortable with it. So the question is : \n\nHow can I modify my settings so that it always opens with VIM ?\n\nWhat I already have done is inserting following line in my ~/.bash_profile \n\n```\nset EDITOR = vim\n\n```\n\nPlease help !",
    "answer": "git config --global core.editor vim\n\nYou can set it from the command line or in your .gitconfig"
  },
  {
    "question": "bash: nano: command not found at Windows git bash I am using git version 2.7.0.windows.1 on a windows pc, I used the following command:\n\n```\n$ nano README\n\n```\n\nwhich results me:\n\n```\nbash: nano: command not found\n\n```\n\nNow how can I install nano text editor to git bash?",
    "answer": "[core]\neditor = \"winpty '/c/Program Files/Git/bin/nano-git-0d9a7347243.exe'\"\nexport PATH=\"/c/Program Files/Git/bin:$PATH\"\nalias nano=\"winpty nano\"\n\nLittle modification of the previous solution (@Simopaa) is worked for me on Windows 10 (without Chocolatey):\n\nDownload nano-git\nMove the nano-git-xxx.exe to (for example) C:\\Program Files\\Git\\bin.\nModify the .gitconfig file with the following (single and double quotes are important):\n\n(Optional step) Make nano available for editing in general:\nCreate or edit the one of the startup script of bash (e.g. ~/.bash_profile) with the followings:"
  },
  {
    "question": "How to turn OFF (or at least override) syntax highlighting in nano via ~/.nanorc? I am struggling finding a clear answer on disabling or overriding the color settings for the nano editor.\nBy default color syntax highlighting is enabled on my system. Clicking ALT+Y disables this, which is exactly what I want my default to be. \nAny ideas?",
    "answer": "set quiet\nsyntax \"disabled\" \".\"\nsyntax \"disabled\" \".\"\ncolor brightgreen,black \".\"\n\nTo disable syntax highlighting write following lines into ~/.nanorc:\n\nThe first line prevent error reporting. The second line defines a new color syntax.\nYou can also define a single syntax containing your favorite color in your ~/.nanorc:\n\nI hope this helps."
  },
  {
    "question": "How to set nano up for git commit messages with line length limits I use nano for git commit messages. Short summary (<=50 chars) plus a new line before the description is relatively straightforward to stick to. However, when it comes to wrapping at 72 chars in the description body I just go off what seems to look right, making for inconsistent logs.\nIn Vagrantfiles I've seen this sort of thing to tell the editor what to do for vi/vim:\n\n```\n# -*- mode: ruby -*-\n# vi: set ft=ruby :\n\n```\n\nIs there something like that for nano, and a template somewhere for git commit, that means I can ensure that nano would be set up for the correct line wrapping when in the context of a git commit message?",
    "answer": "Nano supports the passing of command line arguments when starting it.\nThis can be leveraged as part of the git editor config setting.\ngit config --global core.editor \"nano -r 72\"\nOr:\nexport GIT_EDITOR='nano -r 72'"
  },
  {
    "question": "Scroll using the mouse in nano editor? Probably not the right place to ask this question, but I couldn't find the answer anywhere. As the title says, is there a way to enable mouse wheel scrolling in the nano editor? (specifically ubuntu 14.04)",
    "answer": "Nano doesn't support using the mouse wheel to scroll. You can enable nano's (limited) mouse support with alt-M or with --mouse when starting nano (another invocation returns to whatever mouse support your terminal has), but that's it:\n\u2212m, \u2212\u2212mouse\nEnable mouse support, if available for your system. When enabled, mouse clicks can be used to place the cursor, set the mark (with a double click), and execute shortcuts. The mouse will work in the X Window System, and on the console when gpm is running. Text can still be selected through dragging by holding down the Shift key.\n(https://www.nano-editor.org/dist/v2.6/nano.1.html)\nBy default, mouse services are provided by the terminal window. The mouse works almost the same as in a GUI editor. You can highlight text, right-click to copy and paste, and use the middle mouse button for pasting text from the buffer. However, when you use the middle mouse button to paste text, the text is pasted at the current cursor location, not at the mouse pointer location. Other limitations exist. For example, you cannot use the mouse to cut or delete text, nor can you use the mouse to scroll through the file.\nNano has its own built-in mouse services, but they are limited. They provide only the ability to move the cursor to the point where you click, and to mark the beginning and end of a section of text. Use the Alt-M key combination to toggle between using the terminal's mouse services and nano's built-in mouse services.\n(https://help.ubuntu.com/community/Nano)"
  },
  {
    "question": "Single line create file with content The OS is Ubuntu. I want to create file.txt in /home/z/Desktop where the content of the file is some text here.\nThe first and usual way is run nano /home/z/Desktop/file.txt and type some text here. after that, press ctrl+x, pressy followed by Enter.\nThe second way is run cat > /home/z/Desktop/file.txt, type some text here and press Enter followed by ctrl+c\nI hope I can run single line of command to make it faster. I thought xdotool will work with the cat (the second way), but no, it not works",
    "answer": "echo \"some text here\" > file.txt\necho -n \"some text here\" > file.txt\n\nYou can use \"echo\" in bash. e.g.:\n\nIf you don't want a new line character at the end of the file, use the -n argument:"
  },
  {
    "question": "nano editor line breaks I am trying to learn nano installed on my server (ubuntu 1204), but the problem is whenever I open a file in nano editor it opens the file (say a shell script) as one big line rather than with line breaks which fit the screen.\nIs it possible to open files with linebreaks in nano? I have tried googling this problem, but couldn\u2019t find anything which works.\nAny help would be much appreciated.",
    "answer": "Ctrl+J and the lines \"broken\"."
  },
  {
    "question": "Nano. Copy (Alt+6) Doesn&#39;t work I am using nano version 2.2.6 and I have no idea why copying isn't working.\nWhat I do now is cut and then paste it two times, but before, I was using Alt + 6 without problem, what happens\u00bf? Thank you in advance",
    "answer": "After marking your desired text, copy it by entering (alt+shift+6) or (alt+^)."
  },
  {
    "question": "Nano 2.0.6 - Saving a file in Mac Format Any ideas how to save a file edited with Nano, in \"Mac format\"? I keep getting DOS carriage returns in my newly-saved files on my Linux box. I provided a screen shot of the bottom menu when I go to \"write out\" file:\nM-M Mac Format??? I see that there are save options when saving but cannot figure out how to execute them\n\nM-M                     Toggle the use of Mac format\nThanks",
    "answer": "The command \"M-M\" means \"alt M\". In fact, any command starting with \"M\" means the alt key. Some keyboards gobble up the alt key: in my case, on a Macbook Pro Retina, \"esc\" followed by \"M\" is what worked. More info here.\nHowever, since your screenshot is showing [DOS Format] as the current selection, the key combination you really want is \"alt D\" (or \"esc\" followed by \"D\"). That will toggle off [DOS Format] so that you'll be saving your file with the default line-ending character (ASCII 0xA). Turning on [Mac Format] will just give you a new set of problems."
  },
  {
    "question": "How to disable wrapping lines in nano? My nano did not wrap long lines automatically at first, they just kept going. I do not know what I did but now the automatic wrapping is enabled.\nI want to know how to reconfigure nano to disable automatic line wrapping, this has bothered me a lot.\nI've seen the nano -w command that it comes without wrapping, but what I want is to disable it by default and run nano without having to worry about wrapping.",
    "answer": "set nowrap\nman nanorc\n\nTo change persistent settings for nano, we can edit the user-specific .nanorc file (/home/user/.nanorc). This file contains configuration settings that nano loads on start-up.\nTo disable line-wrapping, we need to specify the nowrap directive. Add this line to .nanorc:\n\nWe may need to create this file if it doesn't exist. For more information about the available settings in this file, see the .nanorc man page:"
  },
  {
    "question": "Launch nano editor passing piped command this is a curiosity.\nCan i start nano editor from bash, passing a piped command?\nthis is my situation:\nI've a log file with dates formatted in tai64. For print my file i launch:\n\n```\n$> cat /var/log/qmail/current | tai64nlocal\n\n```\n\nthat print what i want.\nbut i want to view this in nano or another editor in one command. for example:\n\n```\n$> cat /var/log/qmail/current | tai64nlocal > nano\n\n```\n\nbut this doesn't work.\nAny suggestion?\nThanks in advance",
    "answer": "nano <(cat /var/log/qmail/current | tai64nlocal)\nnano <(tai64nlocal < /var/log/qmail/current)\n\nUse process substitution:\n\nAlso, you don't need to use cat"
  },
  {
    "question": "Passing file directly to nano fails with sighup or sigterm was want to search for a specific file on my server and directly edit it within nano.\ni tried it like this, but it wont work\n\n```\nfind -name file.php | xargs nano $1\n\n```\n\nthe file is found, but it wont work like that\n\n```\nReceived SIGHUP or SIGTERM\n\n```\n\nhow to do that proberly?",
    "answer": "i found the solution in using find intern exec function"
  },
  {
    "question": "I used to use nano to edit text but I switched to vim, which is more powerful. Will I get the same power up if I move to emacs? For about 3+ years, all I thought I needed (and technically that was the case) was nano. I did not understand all the hoopla about vim until I tried it. Although the learning curve is ever-so-slightly higher, it has greatly simplified coding for me and I would not dream of going back to nano.\nI have, however, heard many times over that emacs is the most difficult to learn, yet most useful editor for a programmer. Can anyone who has taken a similar path of evolution through text editors (and find themselves currently choosing emacs) tell me what the advantage is?\nRight now my view on vim is the same as my previous view on nano, which is: Will the marginal utility be great enough to justify putting in the time to learn? With switching from nano to vim, the answer is an obvious yes (for me anyway). Will I look back and say the same thing about vim if I learn emacs?",
    "answer": "I have only started to use EMACS, but my succession was like that - our local editor (home-MSU-made), then i have used vi/vim for several years or sometimes far editor, and finally 3 years ago I've switched to EMACS. The basic commands are learned quite fast, and then the rest of your life you just master it, every day discovering the faster way to do something. There's quite useful tutorial for the first steps in EMACS. Obtaining basic knowledge of LISP is quite fast too, just to customize. But there's also a user interface in EMACS to change preferences and automatically write it to the options file.\nI must admit that i have not tried the modern capabilities of vim, I've heard that in functionality employed by a programmer vim and emacs are very close. So finally it shall be a matter of taste - which of the two to use. Personally i more like LISP than VIMscript:)\nProbably there will be someone who has changed from vim to EMACS or vice versa - their opinion about pros and contras will be more mature than mine."
  },
  {
    "question": "What is a quick way to edit a remote file on Linux? I have a remote file that I edit regularly. I would like to edit it with a quick, simple command that would work likely via SSH. At present, my workflow is to connect to the remote computer via SSH, open the file using an editor (say vim or nano), edit, save and then close the connection.\nI am aware that I can mount the remote computer filesystem using SSHFS or Nautilus capabilities, but I'm really looking for a single command to run in the terminal which shall open the file in an editor, allow me to save and then exit, closing all connections to the remote computer.\nCurrently, I am trying to do this by passing a command to the remote computer via SSH, but I am running  into difficulties. For VIM, the command is something like the following:\n\n```\nssh user1@computer1 \"vim /path/laboratory_notebook_1.md\"\n\n```\n\nUsing this procedure, VIM does not run correctly and presents the following error:\n\n```\nVim: Warning: Output is not to a terminal\nVim: Warning: Input is not from a terminal\n\n```\n\nFor nano, the command is something like the following:\n\n```\nssh user1@computer1 \"nano /path/laboratory_notebook_1.md\"\n\n```\n\nUsing this procedure, nano does not run and the following error is presented:\n\n```\nError opening terminal: unknown.\n\n```\n\nI'm not sure how to proceed on this line of thought. I would appreciate assistance on this method and suggestions on other ways to edit remote files briskly with a minimum amount of interaction.",
    "answer": "-t      Force pseudo-tty allocation.  This can be used to execute arbi-\ntrary screen-based programs on a remote machine, which can be\nvery useful, e.g. when implementing menu services.  Multiple -t\noptions force tty allocation, even if ssh has no local tty.\nssh -t user1@computer1 'vim /path/laboratory_notebook_1.md'\n\nForce Pseudo-TTY Allocation\nYou can force pseudo-tty allocation with one or more -t flags. The SSH(1) man page says:\n\nExample\nUsing your own example, slightly modified, the following would work if you have a local TTY or PTY:\n\nIt works fine for me with OpenSSH_6.2p2. Your mileage (and operating environment) may vary."
  },
  {
    "question": "How to fix /etc/sudoers: syntax error near line number I am new to Ubuntu.\nAnd as while editing anything in visual studio code, I had to enter password.\nso for that I tried to change in sudoers file but after that I am unable to find what happened and also getting error like below.\nI used the command to change i\n\n```\nsudo nano -w /etc/sudoers\n\n```\n\nafter which i pressed ctrl+x\n\n```\n/etc/sudoers: syntax error near line 26 <<<\nsudo: parse error in /etc/sudoers near line 26\nsudo: no valid sudoers sources found, quitting\nsudo: unable to initialize policy plugin\n\n```\n\nThank you.",
    "answer": "we have to write -\npkexec visudo\nAnd it will open the file and one need to change as previous if any made.\nThen type Ctrl+X which will ask to save the file. So type: Y and Enter"
  },
  {
    "question": "How to change default editor for crontab in CentOS7? I want to change the default editor on my CentOS 7 box for crontab as it currently launches vi and I prefer nano.\nI have read a lot of pages online which suggest running export EDITOR='nano' or some similar - some places seem to suggest double quotes (\"), others no quotes at all.\nIn any case, none of these approaches are working, I run the export command, then sudo crontab -e and still it launches vi! What am I doing wrong?\nI realise that if I want the change to persist between sessions, I need to place this export command into my bashrc or bash_profile file, or create a .sh file in /etc/profile.d/, but I just want to get it working in isolation first before I make it persistent - can anyone help?",
    "answer": "The export works like you see, but if you use sudo, you're running crontab as other user, in this case as root, then you need set EDITOR as root too.\n(I post as answer because I can't comment)"
  },
  {
    "question": "nano: update a file after selecting it using the file browser I have no problems when open+change+save a file if I write nano filename on the terminal.\nBut if I open nano and open the File Browser and open a file and try to save it, there is no default file name!:",
    "answer": "I have tried your problem and indeed found that nano unable to \"remember\" the filename. My suspicion is that, when you open the editor just by typing \"nano\" in the command line, it only reads the content of that file, and paste it to the current buffer (without also making a new buffer for opening that file).\nSo try to do this:\n1. Open nano just by typing nano in terminal\n2. Then type alt + F to enable nano to open file into separate buffer\n3. Read your file of interest from nano by ctrl + R, then ctrl + T\n4. Make some modification to the file, and try to save it, It should know the filename of the original file now!"
  },
  {
    "question": "bash ps print info about process with name I need to print UID PID PPID PRI NI VSZ RSS STAT TTY TIME columns using ps of processes with typed name.\n\n```\n  GNU nano 2.0.6                                                     \n  File: file2                                                                                                                        \n\n  ps o uid,pid,ppid,ni,vsz,rss,stat,tty,time | grep $2  > $1\n  cat $1\n  echo \"enter pid of process to kill:\"\n  read pid\n  kill -9 $pid\n\n```\n\nBut it prints nothing, when I use this command with argument $2 = bash (this process exists)\nUPDATE\n\n```\n  GNU nano 2.0.6                               \n  File: file2  \n\nps o uid,pid,ppid,ni,vsz,rss,stat,tty,time,command | grep $2 | awk '{print $1,$2,$3,$4,$5,$6,$7,$8,$9}'  > $1\ncat $1\necho \"enter pid of process to kill:\"\nread pid\nkill -9 $pid\n\n```\n\nThis works for me, but actually this solution IMHO isn't the best one. I use shadow column command, after what grep name and print all columns excluding command.",
    "answer": "ps -o pid,comm | grep \"$2\" | cut -f1 -d' '\n-bash\n-bash\n-bash\nman\nsh\nsh\nsh\n/usr/bin/less\n-bash\nps -o pid,comm | grep bash | cut -f1 -d' '\nps -o uid,pid,ppid,ni,vsz,rss,stat,tty,time,command -p $(ps -o pid,comm | grep bash | cut -f1 -d' ')\nUID   PID  PPID NI      VSZ    RSS STAT TTY           TIME COMMAND\n67676 67675  0  2499876   7212 S+   ttys000    0:00.04 -bash\n71548 71547  0  2500900   8080 S    ttys001    0:01.81 -bash\n71995 71994  0  2457892   3616 S    ttys002    0:00.04 -bash\n74364 74363  0  2466084   7176 S+   ttys003    0:00.06 -bash\nps -o uid,pid,ppid,ni,vsz,rss,stat,tty,time,command -p $(ps -o pid,comm | grep \"$2\" | cut -f1 -d' ')\n\nYou can always use the two-stage approach.\n1.) find the wanted PIDs. For this use the simplest possible ps\n\nthe ps -o pid,comm prints only two columns, like:\n\nso grepping it is easy (and noise-less, without false triggers). The cut just extracts the PIDs. E.g. the\n\nprints\n\n2.) and now you can feed the found PIDs to the another ps using the -p flag, so the full command is:\n\noutput\n\ne.g. the solution using the $2 is"
  },
  {
    "question": "Nano syntax color highlighting in windows How do I get this to work, I did add the color syntax highlighting configuration in nano.rc and .nanorc, but nothing happen.",
    "answer": "C:\\nano\\bin\\       # For the nano.exe\nC:\\nano\\nanorc\\    # For all the *.nanorc files\nC:\\nano\\doc\\       # For the documentation files\n[System.Environment]::SetEnvironmentVariable(\"PATH\", $Env:Path + \";C:\\nano\\bin\", \"Machine\")\nNew-Item -ItemType SymbolicLink -Path \"C:\\ProgramData\\.local\\share\\nano\\filepos_history\" -Target  \"C:\\Users\\<username>\\.local\\share\\nano\\filepos_history\" -Force\n\nNew-Item -ItemType SymbolicLink -Path \"C:\\ProgramData\\.nanorc\" -Target  \"C:\\Users\\<username>\\.nanorc\" -Force\ncd ~\nnano .nanorc\n\ninclude \"/nano/nanorc/*.nanorc\"        # This works!\n\nThe previous answer is outdated and also wrong.\nMost of the problem with the coloring is due to the fact that the native build windows version doesn't support windows paths, as it's using GNU's glob(). So you need to use forward POSIX style paths in your syntax include statement in the .nanorc file.\nHere is the entire install procedure:\n\nDownload the latest Nano build from here OR here.\n\nDownload the latest syntax highlighter files *.nanorc from here\n\nChose an installation location. For example in C:\\nano\\.\n\nExtract the contents into that directory and move it around so that you have:\n\nPut the .nanorc into your home directory in: C:\\Users\\<username>\\.nanorc.\n\nAdd the nano file paths to your System Path:\n\nReboot, restart explorer, or install refreshenv from choco.\n\nOptional: Run nano once, to ensure that a filepos_history file is created.\n\nYou probably want to be able to run Nano with both normal or Administrator privileges, but not having to keep track of more edit locations and 2nd config files. To do this, you need to symlink your own versions of the nano config and history setting files, to the admin versions.\n\nIMPORTANT!\nEdit your .nanorc to include the correct POSIX paths to all your *.nanorc files.\n\nEnjoy!"
  },
  {
    "question": "How to delete several lines containing only whitespaces in nano? I have been using nano to write a bash script and I am at several times indenting various lines at once with Alt + }. The problem is that the following occurs:\n\nSeveral lines without text in them get white-spaces by the amount that I indent the text and they are coloured green. They don't affect the program but they make it look bad. I have been deleting them one by one but it gets frustrating over time. Is there a way to get rid of all the white-spaces at once? maybe some way to select the text (short cut for selecting text in nano is Alt + a) and have a short cut do it?",
    "answer": "Use global search and replace with the expression ^\\s+$.\n\nSwitch to search and replace mode with C-\\.\nSwitch to regex mode with Alt-R.\nType the expression ^\\s+$.\nHit Enter to replace with an empty string.\nOn the match, hit A for All."
  },
  {
    "question": "How To Open A File Using Text Edit In Terminal in MacOS Another small question. I just recently migrated to ZSH and, so far I am loving it! My question is how can I open a file using TextEdit so that I can edit it using a GUI? Make no mistake, Vim is an amazing command-line tool for editing files in the terminal along with nano.\nLet's say that I wanted to open the .zshrc using vim :\n\n```\nvim ~/.zshrc\n\n```\n\nIn the same way, open the .zshrc using nano:\n\n```\nnano ~/.zshrc\n\n```\n\nHow can I open the same ~/.zshrc file using text edit through the terminal?\nThanks a lot and I really appreciate your help :)",
    "answer": "open -a TextEdit ~/.zshrc\n\nHere is the solution, I know it's a bit tedious but, you can open the ~/.zshrc using this command :"
  },
  {
    "question": "dejustify (unjustify): replace each *single* linefeed with a space, but don&#39;t touch groups of linefeeds. sed, awk, or something else? I'm looking for a linux command-line solution to the problem:\n\"Replace each single linefeed with a space, but don't modify any groups of consecutive linefeeds i.e. do not modify any linefeed which has another linefeed next to it.\" As an example:\n\n```\none two\nthree four\nfive six\n\nseven eight\nnine ten\n\n```\n\nshould become:\n\n```\none two three four five six\n\nseven eight nine ten\n\n```\n\nI am already aware that every valid text file should end with a linefeed, but if your proposed solution deletes that final-character-linefeed, that would not be a problem (it would be easy for me to append it back on afterwards).\nI think that this is \"too complex a task\" for tr, but I assume something should be possible in sed or awk (if not, then I'll need to \"rustle up\" something in python or c). Unfortunately, my sed-fu is weak (as is my awk-fu) - are there any sed/awk black-belts around that could please help me?\nI have already found How can I replace each newline (\\n) with a space using sed? but of course the suggested answers to that question wipe out my \"multiple consecutive linefeeds\" (which I want to preserve).\nI am also aware that \"Sed is line-based therefore it is hard for it to grasp newlines\" - perhaps sed is not the best tool for this job.\nI have also found Replace only single instance of a character using sed but of course the character being replaced in that question is not a (problematic) linefeed.\n(Why do I want this? The nano editor has a justify function which adds and removes single linefeeds so that any line \"fills\" the chosen line length but does not overrun it. nano does have a \"built-in\" unjustify function, but this is really just an \"undo\", not a \"real\" unjustify. What I am trying to find is the closest thing to a \"genuine\" unjustify command.)\nUpdate: all the current solutions work perfectly, and thank you to all those who provided them. I've accepted Ed Morton's for the reasons that he gives - his processes only 1 line of input at a time, and it's portable to a non-gnu version of its tool. The solution to my nano problem is:\n\n```\ncat << 'EOF' > $HOME/.local/bin/dejustify\n#!/bin/sh\nawk -v RS= 'NR>1{print \"\"} {$1=$1} 1' < \"${1:-/dev/stdin}\"\nEOF\nchmod u+x $HOME/.local/bin/dejustify\n\n```\n\n(I found the < \"${1:-/dev/stdin}\" here.)\nI can now use it in a pipeline (printf \"one\\ntwo\\nthree\\nfour\\n\" | dejustify) or just dejustify <filename>.\nInside nano, I can <Ctrl>+<t> then enter |dejustify to dejustify my text. Success! \ud83d\ude4f",
    "answer": "awk -v RS= -v ORS='\\n\\n' -F'\\n' '{$1=$1} 1' file\none two three four five six\n\nseven eight nine ten\nawk -v RS= -F'\\n' 'NR>1{print \"\"} {$1=$1} 1' file\none two three four five six\n\nseven eight nine ten\n\nUsing any awk:\n\nBreaking it down:\n\n-v RS= treat the input as [possibly multi-line] records separated by 1 or more empty lines.\n-v ORS='\\n\\n' put 2 newlines at the end of each output record.\n-F'\\n' set the field separator to a newline so that ONLY newlines get replaced in the next step, otherwise all chains of contiguous white space within each record would be replaced.\n{$1=$1} update the value of a field, $1, thereby causing awk to rebuild the current record replacing all strings that match the FS (a newline) with an OFS (a blank char).\n1 a true condition causing awk to execute it's default action of printing the current record.\n\nThe above will print a blank line at the end of the output, if that's a problem you can always do this instead:\n\nwhich prints a blank line before each record except the first instead of printing a blank line after every record:\n\nNR>1{print \"\"} if this is the second or subsequent record then print a blank line before it."
  },
  {
    "question": "nano settings are not adopted from nanorc file I have a new Macbook M1 and usually edit files and write short scripts with nano. However, I am stuck in default settings, which are of course not really feasible. I created a file ~/.nanorc and since it didn't work also a file ~/etc/nanorc with the following content:\n\n```\nset linenumbers\nset tabsize 4\nset tabstospaces\nunset mouse\n\n```\n\nUnfortunately, it has no effect. I don't remember, if I faced the same problem when customising nano at my old macbook. Can someone help me out here?\nThanks!!",
    "answer": "I was having the same issue after I installed nano using brew (Monterey on M1). It turned out that nano command is by default symlinked to pico editor in /usr/bin, see https://ss64.com/osx/pico.html.\nYou can try checking if that is the case for you by using which nano. In my case it was pointing to /usr/bin/nano which is actually just symlink to pico (you can check this with readlink /usr/bin/nano)\nThis is probably an issue with homebrew, check this post Homebrew: Could not symlink, /usr/local/bin is not writable\nAs a quick fix (to verify if this is actually the case) you can just create a symlink pointing to homebrew installation of nano: ln -s /opt/homebrew/bin/nano /usr/local/bin/nano.\nThis should open nano instead of pico when using nano command and settings in .nanorc should now also be taken into account."
  },
  {
    "question": "difference between DOS Format and MAC Format When I save a file using nano on linux (bash), I get a message asking me to choose the format. There are two options: DOS Format and MAC Format. Can someone explain to me the difference between them?",
    "answer": "If you just press enter and no option, you save it in the same file-format which was detected. (That is the third option, and the option to prefer for Linux.)\nIf you selected an option you can deselect it again with pressing again \"Alt\" & \"m\" for deselecting [MAC-Format] or \"Alt\" & \"d\" for deselecting [DOS-Format]. If there is no option selected you use the UNIX-Format \"line flyback\" (LF or \\n), which is most likely the format to prefer on Linux. The selected option is written just before the file name in the bottom.\nsee also: https://unix.stackexchange.com/questions/396551/gnu-nano-2-dos-format-or-mac-format-on-linux"
  },
  {
    "question": "Nano on server ignores certain syntax coloring I'm using nano on a server via ssh; on that system, nano doesn't have syntax color enabled by default. So I copied these nanosyntax files (for alternative, see also @CraigBarnes' answer) on the server, and had set up ~/.nanorc as: \n\n```\ninclude \"~/nanosyntax/syntax-nanorc/php.nanorc\"\ninclude \"~/nanosyntax/syntax-nanorc/php2.nanorc\"\ninclude \"~/nanosyntax/syntax-nanorc/sh.nanorc\"\ninclude \"~/nanosyntax/syntax-nanorc/python.nanorc\"\ninclude \"~/nanosyntax/syntax-nanorc/html.nanorc\"\ninclude \"~/nanosyntax/syntax-nanorc/perl.nanorc\"\ninclude \"~/nanosyntax/syntax-nanorc/ruby.nanorc\"\ninclude \"~/nanosyntax/syntax-nanorc/js.nanorc\"\n\n```\n\nNow, this is the thing; if I just call: \n\n```\nnano somefile.php\n\n```\n\n... no php syntax coloring is done. If I try to force:\n\n```\nnano --syntax=php somefile.php\n\n```\n\n... still no syntax coloring (shown as plain text). However, if I do: \n\n```\nnano ~/.nanorc\n\n```\n\n... then I do get syntax coloring (that corresponds to .nanorc type file) ?!\nSo obviously, syntax coloring as such works (i.e. shell and nano are capable of it) - except, it seems to be ignored for some languages, like in this case php ?!\nSo, does anyone know what is going on - and how could I get syntax coloring also for php files? \nThanks,\nCheers!",
    "answer": "include \"~/.nano/nanorc.nanorc\"\ninclude \"~/.nano/sh.nanorc\"\ninclude \"~/.nano/sh.nanorc\"\ninclude \"~/.nano/nanorc.nanorc\"\n\nI just ran into the same problem, and I fiddled around a bit with the includes to find the error. Surprisingly, turns out that changing the inclusion order fixed the issue:\nThis works:\n\nThis fails to highlight sh files:\n\nSo I guess it's probably a bug (in nano 2.2.2; worked fine in nano 2.1.7)"
  },
  {
    "question": "Special characters not being displayed properly inside text editors on a docker container I'm trying to display these Special Characters,\u00b5\u00df\u00a3\u00ac, on my Docker Container running java-centos' vi editor and nano editor, but the \u00df character is replaced with sort of a question mark \ufffd instead. \nexpected output: \u00b5\u00df\u00a3\u00ac\nactual output: \u00b5\ufffd~_\u00a3\u00ac.\nBut I can echo these characters on the terminal?!?! So it's just to do with the editors not being able to display the \u00df character.\nI've tried all the suggestions from changing the default locale from POSIX to C.UTF-8 to setting the locale to German, to no avail.\nDockerfile:\n\n```\nFROM nimmis/java-centos:oracle-8-jdk\nENV LANG C.UTF-8\n\n```",
    "answer": "Okay so I've got it to work by adding this line to the Dockerfile, it's to do with the locale not being set properly on Centos7.\nENV LANG en_us.utf-8"
  },
  {
    "question": "Long-line handling from Nano in Vim? I'm a hardcore VIM-user. However, I wonder whether it's possible to configure VIM to provide a lone-line handling like Nano. Imagine the following text file:\n\n```\n1 This is a short line\n2 This is a much much much much much much much (...) much longer line\n\n```\n\nWhen I open it in VIM, and like to get to the end of the second line, it's displayed like this (with \u25a0 being where the cursor is):\n\n```\n1 ort line\n2 ch much much much much much much (...) much longer line\u25a0\n\n```\n\nWhen I open in Nano, and navigate to the end of the second line, only this specific line is displayed \"shifted\", like so:\n\n```\n1 This is a short line\n2 >h much much much much much much (...) much longer line\u25a0\n\n```\n\nCan I do this with VIM too?",
    "answer": "Vim is a fixed-width text editor. The problem with your suggested visualization is that when moving to a previous / next line (e.g. via k / j), the cursor would \"jump\" horizontally on the screen, instead of going straight up.\nOne could get accustomed to that (the cursor already jumps when the new line is shorter (unless 'virtualedit' is set)), but how would a blockwise visual selection be represented? With jagged edges on both sides?\nThat's why it's unlikely to be accepted, but don't let this discourage anyone from writing the (non-trivial) patch :-)"
  },
  {
    "question": "Nano syntax highlighting rules for D I'm just getting started learning the D language, and I'm curious whether a .nanorc file with syntax highlighting rules for D is available. Nothing turns up with a quick google search.",
    "answer": "I've just made one, for you and me (since I don't believe that anyone else in the world uses nano for D :-)\nHave a look: https://github.com/vinesworth/d-nanorc"
  },
  {
    "question": "How to save file in crontab using Ubuntu I am editing a crontab file using the nano editor on Ubuntu.  I need to save my text as a crontab file.  When I attempt to save the file, it is asking me for the file name, but after I give the file name it's not taking or I'm not giving the proper path or something.  I have included a screen shot below.\n\nThanks for helping!",
    "answer": "export VISUAL=vi\ncrontab -e\n\nTry to do as follow:\nUse VI as editor:\n\npress i for write; than press ESC and :wq for save and exit."
  },
  {
    "question": "Why does every text editor write an additional byte (UTF-8)? I'm working on Ubuntu 16.04 (Xenial Xerus). I found out that text editors write additional bytes (UTF-8) to the text file. It made some problems for me, when I tried to pass tests.\nSo we have a string, \"Extra byte\", with the size = 10 bytes in UTF-8.\nWhen I try to write it in file by gedit, for example, I get a file with the size = 11 byte. Also, nano makes the same size. Even \"echo \"Extra byte\" > filename\" returns 11 bytes.\nHowever, when we try something like this:\n\n```\n#include <fstream>\n\nint main(){\n    std::ofstream file(\"filename\");\n\n    file<<\"Extra byte\";\n    return 0;\n}\n\n```\n\nor this:\n\n```\nwith open(\"filename_py\",'w+',encoding='UTF-8') as file:\n    file.write('Extra byte')\n\n```\n\nWe get the file with size = 10 bytes. Why?",
    "answer": "echo 'foo' > /tmp/test.txt\nxxd /tmp/test.txt\n00000000: 666f 6f0a                                foo.\necho -n 'foo' > /tmp/test.txt\nxxd /tmp/test.txt\n00000000: 666f 6f                                  foo\nprintf 'foo' > /tmp/test.txt\nxxd /tmp/test.txt\n00000000: 666f 6f                                  foo\n\nYou are seeing a newline character (often expressed in programming languages as \\n, in ASCII it is hex 0a, decimal 10):\n\nThe hex-dump tool xxd shows that the file consists of 4 bytes, hex 66 (ASCII lowercase f), two times hex 65 (lowercase letter o) and the newline.\nYou can use the -n command-line switch to disable adding the newline:\n\nor you can use printf instead (which is more POSIX compliant):\n\nAlso see 'echo' without newline in a shell script.\nMost text editors will also add a newline to the end of a file; how to prevent this depends on the exact editor (often you can just use delete at the end of the file before saving). There are also various command-line options to remove the newline after the fact, see How can I delete a newline if it is the last character in a file?.\nText editors generally add a newline because they deal with text lines, and the POSIX standard defines that text lines end with a newline:\n\n3.206 Line\nA sequence of zero or more non- <newline> characters plus a terminating <newline> character.\n\nAlso see Why should text files end with a newline?"
  },
  {
    "question": "Why nano and rnano are different? I tried to edit files by using nano and rnano. The latter is really in restrict mode, but they are exactly same binary files. Why do they behave differently?\n\n```\n# which nano rnano\n/usr/bin/nano\n/bin/rnano\n\n# md5sum /usr/bin/nano /bin/rnano\nfa670e309a033718bad4b2051f5974fd  /usr/bin/nano\nfa670e309a033718bad4b2051f5974fd  /bin/rnano\n\n```\n\n(In ubuntu 12.04 x64 LTS)",
    "answer": "/* If the executable filename starts with 'r', enable restricted\n* mode.\n*/\nif (*(tail(argv[0])) == 'r')\nSET(RESTRICTED);\n\nThey behave differently because of the argument vector (argv), whose first element (argv[0]) contains the name of the file being executed.\nBecause rnano is a separate file from nano (even though it is just a symbolic link), it has its own, separate argv[0].\nYou can see this check in nano.c's main function:\n\nYou can also test this with a simple shell script. Create a shell script with only one statement, echo $0. Then, create a symbolic link to it with a separate name. Observe the difference."
  },
  {
    "question": "Why does writing to file change the content (hash)? I noticed, that when trying to generate some hashes after copy-pasting them into a file with vim, the hash is not as it is supposed to be. Same when file was opened and written out. Same behavior with nano, so there must be something I am mising.\n\n```\n$ echo -n  \"foo\" | openssl dgst -sha256\n2c26...e7ae\n$ echo -n \"foo\" > hash.txt\n$ openssl dgst -sha256 hash.txt\nSHA256(hash.txt)= 2c26...e7ae\n\n```\n\nBut when I open hash.txt with nano or vim and quit without inserting anything, I subsequently get the following hash: b5bb...944c\nI also noticed that without opening the file and writing out I do not see the output when using cat or head. Was the encoding changed?",
    "answer": "Most text editors, including Vim, save the file with a newline at the end when you quit.  That's because according to POSIX, a text file is either empty or ends with a newline, so most users expect and want that behavior.\nAs you've noticed, adding a newline at the end changes the hash, and the hash you're getting is consistent with the sequence foo\\n, where \\n is a newline.\nIf you have a recent enough Vim, you can control the behavior mentioned above by setting nofixeol, which will preserve the existing line ending, or lack thereof, on the last line."
  },
  {
    "question": "Redirect stdout into buffer editor before piping to other commands I'm trying to clean up my local set of git branches. There are quite a few, and I don't want to do git branch -D branchname for all of them. There's also no pattern that captures all and only the ones I want to kill. Then I remembered that I once saw someone pipe stdout into a buffer editor, and I thought that sounded perfect.\nI'd like to take the output of git branch -l, pipe it into nano, edit the buffer, then (upon exiting nano) have the final buffer be supplied as the input into another command (specifically, xargs git branch -D).\nHaving played around for a while without luck, this has become an academic question related to editing buffers on the fly -- so, no git-specific solutions, please. I want to know how to edit buffers on the fly in bash one-liners, because it seems like the kind of bash-fu that could elevate me to godhood. Acceptable answers should also work when sandwiched between e.g. find and file or path operations (like rm, cat, or grep).",
    "answer": "git branch -l > /tmp/branches; nano /tmp/branches; whatever /tmp/branches; rm /tmp/branches\n\nThe semi-colon is a perfectly acceptable operator."
  },
  {
    "question": "Prevent nano from creating save-file when SIGHUP or SIGTERM is received G'day\nThe text editor nano has this feature, that creates a save-file when the editor receives a SIGHUP or SIGTERM or runs out of memory.\nIn this case nano writes the current editor content to a temporary file called nano.save or, if editing an existing file, nano is adding the .save suffix to this filename: filename.save.\nBasically this is a great feature. The problem is, that I am editing files in /etc/cron.d/ with nano and therefore find a lot of *.save-files in this folder.\nSo this eventually leads to double cron jobs. \nSo my questions are:\n\nIs there a way to prevent nano from saving those buffers to the filesystem?\nOr is it possible to change the folder of those files to another, fixed location?\nOr is there another way to tell cron to not run files containing /.save/ \n\nThanks in advance",
    "answer": "According to the man page: https://www.nano-editor.org/dist/v2.2/nano.1.html\n\n\u2212R (\u2212\u2212restricted)\nRestricted mode: don\u2019t read or write to any file not specified on the command line; read any nanorc files; allow suspending; allow a file to be appended to, prepended to, or saved under a different name if it already has one; or use backup files or spell checking. Also accessible by invoking nano with any name beginning with \u2019r\u2019 (e.g. \"rnano\").\n\nSo one solution is to simply use \"rnano\" or \"nano -R\" instead."
  },
  {
    "question": "How do I list all cron jobs for all users? Is there a command or an existing script that will let me view all of a *NIX system's scheduled cron jobs at once? I'd like it to include all of the user crontabs, as well as /etc/crontab, and whatever's in /etc/cron.d. It would also be nice to see the specific commands run by run-parts in /etc/crontab.\nIdeally, I'd like the output in a nice column form and ordered in some meaningful way.\nI could then merge these listings from multiple servers to view the overall \"schedule of events.\"\nI was about to write such a script myself, but if someone's already gone to the trouble...",
    "answer": "mi     h    d  m  w  user      command\n09,39  *    *  *  *  root      [ -d /var/lib/php5 ] && find /var/lib/php5/ -type f -cmin +$(/usr/lib/php5/maxlifetime) -print0 | xargs -r -0 rm\n*/8  *  *  *  root      rsync -axE --delete --ignore-errors / /mirror/ >/dev/null\n1    *  *  *  root      /etc/cron.daily/apt\n1    *  *  *  root      /etc/cron.daily/aptitude\n1    *  *  *  root      /etc/cron.daily/find\n1    *  *  *  root      /etc/cron.daily/logrotate\n1    *  *  *  root      /etc/cron.daily/man-db\n1    *  *  *  root      /etc/cron.daily/ntp\n1    *  *  *  root      /etc/cron.daily/standard\n1    *  *  *  root      /etc/cron.daily/sysklogd\n2    *  *  7  root      /etc/cron.weekly/man-db\n2    *  *  7  root      /etc/cron.weekly/sysklogd\n3    *  *  *  archiver  /usr/local/bin/offsite-backup 2>&1\n3    1  *  *  root      /etc/cron.monthly/standard\n4    *  *  *  yukon     /home/yukon/bin/do-daily-stuff\n5    *  *  *  archiver  /usr/local/bin/update-logs >/dev/null\nCRONTAB='/etc/crontab'\nCRONDIR='/etc/cron.d'\n\ntab=$(echo -en \"\\t\")\n\nfunction clean_cron_lines() {\nwhile read line ; do\necho \"${line}\" |\negrep --invert-match '^($|\\s*#|\\s*[[:alnum:]_]+=)' |\nsed --regexp-extended \"s/\\s+/ /g\" |\nsed --regexp-extended \"s/^ //\"\ndone;\n}\n\nfunction lookup_run_parts() {\nwhile read line ; do\nmatch=$(echo \"${line}\" | egrep -o 'run-parts (-{1,2}\\S+ )*\\S+')\n\nif [[ -z \"${match}\" ]] ; then\necho \"${line}\"\nelse\ncron_fields=$(echo \"${line}\" | cut -f1-6 -d' ')\ncron_job_dir=$(echo  \"${match}\" | awk '{print $NF}')\n\nif [[ -d \"${cron_job_dir}\" ]] ; then\nfor cron_job_file in \"${cron_job_dir}\"/* ; do  # */ <not a comment>\n[[ -f \"${cron_job_file}\" ]] && echo \"${cron_fields} ${cron_job_file}\"\ndone\nfi\nfi\ndone;\n}\n\ntemp=$(mktemp) || exit 1\n\ncat \"${CRONTAB}\" | clean_cron_lines | lookup_run_parts >\"${temp}\"\n\ncat \"${CRONDIR}\"/* | clean_cron_lines >>\"${temp}\"  # */ <not a comment>\n\nwhile read user ; do\ncrontab -l -u \"${user}\" 2>/dev/null |\nclean_cron_lines |\nsed --regexp-extended \"s/^((\\S+ +){5})(.+)$/\\1${user} \\3/\" >>\"${temp}\"\ndone < <(cut --fields=1 --delimiter=: /etc/passwd)\n\ncat \"${temp}\" |\nsed --regexp-extended \"s/^(\\S+) +(\\S+) +(\\S+) +(\\S+) +(\\S+) +(\\S+) +(.*)$/\\1\\t\\2\\t\\3\\t\\4\\t\\5\\t\\6\\t\\7/\" |\nsort --numeric-sort --field-separator=\"${tab}\" --key=2,1 |\nsed \"1i\\mi\\th\\td\\tm\\tw\\tuser\\tcommand\" |\ncolumn -s\"${tab}\" -t\n\nrm --force \"${temp}\"\n\nI ended up writing a script (I'm trying to teach myself the finer points of bash scripting, so that's why you don't see something like Perl here). It's not exactly a simple affair, but it does most of what I need. It uses Kyle's suggestion for looking up individual users' crontabs, but also deals with /etc/crontab (including the scripts launched by run-parts in /etc/cron.hourly, /etc/cron.daily, etc.) and the jobs in the /etc/cron.d directory. It takes all of those and merges them into a display something like the following:\n\nNote that it shows the user, and more-or-less sorts by hour and minute so that I can see the daily schedule.\nSo far, I've tested it on Ubuntu, Debian, and Red Hat AS."
  },
  {
    "question": "Restarting cron after changing crontab file? Do I have to restart cron after changing the crontable file?",
    "answer": "sudo service cron reload\n/etc/init.d/cron reload\n\nNo.\nFrom the cron man page:\n\n...cron will then examine the modification time on all crontabs\nand reload those which have changed.  Thus cron need not be restarted\nwhenever a crontab file is modified\n\nBut if you just want to make sure its done anyway,\n\nor"
  },
  {
    "question": "How to run a cron job inside a docker container? I am trying to run a cronjob inside a docker container that invokes a shell script.\nHow can I do this?",
    "answer": "* * * * * echo \"Hello world\" >> /var/log/cron.log 2>&1\nFROM ubuntu:latest\nMAINTAINER docker@ekito.fr\n\nRUN apt-get update && apt-get -y install cron\n\nCOPY hello-cron /etc/cron.d/hello-cron\n\nRUN chmod 0644 /etc/cron.d/hello-cron\n\nRUN crontab /etc/cron.d/hello-cron\n\nRUN touch /var/log/cron.log\n\nCMD cron && tail -f /var/log/cron.log\nCOPY hello-cron /etc/cron.d/hello-cron\nRUN crontab /etc/cron.d/hello-cron\nCOPY hello-cron /etc/cron.d/hello-cron\nRUN chmod 0644 /etc/cron.d/hello-cron\nADD hello-cron /etc/cronjob\nRUN crontab /etc/cronjob\n* * * * * root echo hello > /proc/1/fd/1 2>/proc/1/fd/2\nCMD [\"cron\", \"-f\"]\nsudo docker build --rm -t ekito/cron-example .\nsudo docker run -t -i ekito/cron-example\nHello world\nHello world\ncron -f -l 2\n\nYou can copy your crontab into an image, in order for the container launched from said image to run the job.\n\nImportant: as noted in docker-cron issue 3: use LF, not CRLF for your cron file.\n\nSee \"Run a cron job with Docker\" from Julien Boulay in his Ekito/docker-cron:\n\nLet\u2019s create a new file called \"hello-cron\" to describe our job.\n\nIf you are wondering what is 2>&1, Ayman Hourieh explains.\n\nThe following Dockerfile describes all the steps to build your image\n\nBut: if cron dies, the container keeps running.\n(see Gaafar's comment and How do I make apt-get install less noisy?:\napt-get -y install -qq --force-yes cron can work too)\nAs noted by Nathan Lloyd in the comments:\n\nQuick note about a gotcha:\nIf you're adding a script file and telling cron to run it, remember to\nRUN chmod 0744 /the_script\nCron fails silently if you forget.\n\nWarning: as noted in the comments by user8046323\n\nThis config schedules tasks two times.\n\nOne time with crontab and\none time with cron.d\n\nPlease use only one of the ways to evade scheduling your tasks twice\n\nTrue: the problem is with those two lines in the above Dockerfile:\n\nBy placing the hello-cron file in the /etc/cron.d directory, you automatically schedule the cron jobs contained in this file. The cron daemon checks this directory for any files containing cron schedules and automatically loads them.\n\nThe crontab command with /etc/cron.d/hello-cron takes the contents of the hello-cron file and loads them into the main crontab. This means the same jobs are now scheduled directly in the crontab as well, effectively duplicating them.\n\nyou should choose one method to manage your cron jobs, depending on your specific needs:\n\nIf you prefer using /etc/cron.d (often easier for managing multiple separate cron job files):\n\nIf you prefer using crontab (gives you a consolidated view of all cron jobs and can be easier for a single or a few jobs):\n\nOR, make sure your job itself redirect directly to stdout/stderr instead of a log file, as described in hugoShaka's answer:\n\nReplace the last Dockerfile line with\n\nBut: it doesn't work if you want to run tasks as a non-root.\nSee also (about cron -f, which is to say cron \"foreground\") \"docker ubuntu cron -f is not working\"\n\nBuild and run it:\n\nBe patient, wait for 2 minutes and your command-line should display:\n\nEric adds in the comments:\n\nDo note that tail may not display the correct file if it is created during image build.\nIf that is the case, you need to create or touch the file during container runtime in order for tail to pick up the correct file.\n\nSee \"Output of tail -f at the end of a docker CMD is not showing\".\n\nSee more in \"Running Cron in Docker\" (Apr. 2021) from Jason Kulatunga, as he commented below\nSee Jason's image AnalogJ/docker-cron based on:\n\nDockerfile installing cronie/crond, depending on distribution.\n\nan entrypoint initializing /etc/environment and then calling"
  },
  {
    "question": "How to create a cron job using Bash automatically without the interactive editor? Does crontab have an argument for creating cron jobs without using the editor (crontab -e)? If so, what would be the code to create a cron job from a Bash script?",
    "answer": "crontab -l > mycron\n\necho \"00 09 * * 1-5 echo hello\" >> mycron\n\ncrontab mycron\nrm mycron\n* * * * * \"command to be executed\"\n- - - - -\n| | | | |\n| | | | ----- Day of week (0 - 7) (Sunday=0 or 7)\n| | | ------- Month (1 - 12)\n| | --------- Day of month (1 - 31)\n| ----------- Hour (0 - 23)\n------------- Minute (0 - 59)\n\nYou can add to the crontab as follows:\n\nCron line explaination\n\nSource nixCraft."
  },
  {
    "question": "How to pass in password to pg_dump? I'm trying to create a cronjob to back up my database every night before something catastrophic happens. It looks like this command should meet my needs:\n\n```\n0 3 * * * pg_dump dbname | gzip > ~/backup/db/$(date +%Y-%m-%d).psql.gz\n\n```\n\nExcept after running that, it expects me to type in a password. I can't do that if I run it from cron. How can I pass one in automatically?",
    "answer": "hostname:port:database:username:password\nchmod 600 ~/.pgpass\n\nCreate a .pgpass file in the home directory of the account that pg_dump will run as.\nThe format is:\n\nThen, set the file's mode to 0600. Otherwise, it will be ignored.\n\nSee the Postgresql documentation libpq-pgpass for more details."
  },
  {
    "question": "How do I get a Cron like scheduler in Python? I'm looking for a library in Python which will provide at and cron like functionality.\nI'd quite like have a pure Python solution, rather than relying on tools installed on the box; this way I run on machines with no cron.\nFor those unfamiliar with cron: you can schedule tasks based upon an expression like: \n\n```\n 0 2 * * 7 /usr/bin/run-backup # run the backups at 0200 on Every Sunday\n 0 9-17/2 * * 1-5 /usr/bin/purge-temps # run the purge temps command, every 2 hours between 9am and 5pm on Mondays to Fridays.\n\n```\n\nThe cron time expression syntax is less important, but I would like to have something with this sort of flexibility. \nIf there isn't something that does this for me out-the-box, any suggestions for the building blocks to make something like this would be gratefully received.\nEdit\nI'm not interested in launching processes, just \"jobs\" also written in Python - python functions. By necessity I think this would be a different thread, but not in a different process.\nTo this end, I'm looking for the expressivity of the cron time expression, but in Python. \nCron has been around for years, but I'm trying to be as portable as possible. I cannot rely on its presence.",
    "answer": "import schedule\nimport time\n\ndef job():\nprint(\"I'm working...\")\n\nschedule.every(10).minutes.do(job)\nschedule.every().hour.do(job)\nschedule.every().day.at(\"10:30\").do(job)\n\nwhile 1:\nschedule.run_pending()\ntime.sleep(1)\n\nIf you're looking for something lightweight checkout schedule:\n\nDisclosure: I'm the author of that library."
  },
  {
    "question": "Running a cron every 30 seconds I have a cron job that I need to run every 30 seconds.\nHere is what I have:\n\n```\n*/30 * * * * /bin/bash -l -c 'cd /srv/last_song/releases/20120308133159 && script/rails runner -e production '\\''Song.insert_latest'\\'''\n\n```\n\nIt runs, but is this running every 30 minutes or 30 seconds?\nAlso, I have been reading that cron might not be the best tool to use if I run it that often. Is there another better tool that I can use or install on Ubuntu 11.04 that will be a better option? Is there a way to fix the above cron?",
    "answer": "* * * * *              /path/to/executable param1 param2\n* * * * * ( sleep 30 ; /path/to/executable param1 param2 )\n((maxtime = 20))\nwhile [[ \"$(date +%S)\" != \"00\" ]]; do true; done\n\nwhile true; do\n\nsleep 30 &\n\n((delay = RANDOM % maxtime + 1))\n((maxtime += 1))\necho \"$(date) Sleeping for ${delay} seconds (max ${maxtime}).\"\n[[ ${delay} -gt 30 ]] && echo\nsleep ${delay}\n\nwait\ndone\nTue May 26 20:56:00 AWST 2020 Sleeping for 9 seconds (max 21).\nTue May 26 20:56:30 AWST 2020 Sleeping for 19 seconds (max 22).\nTue May 26 20:57:00 AWST 2020 Sleeping for 9 seconds (max 23).\nTue May 26 20:57:30 AWST 2020 Sleeping for 7 seconds (max 24).\nTue May 26 20:58:00 AWST 2020 Sleeping for 2 seconds (max 25).\nTue May 26 20:58:30 AWST 2020 Sleeping for 8 seconds (max 26).\nTue May 26 20:59:00 AWST 2020 Sleeping for 20 seconds (max 27).\nTue May 26 20:59:30 AWST 2020 Sleeping for 25 seconds (max 28).\nTue May 26 21:00:00 AWST 2020 Sleeping for 5 seconds (max 29).\nTue May 26 21:00:30 AWST 2020 Sleeping for 6 seconds (max 30).\nTue May 26 21:01:00 AWST 2020 Sleeping for 27 seconds (max 31).\nTue May 26 21:01:30 AWST 2020 Sleeping for 25 seconds (max 32).\nTue May 26 21:02:00 AWST 2020 Sleeping for 15 seconds (max 33).\nTue May 26 21:02:30 AWST 2020 Sleeping for 10 seconds (max 34).\nTue May 26 21:03:00 AWST 2020 Sleeping for 5 seconds (max 35).\nTue May 26 21:03:30 AWST 2020 Sleeping for 35 seconds (max 36).\n\nTue May 26 21:04:05 AWST 2020 Sleeping for 2 seconds (max 37).\nTue May 26 21:04:35 AWST 2020 Sleeping for 20 seconds (max 38).\nTue May 26 21:05:05 AWST 2020 Sleeping for 22 seconds (max 39).\nTue May 26 21:05:35 AWST 2020 Sleeping for 18 seconds (max 40).\nTue May 26 21:06:05 AWST 2020 Sleeping for 33 seconds (max 41).\n\nTue May 26 21:06:38 AWST 2020 Sleeping for 31 seconds (max 42).\n\nTue May 26 21:07:09 AWST 2020 Sleeping for 6 seconds (max 43).\n\nYou have */30 in the minutes specifier - that means every minute but with a step of 30 (in other words, every half hour). Since cron does not go down to sub-minute resolutions, you will need to find another way.\nOne possibility, though it's a bit of a kludge(a), is to have two jobs, one offset by 30 seconds:\n\nYou'll see I've added comments and formatted to ensure it's easy to keep them synchronised.\nBoth cron jobs actually run every minute but the latter one will wait half a minute before executing the \"meat\" of the job, /path/to/executable.\nFor other (non-cron-based) options, see the other answers here, particularly the ones mentioning fcron and systemd. These are probably preferable assuming your system has the ability to use them (such as installing fcron or having a distro with systemd in it).\n\nIf you don't want to use the kludgy solution, you can use a loop-based solution with a small modification. You'll still have to manage keeping your process running in some form but, once that's sorted, the following script should work:\n\nThe trick is to use a sleep 30 but to start it in the background before your payload runs. Then, after the payload is finished, just wait for the background sleep to finish.\nIf the payload takes n seconds (where n <= 30), the wait after the payload will then be 30 - n seconds. If it takes more than 30 seconds, then the next cycle will be delayed until the payload is finished, but no longer.\nYou'll see that I have debug code in there to start on a one-minute boundary to make the output initially easier to follow. I also gradually increase the maximum payload time so you'll eventually see the payload exceed the 30-second cycle time (an extra blank line is output so the effect is obvious).\nA sample run follows (where cycles normally start 30 seconds after the previous cycle):\n\nIf you want to avoid the kludgy solution, this is probably better. You'll still need a cron job (or equivalent) to periodically detect if this script is running and, if not, start it. But the script itself then handles the timing.\n\n(a) Some of my workmates would say that kludges are my specialty :-)"
  },
  {
    "question": "Where can I set environment variables that crontab will use? I have a crontab running every hour. The user running it has environment variabless in the .bash_profile that work when the user runs the job from the terminal, however, obviously these don't get picked up by crontab when it runs.\nI've tried setting them in .profile and .bashrc but they still don't seem to get picked up. Does anyone know where I can put environment vars that crontab can pick up?",
    "answer": "*       *       *       *       /usr/bin/ksh /work1/jleffler/bin/Cron/hourly\n1       *       *       *       /usr/bin/ksh /work1/jleffler/bin/Cron/daily\n1       *       *       1-5     /usr/bin/ksh /work1/jleffler/bin/Cron/weekday\n3       *       *       0       /usr/bin/ksh /work1/jleffler/bin/Cron/weekly\n3       1       *       *       /usr/bin/ksh /work1/jleffler/bin/Cron/monthly\n:       \"$Id: runcron.sh,v 2.1 2001/02/27 00:53:22 jleffler Exp $\"\n\n. $HOME/.cronfile\n\nbase=`basename $0`\ncmd=${REAL_HOME:-/real/home}/bin/$base\n\nif [ ! -x $cmd ]\nthen cmd=${HOME}/bin/$base\nfi\n\nexec $cmd ${@:+\"$@\"}\n:       \"@(#)$Id: weekday.sh,v 1.10 2007/09/17 02:42:03 jleffler Exp $\"\n\nn.updics\n:       \"@(#)$Id: daily.sh,v 1.5 1997/06/02 22:04:21 johnl Exp $\"\n\nexit 0\n\nHave 'cron' run a shell script that sets the environment before running the command.\nAlways.\n\nThe scripts in ~/bin/Cron are all links to a single script, 'runcron', which looks like:\n\n(Written using an older coding standard - nowadays, I'd use a shebang '#!' at the start.)\nThe '~/.cronfile' is a variation on my profile for use by cron - rigorously non-interactive and no echoing for the sake of being noisy.  You could arrange to execute the .profile and so on instead.  (The REAL_HOME stuff is an artefact of my environment - you can pretend it is the same as $HOME.)\nSo, this code reads the appropriate environment and then executes the non-Cron version of the command from my home directory.  So, for example, my 'weekday' command looks like:\n\nThe 'daily' command is simpler:"
  },
  {
    "question": "Running a cron job at 2:30 AM every day How to configure a cron job to run every night at 2:30? I know how to make it run at 2, but not 2:30.",
    "answer": "crontab -e\n2 * * * /your/command\n\nadd:"
  },
  {
    "question": "How do I schedule jobs in Jenkins? I added a new job in Jenkins, which I want to schedule periodically.\nFrom Configure job, I am checking the \"Build Periodically\" checkbox and in the Schedule text field added the expression:\n\n15 13 * * *\n\nBut it does not run at the scheduled time.\nIs it the correct procedure to schedule a job?\n\nThe job should run at 4:20 AM, but it is not running.",
    "answer": "By setting the schedule period to 15 13 * * * you tell Jenkins to schedule the build every day of every month of every year at the 15th minute of the 13th hour of the day.\nJenkins used a cron expression (official documentation), and the different fields are:\n\nMINUTES   Minutes in one hour (0-59)\nHOURS     Hours in one day (0-23)\nDAYMONTH  Day in a month (1-31)\nMONTH     Month in a year (1-12)\nDAYWEEK   Day of the week (0-7) where 0 and 7 are sunday\n\nIf you want to schedule your build every 5 minutes, this will do the job : */5 * * * *\nIf you want to schedule your build every day at 8h00, this will do the job : 0 8 * * *\nFor the past few versions (2014), Jenkins have a new parameter, H (extract from the Jenkins code documentation):\n\nTo allow periodically scheduled tasks to produce even load on the system, the symbol H (for \u201chash\u201d) should be used wherever possible.\nFor example, using 0 0 * * * for a dozen daily jobs will cause a large spike at midnight. In contrast, using H H * * * would still execute each job once a day, but not all at the same time, better using limited resources.\n\nNote also that:\n\nThe H symbol can be thought of as a random value over a range, but it actually is a hash of the job name, not a random function, so that the value remains stable for any given project.\n\nMore example of using 'H'"
  },
  {
    "question": "Run Cron job every N minutes plus offset */20 * * * * ensures it runs every 20 minutes, but I'd like to run a task every 20 minutes, starting at 5 past the hour.\nIs this possible with Cron?",
    "answer": "5-59/20 * * * *\n\nTo run a task every 20 minutes starting at 5 past the hour, try this:\n\nExplanation\nAn * in the minute field is the same as 0-59/1 where 0-59 is the range and 1 is the step. The command will run at the first minute in the range (0), then at all successive minutes that are distant from the first by step (1), until the last (59).\nWhich is why */20 * * * * will run at 0 minutes, 20 minutes after, and 40 minutes after -- which is the same as every 20 minutes. However, */25 * * * * will run at 0 minutes, 25 minutes after, and 50 minutes after -- which is not the same as every 25 minutes. That's why it's usually desirable to use a step value in the minute field that divides evenly into 60.\nSo to offset the start time, specify the range explicitly and set the first value to the amount of the offset.\nExamples\n5-59/20 * * * * will run at 5 minutes after, 25 minutes after, and 45 minutes after.\n10-59/25 * * * * will run at 10 minutes after and 35 minutes after.\n1-59/2 * * * * will run every odd minute."
  },
  {
    "question": "How to run crontab job every week on Sunday I'm trying to figure out how to run a crontab job every week on Sunday. I think the following should work, but I'm not sure if I understand correctly. Is the following correct?\n\n```\n5 8 * * 6\n\n```",
    "answer": "Here is an explanation of the crontab format.\n\nSo according to this your 5 8 * * 0 would run 8:05 every Sunday."
  },
  {
    "question": "Cron and virtualenv I am trying to run a Django management command from cron. I am using virtualenv to keep my project sandboxed.\nI have seen examples here and elsewhere that show running management commands from within virtualenv's like:\n\n```\n0 3 * * * source /home/user/project/env/bin/activate && /home/user/project/manage.py command arg\n\n```\n\nHowever, even though syslog shows an entry when the task should have started, this task never actually runs (the log file for the script is empty). If I run the line manually from the shell, it works as expected. \nThe only way I can currently get the command to run via cron, is to break the commands up and put them in a dumb bash wrapper script:\n\n```\n#!/bin/sh\nsource /home/user/project/env/bin/activate\ncd /home/user/project/\n./manage.py command arg\n\n```\n\nEDIT:\nars came up with a working combination of commands:\n\n```\n0 3 * * * cd /home/user/project && /home/user/project/env/bin/python /home/user/project/manage.py command arg\n\n```\n\nAt least in my case, invoking the activate script for the virtualenv did nothing. This works, so on with the show.",
    "answer": "/home/my/virtual/bin/python /home/my/project/manage.py command arg\ncd /home/my/project && /home/my/virtual/bin/python ...\ncd /home/my/project && /home/my/virtual/bin/python /home/my/project/manage.py > /tmp/cronlog.txt 2>&1\n\nYou should be able to do this by using the python in your virtual environment:\n\nEDIT: If your django project isn't in the PYTHONPATH, then you'll need to switch to the right directory:\n\nYou can also try to log the failure from cron:\n\nAnother thing to try is to make the same change in your manage.py script at the very top:"
  },
  {
    "question": "How do I write a bash script to restart a process if it dies? I have a python script that'll be checking a queue and performing an action on each item:\n\n```\n# checkqueue.py\nwhile True:\n  check_queue()\n  do_something()\n\n```\n\nHow do I write a bash script that will check if it's running, and if not, start it.  Roughly the following pseudo code (or maybe it should do something like ps | grep?):\n\n```\n# keepalivescript.sh\nif processidfile exists:\n  if processid is running:\n     exit, all ok\n\nrun checkqueue.py\nwrite processid to processidfile\n\n```\n\nI'll call that from a crontab:\n\n```\n# crontab\n*/5 * * * * /path/to/keepalivescript.sh\n\n```",
    "answer": "until myserver; do\necho \"Server 'myserver' crashed with exit code $?.  Respawning..\" >&2\nsleep 1\ndone\ntrap 'kill $(jobs -p)' EXIT; until myserver & wait; do\necho \"ldap proxy crashed with exit code $?. Respawning..\" >&2\nsleep 1\ndone\ncrontab -e\n@reboot /usr/local/bin/myservermonitor\n\nAvoid PID-files, crons, or anything else that tries to evaluate processes that aren't their children.\nThere is a very good reason why in UNIX, you can ONLY wait on your children.  Any method (ps parsing, pgrep, storing a PID, ...) that tries to work around that is flawed and has gaping holes in it.  Just say no.\nInstead you need the process that monitors your process to be the process' parent.  What does this mean?  It means only the process that starts your process can reliably wait for it to end.  In bash, this is absolutely trivial.\n\nOr to be able to stop it:\n\nThe above piece of bash code runs myserver in an until loop.  The first line starts myserver and waits for it to end.  When it ends, until checks its exit status.  If the exit status is 0, it means it ended gracefully (which means you asked it to shut down somehow, and it did so successfully).  In that case we don't want to restart it (we just asked it to shut down!).  If the exit status is not 0, until will run the loop body, which emits an error message on STDERR and restarts the loop (back to line 1) after 1 second.\nWhy do we wait a second?  Because if something's wrong with the startup sequence of myserver and it crashes immediately, you'll have a very intensive loop of constant restarting and crashing on your hands.  The sleep 1 takes away the strain from that.\nNow all you need to do is start this bash script (asynchronously, probably), and it will monitor myserver and restart it as necessary.  If you want to start the monitor on boot (making the server \"survive\" reboots), you can schedule it in your user's cron(1) with an @reboot rule.  Open your cron rules with crontab:\n\nThen add a rule to start your monitor script:\n\nAlternatively; look at inittab(5) and /etc/inittab.  You can add a line in there to have myserver start at a certain init level and be respawned automatically.\n\nEdit.\nLet me add some information on why not to use PID files.  While they are very popular; they are also very flawed and there's no reason why you wouldn't just do it the correct way.\nConsider this:\n\nPID recycling (killing the wrong process):\n\n/etc/init.d/foo start: start foo, write foo's PID to /var/run/foo.pid\nA while later: foo dies somehow.\nA while later: any random process that starts (call it bar) takes a random PID, imagine it taking foo's old PID.\nYou notice foo's gone: /etc/init.d/foo/restart reads /var/run/foo.pid, checks to see if it's still alive, finds bar, thinks it's foo, kills it, starts a new foo.\n\nPID files go stale.  You need over-complicated (or should I say, non-trivial) logic to check whether the PID file is stale, and any such logic is again vulnerable to 1..\n\nWhat if you don't even have write access or are in a read-only environment?\n\nIt's pointless overcomplication; see how simple my example above is.  No need to complicate that, at all.\n\nSee also: Are PID-files still flawed when doing it 'right'?\nBy the way; even worse than PID files is parsing ps!  Don't ever do this.\n\nps is very unportable.  While you find it on almost every UNIX system; its arguments vary greatly if you want non-standard output.  And standard output is ONLY for human consumption, not for scripted parsing!\nParsing ps leads to a LOT of false positives.  Take the ps aux | grep PID example, and now imagine someone starting a process with a number somewhere as argument that happens to be the same as the PID you stared your daemon with!  Imagine two people starting an X session and you grepping for X to kill yours.  It's just all kinds of bad.\n\nIf you don't want to manage the process yourself; there are some perfectly good systems out there that will act as monitor for your processes.  Look into runit, for example."
  },
  {
    "question": "Using crontab to execute script every minute and another every 24 hours I need a crontab syntax which should execute a specific PHP script /var/www/html/a.php every minute. The execution on every minute must start at 00:00. The other task which must execute a script at 00:00 /var/www/html/reset.php (once every 24 hours).",
    "answer": "every minute:\n* * * * * /path/to/php /var/www/html/a.php\nevery 24hours (every midnight):\n0 0 * * * /path/to/php /var/www/html/reset.php\nSee this reference for how crontab works: http://adminschoice.com/crontab-quick-reference, and this handy tool to build cron jobx: http://www.htmlbasix.com/crontab.shtml"
  },
  {
    "question": "How to log cron jobs? I want to know how I can see exactly what the cron jobs are doing on each execution. Where are the log files located? Or can I send the output to my email? I have set the email address to send the log when the cron job runs but I haven't received anything yet.",
    "answer": "* * * * * myjob.sh >> /var/log/myjob.log 2>&1\n\nwill log all output from the cron job to /var/log/myjob.log\nYou might use mail to send emails. Most systems will send unhandled cron job output by email to root or the corresponding user."
  },
  {
    "question": "Crontab Day of the Week syntax In crontab does the Day of the Week field run from 0 - 6 or 1 -7?\nI am seeing conflicting information on this.  wikipedia states 0-6 and other sites I have seen are 1-7.\nAlso what would be the implication or either using 0 or 7 incorrectly?  i.e. would the cron still run?",
    "answer": "- Sun      Sunday\n- Mon      Monday\n- Tue      Tuesday\n- Wed      Wednesday\n- Thu      Thursday\n- Fri      Friday\n- Sat      Saturday\n- Sun      Sunday\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute (0 - 59)\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hour (0 - 23)\n\u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500 day of month (1 - 31)\n\u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500 month (1 - 12)\n\u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500 day of week (0 - 6 => Sunday - Saturday, or\n\u2502 \u2502 \u2502 \u2502 \u2502                1 - 7 => Monday - Sunday)\n\u2193 \u2193 \u2193 \u2193 \u2193\n* * * * * command to be executed\n\n0 and 7 both stand for Sunday, you can use the one you want, so writing 0-6 or 1-7 has the same result.\nAlso, as suggested by @Henrik, it is possible to replace numbers by shortened name of days, such as MON, THU, etc:\n\nGraphically, * * * * * command to be executed stands for:\n\nminute\nhour\nday of month\nmonth\nday of week\n\n(0-59)\n(0-23)\n(1-31)\n(1-12)\n(1-7)\n\n*\n*\n*\n*\n*\ncommand to be executed\n\nOr using the old style:\n\nFinally, if you want to specify day by day, you can separate days with commas, for example SUN,MON,THU will exectute the command only on sundays, mondays on thursdays.\nYou can read further details in Wikipedia's article about Cron and check a cron expression online with crontab.guru."
  },
  {
    "question": "A cron job for rails: best practices? What's the best way to run scheduled tasks in a Rails environment? Script/runner? Rake? I would like to run the task every few minutes.",
    "answer": "task :cron => :environment do\nputs \"Pulling new requests...\"\nEdiListener.process_new_messages\nputs \"done.\"\nend\n\nI'm using the rake approach (as supported by heroku)\nWith a file called lib/tasks/cron.rake ..\n\nTo execute from the command line, this is just \"rake cron\". This command can then be put on the operating system cron/task scheduler as desired.\nUpdate this is quite an old question and answer! Some new info:\n\nthe heroku cron service I referenced has since been replaced by Heroku Scheduler\nfor frequent tasks (esp. where you want to avoid the Rails environment startup cost) my preferred approach is to use system cron to call a script that will either (a) poke a secure/private webhook API to invoke the required task in the background or (b) directly enqueue a task on your queuing system of choice"
  },
  {
    "question": "What is the Windows version of cron? A Google search turned up software that performs the same functions as cron, but nothing built into Windows.\nI'm running Windows XP Professional, but advice for any version of Windows would be potentially helpful to someone.\nIs there also a way to invoke this feature (which based on answers is called the Task Scheduler) programatically or via the command line?",
    "answer": "For newer Microsoft OS versions, Windows Server 2012 / Windows 8, look at the schtasks command line utility.\nIf using PowerShell, the Scheduled Tasks Cmdlets in Windows PowerShell are made for scripting.\nFor command-line usage before Windows 8, you can schedule with the AT command.\nFor the original question, asking about Windows XP (and Windows 7): Windows Task Scheduler"
  },
  {
    "question": "How would I get a cron job to run every 30 minutes? I'm looking to add a crontab entry to execute a script every 30 minutes, on the hour and 30 minutes past the hour or something close. I have the following, but it doesn't seem to run on 0.\n\n```\n*/30 * * * *\n\n```\n\nWhat string do I need to use?\nThe cron is running on OSX.",
    "answer": "0,30 * * * * your_command\n\nDo:"
  },
  {
    "question": "How to simulate the environment cron executes a script with? I normally have several problems with how cron executes scripts as they normally don't have my environment setup. Is there a way to invoke bash(?) in the same way cron does so I could test scripts before installing them?",
    "answer": "* * * * * env > ~/cronenv\nenv - `cat ~/cronenv` /bin/sh\n\nAdd this to your crontab (temporarily):\n\nAfter it runs, do this:\n\nThis assumes that your cron runs /bin/sh, which is the default regardless of the user's default shell.\nFootnote: if env contains more advanced config, eg PS1=$(__git_ps1 \" (%s)\")$, it will error cryptically env: \": No such file or directory."
  },
  {
    "question": "Test a weekly cron job I have a #!/bin/bash file in cron.week directory.\nIs there a way to test if it works? Can't wait 1 week\nI am on Debian 6 with root",
    "answer": "run-parts -v /etc/cron.weekly\nrun-parts /etc/cron.weekly -v\n\nJust do what cron does, run the following as root:\n\n... or the next one if you receive the \"Not a directory: -v\" error:\n\nOption -v prints the script names before they are run."
  },
  {
    "question": "Crontab - Run in directory I would like to set a job to run daily in the root crontab. But I would like it to execute it from a particular directory so it can find all the files it needs, since the application has a bunch of relative paths.\nAnyway, can I tell crontab to run from a particular directory?",
    "answer": "cd /path/to/directory && ./bin/myapp\n\nAll jobs are executed by a shell, so start that shell snippet by a command to change the directory.\n\nConcerning the use of && instead of ;: normally it doesn't make a difference, but if the cd command fails (e.g. because the directory doesn't exist) with && the application isn't executed, whereas with ; it's executed (but not in the intended directory)."
  },
  {
    "question": "How do I create a crontab through a script I need to add a cron job thru a script I run to set up a server. I am currently using Ubuntu. I can use crontab -e but that will open an editor to edit the current crontab. I want to do this programmatically.\nIs it possible to do so?",
    "answer": "Cron jobs usually are stored in a per-user file under /var/spool/cron\nThe simplest thing for you to do is probably just create a text file with the job configured, then copy it to the cron spool folder and make sure it has the right permissions (600)."
  },
  {
    "question": "How to run cron once, daily at 10pm I had entered:\n\n```\n* 22 * * * test > /dev/null\n\n```\n\nHowever, I am being notified via email that this is running every minute.\nI am confused I guess because I thought this was correct for what I am wanting.",
    "answer": "22 * * * ....\n\nIt's running every minute of the hour 22 I guess. Try the following to run it every first minute of the hour 22:"
  },
  {
    "question": "How to specify in crontab by what user to run script? I have few crontab jobs that run under root, but that gives me some problems. For example all folders created in process of that cron job are under user root and group root. \nHow can i make it to run under user www-data and group www-data so when i run scripts from my website i can manipulate those folders and files?\nMy server runs on Ubuntu.\nCurrent crontab job is:  \n\n```\n*/1 * * * * php5 /var/www/web/includes/crontab/queue_process.php >> /var/www/web/includes/crontab/queue.log 2>&1\n\n```",
    "answer": "Instead of creating a crontab to run as the root user, create a crontab for the user that you want to run the script.  In your case, crontab -u www-data -e will edit the crontab for the www-data user.  Just put your full command in there and remove it from the root user's crontab."
  },
  {
    "question": "Run cron job only if it isn&#39;t already running I'm trying to set up a cron job as a sort of watchdog for a daemon that I've created. If the daemon errors out and fails, I want the cron job to periodically restart it... I'm not sure how possible this is, but I read through a couple of cron tutorials and couldn't find anything that would do what I'm looking for...\nMy daemon gets started from a shell script, so I'm really just looking for a way to run a cron job ONLY if the previous run of that job isn't still running.\nI found this post, which did provide a solution for what I'm trying to do using lock files, not I'm not sure if there is a better way to do it...",
    "answer": "if ps -ef | grep -v grep | grep doctype.php ; then\nexit 0\nelse\n/home/user/bin/doctype.php >> /home/user/bin/spooler.log &\n\n/home/user/bin/simplemail.php \"Print spooler was not running...  Restarted.\"\nexit 0\nfi\n\nI do this for a print spooler program that I wrote, it's just a shell script:\n\nIt runs every two minutes and is quite effective.  I have it email me with special information if for some reason the process is not running."
  },
  {
    "question": "How to get CRON to call in the correct PATHs I'm trying to get cron to call in the correct PATHs. When I run a Python script from shell the script runs fine as it uses the PATHs set in bashrc but when I use cron all the PATHs are not used from bashrc. Is there a file I can enter the PATHs into for cron like bashrc or a way to call the PATHs from bashrc?\nSorry I don't think I worded this correctly, I can get the correct script to run (meaning the PATH to the script in crontab is not the problem here), it's just when that script is running I run a build and this uses the PATHs set in .bashrc. When I run the script when I'm logged in, the .bashrc PATHs are pulled in. Since cron doesn't run in a shell per say it does not pull in .bashrc. Is there a way of pulling this in without having to write a bash script wrapper?",
    "answer": "This has the username field, as used by /etc/crontab.\n\nSHELL=/bin/sh\nPATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin\n6 * * *   root    run-parts --report /etc/cron.daily\n6 * * 7   root    run-parts --report /etc/cron.weekly\n6 1 * *   root    run-parts --report /etc/cron.monthly\n01 * * 1-5 root python /path/to/file.py\n\nI used /etc/crontab. I used vi and entered in the PATHs I needed into this file and ran it as root. The normal crontab overwrites PATHs that you have set up. A good tutorial on how to do this.\nThe systemwide cron file looks like this:"
  },
  {
    "question": "How to automatically remove completed Kubernetes Jobs created by a CronJob? Is there a way to automatically remove completed Jobs besides making a CronJob to clean up completed Jobs?\nThe K8s Job Documentation states that the intended behavior of completed Jobs is for them to remain in a completed state until manually deleted. Because I am running thousands of Jobs a day via CronJobs and I don't want to keep completed Jobs around.",
    "answer": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\nname: hello\nspec:\nschedule: \"*/1 * * * *\"\nsuccessfulJobsHistoryLimit: 0\nfailedJobsHistoryLimit: 0\njobTemplate:\nspec:\ntemplate:\nspec:\ncontainers:\n- name: hello\nimage: busybox\nargs:\n- /bin/sh\n- -c\n- date; echo Hello from the Kubernetes cluster\nrestartPolicy: OnFailure\n\nYou can now set history limits, or disable history altogether, so that failed or successful CronJobs are not kept around indefinitely.  See my answer here. Documentation is here.\nTo set the history limits:\n\nThe .spec.successfulJobsHistoryLimit and .spec.failedJobsHistoryLimit fields are optional. These fields specify how many completed and failed jobs should be kept. By default, they are set to 3 and 1 respectively. Setting a limit to 0 corresponds to keeping none of the corresponding kind of jobs after they finish.\n\nThe config with 0 limits would look like:"
  },
  {
    "question": "How to run a script in the background even after I logout SSH? I have Python script bgservice.py and I want it to run all the time, because it is part of the web service I build. How can I make it run continuously even after I logout SSH?",
    "answer": "Run nohup python bgservice.py & to get the script to ignore the hangup signal and keep running. Output will be put in nohup.out.\nIdeally, you'd run your script with something like supervise so that it can be restarted if (when) it dies."
  },
  {
    "question": "How to write a cron that will run a script every day at midnight? I have heard crontab is a good choice, but how do I write the line and where do I put it on the server?",
    "answer": "00 * * * ruby path/to/your/script.rb\nSyntax:\nmm hh dd mt wd  command\n\nmm minute 0-59\nhh hour 0-23\ndd day of month 1-31\nmt month 1-12\nwd day of week 0-7 (Sunday = 0 or 7)\ncommand: what you want to run\nall numeric values can be replaced by * which means all\n\nHere's a good tutorial on what crontab is and how to use it on Ubuntu. Your crontab line will look something like this:\n\n(00 00 indicates midnight--0 minutes and 0 hours--and the *s mean every day of every month.)"
  },
  {
    "question": "How to schedule a function to run every hour on Flask? I have a Flask web hosting with no access to cron command.\nHow can I execute some Python function every hour?",
    "answer": "import time\nimport atexit\n\nfrom apscheduler.schedulers.background import BackgroundScheduler\n\ndef print_date_time():\nprint(time.strftime(\"%A, %d. %B %Y %I:%M:%S %p\"))\n\nscheduler = BackgroundScheduler()\nscheduler.add_job(func=print_date_time, trigger=\"interval\", seconds=60)\nscheduler.start()\n\natexit.register(lambda: scheduler.shutdown())\n\nYou can use BackgroundScheduler() from APScheduler package (v3.5.3):\n\nNote that two of these schedulers will be launched when Flask is in debug mode. For more information, check out this question."
  },
  {
    "question": "CRON job to run on the last day of the month I need to create a CRON job that will run on the last day of every month.\nI will create it using cPanel.\nAny help is appreciated. \nThanks",
    "answer": "23 30 4,6,9,11        * myjob.sh\n23 31 1,3,5,7,8,10,12 * myjob.sh\n23 28 2               * myjob.sh\n0 1 * * myjob.sh\n23 28-31 * * [[ \"$(date --date=tomorrow +\\%d)\" == \"01\" ]] && myjob.sh\nint main (void) {\n// Get today, somewhere around midday (no DST issues).\n\ntime_t noonish = time (0);\nstruct tm *localtm = localtime (&noonish);\nlocaltm->tm_hour = 12;\n\n// Add one day (86,400 seconds).\n\nnoonish = mktime (localtm) + 86400;\nlocaltm = localtime (&noonish);\n\n// Output just day of month.\n\nprintf (\"%d\\n\", localtm->tm_mday);\n\nreturn 0;\n}\n23 28-31 * * [[ \"$(tomdom)\" == \"1\" ]] && myjob.sh\n\nPossibly the easiest way is to simply do three separate jobs:\n\nThat will run on the 28th of February though, even on leap years so, if that's a problem, you'll need to find another way.\n\nHowever, it's usually both substantially easier and correct to run the job as soon as possible on the first day of each month, with something like:\n\nand modify the script to process the previous month's data.\nThis removes any hassles you may encounter with figuring out which day is the last of the month, and also ensures that all data for that month is available, assuming you're processing data. Running at five minutes to midnight on the last day of the month may see you missing anything that happens between then and midnight.\nThis is the usual way to do it anyway, for most end-of-month jobs.\n\nIf you still really want to run it on the last day of the month, one option is to simply detect if tomorrow is the first (either as part of your script, or in the crontab itself).\nSo, something like:\n\nshould be a good start, assuming you have a relatively intelligent date program.\nIf your date program isn't quite advanced enough to give you relative dates, you can just put together a very simple program to give you tomorrow's day of the month (you don't need the full power of date), such as:\n\nand then use (assuming you've called it tomdom for \"tomorrow's day of month\"):\n\nThough you may want to consider adding error checking since both time() and mktime() can return -1 if something goes wrong. The code above, for reasons of simplicity, does not take that into account."
  },
  {
    "question": "Spring cron vs normal cron? I'm trying to get a cron job working within a legacy Java/Spring/Hibernate project, so I decided to use the spring scheduler.\nI want myTask.doStuff to run at 12:00 on the first Sunday of every month. \nIn my application-context.xml I've configured my task scheduler like: \n\n```\n<task:scheduled-tasks scheduler=\"MyTaskScheduler\">\n    <task:scheduled ref=\"myTask\" method=\"doStuff\" cron=\"0 0 12 ? 1/1 SUN#1 *\"/> <!-- Every first Sundy of the month -->\n</task:scheduled-tasks>\n\n<task:scheduler id=\"MyTaskScheduler\" pool-size=\"10\"/>\n\n```\n\nwith the problem cron expression itself being the: 0 0 12 ? 1/1 SUN#1 *\nand myTask is a bean, which has a method called doStuff that works perfectly when run from unit tests. \nWhen I build and deploy I get a bootime exception from spring: \n\n```\nCaused by: java.lang.IllegalArgumentException: cron expression must consist of 6 fields (found 7 in 0 0 12 ? 1/1 SUN#1 *)\nat org.springframework.scheduling.support.CronSequenceGenerator.parse(CronSequenceGenerator.java:233)\nat org.springframework.scheduling.support.CronSequenceGenerator.<init>(CronSequenceGenerator.java:81)\nat org.springframework.scheduling.support.CronTrigger.<init>(CronTrigger.java:54)\nat org.springframework.scheduling.support.CronTrigger.<init>(CronTrigger.java:44)\nat org.springframework.scheduling.config.ScheduledTaskRegistrar.afterPropertiesSet(ScheduledTaskRegistrar.java:129)\nat org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1477)\nat org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1417)\n\n```\n\nGiven that i'm using cron expressions for the first time, my first assumption was that I was doing something wrong, but I double checked using cronmaker and it gave me the same result. \nAll the documentations says: A cron expression is a string consisting of six or seven subexpressions (fields).1\ndespite this I tried knocking off the 7th element(year) since it's not in any of the examples, and got a different error message: \n\n```\norg.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.scheduling.config.ScheduledTaskRegistrar#0': Invocation of init method failed; nested exception is java.lang.NumberFormatException: For input string: \"0#1\"\n\n```\n\n... does org.springframework.scheduling support a different flavor of cron from everything else? the spring-specific documentation just says 'cron expressions'. \nHow can I get this cron expression to work as expected in this context? Any help at all would be appreciated.\nAt the moment my solution would be to simplify this expression to just run every Sunday, and prepend some Java logic to calculate which Sunday of the month it is, and see if that works - but that sort of defeats the purpose of the configuration approach and seems like an antipattern.",
    "answer": "2 3 4 5 6 Index\n- - - - - -\n* * * * * * command to be executed\n- - - - - -\n| | | | | |\n| | | | | ------- Day of week (MON - SUN)\n| | | | --------- Month (1 - 12)\n| | | ----------- Day of month (1 - 31)\n| |-------------- Hour (0 - 23)\n| --------------- Minute (0 - 59)\n----------------- Seconds (0 - 59)\n2 3 4 5 Index\n- - - - -\n* * * * * command to be executed\n- - - - -\n| | | | |\n| | | | ----- Day of week (0 - 7) (Sunday=0 or 7)\n| | | ------- Month (1 - 12)\n| | --------- Day of month (1 - 31)\n| ----------- Hour (0 - 23)\n------------- Minute (0 - 59)\n('.select2').select2({\nwidth: '100%'\n});\n\n//// Init ////////////\ndropdown = $(\"#secondsSelect\");\nfor (let i = 1; i < 60; i++) {\ndropdown.append($(\"<option />\").val(i).text(i));\n}\ndropdown = $(\"#minSelect\");\nfor (let i = 1; i < 60; i++) {\ndropdown.append($(\"<option />\").val(i).text(i));\n}\ndropdown = $(\"#hoursSelect\");\nfor (let i = 1; i < 24; i++) {\ndropdown.append($(\"<option />\").val(i).text(i));\n}\ndropdown = $(\"#dayOfMonthSelect\");\nfor (let i = 1; i < 32; i++) {\ndropdown.append($(\"<option />\").val(i).text(i));\n}\n//// Init End ////////////\n('.select2').on('select2:select', function(e) {\nlet value = e.params.data.id;\nlet prevValue = $(this).val().length > 0 ? $(this).val()[0] : null;\n\nif (value != parseInt(value)) {\n(this).val(value).change();\n} else if (prevValue != parseInt(prevValue)) {\n(this).val(value).change();\n}\ncalculateSpringCron();\n});\n\nlet r = function(dropdown) {\nreturn dropdown.val().join(\",\");\n\n}\n\nlet calculateSpringCron = function() {\n\nlet result = [\nr($(\"#secondsSelect\")),\nr($(\"#minSelect\")),\nr($(\"#hoursSelect\")),\nr($(\"#dayOfMonthSelect\")),\nr($(\"#monthsSelect\")),\nr($(\"#weekdaySelect\")),\n];\n(\"#result\").val(result.join(\" \"));\n(\"#result-expand\").html(result.join(\" &nbsp; &nbsp;\"))\n\n}\n\ncalculateSpringCron();\n.ms-container {\ndisplay: flex;\nflex-direction: column;\nwidth: 100%;\npadding-left: 3em;\npadding-right: 3em;\nbackground: none !important;\npadding-bottom: 5em;\n}\n<link href=\"https://cdnjs.cloudflare.com/ajax/libs/multi-select/0.9.12/css/multi-select.min.css\" rel=\"stylesheet\" />\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/multi-select/0.9.12/js/jquery.multi-select.min.js\"></script>\n<link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css\" integrity=\"sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T\" crossorigin=\"anonymous\">\n\n<script src=\"https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js\" integrity=\"sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM\" crossorigin=\"anonymous\"></script>\n\n<link href=\"https://cdnjs.cloudflare.com/ajax/libs/select2/4.0.8/css/select2.min.css\" rel=\"stylesheet\" />\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/select2/4.0.8/js/select2.min.js\"></script>\n\n<div class=\"row\">\n<div class=\"col-12\">\n<h1>Spring Schedule Cron Generator</h1>\n</div>\n</div>\n<div class=\"row\">\n<div class=\"col-4\">\nSeconds:\n<select id=\"secondsSelect\" class=\"select2\" name=\"states[]\" multiple=\"multiple\">\n<option value=\"*\" selected=\"selected\">Every seconds</option>\n<option value=\"*/2\">Every even seconds</option>\n<option value=\"1-59/2\">Every odd seconds</option>\n<option value=\"*/5\">Every 5 seconds</option>\n<option value=\"*/10\">Every 10 seconds</option>\n<option value=\"*/15\">Every 15 seconds</option>\n<option value=\"*/30\">Every 30 seconds</option>\n</select>\n</div>\n<div class=\"col-4\">\nMinutes:\n<select id=\"minSelect\" class=\"select2\" name=\"states[]\" multiple=\"multiple\">\n<option value=\"*\" selected=\"selected\">Every minutes</option>\n<option value=\"*/2\">Every even minutes</option>\n<option value=\"1-59/2\">Every odd minutes</option>\n<option value=\"*/5\">Every 5 minutes</option>\n<option value=\"*/10\">Every 10 minutes</option>\n<option value=\"*/15\">Every 15 minutes</option>\n<option value=\"*/30\">Every 30 minutes</option>\n</select>\n</div>\n<div class=\"col-4\">\nHours:\n<select id=\"hoursSelect\" class=\"select2\" name=\"states[]\" multiple=\"multiple\">\n<option value=\"*\" selected=\"selected\">Every hour</option>\n<option value=\"*/2\">Every even hour</option>\n<option value=\"1-11/2\">Every odd hour</option>\n<option value=\"*/3\">Every 3 hour</option>\n<option value=\"*/4\">Every 4 hour</option>\n<option value=\"*/6\">Every 6 hour</option>\n</select>\n</div>\n</div>\n\n<div class=\"row\">\n</div>\n\n<div class=\"row\">\n<div class=\"col-4\">\nDays of month:\n<select id=\"dayOfMonthSelect\" class=\"select2\" name=\"states[]\" multiple=\"multiple\">\n<option value=\"*\" selected=\"selected\">Every day of month</option>\n<option value=\"*/2\">Even day of month</option>\n<option value=\"1-31/2\">Odd day of month</option>\n<option value=\"*/5\">Every 5 days of month (5,10,15...)</option>\n<option value=\"*/10\">Every 10 days of month (10,20,30...)</option>\n</select>\n</div>\n<div class=\"col-4\">\nMonths:\n<select id=\"monthsSelect\" class=\"select2\" name=\"states[]\" multiple=\"multiple\">\n<option value=\"*\" selected=\"selected\">Every month</option>\n<option value=\"*/2\">Even months</option>\n<option value=\"1-11/2\">Odd months</option>\n<option value=\"*/4\">Every 4 months</option>\n<option value=\"*/6\">Every 6 months(half year)</option>\n<option value=\"1\">Jan</option>\n<option value=\"2\">Feb</option>\n<option value=\"3\">Mar</option>\n<option value=\"4\">Apr</option>\n<option value=\"5\">May</option>\n<option value=\"6\">Jun</option>\n<option value=\"7\">Jul</option>\n<option value=\"8\">Aug</option>\n<option value=\"9\">Sep</option>\n<option value=\"10\">Oct</option>\n<option value=\"11\">Nov</option>\n<option value=\"12\">Dec</option>\n</select>\n</div>\n<div class=\"col-4\">\nWeekday:\n<select id=\"weekdaySelect\" class=\"select2\" name=\"states[]\" multiple=\"multiple\">\n<option value=\"*\" selected=\"selected\">Every weekday</option>\n<option value=\"MON-FRI\">Weekdays (MON-FRI)</option>\n<option value=\"SAT,SUN\">Weekend</option>\n<option value=\"SUN\">SUN</option>\n<option value=\"MON\">MON</option>\n<option value=\"TUE\">TUE</option>\n<option value=\"WED\">WED</option>\n<option value=\"THU\">THU</option>\n<option value=\"FRI\">FRI</option>\n<option value=\"SAT\">SAT</option>\n</select>\n</div>\n</div>\n<div class=\"row\">\n<div class=\"col-12\">\nResult:\n<input id=\"result\" class=\"form-control\" /> With a bit of seperation for better viewing:<br/>\n<h1 id=\"result-expand\"></h1>\n</div>\n</div>\n\nTaking some note from: https://www.baeldung.com/cron-expressions\nA Spring Scheduled tasks is like this:\n\nFrom: https://www.cyberciti.biz/faq/how-do-i-add-jobs-to-cron-under-linux-or-unix-oses/\nA Linux Cron job is like this:\n\nSide note:\n\nSome article said it is possible to have a 7 optional param which is year , I have tried using latest spring and it show error, so I don't think it is working.\nIf your Linux cron job expression is simple enough, seems like it is possible to just put an 0 in front and it will convert to the spring scheduled tasks expression\n\nE.g. Every 5 minutes\n\n*/5 * * * * Linux cron job\n0 */5 * * * * Spring schedule tasks\n\nBonus: Spring Schedule Cron Generator\n\nClick on Show code snippet\nClick on Run Code snippet\nHave fun!"
  },
  {
    "question": "How can I programmatically create a new cron job? I want to be able to programatically add a new cron job, what is the best way to do this?\nFrom my research, it seems I could dump the current crontab and then append a new one, piping that back into crontab:\n\n```\n(crontab -l ; echo \"0 * * * * wget -O - -q http://www.example.com/cron.php\") | crontab -\n\n```\n\nIs there a better way?",
    "answer": "The best way if you're running as root, is to drop a file into /etc/cron.d\nif you use a package manager to package your software, you can simply lay down files in that directory and they are interpreted as if they were crontabs, but with an extra field for the username, e.g.:\nFilename: /etc/cron.d/per_minute\nContent:\n* * * * * root /bin/sh /home/root/script.sh"
  },
  {
    "question": "Cron jobs and random times, within given hours I need the ability to run a PHP script 20 times a day at completely random times. I also want it to run only between 9am - 11pm.\nI'm familiar with creating cron jobs in linux.",
    "answer": "9 * * * /path/to/bashscript\nmaxdelay=$((14*60))  # 14 hours from 9am to 11pm, converted to minutes\nfor ((i=1; i<=20; i++)); do\ndelay=$(($RANDOM%maxdelay)) # pick an independent random delay for each of the 20 runs\n(sleep $((delay*60)); /path/to/phpscript.php) & # background a subshell to wait, then run the php script\ndone\n\nIf I understand what you're looking for, you'll need to do something a bit messy, like having a cron job that runs a bash script that randomizes the run times...  Something like this:\ncrontab:\n\nand in /path/to/bashscript:\n\nA few notes: this approach it a little wasteful of resources, as it fires off 20 background processes at 9am, each of which waits around for a random number of minutes (up to 14 hours, i.e. 11pm), then launches the php script and exits.  Also, since it uses a random number of minutes (not seconds), the start times aren't quite as random as they could be.  But $RANDOM only goes up to 32,767, and there are 50,400 seconds between 9am and 11pm, it'd be a little more complicated to randomize the seconds as well.  Finally, since the start times are random and independent of each other, it's possible (but not very likely) that two or more instances of the script will be started simultaneously."
  },
  {
    "question": "Running a cron job on Linux every six hours How can I run command every six hours every day?\nI tried the following, but it did not work:\n\n```\n/6 * * * * *  mycommand\n\n```",
    "answer": "*/6 * * * /path/to/mycommand\n0,6,12,18 * * * /path/to/mycommand\n\nYou forgot a *, and you've too many fields. It's the hour you need to care about\n\nThis means every sixth hour starting from 0, i.e. at hour 0, 6, 12 and 18 which you could write as"
  },
  {
    "question": "Cron job every three days Is it possible to run a cronjob every three days? Or maybe 10 times/month.",
    "answer": "0 */3 * *\nif (((date('j') - 1) % 3))\nexit();\n\nRun it every three days - or less at the end of the month. (It'll run 2 days in a row if the previous month had 31 days.)\n\nHow about that?\nIf you want it to run on specific days of the month, like the 1st, 4th, 7th, etc... then you can just have a conditional in your script that checks for the current day of the month.\n\nor, as @mario points out, you can use date('k') to get the day of the year instead of doing it based on the day of the month."
  },
  {
    "question": "I need a Nodejs scheduler that allows for tasks at different intervals I am looking for a node job schedule that will allow me to schedule a number of tasks at different intervals. For instance, \n\ncall function A every 30 seconds\ncall function B every 60 seconds\ncall function C every 7 days\n\nI also want to be able to start and stop the process.\nSo far, I have looked at:\n\nlater - the syntax confuses me, also apparently you cant schedule tasks beyond a month\nagenda- seems the most promising, however I'm confused about the database functionality\ntimeplan - too simple, can't start and stop\n\nI find the syntax of the latter confusing.",
    "answer": "'* * * * * *' - runs every second\n'*/5 * * * * *' - runs every 5 seconds\n'10,20,30 * * * * *' - run at 10th, 20th and 30th second of every minute\n'0 * * * * *' - runs every minute\n'0 0 * * * *' - runs every hour (at 0 minutes and 0 seconds)\n'00 30 11 * * 1-5' - Runs every weekday (Monday through Friday) at 11:30:00 AM. It does not run on Saturday or Sunday.\nvar cron = require('cron');\nvar cronJob = cron.job(\"0 */10 * * * *\", function(){\n// perform operation e.g. GET request http.get() etc.\nconsole.info('cron job completed');\n});\ncronJob.start();\n\nI would recommend node-cron. It allows to run tasks using Cron patterns e.g.\n\nBut also more complex schedules e.g.\n\nSample code: running job every 10 minutes:\n\nYou can find more examples in node-cron wiki\nMore on cron configuration can be found on cron wiki\nI've been using that library in many projects and it does the job. I hope that will help."
  },
  {
    "question": "How do I set a task to run every so often? How do I have a script run every, say 30 minutes?  I assume there are different ways for different OSs.  I'm using OS X.",
    "answer": "/Library/LaunchDaemons/\n/Library/LaunchAgents/\n~/Library/LaunchAgents/\ncom.example.my-fancy-task.plist\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE plist PUBLIC \"-//Apple Computer//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n<plist version=\"1.0\">\n<dict>\n<key>Label</key>\n<string>com.example.my-fancy-task</string>\n<key>OnDemand</key>\n<true/>\n<key>ProgramArguments</key>\n<array>\n<string>/bin/sh</string>\n<string>/usr/local/bin/my-script.sh</string>\n</array>\n<key>StartInterval</key>\n<integer>1800</integer>\n</dict>\n</plist>\nlaunchctl <command> <parameter>\n\nJust use launchd. It is a very powerful launcher system and meanwhile it is the standard launcher system for Mac OS X (current OS X version wouldn't even boot without it). For those who are not familiar with launchd (or with OS X in general), it is like a crossbreed between init, cron, at, SysVinit (init.d), inetd, upstart and systemd. Borrowing concepts of all these projects, yet also offering things you may not find elsewhere.\nEvery service/task is a file. The location of the file depends on the questions: \"When is this service supposed to run?\" and \"Which privileges will the service require?\"\nSystem tasks go to\n\nif they shall run no matter if any user is logged in to the system or not. They will be started with \"root\" privileges.\nIf they shall only run if any user is logged in, they go to\n\nand will be executed with the privileges of the user that just logged in.\nIf they shall run only if you are logged in, they go to\n\nwhere ~ is your HOME directory. These task will run with your privileges, just as if you had started them yourself by command line or by double clicking a file in Finder.\nNote that there also exists /System/Library/LaunchDaemons and /System/Library/LaunchAgents, but as usual, everything under /System is managed by OS X. You shall not place any files there, you shall not change any files there, unless you really know what you are doing. Messing around in the Systems folder can make your system unusable (get it into a state where it will even refuse to boot up again). These are the directories where Apple places the launchd tasks that get your system up and running during boot, automatically start services as required, perform system maintenance tasks, and so on.\nEvery launchd task is a file in PLIST format. It should have reverse domain name notation. E.g. you can name your task\n\nThis plist can have various options and settings. Writing one per hand is not for beginners, so you may want to get a tool like LaunchControl (commercial, $18) or Lingon (commercial, $14.99) to create your tasks.\nJust as an example, it could look like this\n\nThis agent will run the shell script /usr/local/bin/my-script.sh every 1800 seconds (every 30 minutes). You can also have task run on certain dates/times (basically launchd can do everything cron can do) or you can even disable \"OnDemand\" causing launchd to keep the process permanently running (if it quits or crashes, launchd will immediately restart it). You can even limit how much resources a process may use.\nUpdate: Even though OnDemand is still supported, it is deprecated. The new setting is named KeepAlive, which makes much more sense. It can have a boolean value, in which case it is the exact opposite of OnDemand (setting it to false behaves as if OnDemand is true and the other way round). The great new feature is, that it can also have a dictionary value instead of a boolean one. If it has a dictionary value, you have a couple of extra options that give you more fine grain control under which circumstances the task shall be kept alive. E.g. it is only kept alive as long as the program terminated with an exit code of zero, only as long as a certain file/directory on disk exists, only if another task is also alive, or only if the network is currently up.\nAlso you can manually enable/disable tasks via command line:\n\ncommand can be load or unload, to load a plist or unload it again, in which case parameter is the path to the file. Or command can be start or stop, to just start or stop such a task, in which case parameter is the label (com.example.my-fancy-task). Other commands and options exist as well.\nUpdate: Even though load, unload, start, and stop do still work, they are legacy now. The new commands are bootstrap, bootout, enable, and disable with slightly different syntax and options. One big difference is that disable is persistent, so once a service has been disabled, it will stay disabled, even across reboots until you enable it again. Also you can use kickstart to run a task immediately, regardless how it has been configured to run.\nThe main difference between the new and the old commands is that they separate tasks by \"domain\". The system has domain and so has every user. So equally labeled tasks may exist in different domains and launchctl can still distinguish them. Even different login and different UI sessions of the same user have their own domain (e.g. the same user may once be logged locally and once remote via SSH and different tasks may run for either session) and so does every single running processes. Thus instead of com.example.my-fancy-task, you now would use system/com.example.my-fancy-task or user/501/com.example.my-fancy-task to identify a task, with 501 being the user ID of a specific user.\nSee documentation of the plist format and of the launchctl command line tool."
  },
  {
    "question": "How to run a cronjob every X minutes? I'm running a PHP script in a cronjob and I want to send emails every 5 minutes\nMy current (crontab) cronjob:\n\n```\n10 * * * * /usr/bin/php /mydomain.in/cromail.php > /dev/null 2>&1\n\n```\n\nThe cronmail.php is as follows:\n\n```\n<?php\n$from = 'D'; // sender\n$subject = 'S';\n$message = 'M';\n$message = wordwrap($message, 70);\nmail(\"myemail@gmail.com\", $subject, $message, \"From: $from\\n\");\n?>\n\n```\n\nBut I've not received an email in 30 minutes with this configuration.",
    "answer": "* * * * blah\n*/5 * * * * blah\n0,5,10,15,20,25,30,35,40,45,50,55 * * * * blah\ndate >>/tmp/debug_cron_pax.txt\n\nIn a crontab file, the fields are:\n\nminute of the hour.\nhour of the day.\nday of the month.\nmonth of the year.\nday of the week.\n\nSo:\n\nmeans execute blah at 10 minutes past every hour.\nIf you want every five minutes, use either:\n\nmeaning every minute but only every fifth one, or:\n\nfor older cron executables that don't understand the */x notation.\nIf it still seems to be not working after that, change the command to something like:\n\nand monitor that file to ensure something's being written every five minutes. If so, there's something wrong with your PHP scripts. If not, there's something wrong with your cron daemon."
  },
  {
    "question": "How to conditionally enable or disable scheduled jobs in Spring? I am defining scheduled jobs with cron style patterns in Spring, using the @Scheduled annotation.\nThe cron pattern is stored in a config properties file. Actually there are two properties files: one default config, and one profile config that is environment dependent (e.g. dev, test, prod customer 1, prod customer 2 etc.) and overrides some of the default values.\nI configured a property placeholder bean in my spring context which allows me to use ${} style placeholders to import values from my properties files.\nThe job beans looks like this:\n\n```\n@Component\npublic class ImagesPurgeJob implements Job {\n\n    private Logger logger = Logger.getLogger(this.getClass());\n\n    @Override\n    @Transactional(readOnly=true)\n    @Scheduled(cron = \"${jobs.mediafiles.imagesPurgeJob.schedule}\")\n    public void execute() {\n        //Do something\n            //can use DAO or other autowired beans here\n    }\n}\n\n```\n\nRelevant parts of my context XML :\n\n```\n<!-- Enable configuration of scheduled tasks via annotations -->\n    <task:annotation-driven/>\n\n<!-- Load configuration files and allow '${}' style placeholders -->\n    <bean class=\"org.springframework.context.support.PropertySourcesPlaceholderConfigurer\">\n        <property name=\"locations\">\n            <list>\n                <value>classpath:config/default-config.properties</value>\n                <value>classpath:config/environment-config.properties</value>\n            </list>\n        </property>\n        <property name=\"ignoreUnresolvablePlaceholders\" value=\"true\"/>\n        <property name=\"ignoreResourceNotFound\" value=\"false\"/>\n    </bean>\n\n```\n\nI really like this. It's quite simple and clean with minimal XML.\nHowever I have one more requirement: some of these jobs can be totally disabled in some cases.\nSo, before I used Spring to manage them I created them manually and there is a boolean parameter along with the cron parameter in the config files, to specify if the job has to be enabled or not:\n\n```\njobs.mediafiles.imagesPurgeJob.enable=true or false\njobs.mediafiles.imagesPurgeJob.schedule=0 0 0/12 * * ?\n\n```\n\nHow can I use this parameter in Spring to conditionally create or just plainly ignore the bean, depending on this config parameter?\nOne obvious workaround would be to define a cron pattern that would never evaluate, so the job is never executed. But the bean would still be created and the config would be a bit obscure, so I feel there must be a better solution.",
    "answer": "@Component\npublic class ImagesPurgeJob implements Job {\n\nprivate Logger logger = Logger.getLogger(this.getClass());\n\n@Value(\"${jobs.mediafiles.imagesPurgeJob.enable}\")\nprivate boolean imagesPurgeJobEnable;\n\n@Override\n@Transactional(readOnly=true)\n@Scheduled(cron = \"${jobs.mediafiles.imagesPurgeJob.schedule}\")\npublic void execute() {\n\n//Do something\n//can use DAO or other autowired beans here\nif(imagesPurgeJobEnable){\n\nDo your conditional job here...\n\n}\n}\n}"
  },
  {
    "question": "mysqldump &amp; gzip commands to properly create a compressed file of a MySQL database using crontab I am having problems with getting a crontab to work. I want to automate a MySQL database backup.\nThe setup:\n\nDebian GNU/Linux 7.3 (wheezy) \nMySQL Server version: 5.5.33-0+wheezy1(Debian) \ndirectories user, backup and backup2 have 755 permission\nThe user names for MySQL db and Debian account are the same\n\nFrom the shell this command works\n\n```\nmysqldump -u user -p[user_password] [database_name] | gzip > dumpfilename.sql.gz\n\n```\n\nWhen I place this in a crontab using crontab -e \n\n```\n* * /usr/bin/mysqldump -u user -pupasswd mydatabase | gzip> /home/user/backup/mydatabase-backup-`date +\\%m\\%d_\\%Y`.sql.gz >/dev/null 2>&1\n\n```\n\nA file is created every minute in /home/user/backup directory, but has 0 bytes.\nHowever when I redirect this output to a second directory, backup2, I note that the proper mysqldumpfile duly compressed is created in it. I am unable to figure what is the mistake that I am making that results in a 0 byte file in the first directory and the expected output in the second directory.\n\n```\n* * /usr/bin/mysqldump -u user -pupasswd my-database | gzip> /home/user/backup/mydatabase-backup-`date +\\%m\\%d_\\%Y`.sql.gz >/home/user/backup2/mydatabase-backup-`date +\\%m\\%d_\\%Y`.sql.gz 2>&1\n\n```\n\nI would greatly appreciate an explanation.\nThanks",
    "answer": "mysqldump -u user -pupasswd my-database | gzip > one.gz > two.gz > three.gz\n> ls -l\n-rw-r--r--  1 uname  grp     0 Mar  9 00:37 one.gz\n-rw-r--r--  1 uname  grp  1246 Mar  9 00:37 three.gz\n-rw-r--r--  1 uname  grp     0 Mar  9 00:37 two.gz\nmysqldump -u user -pupasswd my-database | gzip -c > one.gz; gzip -c one.gz > two.gz; gzip -c two.gz > three.gz\n> ls -l\n-rw-r--r--  1 uname  grp  1246 Mar  9 00:44 one.gz\n-rw-r--r--  1 uname  grp  1306 Mar  9 00:44 three.gz\n-rw-r--r--  1 uname  grp  1276 Mar  9 00:44 two.gz\n\nFirst the mysqldump command is executed and the output generated is redirected using the pipe. The pipe is sending the standard output into the gzip command as standard input. Following the filename.gz, is the output redirection operator (>) which is going to continue redirecting the data until the last filename, which is where the data will be saved.\nFor example, this command will dump the database and run it through gzip and the data will finally land in three.gz\n\nMy original answer is an example of redirecting the database dump to many compressed files (without double compressing). (Since I scanned the question and seriously missed - sorry about that)\nThis is an example of recompressing files:\n\nThis is a good resource explaining I/O redirection: http://www.codecoffee.com/tipsforlinux/articles2/042.html"
  },
  {
    "question": "CronJob not running I have set up a cronjob for root user in ubuntu environment as follows by typing crontab -e\n\n```\n  34 11 * * * sh /srv/www/live/CronJobs/daily.sh\n  0 08 * * 2 sh /srv/www/live/CronJobs/weekly.sh\n  0 08 1 * * sh /srv/www/live/CronJobs/monthly.sh\n\n```\n\nBut the cronjob does not run. I have tried checking if the cronjob is running using pgrep cron and that gives process id 3033. The shell script calls a python file and is used to send an email. Running the python file is ok. There's no error in it but the cron doesn't run. The daily.sh file has the following code in it.\n\n```\npython /srv/www/live/CronJobs/daily.py\npython /srv/www/live/CronJobs/notification_email.py\npython /srv/www/live/CronJobs/log_kpi.py\n\n```",
    "answer": "import os\nimport sys\nimport time, datetime\n\nCLASS_PATH = '/srv/www/live/mainapp/classes'\nSETTINGS_PATH = '/srv/www/live/foodtrade'\nsys.path.insert(0, CLASS_PATH)\nsys.path.insert(1,SETTINGS_PATH)\n\nimport other_py_files\n\nThe solution for me was to avoid relative paths. Never use relative path in Python scripts to be executed via crontab.\nI did something like this instead:\n\nAlso, never suppress the crontab code; instead use mailserver and check the mail for the user. That gives clearer insights of what is going."
  },
  {
    "question": "How to redirect output of systemd service to a file I am trying to redirect output of a systemd service to a file but it doesn't seem to work:  \n\n```\n[Unit]\nDescription=customprocess\nAfter=network.target\n\n[Service]\nType=forking\nExecStart=/usr/local/bin/binary1 agent -config-dir /etc/sample.d/server\nStandardOutput=/var/log1.log\nStandardError=/var/log2.log\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n\n```\n\nPlease correct my approach.",
    "answer": "StandardOutput=syslog\nStandardError=syslog\nSyslogIdentifier=<your program identifier> # without any quote\nif $programname == '<your program identifier>' then /path/to/log/file.log\n& stop\n-rw-r----- 1 syslog adm 439K Mar  5 19:35 /var/log/syslog\n\nI think there's a more elegant way to solve the problem: send the stdout/stderr to syslog with an identifier and instruct your syslog manager to split its output by program name.\nUse the following properties in your systemd service unit file:\n\nThen, assuming your distribution is using rsyslog to manage syslogs, create a file in /etc/rsyslog.d/<new_file>.conf with the following content:\n\nNow make the log file writable by syslog:\n\nRestart rsyslog (sudo systemctl restart rsyslog) and enjoy! Your program stdout/stderr will still be available through journalctl (sudo journalctl -u <your program identifier>) but they will also be available in your file of choice.\nSource via archive.org"
  },
  {
    "question": "How can I use iptables on centos 7? I installed CentOS 7 with minimal configuration (os + dev tools). I am trying to open 80 port for httpd service, but something wrong with my iptables service ... what's wrong with it? What am I doing wrong? \n\n```\n# ifconfig/sbin/service iptables save\nbash: ifconfig/sbin/service: No such file or directory\n\n\n# /sbin/service iptables save\nThe service command supports only basic LSB actions (start, stop, restart, try-restart, reload, force-reload, status). For other actions, please try to use systemctl.\n\n# sudo service iptables status\nRedirecting to /bin/systemctl status  iptables.service\niptables.service\n   Loaded: not-found (Reason: No such file or directory)\n   Active: inactive (dead)\n\n# /sbin/service iptables save\nThe service command supports only basic LSB actions (start, stop, restart, try-restart, reload, force-reload, status). For other actions, please try to use systemctl.\n\n# sudo service iptables start\nRedirecting to /bin/systemctl start  iptables.service\nFailed to issue method call: Unit iptables.service failed to load: No such file or directory.\n\n```",
    "answer": "systemctl stop firewalld\nsystemctl mask firewalld\nyum install iptables-services\nsystemctl enable iptables\nsystemctl [stop|start|restart] iptables\nservice iptables save\n/usr/libexec/iptables/iptables.init save\n\nWith RHEL 7 / CentOS 7, firewalld was introduced to manage iptables. IMHO, firewalld is more suited for workstations than for server environments.\nIt is possible to go back to a more classic iptables setup. First, stop and mask the firewalld service:\n\nThen, install the iptables-services package:\n\nEnable the service at boot-time:\n\nManaging the service\n\nSaving your firewall rules can be done as follows:\n\nor"
  },
  {
    "question": "How can I configure a systemd service to restart periodically? I have a simple systemd service that needs to be periodically restarted to keep its process from bugging out. Is there a configuration option for systemd services to periodically restart them? All of the Restart* options seem to pertain to restarting the service when it exits.",
    "answer": "[Unit]\n.\n.\n\n[Service]\nType=notify\n.\n.\nWatchdogSec=10\nRestart=always\n.\n.\n\n[Install]\nWantedBy= ....\n\nYes, you can make your service to restart it periodically by making your service of Type=notify.\nAdd this option in [Service] section of your service file along with Restart=always and give WatchdogSec=xx, where xx is the time period in second you want to restart your service. Here your process will be killed by systemd after xx time period and will be restarted by systemd again.\nfor eg."
  },
  {
    "question": "Start systemd service after specific service? I have a general question. How does one start a systemd unit *.service after a particular *.service has started successfully?\nMore specific question is, how do I start website.service only after mongodb.service has started? In other words website.service should depend on mongodb.service.",
    "answer": "[Unit]\nDescription=My Website\nAfter=syslog.target network.target mongodb.service\n\nIn the .service file under the [Unit] section:\n\nThe important part is the mongodb.service\nThe manpage describes it however due to formatting it's not as clear on first sight\nsystemd.unit - well formatted\nsystemd.unit - not so well formatted"
  },
  {
    "question": "Systemd with multiple execStart Is it possible to create service with the same script started with different input parameters?\nExample:\n\n```\n[Unit]\nDescription=script description\n\n[Service]\nType=simple\nExecStart=/script.py parameters1\nExecStart=/script.py parameters2\nRestart=on-failure\n\n[Install]\nWantedBy=multi-user.target\n\n```\n\nIs it possible?\nWill it be launched in serial-mode? Or in two different process?",
    "answer": "[Unit]\nDescription=script description %I\n\n[Service]\nType=simple\nExecStart=/script.py %i\nRestart=on-failure\n\n[Install]\nWantedBy=multi-user.target\nsystemctl start foo@parameter1.service foo@parameter2.service\n[Unit]\nDescription=bar target\nRequires=multi-user.target\nAfter=multi-user.target\nAllowIsolate=yes\n[Unit]\nDescription=script description %I\n\n[Service]\nType=simple\nExecStart=/script.py %i\nRestart=on-failure\n\n[Install]\nWantedBy=bar.target\nsystemctl daemon-reload\nsystemctl enable foo@param1.service\nsystemctl enable foo@param2.service\nsystemctl start bar.target\n\nif Type=simple in your unit file, you can only specify one ExecStart, but you can add as many ExecStartPre, ExecStartPost, but none of this is suited for long running commands, because they are executed serially and everything one start is killed before starting the next one.\nIf Type=oneshot you can specify multiple ExecStart, they run serially not in parallel.\nIf what you want is to run multiple units in parallel, there a few things you can do:\nIf they differ on 1 param\nYou can use template units, so you create a /etc/systemd/system/foo@.service. NOTE: (the @ is important).\n\nAnd then you exec:\n\nor...\nTarget dependencies\nYou can create multiple units that links to a single target:\n\nAnd then you just modify you .service units to be WantedBy=bar.target like:\n\nThen you just enable the foo services you want in parallel, and start the bar target like this:\n\nNOTE: that this works with any type of units not only template units."
  },
  {
    "question": "What is the difference between systemd&#39;s &#39;oneshot&#39; and &#39;simple&#39; service types? What is the difference between systemd service Type oneshot and simple?\nThis link states to use simple instead of oneshot for timers.  I am not able to understand it correctly.",
    "answer": "The Type=oneshot service unit:\n\nblocks on a start operation until the first process exits, and its state will be reported as \"activating\";\nonce the first process exits, transitions from \"activating\" straight to \"inactive\", unless RemainAfterExit=true is set (in which case it becomes \"active\" with no processes!);\nmay have any number (0 or more) of ExecStart= directives which will be executed sequentially (waiting for each started process to exit before starting the next one);\nmay leave out ExecStart= but have ExecStop= (useful together with RemainAfterExit=true for arranging things to run on system shutdown).\n\nThe Type=simple service unit:\n\ndoes not block on a start operation (i. e. becomes \"active\" immediately after forking off the first process, even if it is still initializing!);\nonce the first process exits, transitions from \"active\" to \"inactive\" (there is no RemainAfterExit= option);\nis generally discouraged because there is no way to distinguish situations like \"exited on start because of a configuration error\" from \"crashed after 500ms of runtime\" and suchlike.\n\nBoth Type=oneshot and Type=simple units:\n\nignore any children of the first process, so do not use these modes with forking processes (note: you may use Type=oneshot with KillMode=none, but only do this if you know what you are doing)."
  },
  {
    "question": "How can systemd and systemctl be enabled and used in Ubuntu Docker containers? Problem\nHow can systemd and systemctl be enabled and used in Ubuntu Docker containers? Why isn't systemd enabled by default in these containers, and why is it not considered a best practice in Docker?\nSetup\nI'm running Docker containers from the ubuntu:16.04 and ubuntu:16.10 images.\nTests\nIf I execute:\nsystemctl status ssh in the 16,04 container\nthe result is the error Failed to connect to bus: No such file or directory\nIn the 16.10 container the error is: bash: systemctl: command not found.\nIf I do which systemctl systemctl is found in the 16.04 container but not in the 16.10 container.\nI have spotted that /lib/systemd exists.\nI have tried installing systemd with:\n\n```\napt-get install systemd libpam-systemd systemd-ui\n\n```\n\nThen which systemctl finds systemctl in 16.10\nbut systemctl status ssh still gives the error Failed to connect to bus: No such file or directory\nQuestions\nCan someone provide a solution for enabling and using systemd in Ubuntu Docker containers?\nI have failed to find any documentation on this topic for Ubuntu / Ubuntu Docker images, only information on the Ubuntu transition from Upstart to systemd.",
    "answer": "This is by design. Docker should be running a process in the foreground in your container and it will be spawned as PID 1 within the container's pid namespace. Docker is designed for process isolation, not for OS virtualization, so there are no other OS processes and daemons running inside the container (like systemd, cron, syslog, etc), only your entrypoint or command you run.\nIf they included systemd commands, you'd find a lot of things not working since your entrypoint replaces init. Systemd also makes use to cgroups which docker restricts inside of containers since the ability to change cgroups could allow a process to escape the container's isolation. Without systemd running as init inside your container, there's no daemon to process your start and stop commands."
  },
  {
    "question": "Fixing a systemd service 203/EXEC failure (no such file or directory) I'm trying to set up a simple systemd timer to run a bash script every day at midnight.\nsystemctl --user status backup.service fails and logs the following:\n\n```\nbackup.service: Failed at step EXEC spawning /home/user/.scripts/backup.sh: No such file or directory.\n\nbackup.service: Main process exited, code=exited, status=203/EXEC\nFailed to start backup.\nbackup.service: Unit entered failed state.\nbackup.service: Failed with result 'exit-code'.\n\n```\n\nI'm lost, since the files and directories exist. The script is executable and, just to check, I've even set permissions to 777.\nSome background:\nThe backup.timer and backup.service unit files are located in /home/user/.config/systemd/user.\nbackup.timer is loaded and active, and currently waiting for midnight.\nHere's what it looks like:\n\n```\n[Unit]\nDescription=Runs backup at 0000\n\n[Timer]\nOnCalendar=daily\nUnit=backup.service\n\n[Install]\nWantedBy=multi-user.target\n\n```\n\nHere's backup.service:\n\n```\n[Unit]\nDescription=backup\n\n[Service]\nType=oneshot\nExecStart=/home/user/.scripts/backup.sh\n\n[Install]\nWantedBy=multi-user.target\n\n```\n\nAnd lastly, this is a paraphrase of backup.sh:\n\n```\n#!/usr/env/bin bash\n\nrsync -a --delete --quiet /home/user/directory/ /mnt/drive/directory-backup/\n\n```\n\nThe script runs fine if I execute it myself.\nNot sure if it matters, but I use fish as my shell (started from .bashrc).\nI'm happy to post the full script if that's helpful.",
    "answer": "I think I found the answer:\nIn the .service file, I needed to add /bin/bash before the path to the script.\nFor example, for backup.service:\nExecStart=/bin/bash /home/user/.scripts/backup.sh\nAs opposed to:\nExecStart=/home/user/.scripts/backup.sh\nI'm not sure why. Perhaps fish. On the other hand, I have another script running for my email, and the service file seems to run fine without /bin/bash. It does use default.target instead multi-user.target, though.\nMost of the tutorials I came across don't prepend /bin/bash, but I then saw this SO answer which had it, and figured it was worth a try.\nThe service file executes the script, and the timer is listed in systemctl --user list-timers, so hopefully this will work.\nUpdate: I can confirm that everything is working now."
  },
  {
    "question": "journalctl - remove logs of a specific unit With journalctl, it is possible to remove the old logs, by specifying either the maximum disk space they can use, or the maximum age they can have, or the maximum number of separate journal files (using the options --vacuum-size, --vacuum-time or --vacuum-files).\nIs there a way to restrain this deletion to a specific systemd unit ?\njournalctl -u <unit> --vacuum-time=2d seems not to work as desired: according to the documentation, -u only works when displaying logs.",
    "answer": "After some research I found that you can't delete logs for a specific systemd unit, because the logs are interlaced and if you delete only one unit you'll corrupt the logs, so journalctl doesn't let you."
  },
  {
    "question": "systemd apparently not finding .service file I've put together a foo.service file for our foo service that runs as a daemon.  The service runs fine when I run systemctl start foo (and stop) but \nsystemtcl enable foo results in Failed to issue method call: Invalid argument.  The unit file is placed in /etc/systemd/system/foo.service, and has permissions 0755.  Setting systemd to debug and running enable gives \n\n```\nLooking for unit files in (highest priority first):`\n    /etc/systemd/system\n    /run/systemd/system\n    /usr/local/lib/systemd/system\n    /usr/lib/systemd/system\nLooking for SysV init scripts in:\n    /etc/rc.d/init.d\nLooking for SysV rcN.d links in:\n    /etc/rd.c\nFailed to issue method call: Invalid argument\n\n```\n\nGoogling around, it seems like systemctl isn't finding the .service file.  Is there any way to verify that?  If so, how can I fix that?  Any other ideas about what might be wrong?  Is there more debugging I can enable?  The debug info given doesn't really help me narrow down the problem.\nfoo.service looks like:\n\n```\n[Unit]\nDescription=Blah Blah Blah\n\n[Service]\nExecStart=/usr/bar/doof/foo\nType=simple\nPIDFile=/var/run/foo.pid\n\n[Install]\nWantedBy=multi-user.target,graphical.target\n\n```\n\nEDIT: Yes, I did run systemctl daemon-reload.",
    "answer": "For people from Google:\n\nValidate with sudo systemd-analyze verify NAME.service\nWhen using a symlink, make sure it uses absolute path\nMake sure the name is like /etc/systemd/system/*.service\nDo sudo systemctl daemon-reload after changes"
  },
  {
    "question": "nginx.service: Failed to read PID from file /run/nginx.pid: Invalid argument I'm working through https://www.digitalocean.com/community/tutorials/how-to-serve-django-applications-with-uwsgi-and-nginx-on-ubuntu-16-04. I've completed the tut but I'm getting a 502 error.\nMy nginx  server block configuration file:\n\n```\nserver {\n    listen 80;\n    server_name 198..xxx.xxx.xxx mysite.org;\n\n    location = /favicon.ico { access_log off; log_not_found off; }\n    location /static/ {\n        root /home/deploy/mysite3;\n    }\n\n    location / {\n        include         uwsgi_params;\n        uwsgi_pass      unix:/run/uwsgi/mysite3.sock;\n    }\n}\n\n```\n\n\n```\ndeploy@server:/etc/nginx/sites-enabled$ sudo systemctl status nginx\n\u25cf nginx.service - A high performance web server and a reverse proxy server\n   Loaded: loaded (/lib/systemd/system/nginx.service; enabled; vendor preset: enabled)\n   Active: active (running) since Mon 2017-02-06 17:30:53 EST; 4s ago\n  Process: 7374 ExecStop=/sbin/start-stop-daemon --quiet --stop --retry QUIT/5 --pidfile /run/nginx.pid (code=exited, status=0/SUCCESS)\n  Process: 7383 ExecStart=/usr/sbin/nginx -g daemon on; master_process on; (code=exited, status=0/SUCCESS)\n  Process: 7380 ExecStartPre=/usr/sbin/nginx -t -q -g daemon on; master_process on; (code=exited, status=0/SUCCESS)\n Main PID: 7384 (nginx)\n   CGroup: /system.slice/nginx.service\n           \u251c\u25007384 nginx: master process /usr/sbin/nginx -g daemon on; master_process on\n           \u2514\u25007385 nginx: worker process\n\nFeb 06 17:30:53 server systemd[1]: Starting A high performance web server and a reverse proxy server...\nFeb 06 17:30:53 server systemd[1]: nginx.service: Failed to read PID from file /run/nginx.pid: Invalid argument\nFeb 06 17:30:53 server systemd[1]: Started A high performance web server and a reverse proxy server.\n\n```\n\nnginx error log shows:\n\n```\n2017/02/06 21:10:32 [error] 7385#7385: *15 upstream prematurely closed connection while reading response header from upstream, client: 64.xxx.xxx.xxx, server: 198.xxx.xxx.xxx, request: \"GET / HTTP/1.1\", upstream: \"uwsgi://unix:/run/uwsgi/mysite3.sock:\", host: \"mysite.org\"\n\n```\n\nIt looks to me that uwsgi is running ok:\n\n```\nFeb 06 17:43:42 server uwsgi[7434]: WSGI app 0 (mountpoint='') ready in 1 seconds on interpreter 0xc7ac10 pid: 7435 (default app)\nFeb 06 17:43:42 server uwsgi[7434]: *** uWSGI is running in multiple interpreter mode ***\nFeb 06 17:43:42 server uwsgi[7434]: spawned uWSGI master process (pid: 7435)\nFeb 06 17:43:42 server uwsgi[7434]: Mon Feb  6 17:43:42 2017 - [emperor] vassal mysite3.ini has been spawned\nFeb 06 17:43:42 server uwsgi[7434]: spawned uWSGI worker 1 (pid: 7439, cores: 1)\nFeb 06 17:43:42 server uwsgi[7434]: spawned uWSGI worker 2 (pid: 7440, cores: 1)\nFeb 06 17:43:42 server uwsgi[7434]: spawned uWSGI worker 3 (pid: 7441, cores: 1)\nFeb 06 17:43:42 server uwsgi[7434]: spawned uWSGI worker 4 (pid: 7442, cores: 1)\nFeb 06 17:43:42 server uwsgi[7434]: spawned uWSGI worker 5 (pid: 7443, cores: 1)\nFeb 06 17:43:42 server uwsgi[7434]: Mon Feb  6 17:43:42 2017 - [emperor] vassal mysite3.ini is ready to accept requests\n\n```\n\nHow can I fix this?\nedit:\n\n```\nroot@server:~# mkdir /etc/systemd/system/nginx.service.d\nroot@server:~# printf \"[Service]\\nExecStartPost=/bin/sleep 0.1\\n\" > /etc/systemd/system/nginx.service.d/override.conf\nroot@server:~# systemctl daemon-reload\nroot@server:~# systemctl restart nginx\nroot@server:~# systemctl status nginx\n\u25cf nginx.service - A high performance web server and a reverse proxy server\n   Loaded: loaded (/lib/systemd/system/nginx.service; enabled; vendor preset: en\n  Drop-In: /etc/systemd/system/nginx.service.d\n           \u2514\u2500override.conf\n   Active: active (running) since Tue 2017-02-07 08:18:26 EST; 6s ago\n  Process: 10076 ExecStop=/sbin/start-stop-daemon --quiet --stop --retry QUIT/5\n  Process: 10084 ExecStartPost=/bin/sleep 0.1 (code=exited, status=0/SUCCESS)\n  Process: 10082 ExecStart=/usr/sbin/nginx -g daemon on; master_process on; (cod\n  Process: 10079 ExecStartPre=/usr/sbin/nginx -t -q -g daemon on; master_process\n Main PID: 10083 (nginx)\n   CGroup: /system.slice/nginx.service\n           \u251c\u250010083 nginx: master process /usr/sbin/nginx -g daemon on; master_pr\n           \u2514\u250010085 nginx: worker process\n\nFeb 07 08:18:26 server systemd[1]: Starting A high performance web server and a\nFeb 07 08:18:26 server systemd[1]: Started A high performance web server and a r\nroot@server:~#\n\n```",
    "answer": "mkdir /etc/systemd/system/nginx.service.d\nprintf \"[Service]\\nExecStartPost=/bin/sleep 0.1\\n\" > /etc/systemd/system/nginx.service.d/override.conf\nsystemctl daemon-reload\nsystemctl restart nginx\n/run/uwsgi/mysite3.sock\n\nThat warning with the nginx.pid file is a know bug (at least for Ubutnu if not for other distros as well). More details here:\nhttps://bugs.launchpad.net/ubuntu/+source/nginx/+bug/1581864\nWorkaround (on a ssh console, as root, use the commands bellow):\n\nThen check if you still see that nginx.pid error and also if nginx is actually running and if you can connect to port 80 on your server.\nI would also check if this actually exists and the permissions on it:\n\nIf nginx is running and uWSGI is running as well then I guess it's a configuration problem\nI understand you want to use Django so I would recommend to review your actual configuration and compare it with the one from here:\nhttp://uwsgi-docs.readthedocs.io/en/latest/tutorials/Django_and_nginx.html\nI hope it helps!"
  },
  {
    "question": "tmux open terminal failed: not a terminal I have a shell script that is enabled as service to start multiple shell scripts e.g.\n\n```\nservice started script -> script1, script2 ,script3 \n\n```\n\nscript1 should open a program in a tmux window, and it does work just fine if I manually start the script via ./script1.sh, however when started at boot via the service started script it does not with the above error:\n\n```\nopen terminal failed: not a terminal\n\n```\n\nWhy is this happening?",
    "answer": "tmux new-session -s username -d\n\nI think the issue is that the service does not have an associated tty. A workaround I've found is to change your tmux call in your script to\n\n(username a user for whom the service was started)"
  },
  {
    "question": "How to restart a service if its dependent service is restarted A service (say bar.service) is dependent on another service (say foo.service), like below\nbar's service file:\n\n```\n[Unit]\nAfter=foo.service\nRequires=foo.service\n...\n\n```\n\nIf foo.service is restarted (either manually or due to a bug), how can bar.service be automatically restarted as well?",
    "answer": "[Unit]\nAfter=foo.service\nRequires=foo.service\nPartOf=foo.service\n\nYou can use PartOf.\n\nFrom the systemd.unit man page:\n\nPartOf=\nConfigures dependencies similar to Requires=, but limited to stopping and restarting of units. When systemd stops or restarts the units listed here, the action is propagated to this unit. Note that this is a one-way dependency \u2014 changes to this unit do not affect the listed units."
  },
  {
    "question": "How to log to journald (systemd) via Python? I would like logging.info() to go to journald (systemd).\nUp to now I only found python modules which read journald (not what I want) or modules which work like this: journal.send('Hello world')",
    "answer": "import logging\nfrom systemd.journal import JournalHandler\n\nlog = logging.getLogger('demo')\nlog.addHandler(JournalHandler())\nlog.setLevel(logging.INFO)\nlog.info(\"sent to journal\")\n\npython-systemd has a JournalHandler you can use with the logging framework.\nFrom the documentation:"
  },
  {
    "question": "systemd: &quot;Environment&quot; directive to set PATH What is the right way to set PATH variable in a systemd unit file?\nAfter seeing a few examples, I tried to use the format below, but the variable doesn't seem to expand.\n\n```\nEnvironment=\"PATH=/local/bin:$PATH\"\n\n```\n\nI am trying this on CoreOS with the below version of systemd.\n\n```\nsystemd 225\n-PAM +AUDIT +SELINUX +IMA -APPARMOR +SMACK -SYSVINIT +UTMP +LIBCRYPTSETUP +GCRYPT -GNUTLS -ACL +XZ -LZ4 +SECCOMP +BLKID -ELFUTILS +KMOD -IDN\n\n```",
    "answer": "[Service]\nEnvironment=\"PATH=/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin\"\n\n> sudo systemctl daemon-reload\n> sudo systemctl restart nagios\n> sudo cat /proc/28647/environ\n...\nPATH=/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin\n...\n\nYou can't use EnvVars in Environment directives. The whole Environment= will be ignored. If you use EnvironmentFile=, then the specified file will be loaded without substitution. So PATH=/local/bin:$PATH would be exactly that, and this is probably not what you want.\nUnder CentOS7 the following works."
  },
  {
    "question": "systemctl status shows inactive dead I am trying to write my own (simple) systemd service which does something simple.( Like writing numbers 1 to 10 to a file, using the shell script).\nMy service file looks like below.\n\n```\n[Unit]\nDescription=NandaGopal\nDocumentation=https://google.com\nAfter=multi-user.target\n\n[Service]\nType=forking  \nRemainAfterExit=yes\nExecStart=/usr/bin/hello.sh &\n\n[Install]\nRequiredBy = multi-user.target\n\n```\n\nThis is my shell script.\n\n```\n#!/usr/bin/env bash\n\nsource /etc/profile\na=0\nwhile [ $a -lt 10 ]\ndo\n   echo $a >> /var/log//t.txt\n        a=`expr $a + 1`\ndone\n\n```\n\nFor some reason, the service doesn't come up and systemctl is showing the below output.\n\n```\nroot@TARGET:~ >systemctl status -l hello\n* hello.service - NandaGopal\n   Loaded: loaded (/usr/lib/systemd/system/hello.service; disabled; vendor     preset: enabled)\n   Active: inactive (dead)\n    Docs: https://google.com\n\n```\n\nBeen trying to figure out what went wrong for the last 2 days.",
    "answer": "You have set Type=Forking, but your service doesn't work. Try\nType=oneshot\nYou have a \"&\" your ExecStart line, which is not necessary.\nThe service is disabled, which means it was not enabled to start at boot. You should run systemctl enable hello to set it to start at boot.\n\nYou can check man systemd.directives to find an index of all the directives that you can use in your unit files."
  },
  {
    "question": "In Ansible, what&#39;s the diffence between the service and the systemd modules? In Ansible, what is the difference between the service and the systemd modules? The service module seems to include the systemd module so what's the point of having systemd by itself?",
    "answer": "The module service is a generic one. According to the Ansible documentation :\n\nSupported init systems include BSD init, OpenRC, SysV, Solaris SMF, systemd, upstart.\n\nThe module systemd is available only from Ansible 2.2 and is dedicated to systemd.\nAccording to the developers of Ansible :\n\nwe are moving away from having everything in a monolithic 'service' module and splitting into specific modules, following the same model the 'package' module has."
  },
  {
    "question": "docker change cgroup driver to systemd I want Docker to start with systemd cgroup driver. For some reason it is using only cgroupfs on my CentOS 7 server.\nHere is startup config file.\n\n```\n# systemctl cat docker\n# /usr/lib/systemd/system/docker.service\n[Unit]\nDescription=Docker Application Container Engine\nDocumentation=http://docs.docker.com\nAfter=network.target\nWants=docker-storage-setup.service\nRequires=docker-cleanup.timer\n\n[Service]\nType=notify\nNotifyAccess=all\nEnvironmentFile=-/etc/sysconfig/docker\nEnvironmentFile=-/etc/sysconfig/docker-storage\nEnvironmentFile=-/etc/sysconfig/docker-network\nEnvironment=GOTRACEBACK=crash\nEnvironment=DOCKER_HTTP_HOST_COMPAT=1\nEnvironment=PATH=/usr/libexec/docker:/usr/bin:/usr/sbin\nExecStart=/usr/bin/dockerd-current \\\n          --add-runtime docker-runc=/usr/libexec/docker/docker-runc-current \\\n          --default-runtime=docker-runc \\\n          --exec-opt native.cgroupdriver=systemd \\\n          --userland-proxy-path=/usr/libexec/docker/docker-proxy-current \\\n          $OPTIONS \\\n          $DOCKER_STORAGE_OPTIONS \\\n          $DOCKER_NETWORK_OPTIONS \\\n          $ADD_REGISTRY \\\n          $BLOCK_REGISTRY \\\n          $INSECURE_REGISTRY\nExecReload=/bin/kill -s HUP $MAINPID\nLimitNOFILE=1048576\nLimitNPROC=1048576\nLimitCORE=infinity\nTimeoutStartSec=0\nRestart=on-abnormal\nMountFlags=slave\n\n[Install]\nWantedBy=multi-user.target\n\n# /etc/systemd/system/docker.service.d/docker-thinpool.conf\n [Service]\n ExecStart=\n ExecStart=/usr/bin/dockerd --storage-driver=devicemapper --storage-opt=dm.thinpooldev=/dev/mapper/docker-thinpool \\\n --storage-opt=dm.use_deferred_removal=true --storage-opt=dm.use_deferred_deletion=true\n EOF\n\n```\n\nWhen I start Docker, it's running like this:\n\n```\n# ps -fed | grep docker\nroot      8436     1  0 19:13 ?        00:00:00 /usr/bin/dockerd-current --storage-driver=devicemapper --storage-opt=dm.thinpooldev=/dev/mapper/docker-thinpool --storage-opt=dm.use_deferred_removal=true --storage-opt=dm.use_deferred_deletion=true\nroot      8439  8436  0 19:13 ?        00:00:00 /usr/bin/docker-containerd-current -l unix:///var/run/docker/libcontainerd/docker-containerd.sock --shim docker-containerd-shim --metrics-interval=0 --start-timeout 2m --state-dir /var/run/docker/libcontainerd/containerd --runtime docker-runc\n\n```\n\nHere is the output of docker info:\n\n```\n# docker info\nContainers: 0\n Running: 0\n Paused: 0\n Stopped: 0\nImages: 1\nServer Version: 1.12.6\nStorage Driver: devicemapper\n Pool Name: docker-thinpool\n Pool Blocksize: 524.3 kB\n Base Device Size: 10.74 GB\n Backing Filesystem: xfs\n Data file:\n Metadata file:\n Data Space Used: 185.6 MB\n Data Space Total: 1.015 GB\n Data Space Available: 829.4 MB\n Metadata Space Used: 77.82 kB\n Metadata Space Total: 8.389 MB\n Metadata Space Available: 8.311 MB\n Thin Pool Minimum Free Space: 101.2 MB\n Udev Sync Supported: true\n Deferred Removal Enabled: true\n Deferred Deletion Enabled: true\n Deferred Deleted Device Count: 0\n Library Version: 1.02.135-RHEL7 (2016-11-16)\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local\n Network: null bridge overlay host\nSwarm: inactive\nRuntimes: runc\nDefault Runtime: runc\nSecurity Options: seccomp\nKernel Version: 3.10.0-514.16.1.el7.x86_64\nOperating System: CentOS Linux 7 (Core)\nOSType: linux\nArchitecture: x86_64\nNumber of Docker Hooks: 2\nCPUs: 1\nTotal Memory: 992.7 MiB\nName: master\nID: 6CFR:H7SN:MEU7:PNJH:UMSO:6MNE:43Q5:SF4K:Z25I:BKHP:53U4:63SO\nDocker Root Dir: /var/lib/docker\nDebug Mode (client): false\nDebug Mode (server): false\nRegistry: https://index.docker.io/v1/\nInsecure Registries:\n 127.0.0.0/8\nRegistries: docker.io (secure)\n\n```\n\nHow can I make it run with systemd?\nThanks\nSR",
    "answer": "--exec-opt native.cgroupdriver=systemd \\\n\nSince I have two configuration file I need to add the entry in the second config file also -- /etc/systemd/system/docker.service.d/docker-thinpool.conf:"
  },
  {
    "question": "&quot;sudo systemctl enable docker&quot; not available: Automatically run Docker at boot on WSL2 (using a &quot;sysvinit&quot; / &quot;init&quot; command or a workaround) I am using Ubuntu on WSL2 (not on Docker Desktop).\nAccording to How to fix docker \u2018Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\u2019 on Ubuntu, I can automatically start the docker daemon at boot using\n\n```\nsudo systemctl enable docker\n\n```\n\ninstead of just starting it again at every boot with\n\n```\nsudo systemctl start docker\n\n```\n\nwith both commands avoiding \"Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\".\nWhen using any of the two, I get\n\nSynchronizing state of docker.service with SysV service script with\n/lib/systemd/systemd-sysv-install. Executing:\n/lib/systemd/systemd-sysv-install enable docker\n\nand a test run shows, that docker is not yet running:\n\n\n```\ndocker run hello-world \n\n```\n\ndocker: Cannot connect to the Docker daemon at\nunix:///var/run/docker.sock. Is the docker daemon running?. See\n'docker run --help'.\n\nSome steps before, I also got a different message at this point:\n\nSystem has not been booted with systemd as init system (PID 1). Can't\noperate.Failed to connect to bus: Host is down\"\n\nwhich brought me to Fixing \"System has not been booted with systemd as init system\" Error:\n\nReason: Your Linux system is not using systemd\nHow to know which init system you are using? You may use this command\nto know the process name associated with PID 1 (the first process that\nruns on your system):\n\n```\nps -p 1 -o comm=\n\n```\n\nIt should show systemd or sysv (or something like that) in the output.\n\nps -p 1 -o comm= gave me init.\nAccording to this and this table\n\n\n```\nSystemd command\n    Sysvinit command\n\nsystemctl start service_name\n    service service_name start\n\nsystemctl stop service_name\n    service service_name stop\n\nsystemctl restart service_name\n    service service_name restart\n\nsystemctl status service_name\n    service service_name status\n\nsystemctl enable service_name\n    chkconfig service_name on\n\nsystemctl disable service_name\n    chkconfig service_name off\n\n```\n\nI can choose service docker start to run docker, which works. But I cannot find something like \"systemd\"'s sudo systemctl enable docker for \"sysvinit\". I would expect it to be like:\n\n```\nsudo service docker enable\n\n```\n\nBut that \"enable\" is not available for \"sysvinit\" / \"init\".\nWhile sudo service docker start works like sudo systemctl start docker, there is no such command that uses \"enable\". At the moment, I need to run sudo service docker start whenever I start WSL2.\nThe question:\nWhat is the command that reaches sudo systemctl enable docker using sudo service docker ..., or if that does not exist, what is a workaround here to automatically start docker when opening Ubuntu on WSL2?",
    "answer": "sudo systemctl enable docker\nsudo systemctl start docker\n[boot]\ncommand= service docker start\n[boot]\ncommand= service docker start; service cron start\n\nThis answer requires the latest version of Windows and WSL at the time of this posting, and it now works under both Windows 10 and 11.  Run wsl --version and confirm that you are on WSL 1.0.0 (not to be confused with WSL1) or later.\nIf you are on an older release of Windows or WSL, then wsl --version will likely just show the Help text.  See this answer for information on how to upgrade.\nIf you cannot upgrade at this time, then please see my original answer for a workaround for Windows 10.\n\nwhat is a workaround here to automatically start docker when opening Ubuntu on WSL2?\n\nOption 1: Enable Systemd support in WSL2\nThe latest release of WSL2 includes support for Systemd.  You can read how to enable it in this Community Wiki answer or my original Ask Ubuntu answer.\nHowever, my personal recommendation is to consider whether you really need Systemd.  It will add additional overhead and potentially other complications, and it isn't strictly necessary for Ubuntu to run (well) on WSL, as we've been doing for quite a few years without it.  Option 2 may be a better (and faster) option for many services.\nIf you do have Systemd enabled, then the commands in the original question should work for you:\n\nDocker Engine should automatically start for you the next time you restart your WSL2 distribution.  However, please see the bottom of this answer for an important note on keeping the services running.\n\nOption 2:  Add the necessary commands to the [boot] section in /etc/wsl.conf:\n\nTo run multiple commands, separate them with a semicolon as in:\n\nImportant Note: If you run a service (e.g. cron or docker) using either of these methods, please note that the WSL distribution will still auto-terminate when the last process that was started interactively completes.  You can see more discussion (and a workaround using keychain) for this in my answer to the Ask Ubuntu question Is it possible to run a WSL app in the background?."
  },
  {
    "question": "What benefit do I get from JSVC over just using systemd? The Tomcat documentation describes the process of compiling and installing JSVC which can be used to run Tomcat as a daemon. As per my understanding, JSVC has two benefits:\n\nIt launches as root allowing for the use of a privileged port (like 80 or 443).\nIt creates a \"controller process\" which will monitor a \"controlled process\" (the main Java thread) and restart the process on failure.\n\nI've been learning systemd, including the service unit configuration. Based on my limited understanding, systemd is able to perform the same tasks as JSVC if I set User=tomcat (using the desired username) and Restart=on-failure in my tomcat.service configuration file.\nUsing JSVC, I would expect tomcat.service to look something like this:\n\n```\n[Unit]\nDescription=Apache Tomcat\nAfter=network.target\n\n[Service]\nEnvironment=CATALINA_PID=/var/run/tomcat.pid\nEnvironment=JAVA_HOME=/path/to/java\nEnvironment=CATALINA_HOME=/opt/tomcat\n...\n\nExecStart=/opt/tomcat/bin/jsvc \\\n    -Dcatalina.home=${CATALINA_HOME} \\\n    -user tomcat \\\n    -java-home ${JAVA_HOME} \\\n    -pidfile ${CATALINA_PID} \\\n    ...\n    org.apache.catalina.startup.Bootstrap\n\nExecStop=/opt/tomcat/bin/jsvc \\\n    -pidfile ${CATALINA_PID} \\\n    ...\n    -stop \\\n    org.apache.catalina.startup.Bootstrap\n\n[Install]\nWantedBy=multi-user.target\n\n```\n\nUsing systemd, I would expect tomcat.service to look something like this:\n\n```\n[Unit]\nDescription=Apache Tomcat\nAfter=network.target\n\n[Service]\nType=forking  \nPIDFile=/var/run/tomcat.pid\nUser=tomcat\nGroup=tomcat\nEnvironment=JAVA_HOME=/path/to/java\nEnvironment=CATALINA_HOME=/opt/tomcat\n...\n\nRestart=on-failure\n\nExecStart=/opt/tomcat/bin/startup.sh\nExecStop=/opt/tomcat/bin/shutdown.sh\n\n[Install]\nWantedBy=multi-user.target\n\n```\n\nMy preference is using just systemd as it's already there and I have to (should) use it anyway. I am however uncertain as to whether or not I will be missing any benefit of using JSVC that I am overlooking.\nWhat can be achieved by JSVC that cannot be achieved by systemd if I want to run Tomcat as a daemon?\nAlso, if systemd is able to perform the same tasks as JSVC as well as JSVC, I'd also like to ask for any configuration tips you may offer to best achieve the benefits of JSVC using just systemd.",
    "answer": "ExecStart=/opt/tomcat/bin/startup.sh\nExecStop=/opt/tomcat/bin/shutdown.sh\n\nIn general, most of the functionality provided by jsvc is provided by systemd, with the exception of opening of privileged ports (see below). If possible, it is a very good idea to switch to using systemd functionality directly, since things become simpler and more efficient.\nYour unit file looks mostly OK, with the exception of\n\nThis part looks like another wrapper which can be replaced with a direct to\njava -jar ....\nOpening privileged sockets\nUnder Systemd this is usually done through socket activation. Systemd opens the socket and hands it to the daemon as an open file descriptor (like stdin, stdout, stderr).\nThe daemon can then be started as unprivileged user, and does not drop privileges itself. The daemon has to support this, and instead of opening the socket by itself, it should use the one it was given. Under Java this is made very problematic by the lack of support in the Java stdlib.\nAFAIK, tomcat does not support socket activation, so if you want to use an privileged port and run the daemon under an unprivileged user, jsvc might still be necessary."
  },
  {
    "question": "Is it recommended to run systemd inside docker container? I am planning to use 'systemd' inside the container. Based on the articles I have read, it is preferable to limit only one process per container. \nBut if I configure 'systemd' inside the container, I will end up running many processes. \nIt would be great to understand the pros and cons of using systemd inside the container before I take any decision.",
    "answer": "I'd advise you to avoid systemd in a container if at all possible.\nSystemd mounts filesystems, controls several kernel parameters, has its own internal system for capturing process output, configures system swap space, configures huge pages and POSIX message queues, starts an inter-process message bus, starts per-terminal login prompts, and manages a swath of system services.  Many of these are things Docker does for you; others are system-level controls that Docker by default prevents (for good reason).\nUsually you want a container to do one thing, which occasionally requires multiple coordinating processes, but you usually don't want it to do any of the things systemd does beyond provide the process manager.  Since systemd changes so many host-level parameters you often need to run it as --privileged which breaks the Docker isolation, which is usually a bad idea.\nAs you say in the question, running one \"piece\" per container is usually considered best.  If you can't do this then a light-weight process manager like supervisord that does the very minimum an init process is required to is a better match, both for the Docker and Unix philosophies."
  },
  {
    "question": "DOCKER_OPTS do not work in config file /etc/default/docker I have changed /etc/default/docker with DOCKER_OPTS=\"-H tcp://127.0.0.1:2375 -H unix:///var/run/docker.sock\" (docker version 1.4.1 in ubuntu 14.04), but it do not take any effect for me (not listening at port 2375). It seems that docker do not read this initial config file because I found export http_proxy enviroment do not work too.\nOnly sudo docker -H tcp://127.0.0.1:2375 -H unix:///var/run/docker.sock -d works.\nIt really confused me!",
    "answer": "[Service]\nExecStart=\nExecStart=/usr/bin/dockerd -H tcp://127.0.0.1:2375 -H unix:///var/run/docker.sock\nsudo systemctl daemon-reload\nsudo systemctl restart docker.service\nsudo netstat -lntp | grep dockerd\ntcp        0      0 127.0.0.1:2375          0.0.0.0:*               LISTEN      3758/dockerd\n\nAccording to docker documentation, The recommended way  to configure the daemon flags and environment variables for your Docker daemon is to use a systemd drop-in file.\nSo, for this specific case, do the following:\n\nUse the command sudo systemctl edit docker.service to open an override file for docker.service in a text editor.\nAdd or modify the following lines, substituting your own values.\n\nSave the file.\nReload the systemctl configuration.\n\nRestart Docker:\n\nCheck to see whether the change was honored by reviewing the output of netstat to confirm dockerd is listening on the configured port."
  },
  {
    "question": "Referencing Other Environment Variables in Systemd Is is possible to reference other environment variables when setting new ones in systemd?\n\n```\n[Service]\nEnvironmentFile=/etc/environment\nEnvironment=HOSTNAME=$COREOS_PRIVATE_IPV4\nEnvironment=IP=$COREOS_PRIVATE_IPV4\nEnvironment=FELIX_FELIXHOSTNAME=$COREOS_PRIVATE_IPV4\n\n```\n\nThe above code does not seem to be working.",
    "answer": "Environment=\nSets environment variables for executed processes. Takes a space-separated list of variable assignments. This\noption may be specified more than once, in which case all listed variables will be set. If the same variable is\nset twice, the later setting will override the earlier setting. If the empty string is assigned to this option,\nthe list of environment variables is reset, all prior assignments have no effect. Variable expansion is not\nperformed inside the strings, however, specifier expansion is possible. The $ character has no special meaning.\nIf you need to assign a value containing spaces to a variable, use double quotes (\") for the assignment.\n\nExample:\n\nEnvironment=\"VAR1=word1 word2\" VAR2=word3 \"VAR3=$word 5 6\"\n\ngives three variables \"VAR1\", \"VAR2\", \"VAR3\" with the values \"word1 word2\", \"word3\", \"$word 5 6\".\nExample:\n\nEnvironment=\"ONE=one\" 'TWO=two two'\nExecStart=/bin/echo $ONE $TWO ${TWO}\n\nThis will execute /bin/echo with four arguments: \"one\", \"two\", \"two\", and \"two two\".\n\nExample:\n\nEnvironment=ONE='one' \"TWO='two two' too\" THREE=\nExecStart=/bin/echo ${ONE} ${TWO} ${THREE}\nExecStart=/bin/echo $ONE $TWO $THREE\n\nThis results in echo being called twice, the first time with arguments \"'one'\", \"'two two' too\", \"\", and the second\ntime with arguments \"one\", \"two two\", \"too\".\n\nThis is really a question for unix & linux.  But nevertheless: No, systemd will not perform environment variable expansion inside Environment=.  From man systemd.exec:\n\nAs you see from the example in the documentation $word just means $word no expansion will be performed.  The specifiers that that man talks about are the %i, %n, %u, etc.  They're in man systemd.unit (under their own man section).\n\nOn the other hand ExecStart= and its derivatives will perform environment variable expansion.  Using environment variables on the ExecStart= is the common workaround for extra environment variables in systemd.  I believe, that is also one of the reasons why so many recent programs accept the same parameters from the environment and from command line parameters.\nAn example of expansion in ExecStart=, from man systemd.service:\n\nsystemd has its documentation spread out across several man's, but one gets used to them after a while."
  },
  {
    "question": "Starting a systemd service via python Is there a way to start/restart a systemd service via python?\nI know that I can make a system call - but then I also could write this in shell script...\n\n```\nfrom subprocess import call\ncall([\"systemctl\", \"restart service\"])\n\n```\n\nI heared systemd has some python binds, but as far as I saw it they only cover the journal",
    "answer": "import dbus\nsysbus = dbus.SystemBus()\nsystemd1 = sysbus.get_object('org.freedesktop.systemd1', '/org/freedesktop/systemd1')\nmanager = dbus.Interface(systemd1, 'org.freedesktop.systemd1.Manager')\njob = manager.RestartUnit('sshd.service', 'fail')\n\nYou can use systemd's DBus API to call the RestartUnit method of the Manager (need of sufficient privileges, else it won't work)"
  },
  {
    "question": "Unable to start postgresql.service? I'm using arch linux (4.8.13-1-ARCH). I'm trying to set up PostgreSQL as instructed here.\nAfter performing\n\n```\n[postgres@BitBox ~]$ initdb --locale $LANG -E UTF8 -D '/var/lib/postgres/data'\nThe files belonging to this database system will be owned by user \"postgres\".\nThis user must also own the server process.\n\nThe database cluster will be initialized with locale \"en_US.UTF-8\".\nThe default text search configuration will be set to \"english\".\n\nData page checksums are disabled.\n\nfixing permissions on existing directory /var/lib/postgres/data ... ok\ncreating subdirectories ... ok\nselecting default max_connections ... 100\nselecting default shared_buffers ... 128MB\nselecting dynamic shared memory implementation ... posix\ncreating configuration files ... ok\nrunning bootstrap script ... ok\nperforming post-bootstrap initialization ... ok\nsyncing data to disk ... ok\n\nWARNING: enabling \"trust\" authentication for local connections\nYou can change this by editing pg_hba.conf or using the option -A, or\n--auth-local and --auth-host, the next time you run initdb.\n\nSuccess. You can now start the database server using:\n\npg_ctl -D /var/lib/postgres/data -l logfile start\n\n```\n\nsuccessfully, I returned to the my regular user using exit command.\n\n```\n[postgres@BitBox ~]$ exit\nlogout\n\n```\n\nThen, while trying to start postgresql.service, I got the following error:\n\n```\n[code_master5@BitBox ~]$ sudo systemctl start postgresql.service\nFailed to start postgresql.service: Unit postgresql.service not found.\n\n```\n\nI'm not even getting the status of the service:\n\n```\n[code_master5@BitBox ~]$ sudo systemctl status postgresql.service\nUnit postgresql.service could not be found.\n\n```\n\nI'm stuck!",
    "answer": "/usr/lib/systemd/system/postgresql-9.6.service\n[code_master5@BitBox ~]$ sudo systemctl start postgresql-9.6.service\nFailed to start postgresql-9.6.service: Unit postgresql-9.6.service not found.\n[code_master5@BitBox ~]$ sudo systemctl status postgresql.service\n[sudo] password for code_master5:\n\u25cf postgresql.service - PostgreSQL database server\nLoaded: loaded (/usr/lib/systemd/system/postgresql.service; enabled; vendor p\nActive: active (running) since Sat 2017-01-28 09:31:30 IST; 7h ago\nMain PID: 342 (postgres)\nTasks: 6 (limit: 4915)\nCGroup: /system.slice/postgresql.service\n\u251c\u2500342 /usr/bin/postgres -D /var/lib/postgres/data\n\u251c\u2500358 postgres: checkpointer process\n\u251c\u2500359 postgres: writer process\n\u251c\u2500360 postgres: wal writer process\n\u251c\u2500361 postgres: autovacuum launcher process\n\u2514\u2500362 postgres: stats collector process\n\nJan 28 09:31:26 BitBox systemd[1]: Starting PostgreSQL database server...\nJan 28 09:31:28 BitBox postgres[340]: FATAL:  the database system is starting up\nJan 28 09:31:28 BitBox postgres[340]: LOG:  database system was shut down at 201\nJan 28 09:31:29 BitBox postgres[340]: FATAL:  the database system is starting up\nJan 28 09:31:29 BitBox postgres[340]: LOG:  MultiXact member wraparound protecti\nJan 28 09:31:29 BitBox postgres[340]: LOG:  database system is ready to accept c\nJan 28 09:31:29 BitBox postgres[340]: LOG:  autovacuum launcher started\nJan 28 09:31:30 BitBox systemd[1]: Started PostgreSQL database server.\n\nFinally, I figured this one out. There was already a file present\n\nSo, may be due to the presence of this file, I was not able to start postgresql.service. Then I tried to start postgresql-9.6.service as follows:\n\nAnd, as you can see the output, again it failed.\nI simply deleted the file using sudo as I thought may be postgresql.service file is not being created by relevant program due to the presence of this file. Then I restarted the system. It's working fine since then, as you can see the output below:\n\nI would surely like to warn all those having same problem. Please do whatever I did at your own risk. Since these are system files. Messing with these can spoil your weekend!\nI am still a bit confused on this though. Explanations are welcome!"
  },
  {
    "question": "What is the Difference between ConditionPathExists= and ConditionPathExists=| in systemd? I need check a file not exist before i start my service in Systemd. I see two case in [Unit]:\n\n```\nConditionPathExists=!/tmp/abc\n\n```\n\nand\n\n```\nConditionPathExists=|!/tmp/abc\n\n```\n\nare they the same? Can anybody help me explain if they are different?",
    "answer": "ConditionPathExists=!/tmp/abc\nConditionPathExists=!/tmp/abe\nConditionPathExists=|!/tmp/abc\nConditionPathExists=|!/tmp/abe\n\nSometime you specify multiple files like:\n\nNow if any of the condition isn't satisfied, it doesn't start service. It's like and operations.\nNow if you use:\n\nIf any of these conditions is satisfied, it will run the service.\n\nCondition checks can be prefixed with a pipe symbol (|) in which case\na condition becomes a triggering condition. If at least one triggering\ncondition is defined for a unit, then the unit will be executed if at\nleast one of the triggering conditions apply and all of the\nnon-triggering conditions\n\nIt's like OR operations"
  },
  {
    "question": "How stop systemd service I am trying to create systemd service file for Flume, have created /etc/systemd/system/flume-ng.service with following contents \n\n```\n[Unit]\nDescription=Apache Flume\n\n[Service]\nEnvironment=FLUME_CLASSPATH=/opt/flume/current/lib/\nExecStart=/usr/bin/nohup /usr/bin/flume-ng agent -c /etc/flume-ng/conf -f /etc/flume-ng/conf/flume.conf --name a1 &\n\n[Install]\nWantedBy=multi-user.target\n\n```\n\nwhich start the Flume service but how do I stop it as a service ?\nIn this case I had to kill with PID.\nthanks",
    "answer": "[Service]\nEnvironment=FLUME_CLASSPATH=/opt/flume/current/lib/\nExecStart=/usr/bin/nohup /usr/bin/flume-ng agent -c /etc/flume-ng/conf -f /etc/flume-ng/conf/flume.conf --name a1 &\nExecStop=/usr/bin/flume-ng agent stop\n\nYou need to put in a ExecStop option in the [Service] section with the command you want to use to stop the service.\nSomething like:\n\nor whatever the command is to stop the flume-ng\nThen you can stop the service with systemctl stop flume-ng.\nRead the manual at https://www.freedesktop.org/software/systemd/man/systemd.service.html for the full set of options available to control the service."
  },
  {
    "question": "Can&#39;t detach child process when main process is started from systemd I want to spawn long-running child processes that survive when the main process restarts/dies. This works fine when running from the terminal:\n\n```\n$ cat exectest.go\npackage main\n\nimport (\n        \"log\"\n        \"os\"\n        \"os/exec\"\n        \"syscall\"\n        \"time\"\n)\n\nfunc main() {\n        if len(os.Args) == 2 && os.Args[1] == \"child\" {\n                for {   \n                        time.Sleep(time.Second)\n                }\n        } else {\n                cmd := exec.Command(os.Args[0], \"child\")\n                cmd.SysProcAttr = &syscall.SysProcAttr{Setsid: true}\n                log.Printf(\"child exited: %v\", cmd.Run())\n        }\n}\n$ go build\n$ ./exectest\n^Z\n[1]+  Stopped                 ./exectest\n$ bg\n[1]+ ./exectest &\n$ ps -ef | grep exectest | grep -v grep | grep -v vim\nsnowm     7914  5650  0 23:44 pts/7    00:00:00 ./exectest\nsnowm     7916  7914  0 23:44 ?        00:00:00 ./exectest child\n$ kill -INT 7914 # kill parent process\n[1]+  Exit 2                  ./exectest\n$ ps -ef | grep exectest | grep -v grep | grep -v vim\nsnowm     7916     1  0 23:44 ?        00:00:00 ./exectest child\n\n```\n\nNote that the child process is still alive after parent process was killed. However, if I start the main process from systemd like this...\n\n```\n[snowm@localhost exectest]$ cat /etc/systemd/system/exectest.service \n[Unit]\nDescription=ExecTest\n\n[Service]                        \nType=simple\nExecStart=/home/snowm/src/exectest/exectest\nUser=snowm\n\n[Install]\nWantedBy=multi-user.target\n$ sudo systemctl enable exectest\nln -s '/etc/systemd/system/exectest.service' '/etc/systemd/system/multi-user.target.wants/exectest.service'\n$ sudo systemctl start exectest\n\n```\n\n... then the child also dies when I kill the main process:\n\n```\n$ ps -ef | grep exectest | grep -v grep | grep -v vim\nsnowm     8132     1  0 23:55 ?        00:00:00 /home/snowm/src/exectest/exectest\nsnowm     8134  8132  0 23:55 ?        00:00:00 /home/snowm/src/exectest/exectest child\n$ kill -INT 8132\n$ ps -ef | grep exectest | grep -v grep | grep -v vim\n$\n\n```\n\nHow can I make the child survive?\nRunning go version go1.4.2 linux/amd64 under CentOS Linux release 7.1.1503 (Core).",
    "answer": "KillMode=process\n\nSolution is to add\n\nto the service block. Default value is control-group which means systemd cleans up any child processes.\nFrom man systemd.kill\n\nKillMode= Specifies how processes of this unit shall be killed. One of\ncontrol-group, process, mixed, none.\nIf set to control-group, all remaining processes in the control group\nof this unit will be killed on unit stop (for services: after the stop\ncommand is executed, as configured with ExecStop=). If set to process,\nonly the main process itself is killed. If set to mixed, the SIGTERM\nsignal (see below) is sent to the main process while the subsequent\nSIGKILL signal (see below) is sent to all remaining processes of the\nunit's control group. If set to none, no process is killed. In this\ncase, only the stop command will be executed on unit stop, but no\nprocess be killed otherwise. Processes remaining alive after stop are\nleft in their control group and the control group continues to exist\nafter stop unless it is empty."
  },
  {
    "question": "set umask for Tomcat via tomcat.service in SystemD I am trying to set a custom umask for a tomcat 8 instance, tried to make it the good way by using the UMask directive in systemd tomcat unit as seen here without luck.\nI'd like to set a 022 umask cause the company dev needs to access tomcat / application logs and they are not in the same group as the tomcat user....\nthe crazy thing is that the systemd doc says :\nControls the file mode creation mask. Takes an access mode in octal notation. See umask(2) for details. Defaults to 0022.\nBut the logs (application / tomcat) are set to 640 (not the expected 755) :\n\n```\n-rw-r----- 1 top top 21416 Feb  1 09:58 catalina.out\n\n```\n\nMy service file :\n\n```\n# Systemd unit file for tomcat\n[Unit]\nDescription=Apache Tomcat Web Application Container\nAfter=syslog.target network.target\n\n[...]\n\nUser=top\nGroup=top\nUMask=0022\n\n[Install]\nWantedBy=multi-user.target\n\n```\n\nAny thoughts about this ?\nThanks",
    "answer": "[Service]\n...\nEnvironment='UMASK=0022'\n...\nif [ -z \"$UMASK\" ]; then\nUMASK=\"0027\"\nfi\numask $UMASK\n\nTry adding UMASK as Environment variable into tomcat's service file:\n\nDefault catalina.sh is checking for environment's $UMASK:\n\n(It seems to me, that UMask from systemd is not used by Tomcat, but I am not completely sure.)"
  },
  {
    "question": "Systemd http health check I have a service on a Redhat 7.1 which I use systemctl start, stop, restart and status to control. One time the systemctl status returned active, but the application \"behind\" the service responded http code different from 200.\nI know that I can use Monit or Nagios to check this and do the systemctl restart - but I would like to know if there exist something per default when using systemd, so that I do not need to have other tools installed.\nMy preferred solution would be to have my service restarted if http return code is different from 200 totally automatically without other tools than systemd itself - (and maybe with a possibility to notify a Hipchat room or send a email...)",
    "answer": "The Short Answer\nsystemd has a native (socket-based) healthcheck method, but it's not HTTP-based. You can write a shim that polls status over HTTP and forwards it to the native mechanism, however.\n\nThe Long Answer\nThe Right Thing in the systemd world is to use the sd_notify socket mechanism to inform the init system when your application is fully available. Use Type=notify for your service to enable this functionality.\nYou can write to this socket directly using the sd_notify() call, or you can inspect the NOTIFY_SOCKET environment variable to get the name and have your own code write READY=1 to that socket when the application is returning 200s.\nIf you want to put this off to a separate process that polls your process over HTTP and then writes to the socket, you can do that -- ensure that NotifyAccess is set appropriately (by default, only the main process of the service is allowed to write to the socket).\n\nInasmuch as you're interested in detecting cases where the application fails after it was fully initialized, and triggering a restart, the sd_notify socket is appropriate in this scenario as well:\nSend WATCHDOG_USEC=... to set the amount of time which is permissible between successful tests, then WATCHDOG=1 whenever you have a successful self-test; whenever no successful test is seen for the configured period, your service will be restarted."
  },
  {
    "question": "Docker (CentOS 7 with SYSTEMCTL) : Failed to mount tmpfs &amp; cgroup (I'm a Docker beginner. Then I followed some tutorials for CentOS-7)\nIn my CentOS 7.2, I tried to learn Docker by following the steps below.\n\n```\n# docker version\n\nClient:\n Version:      1.10.3\n API version:  1.22\n Go version:   go1.5.3\n Git commit:   20f81dd\n Built:        Thu Mar 10 15:39:25 2016\n OS/Arch:      linux/amd64\n\nServer:\n Version:      1.10.3\n API version:  1.22\n Go version:   go1.5.3\n Git commit:   20f81dd\n Built:        Thu Mar 10 15:39:25 2016\n OS/Arch:      linux/amd64\n\n# docker pull centos:latest\n# docker images\ncentos     latest    778a53015523    12 days ago    196.7 MB\n\n# mkdir ~/docker/centos7-systemd\n# cd ~/docker/centos7-systemd\n# vi Dockerfile\nFROM centos\nMAINTAINER \"XXXX XXXX\" <xxxx@xxxx.com>\nENV container docker\nRUN (cd /lib/systemd/system/sysinit.target.wants/; for i in *; do [ $i == systemd-tmpfiles-setup.service ] || rm -f $i; done); \\\nrm -f /lib/systemd/system/multi-user.target.wants/*;\\\nrm -f /etc/systemd/system/*.wants/*;\\\nrm -f /lib/systemd/system/local-fs.target.wants/*; \\\nrm -f /lib/systemd/system/sockets.target.wants/*udev*; \\\nrm -f /lib/systemd/system/sockets.target.wants/*initctl*; \\\nrm -f /lib/systemd/system/basic.target.wants/*;\\\nrm -f /lib/systemd/system/anaconda.target.wants/*;\nVOLUME [ \"/sys/fs/cgroup\" ]\nCMD [\"/usr/sbin/init\"]\n\n# docker build --rm -t local/centos7-systemd .\n..\nSuccessfully built 1a9f1c4938b3\n\n# docker images\ncentos                  latest    778a53015523    12 days ago    196.7 MB\nlocal/centos7-systemd   latest    1a9f1c4938b3    8 seconds ago  196.7 MB\n\n```\n\nSo up to this point, everything (seems) ok.\nNow the problem comes when I run:\n\n```\n# docker run -ti -v /sys/fs/cgroup:/sys/fs/cgroup:ro -p 80:80 local/centos7-systemd\nFailed to mount tmpfs at /run: Operation not permitted\nFailed to mount cgroup at /sys/fs/cgroup/systemd: Operation not permitted\n[!!!!!!] Failed to mount API filesystems, freezing.\n\n```\n\nWhat does this even mean, and more importantly, what is happening and how can I solve this, please?\nThank you all :)",
    "answer": "docker run -ti --privileged=true -v /sys/fs/cgroup:/sys/fs/cgroup:ro -p 80:80 local/centos7-systemd\n\ntry to run your container in privileged mode:\n\nthis should solve your problem"
  },
  {
    "question": "Systemd: Using both After and Requires I have a service foo.service which depends on service bar.service. I need to make sure that bar.service is started before foo.service and that bar.service launched successfully.\nFrom this source it says that Requires:\n\nThis directive lists any units upon which this unit essentially depends. If the current unit is activated, the units listed here must successfully activate as well, else this unit will fail. These units are started in parallel with the current unit by default.\n\nand that After:\n\nThe units listed in this directive will be started before starting the current unit. This does not imply a dependency relationship and one must be established through the above directives if this is required.\n\nIs it correct to have both the Requires and After sections in the same unit file? Requires says that the service will be launched in parallel, but After says it will be launched before. If bar.service fails to start during the After condition, will it attempt to launch it again during the Requires section? If so I need to find another way to launch foo.service\nfoo.service\n\n```\n[Unit]\nAfter=bar.service\nRequires=bar.service\n\n```",
    "answer": "/lib/systemd/system/basic.target\n[Unit]\n...\nRequires=sysinit.target\nAfter=sysinit.target\n...\n\nIt is perfectly fine to use both After= and Requires=. They have different purposes. Requires= sets up a start dependency. systemd makes sure that if any body is trying to start foo.service, it should start bar.service too. Should bar.service fails at some point, then foo.service is taken down too.\nAfter= is putting a start order between services. If both of the services are scheduled to start, then After= makes sure the start order is set.\nYou can look at systemd's own service file as an example."
  },
  {
    "question": "Is there any way to list systemd services in linux &quot;in-the-order-of&quot; they were loaded? I am trying to understand the dependencies between different systemd services in my new project. We are using yocto build system and systemd/system services.\nIf I perform, \n\nsystemctl -l\n\nIt will list all the services in alphabetic order. But I am specifically looking for any commands/scripts that can be used for listing systemd services \"in-the-order-of\" they were loaded. \nPlease help.",
    "answer": "systemd-analyze plot > startup_order.svg\nsystemd-analyze dot | dot -Tsvg > systemd.svg\n\nUnfortunately, due to the parallel nature of the boot up process and the potentially complex dependency relationships among services, the service start up order isn't very deterministic.  However, systemd-analyze, if present on your system, can graphically plot the start up order of services:\n\nIt can also be used to graphically plot service dependencies:"
  },
  {
    "question": "Cannot disable systemd serial-getty service On Raspberry Pi with Arch Linux there is a service active called serial-getty@AMA0.\nThe unit file is: /usr/lib/systemd/system/serial-getty@.service\nAs root I can invoke\n\n```\nsystemctl stop serial-getty@ttyAMA0\nsystemctl disable serial-getty@ttyAMA0\n\n```\n\nBut after reboot the service is enabled and running again.\nWhy is the service enabled after disabling it? How can I disable it permanent?\nUPDATE\nsystemd uses generators at /usr/lib/systemd/system-generators/ is a binary called systemd-getty-generator. This binary runs at system start and adds the symlink serial-getty@ttyAMA0.service to /run/systemd/generator/getty.target.wants.\nI eventually found a dirty solution. I commented out all actions in /usr/lib/systemd/system/serial-getty@.service. The service did appear to start anyway, but without blocking ttyAMA0.",
    "answer": "systemctl mask serial-getty@ttyAMA0.service\n\nThe correct way to stop a service ever being enabled again is to use:\n\n(using ttyAMA0 as the example in this case). This will add a link to null to the entry for that service."
  },
  {
    "question": "systemd: start service at boot time after network is really up (for WoL purpose) I have a computer at work which I sometimes wakeup from home in order to access it but when boots and gets another IP address from our DHCP server, how can I access it?\nThe situation and my \u201cworkflow\u201d is as follows:\n\nFrom my home PC I connect to the office's VPN\nssh to a dedicated server in office's LAN (it has a fixed IP address)\non that server, I call a script that broadcasts a WoL packet with my office PC's MAC address\nmy office PC starts up (it really does, for sure!)\n\nNow in theory I'd be able to SSH to my office PC if only I'd knew its IP address.\nSometimes it gets the same, sometimes it changes.\nTo circumvent this I had the following idea:\n\nafter booting, my office PC does an SSH to the office server and writes its own IP address into some text file on the server\nI examine that file on the server (which I can connect to because of its fixed IP), find the IP address of my office PC, and can then SSH from my home PC to my office PC\n\nAll computers run Linux; Ubuntu 14.04 at home, SLES on the office server, OpenSUSE 13.1 on my office PC. This is all not a problem.\nFor this all to work I simply need a script on my office PC that runs at boot time when the network is up and running.\nMy script (publish_ip.sh) is like follows:\n\n```\n# get own IP address:\nip=$(ip addr show | awk '$1==\"inet\" && $2 !~ /127\\.0\\.0\\.1/ { split($2, a, \"/\"); print a[1]}');\n\n# SSH to the office server (10.64.5.84) and write own IP address to a file there:\nssh -x -e none 10.64.5.84 \"echo $(date) $ip >> office_pc_address.txt\"\n\n```\n\nTo run this script at boot time I created a systemd service file, publish-ip.service, for my office PC:\n\n```\n[Unit]\nDescription=publishes own IP address\nWants=network.target\nAfter=network.target\n\n[Service]\nType=oneshot\nExecStartPre=/usr/bin/sleep 30\nExecStart=/home/perldog/bin/publish_ip.sh\nUser=perldog\n\n[Install]\nWantedBy=multi-user.target\n\n```\n\nBut this is what I always get on my office PC:\n\n```\nlinux-tz7m:/usr/lib/systemd/system # systemctl status publish-ip.service\npublish-ip.service - publishes own IP address\n   Loaded: loaded (/usr/lib/systemd/system/publish-ip.service; enabled)\n   Active: failed (Result: exit-code) since Mon 2016-02-29 12:17:34 CET; 4 days ago\n  Process: 1688 ExecStart=/home/perldog/bin/publish_ip.sh (code=exited, status=255)\n  Process: 1016 ExecStartPre=/usr/bin/sleep 30 (code=exited, status=0/SUCCESS)\n Main PID: 1688 (code=exited, status=255)\n\nFeb 29 12:17:34 linux-tz7m publish_ip.sh[1688]: ssh: connect to host 10.64.5.84 port 22: Network is unreachable\n\n```\n\nObviously my service starts and also calls my script but the SSH command in that script fails with Network is unreachable.\nI tried everything in my service file so that it runs only after the network is up, but I don't get it. I tried Wants=network.target,\nAfter=network.target, WantedBy=multi-user.target, and even inserted an ExecStartPre=/usr/bin/sleep 30. Nothing worked.\nI always get Network is unreachable when my script is called and tries to SSH to the office server.\nQuestion: What settings are required in my service file so that it runs only after the office server is reachable with SSH?\nNote: When I'm at the office and my office PC is up-and-running, both my script and the service work perfectly, i.e. systemctl start publish-ip.service works without any error.",
    "answer": "systemctl enable systemd-networkd.service systemd-networkd-wait-online.service\n[Unit]\nAfter=systemd-networkd-wait-online.service\nWants=systemd-networkd-wait-online.service\n\nI tried all these targets, and they all were reached before DHCP got an IP address. Go figure:\n\nnetwork-online.target\nremote-fs.target\nnfs-client.target\ndbus.service\n\nWhat did work was enabling these two:\n\nAnd then add the following into your .service file:\n\nNow it got started after DHCP got an IP address. (A mount point in my case, but could have been your service too)\n(On debian9/stretch)"
  },
  {
    "question": "How do I list all systemd masked units? Is there an easy way to list all systemd masked units?\nI can think of:\n\n```\nls -l /etc/systemd/system/* | grep /dev/null\n\n```\n\nOr (for unit names only):\n\n```\nls -l /etc/systemd/system/* | grep /dev/null | cut -d' ' -f12 | awk -F'/' '{ print $(NF) }'\n\n```\n\nIs there a clearer way?",
    "answer": "systemctl list-unit-files --state=masked\n\nThe --state option would do the job"
  },
  {
    "question": "Supervisor fails to restart half of the time I'm trying to deploy a Django app using Uwsgi and supervisor on a machine running Debian 8.1.\nWhen I restart via sudo systemctl restart supervisor it fails to restart half of the time.\n\n```\n$ root@host:/# systemctl start supervisor\n    Job for supervisor.service failed. See 'systemctl status supervisor.service' and 'journalctl -xn' for details.\n$ root@host:/# systemctl status supervisor.service\n    \u25cf supervisor.service - LSB: Start/stop supervisor\n       Loaded: loaded (/etc/init.d/supervisor)\n       Active: failed (Result: exit-code) since Wed 2015-09-23 11:12:01 UTC; 16s ago\n      Process: 21505 ExecStop=/etc/init.d/supervisor stop (code=exited, status=0/SUCCESS)\n      Process: 21511 ExecStart=/etc/init.d/supervisor start (code=exited, status=1/FAILURE)\n    Sep 23 11:12:01 host supervisor[21511]: Starting supervisor:\n    Sep 23 11:12:01 host systemd[1]: supervisor.service: control process exited, code=exited status=1\n    Sep 23 11:12:01 host systemd[1]: Failed to start LSB: Start/stop supervisor.\n    Sep 23 11:12:01 host systemd[1]: Unit supervisor.service entered failed state.\n\n```\n\nHowever there is nothing in the supervisor or uwsgi logs.\nSupervisor 3.0 is running with this configuration for uwsgi :\n\n```\n[program:uwsgi]\nstopsignal=QUIT\ncommand = uwsgi --ini uwsgi.ini\ndirectory = /dir/\nenvironment=ENVIRONMENT=STAGING\nlogfile-maxbytes = 300MB\n\n```\n\nstopsignal=QUIT has been added because UWSGI ignores the default signal (SIGTERM) on stop and gets killed brutally with SIGKILL leaving orphan workers.\nIs there a way I could investigate what's happening ?\nEDIT:\nTried as mnencia advised : /etc/init.d/supervisor stop && while /etc/init.d/supervisor status ; do sleep 1; done && /etc/init.d/supervisor start\nbut it still fails half of the time.\n\n```\n root@host:~# /etc/init.d/supervisor stop && while /etc/init.d/supervisor status ; do sleep 1; done && /etc/init.d/supervisor start\n    [ ok ] Stopping supervisor (via systemctl): supervisor.service.\n    \u25cf supervisor.service - LSB: Start/stop supervisor\n       Loaded: loaded (/etc/init.d/supervisor)\n       Active: inactive (dead) since Tue 2015-11-24 13:04:32 UTC; 89ms ago\n      Process: 23490 ExecStop=/etc/init.d/supervisor stop (code=exited, status=0/SUCCESS)\n      Process: 23349 ExecStart=/etc/init.d/supervisor start (code=exited, status=0/SUCCESS)\n\n    Nov 24 13:04:30 xxx supervisor[23349]: Starting supervisor: supervisord.\n    Nov 24 13:04:30 xxx systemd[1]: Started LSB: Start/stop supervisor.\n    Nov 24 13:04:32 xxx systemd[1]: Stopping LSB: Start/stop supervisor...\n    Nov 24 13:04:32 xxx supervisor[23490]: Stopping supervisor: supervisord.\n    Nov 24 13:04:32 xxx systemd[1]: Stopped LSB: Start/stop supervisor.\n    [....] Starting supervisor (via systemctl): supervisor.serviceJob for supervisor.service failed. See 'systemctl status supervisor.service' and 'journalctl -xn' for details.\n     failed!\n    root@host:~# /etc/init.d/supervisor stop && while /etc/init.d/supervisor status ; do sleep 1; done && /etc/init.d/supervisor start\n    [ ok ] Stopping supervisor (via systemctl): supervisor.service.\n    \u25cf supervisor.service - LSB: Start/stop supervisor\n       Loaded: loaded (/etc/init.d/supervisor)\n       Active: failed (Result: exit-code) since Tue 2015-11-24 13:04:32 UTC; 1s ago\n      Process: 23490 ExecStop=/etc/init.d/supervisor stop (code=exited, status=0/SUCCESS)\n      Process: 23526 ExecStart=/etc/init.d/supervisor start (code=exited, status=1/FAILURE)\n\nNov 24 13:04:32 xxx systemd[1]: supervisor.service: control process exited, code=exited status=1\nNov 24 13:04:32 xxx systemd[1]: Failed to start LSB: Start/stop supervisor.\nNov 24 13:04:32 xxx systemd[1]: Unit supervisor.service entered failed state.\nNov 24 13:04:32 xxx supervisor[23526]: Starting supervisor:\nNov 24 13:04:33 xxx systemd[1]: Stopped LSB: Start/stop supervisor.\n[ ok ] Starting supervisor (via systemctl): supervisor.service.\n\n```",
    "answer": "/etc/init.d/supervisor force-stop && \\\n/etc/init.d/supervisor stop && \\\n/etc/init.d/supervisor start\n\nThis is not necessarily an error from supervisor. I see from your systemctl status output that supervisor is started through the sysv-init compatibility layer, so the failure could be in the /etc/init.d/supervisor script. It would explain the absence of errors in the supervisord logs.\nTo debug the init script, the easiest way is to add a set -x as first non-comment instruction in that file, and look in the journalctl output the trace of the script execution.\nEDIT:\nI've reproduced and debugged it on a test system with Debian Sid.\nThe issue is that the stop target of the supervisor init-script does not check if the daemon has been really terminated but only send a signal if the process exists. If the daemon process takes a while to shutdown, the subsequent start action will fail due to the dying daemon process, which is counted as already running.\nI've opened a bug on Debian Bug Tracker: http://bugs.debian.org/805920\nWORKAROUND:\nYou can workaround the issue with:\n\nforce-stop will ensure the supervisord has been terminated (outside systemd).\nstop make sure systemd know it's terminated\nstart starts it again\n\nThe stop after the force-stop is required otherwise systemd will ignore any subsequent start request. stop and start can be combined using restart, but here I've put both of them to show how it works."
  },
  {
    "question": "How to define a d-bus activated systemd service? Does anyone have an example, or a link to an example of how to define a systemd .service which is activated by d-bus?\nMy understanding is that if I create a test.service file here:\n\n```\n/usr/share/dbus-1/services/test.service\n\n```\n\nWith the following contents:\n\n```\n[D-BUS Service]\nName=org.me.test\nExec=\"/tmp/testamundo.sh\"\n\n```\n\nCan the service now be started/stopped via d-bus calls to systemd.Manager? If so, how?",
    "answer": "[D-BUS Service]\nName=org.freedesktop.hostname1\nExec=/bin/false\nUser=root\nSystemdService=dbus-org.freedesktop.hostname1.service\nsystemd-hostnamed.service\n...\n...\n[Service]\nBusName=org.freedesktop.hostname1\n...\n...\n\nLets take a look at one of the services coming with systemd, hostnamed.\n\nThe magic is SystemdService= directive. The service specified with SystemdService= is what dbus-daemon asks systemd to activate.\nWe are expecting a service called dbus-org.freedesktop.hostname1.service in systemd service directory.\n\nThere you go, this way a dbus service org.freedesktop.hostname1.service tells systemd to activate a systemd service systemd-hostnamed.service.\nAnd the systemd service looks like\n\nThe magic on the systemd service file is BusName= directive. This directive tells systemd to wait until the given bus name to appear on the bus before proceeding.\nNote: A dbus service has completely different syntax than systemd service. You need both to be able to have a dbus activated daemon."
  },
  {
    "question": "Starting Docker-Engine on boot When I restart my host, I want my docker engine to start on boot.\nIs this possible?\nCan anyone point me in right direction ?\nMy OS is RHEL 7.3 and \nmy /usr/lib/systemd/system/docker.service fiel looks like:\n\n```\n[Unit]\nDescription=Docker Application Container Engine\nDocumentation=https://docs.docker.com\nAfter=network.target firewalld.service\n\n[Service]\nType=notify\n# the default is not to use systemd for cgroups because the delegate issues still\n# exists and systemd currently does not support the cgroup feature set required\n# for containers run by docker\nExecStart=/usr/bin/dockerd\nExecReload=/bin/kill -s HUP $MAINPID\n# Having non-zero Limit*s causes performance problems due to accounting overhead\n# in the kernel. We recommend using cgroups to do container-local accounting.\nLimitNOFILE=infinity\nLimitNPROC=infinity\nLimitCORE=infinity\n# Uncomment TasksMax if your systemd version supports it.\n# Only systemd 226 and above support this version.\n#TasksMax=infinity\nRestart=always\nTimeoutStartSec=0\n# set delegate yes so that systemd does not reset the cgroups of docker containers\nDelegate=yes\n# kill only the docker process, not all processes in the cgroup\nKillMode=process\n\n[Install]\nWantedBy=multi-user.target\n\n```",
    "answer": "sudo systemctl enable /usr/lib/systemd/system/docker.service\n\nYes. You can use this command to enable automatic start of the docker service after startup:"
  },
  {
    "question": "Systemd: Start operation timed out. Terminating I'm trying to create an autostart service for my python-flask-socketio server.\nI need to start a python script through systemd. Here's my service code:\n\n```\n[Unit]\nDescription=AppName\n\n\n[Service]\nType=forking\nExecStart=/usr/bin/python3 /opt/myapp/app.py\n\n[Install]\nWantedBy=multi-user.target\n\n```\n\nIf I try to start it manually using sudo service myservice start - it works just fine. It halts my terminal\\ssh window but I can close it and it works like expected.\nBut when I reboot my PC it does not start. When checking it's status with systemctl status myservice I get the following:\n\n```\nsystemd[1]: Starting My Service...\nsystemd[1]: myserivce.service: Start operation timed out. Terminating.\nsystemd[1]: Failed to start My Service.\nsystemd[1]: myserivce.service: Unit entered failed state.\nsystemd[1]: myserivce.service: Failed with result 'timeout'.\n\n```\n\nWhat am I doing wrong here?",
    "answer": "Your type seems wrong, forking is for programs that detach immediately by themselves. Flask does not, it stays attached to your console.\nYour service type should probably be simple"
  },
  {
    "question": "Passing a JSON file as environment variable in Docker I would like to pass the content of a JSON file as an environment variable during the docker run. The docker run is initialed inside a systemd service file.\nI did something like:\n\n```\nexport TEMP_CONFIG=$(cat /etc/config.json)\n\n```\n\nand run docker container as follow:\n\n```\ndocker run \\\n        --env SERVICE_NAME=${CONTAINER_NAME} \\\n        --env TEMP_CONFIG \\\n\n```\n\nBut when I am inside the docker container and try to echo the variable ${TEMP_CONFIG} It's empty.\n\n```\nroot@ip-10-109-7-77:/usr/local/nginx/conf# echo ${TEMP_CONFIG}\n\nroot@ip-10-109-7-77:/usr/local/nginx/conf#\n\n```\n\nis there a way to pass content of a JSON file as environment variable?\nBTW:\n\n```\n--env TEMP_CONFIG=$(cat /etc/config.json) \\ \n\n```\n\nDoing above throws an exception:\n\n```\ndocker: Error parsing reference: \"\\\"conf\\\"\" is not a valid repository/tag.\n\n```\n\nThe content of config.json is:\n\n```\n{\n    \"conf\" :\n    {\n        \"appname\" :\n        {\n            \"dbhost\" : \"xxxx\",\n            \"dbname\" : \"dbname\",\n            \"dbuser\" : \"user\",\n            \"dbpassword\" : \"xxxxx\",\n            \"hostname\" : \"xxxxxx\"\n        },\n        \"cacheBaseDir\" : \"/storage/\",\n        \"iccprofile\" : \"/etc/nginx/RGB.V1.0.icc\",\n        \"tmpDir\" : \"/tmp\",\n        \"mdb\" :\n        {\n            \"user\" : \"user\",\n            \"password\" : \"xxxxx\",\n            \"rights\" : \"GlobalAdministrator\",\n            \"company\" : \"somecompany\"\n        }\n    }\n}\n\n```\n\nAny help is definitely appreciated.",
    "answer": "ExecStart=/bin/bash -c \"docker run -e \\\"TEMP_CONFIG=$(</etc/config.json)\\\" ...\"\n--env TEMP_CONFIG={ \"conf\" : { \"...\n--env \"TEMP_CONFIG=$(cat /etc/config.json)\"\n--env \"TEMP_CONFIG=$(</etc/config.json)\"\n\nUpdated answer\nYou mentioned that you use the docker run command in a systemd unit file. A systemd ExecStart options is not started in a shell. Environment variable substitution is supported by name. Also see the documentation on this:\n\nBasic environment variable substitution is supported. Use \"${FOO}\" as part of a word, or as a word of its own, on the command line, in which case it will be replaced by the value of the environment variable including all whitespace it contains, resulting in a single argument.\n\nThe doc also says that StartExec is not executed in a shell:\n\nThis syntax is intended to be very similar to shell syntax, but only the meta-characters and expansions described in the following paragraphs are understood. Specifically, redirection using \"<\", \"<<\", \">\", and \">>\", pipes using \"|\", running programs in the background using \"&\", and other elements of shell syntax are not supported. [...] Note that shell command lines are not directly supported.\n\nHowever, you can use ExecStart to start a shell and then pass a command using the -c flag (you still need to quote the variable as mentioned in my original answer below):\n\nOriginal answer\nYour JSON string contains spaces, and without quoting your shell will interpret everything after the first space as subsequent arguments. So TEMP_CONFIG=$(cat /etc/config.json) is essentially equivalent to:\n\nIn this case, the TEMP_CONFIG environmant variable will have the value {, and docker run will assume \"conf\" to be the next argument (in this case, the image name).\nSolution: Quote your bash variables:\n\nAlso, don't use cat when you don't have to:"
  },
  {
    "question": "Why use ExecStart= (with no value) before another ExecStart=/new/value in a systemd override? I would like to go the systemd override way to let dockerd to listen to port 2376.\nSo I followed this instruction.\nOn the other hand, I would like to dig into systemd to know what's going on under the hood.\nSo I tried to inspect the unit file of docker by this command:\nsystemctl cat docker.service\nAccording to the output of the command, two files are involved.\n\n/lib/systemd/system/docker.service\n/etc/systemd/system/docker.service.d/override.conf\n\nI believe the first one is default unit file for docker and the second one is the I created.\nMy problem is:\nBoth files include sentances - ExecStart= and twice in the second file like:\n\n```\nExecStart=\nExecStart=/usr/bin/dockerd -H fd://\n\n```\n\nIs it necessary to assign empty to ExecStart= before setting meaningful value ExecStart=/usr/bin/dockerd -H fd:// ?\nI have spilit this post into two questions and the other one here.",
    "answer": "[Service]\nEnvironmentFile=/etc/foo.env\n[Service]\nEnvironmentFile=/etc/bar.env\n[Service]\nEnvironmentFile=/etc/foo.env\nEnvironmentFile=/etc/bar.env\n[Service]\nExecStart=/new/command/line\nsystemd: example.service has more than one ExecStart= setting, which is only allowed for Type=oneshot services. Refusing.\n[Service]\nExecStart=/bin/foo\n[Service]\nExecStart=\nExecStart=/bin/bar\n[Service]\nExecStart=/bin/bar\n\nWhen you add entries to an override file, they are by default appended to any existing entries. That is, if your service example.service has:\n\nAnd you create /etc/systemd/system/example.service.d/override.conf\nwith:\n\nThen the effective configuration is:\n\nThat's fine for many directives, but a service can have only one\nExecStart (unless it's a Type-oneshot service), so if you try to create an override file like this:\n\nThat will fail with an error along the lines of:\n\nBy specifying an empty ExecStart, you are \"clearing out\" all\nprevious entries. So if your example.service has:\n\nAnd you create an override like:\n\nThe effective configuration is:"
  },
  {
    "question": "using sudo with ExecStart (systemd) I am trying to get a node.js site live on port 80 (I am using Digital Ocean). I doing this using systemd with in service file\n\n```\n...\nExecStart=/usr/bin/nodejs /var/www/bin/app.js\n...\n\n```\n\nOn localhost this works fine on port 80 if I use sudo to start the site, but not without sudo. Apparently you need to run as root for ports below 1024. \nHow do I allow sudo in the ExecStart? Or am I going completely the wrong way here and if so, how do I get the express app on port 80?\nCheers, Mike",
    "answer": "...\nExecStart=/usr/bin/sudo /usr/bin/nodejs /var/www/bin/app.js\n...\n\nSystemd starts the executable stated in ExecStart= as root by default.\nHowever, if you have specified User= or Group= in your service file overriding that default, and still need to run an executable that requires sudo, prepend the command with the absolute path to your sudo location:"
  },
  {
    "question": "How to use Systemd to restart a service when down? On my server I use elasticSearch which regularly goes down and the result is a 500 error for my users. I understand Systemd is now the reference for managing services.\nHow can I use Systemd to restart my elastic search service automatically when it goes down? I  found ways to restart it, but not automatically without me checking if it's down.",
    "answer": "[Service]\nType=simple\nExecStart=here will be your service executable name\nRestart=always\nRestartSec=0\n\nIf you are using a systemd service file to start your service, then add the lines below to your service file from where you are starting your service:\n\nRestart=\nConfigures whether the service shall be restarted when the service process exits, is killed, or a timeout is reached. Takes one of the following values: no, on-success, on-failure, on-abnormal, on-watchdog, on-abort or always. If set to no (the default).\nRestartSec=\nConfigures the time to sleep before restarting a service (as configured with Restart=). Takes a unit-less value in seconds.\n\nThese two options have to be under the [Service] tag in a service file."
  },
  {
    "question": "Launch docker automatically when starting ec2 server Everytime I restart my ec2 server I have to do:\nsudo systemctl start docker and then docker-compose up -d to launch all my containers.\nWould there be a way to automatically run these two commands at the start of the instance?\nI have read this answer and I think ideally I would like to know how to do that:\n\nCreate a systemd service and enable it. All the enabled systems\nservices will be started on powering.\n\nDo you know how to create such systemd service?\n[EDIT 1]: Following Chris William's comment, here is what I have done:\nThanks Chris, so I created a docker_boot.service with the following content:\n\n```\n[Unit]\nDescription=docker boot\nAfter=docker.service\n\n[Service]\nType=simple\nRestart=always\nRestartSec=1\nUser=ec2-user\nExecStart=/usr/bin/docker-compose -f docker-compose.yml up\n\n[Install]\nWantedBy=multi-user.target\n\n```\n\nI created it in /etc/systemd/system folder\nI then did:\n\n```\nsudo systemctl enable docker\nsudo systemctl enable docker_boot\n\n```\n\nWhen I turn on the server, the only Docker images that are running are certbot/certbot and telethonkids/shinyproxy\nPlease find below the content of my docker-compose.yml file.\nDo you see what is missing so that all images are up and running?\n\n```\nversion: \"3.5\"\nservices:\n  rstudio:\n    environment:\n      - USER=username\n      - PASSWORD=password\n    image: \"rocker/tidyverse:latest\"\n    build:\n     context: ./Docker_RStudio\n     dockerfile: Dockerfile\n    volumes:\n      - /home/ec2-user/R_and_Jupyter_scripts:/home/maxence/R_and_Jupyter_scripts\n    working_dir: /home/ec2-user/R_and_Jupyter_scripts\n    container_name: rstudio\n    ports:\n      - 8787:8787\n\n  jupyter:\n    image: 'jupyter/datascience-notebook:latest'\n    ports:\n      - 8888:8888\n    volumes:\n     - /home/ec2-user/R_and_Jupyter_scripts:/home/joyvan/R_and_Jupyter_scripts\n    working_dir: /home/joyvan/R_and_Jupyter_scripts\n    container_name: jupyter\n\n  shiny:\n    image: \"rocker/shiny:latest\"\n    build:\n     context: ./Docker_Shiny\n     dockerfile: Dockerfile\n    container_name: shiny\n    ports:\n     - 3838:3838\n\n  nginx:\n    image: nginx:alpine\n    container_name: nginx\n    restart: on-failure\n    networks:\n     - net\n    volumes:\n     - ./nginx.conf:/etc/nginx/nginx.conf\n     - ./data/certbot/conf:/etc/letsencrypt\n     - ./data/certbot/www:/var/www/certbot\n    ports:\n     - 80:80\n     - 443:443\n    command: \"/bin/sh -c 'while :; do sleep 6h & wait $${!}; nginx -s reload; done & nginx -g \\\"daemon off;\\\"'\"\n    depends_on:\n     - shinyproxy\n\n  certbot:\n    image: certbot/certbot\n    container_name: certbot\n    restart: on-failure\n    volumes:\n     - ./data/certbot/conf:/etc/letsencrypt\n     - ./data/certbot/www:/var/www/certbot\n    entrypoint: \"/bin/sh -c 'trap exit TERM; while :; do certbot renew; sleep 12h & wait $${!}; done;'\"\n\n  shinyproxy:\n      image: telethonkids/shinyproxy\n      container_name: shinyproxy\n      restart: on-failure\n      networks:\n       - net\n      volumes:\n       - ./application.yml:/opt/shinyproxy/application.yml\n       - /var/run/docker.sock:/var/run/docker.sock\n      expose:\n        - 8080\n\n  cron:\n   build:\n     context: ./cron\n     dockerfile: Dockerfile\n   container_name: cron\n   volumes:\n     - ./Docker_Shiny/app:/home\n   networks:\n     - net\n\nnetworks:\n net:\n   name: net\n\n```",
    "answer": "sudo yum update -y\nsudo yum install -y docker\nsudo systemctl enable docker\nsudo systemctl start docker\nsudo curl -L https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m) -o /usr/bin/docker-compose\n\nsudo chmod +x /usr/bin/docker-compose\nmkdir myapp\nversion: '3.3'\n\nservices:\ndb:\nimage: mysql:5.7\nvolumes:\n- db_data:/var/lib/mysql\nrestart: always\nenvironment:\nMYSQL_ROOT_PASSWORD: somewordpress\nMYSQL_DATABASE: wordpress\nMYSQL_USER: wordpress\nMYSQL_PASSWORD: wordpress\n\nwordpress:\ndepends_on:\n- db\nimage: wordpress:latest\nports:\n- \"8000:80\"\nrestart: always\nenvironment:\nWORDPRESS_DB_HOST: db:3306\nWORDPRESS_DB_USER: wordpress\nWORDPRESS_DB_PASSWORD: wordpress\nWORDPRESS_DB_NAME: wordpress\nvolumes:\ndb_data: {}\n[Unit]\nDescription=docker boot\nAfter=docker.service\n\n[Service]\nType=oneshot\nRemainAfterExit=yes\nWorkingDirectory=/home/ec2-user/myapp\nExecStart=/usr/bin/docker-compose -f /home/ec2-user/myapp/docker-compose.yml up -d --remove-orphans\n\n[Install]\nWantedBy=multi-user.target\nsudo cp -v ./myapp/docker_boot.service /etc/systemd/system\nsudo systemctl enable docker_boot.service\nsudo systemctl start docker_boot.service\nsudo systemctl status docker_boot.service\ncurl -L localhost:8000\n\nUsing Amazon Linux 2 I tried to replicate the issue. Obviously, I don't have all the dependencies to run your exact docker-compose.yml, thus I used the docker-compose.yml from here for my verification. The file setups wordpress with mysql .\nSteps I took were following (executed as ec2-user in home folder):\n1. Install docker\n\n2. Install docker-compose\n\n3. Create docker-compose.yml\n\nCreate file ./myapp/docker-compose.yml:\n\n4. Create docker_boot.service\nThe file is different then yours, as there were few potential issues in your file:\n\nnot using absolute paths\nec2-user may have no permissions to run docker\n\nCreate file ./myapp/docker_boot.service:\n\n5. Copy docker_boot.service to systemd\n\n6. Enable and start docker_boot.service\n\nNote: First start may take some time, as it will pull all docker images required. Alternatively start docker-compose manually first to avoid this.\n7. Check status of the docker_boot.service\n\n8. Check if the wordpress is up\n\n9. Reboot\nCheck if the docker_boot.service is running after instance reboot by logging in into the instance and using sudo systemctl status docker_boot.service and/or curl -L localhost:8000."
  },
  {
    "question": "Using variable in command path for ExecStart in systemd service Im trying to make a systemd service like below : \n\n```\n[Unit]\nDescription=Syslog\n\n[Service]\nType=simple\nEnvironment=\"TESTEXTSERVICESFILES=/opt/test/extservices\"\nEnvironment=\"TESTCONFDATA=/storage/test/conf\"\n\nExecStartPre=/bin/echo ${TESTEXTSERVICESFILES}/syslog/bin/nxlog $TESTCONFDATA\nExecStart=/opt/test/extservices/syslog/bin/nxlog -c ${TESTCONFDATA}/syslog/nxlog.conf\n#ExecStart=/${TESTEXTSERVICESFILES}/syslog/bin/nxlog -c ${TESTCONFDATA}/syslog/nxlog.conf\n\n\n[Install]\nWantedBy=multi-user.target\n\n```\n\nAfter running 'sudo systemctl daemon-reload ; sudo systemctl start test-syslog ; sudo systemctl status test-syslog', I get the following success output: \n\n```\n\u25cf test-syslog.service - TestSyslog\n   Loaded: loaded (/usr/lib/systemd/system/test-syslog.service; enabled; vendor preset: disabled)\n   Active: deactivating (stop-sigterm) since Fri 2018-02-23 10:15:09 UTC; 11ms ago\n  Process: 9474 ExecStart=/./opt/test/extservices/test-syslog/bin/nxlog -c ${TESTCONFDATA}/test-syslog/nxlog.conf (code=exited, status=0/SUCCESS)\n  Process: 9471 ExecStartPre=/bin/echo /.${TESTEXTSERVICESFILES}/test-syslog/bin/nxlog $TESTCONFDATA (code=exited, status=0/SUCCESS)\n Main PID: 9474 (code=exited, status=0/SUCCESS)\n   CGroup: /system.slice/test-syslog.service\n           \u2514\u25009478 /./opt/test/extservices/test-syslog/bin/nxlog -c /storage/test/conf/test-syslog/nxlog.conf\n\nFeb 23 10:15:09 lt-x260-1606.test.local systemd[1]: test-syslog.service: control process exited, code=exited status=0\nFeb 23 10:15:09 lt-x260-1606.test.local systemd[1]: test-syslog.service got final SIGCHLD for state start-pre\nFeb 23 10:15:09 lt-x260-1606.test.local systemd[1]: About to execute: /./opt/test/extservices/test-syslog/bin/nxlog -c ${TESTCONFDATA}/test-syslog/nxlog.conf\nFeb 23 10:15:09 lt-x260-1606.test.local systemd[1]: Forked /./opt/test/extservices/test-syslog/bin/nxlog as 9474\nFeb 23 10:15:09 lt-x260-1606.test.local systemd[1]: test-syslog.service changed start-pre -> running\nFeb 23 10:15:09 lt-x260-1606.test.local systemd[1]: Job test-syslog.service/start finished, result=done\nFeb 23 10:15:09 lt-x260-1606.test.local systemd[1]: Started Test Syslog.\nFeb 23 10:15:09 lt-x260-1606.test.local systemd[1]: Child 9474 belongs to test-syslog.service\nFeb 23 10:15:09 lt-x260-1606.test.local systemd[1]: test-syslog.service: main process exited, code=exited, status=0/SUCCESS\nFeb 23 10:15:09 lt-x260-1606.test.local systemd[1]: test-syslog.service changed running -> stop-sigterm\n\n```\n\nHere the service has started successfully. But when I comment the first ExecStart directive and uncomment the second one I get \nas failure :\n\n```\n\u25cf test-syslog.service - Test Syslog\n   Loaded: loaded (/usr/lib/systemd/system/test-syslog.service; enabled; vendor preset: disabled)\n   Active: failed (Result: exit-code) since Fri 2018-02-23 10:11:44 UTC; 11ms ago\n  Process: 9243 ExecStart=/$TESTEXTSERVICESFILES/test-syslog/bin/nxlog -c $TESTCONFDATA/test-syslog/nxlog.conf (code=exited, status=203/EXEC)\n  Process: 9239 ExecStartPre=/bin/echo /.${TESTEXTSERVICESFILES}/test-syslog/bin/nxlog $TESTCONFDATA (code=exited, status=0/SUCCESS)\n Main PID: 9243 (code=exited, status=203/EXEC)\n\nFeb 23 10:11:44 lt-x260-1606.test.local echo[9239]: /./opt/test/extservices/test-syslog/bin/nxlog /storage/test/conf\n\n```\n\nThis time the service cannot start, like it doesnt want to start the process starting by ${TESTEXTSERVICESFILES} variable. Does someone have any idea why it is not working even if command lines are the same in both cases ?",
    "answer": "ExecStart=/bin/bash -c '/${TESTEXTSERVICESFILES}/syslog/bin/nxlog -c ${TESTCONFDATA}/syslog/nxlog.conf'\n\nYou can't use variables in the actual command. systemd.service:\n\nThe command to execute must be an absolute path name. It may contain\nspaces, but control characters are not allowed.\n\nYou might wan't to wrap it in a shell command (which does parameter expansion):"
  },
  {
    "question": "Configure Prometheus to use non-default port I would like to install Prometheus on port 8080 instead of 9090 (its normal  default). To this end I have edited /etc/systemd/system/prometheus.service to contain this line:\n\n```\nExecStart=/usr/local/bin/prometheus \\\n  --config.file=/etc/prometheus.yaml --web.enable-admin-api \\\n  --web.listen-address=\":8080\"\n\n```\n\nI.e., I am using option --web.listen-address to specifiy the non-default port.\nHowever, when I start Prometheus (2.0 beta) with systemctl start prometheus I receive this error message:\n\n```\nparse external URL \"\": invalid external URL \"http://<myhost>:8080\\\"/\"\n\n```\n\nSo how can I configure Prometheus such that I can reach its web UI at http://<myhost>:8080/ (instead of http://<myhost>:9090)?",
    "answer": "ExecStart=/usr/local/bin/prometheus \\\n--config.file=/etc/prometheus.yaml --web.enable-admin-api \\\n--web.listen-address=:8080\n\nThe quotes were superfluous. This line will work:"
  },
  {
    "question": "Start Docker container using systemd socket activation? Can an individual Docker container, for example a web server, that exposes (listens on) a port be started using systemd's socket activation feature?  The idea is to save resources by starting a container only when it is actually needed for the first time (and possibly stop it again when idle to save resources).\nNote: This question is not about launching the Docker daemon itself using socket activation (which is already supported), but about starting individual containers on demand.",
    "answer": "In short, you can't.\nBut, if you wanted to approach a solution, you would first need to run a tool like CoreOS or geard that runs each Docker container in a systemd service.\nEven then, Docker's support for inheriting the socket has come and gone. I know geard is working on stable support. CoreOS has published generalized support for socket activation in Go. Red Hat folks have also added in related patches to Fedora's Docker packages that use Go's socket activation library and improve \"foreground mode,\" a key component in making it work.\n(I am the David Strauss from Lennart's early article on socket activation of containers, and this topic interests me a lot. I've emailed the author of the patch at Red Hat and contacted the geard team. I'll try to keep this answer updated.)"
  },
  {
    "question": "How to get actual path to executable when using .netcore 3.0 and using the /p:PublishSingleFile=true flag? I recently upgraded an application to dotnet core 3 and started using the PublishSingleFile flag during the build process. With these two changes the way the executable path is found has changed. Now instead of getting the path where the executable file is, I get redirected to a random directory in /var/tmp/.net/ where as I used to get /opt/appdir/.\n\n```\nDirectory.SetCurrentDirectory(Path.GetDirectoryName(Assembly.GetEntryAssembly().Location));\n\n```\n\nEdit: So it seems like the random dir it is spitting out actually points to an unpacked version of my application which does not contain vital resources that are in the other directory.",
    "answer": "System.Diagnostics.Process.GetCurrentProcess().MainModule.FileName\n\nThe following seems to give the path you're after, to the original executable:"
  },
  {
    "question": "Nginx log to stderr I want to redirect nginx access logs to stdout to be able to analyze them through journalctl (systemd). \nThere is the same question with approved answer.\nHave nginx access_log and error_log log to STDOUT and STDERR of master process\nBut it does not work for me. With /dev/stderr I get open() \"/dev/stderr\" failed (6: No such device or address). With /dev/stdout I get no access logs in journalctl -u nginx.\nnginx.conf\n\n```\ndaemon off;\n\nhttp {\n    access_log /dev/stdout;\n    error_log /dev/stdout;\n    ...\n}\n...\n\n```\n\nsitename.conf\n\n```\nserver {\n    server_name sitename.com;\n    root /home/username/sitename.com;\n\n    location / {\n        proxy_pass http://localhost:3000/;\n        proxy_set_header Host $host;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        access_log on;\n    }\n}\n\n```\n\nnginx.service\n\n```\n[Service]\nType=forking\nPIDFile=/run/nginx.pid\nStandardOutput=syslog\nStandardError=syslog\nSyslogIdentifier=nginx\nExecStartPre=/usr/sbin/nginx -t -q -g 'master_process on;'\nExecStart=/usr/sbin/nginx -g 'master_process on;'\nExecReload=/usr/sbin/nginx -g 'master_process on;' -s reload\nExecStop=-/sbin/start-stop-daemon --quiet --stop --retry QUIT/5 --pidfile /run/nginx.pid\nTimeoutStopSec=5\n\n```\n\nI've tried my best to workaround that by changing every possible parameter in the code above and with different nginx versions (1.2, 1.6) but without any success.\nI'm really, really interested how to make this work so I raise this question again on a different thread as I consider previous answer is wrong, speculative or environment specific.\n\n```\n$ journalctl -u nginx\n\n```\n\ncontains only lines like \n\n```\n Feb 08 13:05:23 Username systemd[1]: Started A high performance web server and a reverse proxy server.\n\n```\n\nand no sign of access logs :(",
    "answer": "http {\nerror_log stderr;\n...\n}\n\nAccording to the nginx documentation, the error_log directive supports stderr as its argument. The following configuration should therefore log error messages to stderr:\n\nUnfortunately, access_log doesn't support stdout as its argument. However, it should be possible to set it to syslog (documentation) and get systemd to include syslog calls into its journal."
  },
  {
    "question": "Network usage top/htop on Linux Is there a htop/top on Linux where I get to sort processes by network usage?",
    "answer": "jnettop is another candidate.\nedit: it only shows the streams, not the owner processes."
  },
  {
    "question": "How do I interpret the memory usage information from htop We have multiple servers in our lab and I tried to determine which one has more resources currently available. I tried to interpret the information htop displays but I'm not 100% understanding all those numbers. \nI have taken a screen shot for each server after issuing htop:\nServer #1:\n\nServer #2: \n \nDoes server #1 have more memory available than server #2? Should I look at Avg or Mem? Or what other parameter should I look at?\nThanks!",
    "answer": "htop author here.\n\nDoes server #1 have more memory available than server #2?\n\nYes.\nFrom the htop faq:\n\nThe memory meter in htop says a low number, such as 9%, when top shows something like 90%! (Or: the MEM% number is low, but the bar looks almost full. What's going on?)\nThe number showed by the memory meter is the total memory used by processes. The additional available memory is used by the Linux kernel for buffering and disk cache, so in total almost the entire memory is in use by the kernel. I believe the number displayed by htop is a more meaningful metric of resources used: the number corresponds to the green bars; the blue and brown bars correspond to buffers and cache, respectively (as explained in the Help screen accessible through the F1 key). Numeric data about these is also available when configuring the memory meter to display as text (in the Setup screen, F2).\n\nHope that clears things up! Cheers!"
  },
  {
    "question": "Htop showing multiple java processes with different pids Htop on my linux machine shows many \"processes\" after launching one java program/JVM. I do understand that the JVM runs multiple\nthreads (for the actual program, for garbage collection, etc). \nBut how come htop lists them as multiple processes with different pids. What exactly are those processes?",
    "answer": "Actually, it's just that htop by default shows each thread as a separate process.  You can add the line hide_userland_threads=1 to your ~/.config/htop/htoprc file to show only one line.\nSee https://unix.stackexchange.com/questions/10362/why-does-htop-show-more-process-than-ps"
  },
  {
    "question": "Trick to loop/autorefresh docker ps view like top/htop in bash Is it possible - and if yes, how - to have a self-refreshing view of current Docker containers printed by \"docker ps\" alike top/htop utilities?",
    "answer": "watch docker ps\n\nUse watch:\n\nSee man watch"
  },
  {
    "question": "Why does HTOP show exclamation mark next to uptime Here is the screenshot \nWhat does the (!) means next to the uptime ?",
    "answer": "htop adds an exclamation mark if the uptime is greater than 100 days.\nFrom: https://github.com/hishamhm/htop/blob/0e8a02367ec7ca8f52b10de70938dfd07faed3ab/UptimeMeter.c#L41"
  },
  {
    "question": "Python thread name doesn&#39;t show up on ps or htop When I set the name for a Python thread, it doesn't show up on htop or ps. The ps output only shows python as the thread name. Is there any way to set a thread name so that it shows up on system reports like them?\n\n```\nfrom threading import Thread\nimport time\n\n\ndef sleeper():\n    while True:\n        time.sleep(10)\n        print \"sleeping\"\n\nt = Thread(target=sleeper, name=\"Sleeper01\")\nt.start()\nt.join()\n\n```\n\nps -T -p {PID} output\n\n```\n  PID  SPID TTY          TIME CMD\n31420 31420 pts/30   00:00:00 python\n31420 31421 pts/30   00:00:00 python\n\n```",
    "answer": "from threading import Thread\nimport time\nimport prctl\n\ndef sleeper():\nprctl.set_name(\"sleeping tiger\")\nwhile True:\ntime.sleep(10)\nprint \"sleeping\"\n\nt = Thread(target=sleeper, name=\"Sleeper01\")\nt.start()\nt.join()\nps -T\nPID  SPID TTY          TIME CMD\n22684 pts/29   00:00:00 bash\n23302 pts/29   00:00:00 python\n23303 pts/29   00:00:00 sleeping tiger\n23304 pts/29   00:00:00 ps\n\nFirst install the prctl module.  (On debian/ubuntu just type sudo apt-get install python-prctl)\n\nThis prints\n\nNote: python3 users may wish to use pyprctl."
  },
  {
    "question": "How to calculate system memory usage from /proc/meminfo (like htop) Running the htop command gives you a picture of the memory usage in a format like this: \n\n```\n1.92G/5.83G\n\n```\n\nQuestion: how should I interpret the values taken from /proc/meminfo in order to calculate programmatically the memory used?\nI am looking for something similar to this: Accurate calculation of CPU usage given in percentage in Linux? meaning that pseudocode is ok, I do not need something that can be compiled, just the logic. The source code of htop is a place to look for but I had no luck spotting the lines of code written for this...\n\n```\n$ cat /proc/meminfo \nMemTotal:        6110716 kB\nMemFree:         2076448 kB\nMemAvailable:    3800944 kB\nBuffers:          382240 kB\nCached:          1496216 kB\nSwapCached:            0 kB\nActive:          2830192 kB\nInactive:         796648 kB\nActive(anon):    1749940 kB\nInactive(anon):   109808 kB\nActive(file):    1080252 kB\nInactive(file):   686840 kB\nUnevictable:          48 kB\n.\n.\n.\n...\n\n```",
    "answer": "htop author here. These are the calculations I make to get the numbers for the green, blue and yellow bars in the memory meter:\n\nTotal used memory = MemTotal - MemFree\nNon cache/buffer memory (green) = Total used memory - (Buffers + Cached memory)\nBuffers (blue) = Buffers\nCached memory (yellow) = Cached + SReclaimable - Shmem\nSwap = SwapTotal - SwapFree\n\nIn the htop source code: linux/LinuxProcessList.c and linux/Platform.c."
  },
  {
    "question": "What does a C process status mean in htop? I am using htop on osx and I can't seem to find out what a 'C' status in the 'S' status column means for a process status. \nWhat does a C process status mean in htop?",
    "answer": "htop author here. I am not aware of such status code in the htop codebase.\nKeep in mind that htop is written for Linux only, so there is no support for macOS/OSX. When I hear of people running it on OSX they are often using an outdated, unsupported fork (the latest version of htop is 2.0.1, including macOS support)."
  },
  {
    "question": "When using htop command, do red values in the time+ column mean there&#39;s something wrong? Below is my server htop display. The nginx process uses CPU time more then 18 hours, and is shown in red color, but CPU and memory all look OK. Is the value within the normal range?",
    "answer": "if (hours >= 100) {\nsnprintf(buffer, 10, \"%7lluh \", hours);\nRichString_append(str, CRT_colors[LARGE_NUMBER], buffer);\n} else {\nif (hours) {\nsnprintf(buffer, 10, \"%2lluh\", hours);\nRichString_append(str, CRT_colors[LARGE_NUMBER], buffer);\nsnprintf(buffer, 10, \"%02d:%02d \", minutes, seconds);\n} else {\nsnprintf(buffer, 10, \"%2d:%02d.%02d \", minutes, seconds, hundredths);\n}\nRichString_append(str, CRT_colors[DEFAULT_COLOR], buffer);\n}\n\nI was curious about this too, so I dug into the source code and found this:\n\nSo, it looks like whenever the CPU time exceeds one hour, the hour portion is just highlighted in red (or whatever CRT_colors[LARGE_NUMBER] happens to be.)\nNotice that the time format changes as the time goes:\n4:33.42 is minutes/seconds/millisconds\n18h26:41 is hours/minutes/seconds\n101h  would be hours > 100"
  },
  {
    "question": "Find tmux session that a PID belongs to I am using htop so see what processes are taking up a lot of memory so I can kill them. I have a lot of tmux sessions and lots of similar processes. How can I check which tmux pane a PID is in so I can be sure I am killing stuff I want to kill?",
    "answer": "tmux list-panes -a -F \"#{pane_pid} #{pane_id}\" | grep ^PID\n%30\ntmux kill-pane -t %30\ntmux list-panes -a -F \"#{pane_pid} #{session_name}\" | grep ^PID\ntmux list-panes -a -F \"#{pane_pid} #{session_name}:#{window_index}:#{pane_index}\" | grep ^PID\n\nGiven that PID in the below line is the target pid number:\n\nThe above will identify the pane where the PID is running. The output will be two strings. The first number should be the same as PID and the second one (with a percent sign) is \"tmux pane id\". Example output:\n\nNow, you can use \"tmux pane id\" to kill the pane without \"manually\" searching for it:\n\nTo answer your question completely, in order to find *tmux session* that a PID belongs to, this command can be used:\n\nHere's another possibly useful \"line\":\n\nThe descriptions for all of the interpolation strings (example #{pane_pid}) can be looked up in tmux man page in the FORMATS section."
  },
  {
    "question": "htop with web interface Is there any simple and lightweight monitoring tool like well-known htop, but with web interface? For Debian / Repberry Pi. All solutions I've seen was complicated and resource-intensive.",
    "answer": "[sudo] service shellinaboxd stop\n[sudo] service shellinabox stop\n[sudo] update-rc.d -f shellinabox remove\n\nThanks everything works well!\nIn debian wheezy:\n\nBecomes (without the letter 'd')\n\nThe same applies to update-rc.d line"
  },
  {
    "question": "Building htop from source in Cygwin I am trying to build htop from the source package (latest stable) under Cygwin on  Windows 7 64bit. I am following the instructions given in README, but I'm hitting the following error when I run the make script:\n\n```\nAdministrator@x230-WIN7 ~/tmp/htop-1.0.3\n$ make\nmake  all-am\nmake[1]: Entering directory '/home/Administrator/tmp/htop-1.0.3'\ngcc -DHAVE_CONFIG_H -I.  -DNDEBUG  -pedantic -Wall -Wextra -std=c99 -rdynamic -D_XOPEN_SOURCE_EXTENDED -DSYSCONFDIR=\\\"/usr/local/etc\\\" -g -O2 -MT htop-AvailableMetersPanel.o -MD -MP -MF .deps/htop-AvailableMetersPanel.Tpo -c -o htop-AvailableMetersPanel.o `test -f 'AvailableMetersPanel.c' || echo './'`AvailableMetersPanel.c\ngcc: error: unrecognized command line option \u2018-rdynamic\u2019\nMakefile:467: recipe for target 'htop-AvailableMetersPanel.o' failed\nmake[1]: *** [htop-AvailableMetersPanel.o] Error 1\nmake[1]: Leaving directory '/home/Administrator/tmp/htop-1.0.3'\nMakefile:292: recipe for target 'all' failed\nmake: *** [all] Error 2\n\n```\n\nAll suggestions would be appreciated!",
    "answer": "htop author here \u2014 htop has a lot of platform-specific code, so porting it is not a matter of simply recompiling it. The current version of htop does not support Cygwin. The latest release version, 2.0.1, supports Linux, Mac OS X, FreeBSD and OpenBSD.\nI'm happy to accept patches if someone writes a Cygwin port, though!"
  },
  {
    "question": "Why does Java spawn so many processes? I've written a Java server application that I run on a Debian 7 VServer.\nThe application itself works just fine, but I notices something very strange.\nIssue\nAfter calling java -Xmx200M -jar \"CCU.jar I'd expect to see one Java process running my CCU.jar.\nWhen calling top there is just one java process (as expected). But when I call htop I get this:\n\nWhat are all these subprocesses that seem to be the same process (they have the exactly same data showed in the table) but have different PIDs?",
    "answer": "A java application is never single threaded (Garbage Collector thread among other). So you always end up with several thread. Moreover, Linux implement thread as Light-weight process therefore, each Java thread will appear as a process with a unique PID in htop."
  },
  {
    "question": "Missing interface elements in htop After reinstalling the system (on client) had a problem with the interface htop.\nNot show some of the elements: username, load average\nclient: win7pro, putty 9708\nserver: ubuntu server 12.04",
    "answer": "Putty is rendering \"gray\" color (sometimes referred to as \"bright black\") as black. Go to the Putty settings and tweak the colors, it should fix it."
  },
  {
    "question": "How to fetch list of running processes on a system and sort them by various parameters I use htop to view information about the processes currently running in my osx machine, also to sort them by CPU, memory usage, etc.\nIs there any way to fetch the output of htop programatically in Ruby?. Also I would like to be able to use the API to sort the processes using various parameters like CPU, memory usage, etc.\nI can do IO.popen('ps -a') and parse the output, but want to know if there is a better way than directly parsing the output of a system command run programmatically.",
    "answer": "require 'sys/proctable'\n\nSys::ProcTable.ps\nSys::ProcTable.ps.sort_by(&:starttime)\n\nCheck out sys-proctable:\n\nTo sort by starttime:"
  },
  {
    "question": "Way too much resources are used with Pytorch I am using pytorch to train a DQN model. With ubuntu, if I use htop, I get\n\nAs you can see, all resources are used and I am a bit worried about that.\nHere is my code.\nIs there a way to use less resource? Do I have to add my requirement using pytorch?\nBe aware that there's no GPUs on my machine, just CPUs",
    "answer": "Yes, there is. You can use torch.set_num_threads(...) to specify the number of threads. Depending on the PyTorch version you use, maybe this function will not work correctly. See why in this issue. In there, you'll see that if needed you can use environment variables to limit OpenMP or MKL threads usage via OMP_NUM_THREADS=? and MKL_NUM_THREADS=? respectively, where ? is the number of threads.\nKeep in mind that these things are expected to run on GPUs with thousands of cores, so I would limit CPU usage only when extremely necessary."
  },
  {
    "question": "Is it normal for Rails application to keep so many idle Puma and Postgres connections? I have a Rails app with Puma server and DelayedJob. \nI did some load testing of it - multiple requests at the same time etc. And when I looked at htop I found a number of processes which made me suspicious that puma is leaking  /not killing processes. On the other hand it may be normal behavior. I did see memory go up though. \nI have 2 Puma workers total in Rails configuration and 2 Delayed job workers.\nCan someone with experience with puma confirm / discard my concerns over memory leak? \n\n```\n  CPU[|                                                           1.3%]     Tasks: 54, 19 thr; 1 running\n  Mem[||||||||||||||||||||||||||||||||||||||||||||||||||||| 746/1652MB]     Load average: 0.02 0.03 0.05 \n  Swp[                                                        0/2943MB]     Uptime: 1 day, 12:48:05\n\n\n 1024 admin      20   0  828M  183M  3840 S  0.0 11.1  0:00.00 puma: cluster worker 0: 819\n 1025 admin      20   0  828M  183M  3840 S  0.0 11.1  0:00.00 puma: cluster worker 0: 819\n 1026 admin      20   0  828M  183M  3840 S  0.0 11.1  0:02.68 puma: cluster worker 0: 819\n 1027 admin      20   0  828M  183M  3840 S  0.0 11.1  0:00.43 puma: cluster worker 0: 819\n 1028 admin      20   0  828M  183M  3840 S  0.0 11.1  0:07.04 puma: cluster worker 0: 819\n 1029 admin      20   0  828M  183M  3840 S  0.0 11.1  0:00.05 puma: cluster worker 0: 819\n 1022 admin      20   0  828M  183M  3840 S  0.0 11.1  0:13.23 puma: cluster worker 0: 819\n 1034 admin      20   0  829M  178M  3900 S  0.0 10.8  0:00.00 puma: cluster worker 1: 819\n 1035 admin      20   0  829M  178M  3900 S  0.0 10.8  0:00.00 puma: cluster worker 1: 819\n 1037 admin      20   0  829M  178M  3900 S  0.0 10.8  0:02.68 puma: cluster worker 1: 819\n 1038 admin      20   0  829M  178M  3900 S  0.0 10.8  0:00.44 puma: cluster worker 1: 819\n 1039 admin      20   0  829M  178M  3900 S  0.0 10.8  0:07.12 puma: cluster worker 1: 819\n 1040 admin      20   0  829M  178M  3900 S  0.0 10.8  0:00.00 puma: cluster worker 1: 819\n 1033 admin      20   0  829M  178M  3900 S  0.0 10.8  0:14.28 puma: cluster worker 1: 819\n 1043 admin      20   0  435M  117M  3912 S  0.0  7.1  0:00.00 delayed_job.0\n 1041 admin      20   0  435M  117M  3912 S  0.0  7.1  0:52.71 delayed_job.0\n 1049 admin      20   0  435M  116M  3872 S  0.0  7.1  0:00.00 delayed_job.1\n 1047 admin      20   0  435M  116M  3872 S  0.0  7.1  0:52.98 delayed_job.1\n 1789 postgres   20   0  125M 10964  7564 S  0.0  0.6  0:00.26 postgres: admin app_production_ [local] idle\n 1794 postgres   20   0  127M 11160  6460 S  0.0  0.7  0:00.18 postgres: admin app_production_ [local] idle\n 1798 postgres   20   0  125M 10748  7484 S  0.0  0.6  0:00.24 postgres: admin app_production_ [local] idle\n 1811 postgres   20   0  127M 10996  6424 S  0.0  0.6  0:00.11 postgres: admin app_production_ [local] idle\n 1817 postgres   20   0  127M 11032  6460 S  0.0  0.7  0:00.12 postgres: admin app_production_ [local] idle\n 1830 postgres   20   0  127M 11032  6460 S  0.0  0.7  0:00.14 postgres: admin app_production_ [local] idle\n 1831 postgres   20   0  127M 11036  6468 S  0.0  0.7  0:00.20 postgres: admin app_production_ [local] idle\n 1835 postgres   20   0  127M 11028  6460 S  0.0  0.7  0:00.06 postgres: admin app_production_ [local] idle\n 1840 postgres   20   0  125M  7288  4412 S  0.0  0.4  0:00.04 postgres: admin app_production_ [local] idle\n 1847 postgres   20   0  125M  7308  4432 S  0.0  0.4  0:00.06 postgres: admin app_production_ [local] idle\n 1866 postgres   20   0  125M  7292  4416 S  0.0  0.4  0:00.06 postgres: admin app_production_ [local] idle\n 1875 postgres   20   0  125M  7300  4424 S  0.0  0.4  0:00.04 postgres: admin app_production_ [local] idle\n\n```",
    "answer": "If the number of processes matches your concurrency configuration i would say that's ok, if it keeps growing with every request then you may have an issue with processes hanging. The default for puma i believe is 16. It also looks like you are using clustered mode so it would have multiple processes and multiple threads per process."
  },
  {
    "question": "Why does &quot;htop&quot; show me dozens of PIDs in use by my app, but &quot;ps&quot; only shows me one? I have a Clojure app that I am developing. I am testing it on the server, mostly by going into a \"screen\" session and typing:\njava -jar lo_login_service-0.2-standalone.jar\nand then I kill it by hitting Control-C. Then I make some changes. Then I test it again. \nI assume only 1 PID is in use. If I do:\n\n```\nps aux\n\n```\n\nI only see 1 PID in use:\n\n```\ndas      15028  0.2 22.1 1185300 133520 pts/5  Sl+  Jul26   3:19 java -jar lo_login_service-0.2-standalone.jar\n\n```\n\nBut if I run \"htop\", then I see:\n\n```\n15029 das        20   0 1157M  130M  9960 S  0.0 22.2  0:25.85 java -jar lo_login_service-0.2-standalone.jar\n\n15030 das        20   0 1157M  130M  9960 S  0.0 22.2  0:07.29 java -jar lo_login_service-0.2-standalone.jar\n\n15031 das        20   0 1157M  130M  9960 S  0.0 22.2  0:00.02 java -jar lo_login_service-0.2-standalone.jar\n\n15032 das        20   0 1157M  130M  9960 S  0.0 22.2  0:00.25 java -jar lo_login_service-0.2-standalone.jar\n\n15033 das        20   0 1157M  130M  9960 S  0.0 22.2  0:00.00 java -jar lo_login_service-0.2-standalone.jar\n\n15034 das        20   0 1157M  130M  9960 S  0.0 22.2  0:14.68 java -jar lo_login_service-0.2-standalone.jar\n\n15035 das        20   0 1157M  130M  9960 S  0.0 22.2  0:11.46 java -jar lo_login_service-0.2-standalone.jar\n\n15036 das        20   0 1157M  130M  9960 S  0.0 22.2  0:00.00 java -jar lo_login_service-0.2-standalone.jar\n\n15038 das        20   0 1157M  130M  9960 S  0.0 22.2  0:08.46 java -jar lo_login_service-0.2-standalone.jar\n\n15039 das        20   0 1157M  130M  9960 S  0.0 22.2  0:04.50 java -jar lo_login_service-0.2-standalone.jar\n\n15040 das        20   0 1157M  130M  9960 S  0.0 22.2  0:14.81 java -jar lo_login_service-0.2-standalone.jar\n\n15041 das        20   0 1157M  130M  9960 S  0.0 22.2  0:03.93 java -jar lo_login_service-0.2-standalone.jar\n\n15042 das        20   0 1157M  130M  9960 S  0.0 22.2  0:00.09 java -jar lo_login_service-0.2-standalone.jar\n\n15043 das        20   0 1157M  130M  9960 S  0.0 22.2  0:00.00 java -jar lo_login_service-0.2-standalone.jar\n\n15044 das        20   0 1157M  130M  9960 S  0.0 22.2  0:00.00 java -jar lo_login_service-0.2-standalone.jar\n\n15045 das        20   0 1157M  130M  9960 S  0.0 22.2  0:00.00 java -jar lo_login_service-0.2-standalone.jar\n\n15046 das        20   0 1157M  130M  9960 S  0.0 22.2  0:00.00 java -jar lo_login_service-0.2-standalone.jar\n\n15047 das        20   0 1157M  130M  9960 S  0.0 22.2  0:00.00 java -jar lo_login_service-0.2-standalone.jar\n\n15048 das        20   0 1157M  130M  9960 S  0.0 22.2  0:00.00 java -jar lo_login_service-0.2-standalone.jar\n\n```\n\nWhy does htop show me so many PIDs in use?",
    "answer": "ls /proc/2321/task\n2323  2325  2326  2327\n\nThose probably are the threads used by your application. To only display the process you can press F2 (Setup) > Display options > Hide userland threads > F10 (Save).\nYou can see here an example with hidden/shown user threads (they are shown here as a tree because the option Tree view from the same menu is enabled):\n\nEDIT: I forgot to mention that the numbers in the PID column are not PID's of the threads, but a kernel thread \"ID\" (not sure if this is the right term). Using the scenario of the image above as source, you would find them in /proc/2321/tasks:"
  },
  {
    "question": "Htop process viewer - set column width I try to enlarge the htop column's width. I've found a solution here, but it seems very specific and also too difficult. Is there any simpler way to make all the characters in a column visible?",
    "answer": "Got here from this question: https://serverfault.com/questions/740496/htop-cgroup-column-widen/740533?noredirect=1#comment926040_740533 which is a duplicate of yours.\nIn short, in current versions the column width is fixed. You can ask for this enhancement to the devs or implement it yourself :-)\nYou can use ps to find the processes in control groups you're interested in."
  },
  {
    "question": "using php to stream data of programs such as `htop` using one of those codes\n\n```\nsystem(\"htop\");\n//or\nexec('htop');\n\n```\n\nhow to keep the data from htop being written into a file or something (time limit of the script is set to 0 don't worry)",
    "answer": "resp = system(\"top -n 1\");\nprint $resp;\n\nhtop is an interactive program that runs in a loop until exited, and doesn't seem to offer any flags to do a single iteration then exit which makes capturing the output problematic.\nHave you considered using top? You can limit it to 1 iteration with -n 1:"
  },
  {
    "question": "Segfault using htop on AWS Sagemaker pytorch-1.10-cpu-py38 app I am trying to launch the htop command in the Pytorch 1.10 - Python 3.8 CPU optimized AWS Sagemaker container. This works fine in other images I have used till now, but in this one, the command fails with a segfault:\n\n```\nhtop \nhtop: /opt/conda/lib/libncursesw.so.6: no version information available (required by htop) \nhtop: /opt/conda/lib/libncursesw.so.6: no version information available (required by htop) \nhtop: /opt/conda/lib/libncursesw.so.6: no version information available (required by htop) \nSegmentation fault (core dumped)\n\n```\n\nMore info :\n\n```\nhtop --version\nhtop: /opt/conda/lib/libncursesw.so.6: no version information available (required by htop)\nhtop: /opt/conda/lib/libncursesw.so.6: no version information available (required by htop)\nhtop: /opt/conda/lib/libncursesw.so.6: no version information available (required by htop)\nhtop 2.2.0 - (C) 2004-2019 Hisham Muhammad\nReleased under the GNU GPL.\n\n```",
    "answer": "ln -fs /lib/x86_64-linux-gnu/libncursesw.so.6 /opt/conda/lib/libncursesw.so.6\n\nI fixed this with"
  },
  {
    "question": "htop ubuntu wily percentage numbers missing I just upgraded to Ubuntu 15.10 (wily).\nI notice that htop no longer shows the percentage numbers for CPU, memory and swap.\nIs this a bug or \"the new\" way?",
    "answer": "htop author here. This is most likely a problem in the way your terminal handles gray text. In terminal ANSI colors (man console_codes) gray text is made by combining black foreground and bold, and this is what htop uses. However, some terminals display this combination as \"bold black\", rendering the text invisible when written over a black background.\nIn the version of htop currently in git, there is a \"Broken Gray\" color scheme, that is identical to the default, but does not use gray at all. You may want to use that, or try to configure your terminal colors so that it properly displays gray."
  },
  {
    "question": "Why is fresh ElasticSearch Install running so Many processes? I just installed ElasticSearch and have not loaded in any data at all. htop shows ElasticSearch running a bunch of threads. \nView htop output\nWhy is ElasticSearch running all these processes? What is it doing?\nI know that you can configure htop to group all of the threads into just one line. But that still does not answer the question of why anything is running at all.",
    "answer": "The simple answer is that in order to be efficient ES uses many threadpools for carrying out the many things it needs to do.\nAs you probably know ES provides a very powerful search engine.\nSo in order to enable a potentially massive amount of users to run a potentially massive amount of queries efficiently, ES uses a pool of threads to carry out that work.\nThat's not the end of the story. While all those users might search like mads, other users or processes can also index a potentially massive amount of data at the same time. For that reason ES needs another thread pool for handling the many indexing requests it can get. Those indexing requests can come in two forms: indexing a single document, indexing many documents in bulk. For those two indexing processes, ES uses two different thread pools.\nThat's still not the end of the story. While some users are searching and some others are indexing data, there might be a backup process running (what ES calls snapshotting). For that there's another threadpool.\nAnd so on. The list is not exhaustive, but you can trust that ES has several threadpools in order to handle what it needs to handle and it knows how to do it efficiently as it will only create as many threads as your available processors can handle.\nYou can review the full list of threadpools that ES is managing and you'll probably better understand what it is doing. You can also use the /_cat/thread_pool and the /_nodes/hot_threads endpoints in order to better visualise what those threads are doing."
  },
  {
    "question": "What am I setting when I limit the number of &quot;threads&quot;? I have a somewhat large code that uses the libraries numpy, scipy, sklearn, matplotlib. I need to limit the CPU usage to stop it from consuming all the available processing power in my computational cluster. Following this answer I implemented the following block of code that is executed as soon as the script is run:\n\n```\nimport os\nparallel_procs = \"4\"\nos.environ[\"OMP_NUM_THREADS\"] = parallel_procs\nos.environ[\"MKL_NUM_THREADS\"] = parallel_procs\nos.environ[\"OPENBLAS_NUM_THREADS\"] = parallel_procs\nos.environ[\"VECLIB_MAXIMUM_THREADS\"] = parallel_procs\nos.environ[\"NUMEXPR_NUM_THREADS\"] = parallel_procs\n\n```\n\nMy understanding is that this should limit the number of cores used to 4, but apparently this is not happening. This is what htop shows for my user and that script:\n\nThere are 16 processes, 4 of which show percentages of CPU above 100%. This is an excerpt of lscpu:\n\n```\nCPU(s):              48\nOn-line CPU(s) list: 0-47\nThread(s) per core:  2\nCore(s) per socket:  12\nSocket(s):           2\n\n```\n\nI am also using the multiprocessing library down the road in my code. I set the same number of processes using multiprocessing.Pool(processes=4). Without the block of code shown in above, the script insisted on using as many cores as possible apparently ignoring multiprocessing entirely.\nMy questions are then: what am I limiting when I use the code above? How should I interpret the htop output?",
    "answer": "n = '1'\nos.environ[\"OMP_NUM_THREADS\"] = n\nos.environ[\"MKL_NUM_THREADS\"] = n\n\n(This might be better as a comment, feel free to remove this if a better answer comes up, as it's based on my experience using the libraries.)\nI had a similar issue when multiprocessing parts of my code. The numpy/scipy libraries appear to spin up extra threads when you do vectorised operations if you compiled the libraries with BLAS or MKL (or if the conda repo you pulled them from also included a BLAS/MKL library), to accelerate certain calculations.\nThis is fine when running your script in a single process, since it will spawn threads up to the number specified by OPENBLAS_NUM_THREADS or MKL_NUM_THREADS (depending on if you have a BLAS library or MKL library - you can identify which by using numpy.__config__.show()), but if you are explicitly using a multiprocesing.Pool, then you likely want to control the number of processes in multiprocessing - in this case, it makes sense to set n=1 (before importing numpy & scipy), or some small number to make sure you are not oversubscribing:\n\nIf you set multiprocessing.Pool(processes=4), it will use 4*n processes (n threads in each process). In your case, it seems like you have a pool of 4 processes and they fire up 4 threads each, hence the 16 python processes.\nThe htop output gives 100% assuming a single CPU per core. As a Linux machine interprets a thread as a CPU (I might be wrong in the terminology here), if you have 4 threads per CPU, it means that the full load is actually 400%. This might not be maxed out, depending on the operations being performed (and on caching, as your machine looks hyperthreaded).\nSo if you're doing the numpy/scipy operation in parts of the code which are in a single process/single thread, you are better off setting a larger n, but for the multiprocessing sections, it might be better to set a larger pool and single or small n. Unfortunately, you can only set this once, at the beginning of your script if you're passing in flags through the environmental flags. If you want to set it dynamically, I saw in a numpy issues discussion somewhere that you should use threadpoolctl (I'll add a link if I can find it again)."
  },
  {
    "question": "Htop says &quot;530G&quot; in &quot;VIRT&quot; for &quot;vagrant ssh&quot; I use Vagrant on a MacOS with an ubuntu64 16.04. Running htop, I can see vagrant ssh process can use virtually 530G (in VIRT Column).\nIs it the normal behavior of Vagrant? Should I panic? Is it \"normal\" to have virtually 530G on a mac with 120G of disk and 16G of RAM? Or maybe did I not understand the meaning of VIRT?\nThe vagrant box runs on virtual box and has only 1G of RAM allocated.",
    "answer": "Answer by chrisroberts on github:\n\nHi! I was able to reproduce this behavior, but with any vagrant command executed. The vagrant ssh command is the easiest to see this behavior simply because the process is left running for as long as the ssh session is alive.\nThe tl;dr version of below is simply: Don't worry about it. VIRT isn't allocated memory. If it were, you would either need massive swap space, or nothing would be working.\nSo, what's going on here? The vagrant installer includes a small go executable (vagrant) whose job is to setup the current environment with the proper locations of everything it needs. The installers bin directory, the lib directory for ruby and all the other friends, all the gems, and the vagrant gem itself. Once it has all this configured, it spawns off a new process, the actual Ruby vagrant process.\nBecause your example was referencing vagrant ssh, and as was previously pointed out (#7296 (comment)) a Kernel.exec happens meaning the Ruby process does not persist, I figured it must be the wrapper that was the culprit. After a bit of searching (mostly to find stackoverflow items saying \"don't worry about VIRT\") I stumbled upon:\nkeybase/keybase-issues#1908\nThey refer to the golang FAQ that talks about a bunch of VIRT being claimed up front and it not being a big deal, but never any absolutes about how much was actually being claimed. A link to lwn was dropped in there (keybase/keybase-issues#1908 (comment)) regarding golang's behavior on startup of claiming a huge chunk of VIRT, but still everything referenced a much lower amount than I was seeing locally. So I decided to go dig into the golang runtime code, and within malloc.go we find the answer:\ngolang src/runtime/malloc.go\nThe why it's happening is because of the go wrapper used to start vagrant. Because the VIRT you see is simply a reservation and not actually allocated, it's not a problem and not something that should be worried about.\n(There are some interesting conversations on the golang ML around the pros and cons of this approach, all pretty great reads).\n\nIt's just a copy/paste (and bolded the TLDR), hope it could help someone else."
  },
  {
    "question": "Memory report confusion shown by top and htop commands? I'm on Slicehost 256 plan running one single Rails app on Ubuntu Hardy 64 bit server.\nThis is the shot taken using top command sorted by memory% (Shift+M)\nMemory by top http://www.freeimagehosting.net/uploads/7d35d548bf.png\nAnd this is the screenshot taken while running htop command sorted by memory% used.\nMemory by htop http://www.freeimagehosting.net/uploads/f598fea0da.png\nThe memory consumed by mysql using top shows 3.8% but the htop shows around 17 processes each eating up around 3.2% of memory.\nWhats the difference between those stat shown by top and htop??\nIs mysql eating up my memory as shown by htop??",
    "answer": "It looks like htop defaults to showing you the threads view. You can get the same effect in top by pressing shift-H. There is one mysql process using 3.2% memory and 17 threads.\nAlternatively, you can make htop display only a single line per process, like the default behavior in top."
  },
  {
    "question": "htop reports 100% cpu steal time, top reports 0% after virsh restore After a virsh save + restore from file, htop reports ~100% cpu steal time (st), while top reports ~100% idle time.\nMay this be a bug in htop? Are there any fixes available, or is this simply some strange qemu/kvm behavior?\n\n\nDebian 8, Linux 3.16.0-4-amd64, htop 1.0.3, top (procps-ng) 3.3.9, virsh 1.2.9",
    "answer": "According to this thread, this is caused by a bug in KVM and is fixed in KVM 2.1.3 and 2.2.\nThe latest Debian package (kvm-qemu 1:2.1+dfsg-12+deb8u2) does not seem to include that fix."
  },
  {
    "question": "sails.js multiple child process, what are those? Ubuntu By running sails with forever or calling it directly with node app.js --prod, according to htop i have a 4 child process that look like clones under it.\n\nIf i leave out the --prod argument, i get an equal quantity of grunt processes.\nAs you can see they all have the same amount of memory used, different PID and different TIME+ readings.\nI'm pretty sure sails its not auto clustering, or is it? This is a single core machine (virtual), so i have no idea what are those processes, or is it an error in htop?\nAccording to this question htop-showing-multiple-java-processes-with-different-pids?rq=1\nhtop shows threads as different process, but isn't node single threaded? If those are threads where are they comming from?",
    "answer": "The threads you see using htop is userland threads(user's thread) but when you run your application in cluster mode(say 4 instances),it makes kernel thread.\nmore on user threads\nwhen you hide userland threads then whatever you see is kernel thread.while running application in cluster mode you will create multiple kernel threads of an application.And for each application's kernel thread there are several userland threads associated with it.\n(for htop you can hide userland thread by F2 and display options)\nplease see the screen shots for explanation.\nfour instances of my app in cluster mode since number of cores==4\n\nps commands showing kernel threads only\n\nhtop showing kernel threads only\n\nhtop showing kernel threads along with userland threads\n\nIn last screenshot you see a few kernel and userland's threads but there are more.\nI mean for each kernel thread there are some userland's thread associated with it."
  },
  {
    "question": "Why my java long running Threads (5k+ thread) not utilizing all machines cores (12 cores)? I've Worte a simple multithread java application, The main method just creates 5k threads, each thread will loop over a list having 5M records to process.\nMy Machine specs: \n\nCPU cores: 12 cores \nMemory: 13Gb RAM\nOS: Debian 64-bit\n\nMy jar is now running, And I use hTop to monitor my application and this is what I can see while its running \n\nAnd This is how I construct a Thread:\n\n```\nExecutorService executor = Executors.newCachedThreadPool();\nFuture<MatchResult> future = executor.submit(() -> {\n            Match match = new Match();\n            return match.find(this);\n        });\n\n```\n\nMatch.class\n\n```\nfind(Main main){\n// looping over a list of 5M \n// process this values and doing some calculations \n// send the result back to the caller \n// this function has no problem and it just takes a long time to run (~160 min)\n}\n\n```\n\nAnd now I have some questions:\n1- Based on my understanding if I have a multiThreaded process, it'll fully utilize all my cores until the task is completed, so why the work load is only around 0.5 (only half a core is used)?\n2- Why my Java app state is \"S\" (sleeping) while its actually running and filling up the logs file? \n3- Why I can only see 2037 threads out of 5k are running (this number was actually less than this and its getting increased over time)\nMy Target: to utilize all cores and get all this 5k+ done as fast as it can be :)",
    "answer": "Thank you guys, I've fixed the problem and now Im having the 12 cores running up to maximum as you see in the picture. :)\n\nI actually tried to run this command jstack <Pid> to see the status of my all running threads in this process ID, and I found that 95% of my threads are actually BLOCKED at the logging line, I did some googling and found that I can use AsynchAppender in log4J so logging will not block the thread"
  },
  {
    "question": "Huge difference between htop and ps aux output I am running a test on Ubuntu 14.04. When I check my CPU usage using \n'ps aux|grep service' then CPU usage is 0.1 of a process, while in htop for the same process the CPU% is 12.3.\nCan anyone tell me the reason? or which value should I consider the right one?\nThanks",
    "answer": "CPU usage is currently expressed as the percentage of time spent\nrunning during the entire lifetime of a process.  This is not ideal,\nand it does not conform to the standards that ps otherwise conforms to.\nCPU usage is unlikely to add up to exactly 100%.\nPERCENT_CPU (CPU%)\nThe  percentage  of  the  CPU  time  that the process is currently\nusing.\n\nThey are measuring different things.\nFrom the ps man-page:\n\nFrom the htop man-page (I am the author of htop):\n\nSo, in htop this is the percentage of total CPU time used by the program between the last refresh of the screen and now.\nPercentageInHtop = (non-idle CPU time used by process during the last 1.5s) / 1.5s\nIn ps this is the percentage of CPU time used by the program relative to the total time it exists (ie, since it was launched).\nPercentageInPs = (non-idle CPU time used by process since process startup) / (time elapsed since process startup)\nThat is, in your reading it means that htop is saying that the service is taking 12.3% of your CPU now, while ps is saying that your service has spent 99.9% of its total life idle."
  },
  {
    "question": "How does htop display work? I want to write a program that displays its output in the shell like htop does, as opposed to just running the program and using the watch command. I have looked through the htop source code and am still a little lost. Is there a output beyond tty in the shell that is used, or are all the htop panels custom and opening an output like that is not a native task for a shell like bash?\nThanks.",
    "answer": "htop author here. What you're looking for is the NCurses library.\nWith NCurses, you can implement a full-screen textmode application in C.\nIt provides a set of primitives for \"drawing\" on the terminal: ie, move() to go to a X-Y coordinate on the screen, functions to change colors, to erase part of the screen, etc. It also provides some high-level constructs such as \"windows\" through which you can scroll parts of the screen separately, but in htop I don't use that and implement my scrollable panels drawing them \"by hand\" using the lower-level primitives."
  },
  {
    "question": "exit and clean up python fork i am trying to code socket server with fork in python. somehow a new fork will be created when a client is connected and this fork process will handle the connection including send/receive.\ni ran this script on Linux centOS and monitor resources with htop/top to see how many forks (task) are shown. the\nproblem is  when i kill some fork by using os._exit(0) htop won't be changed (naturally it has to be decreased by killing forks) and when i close python script every thing will be back to normal ( Ram usage and tasks ).\nso what i have to do that when i kill some fork by using os._exit(0), it effects on htop in other hand releases all resources and do not wait until its own parent is killed ?\nhere it's the code to create forks:\n\n```\ndef test(sock):\n     //handle socket then return\nfor i in range (1000):\n     sock,addr=socket.accept()\n     pid=os.fork()\n     if pid==0:\n          test(sock)\n          os._exit(0)\n     elif pid !=-1:\n          os.waitpid(-1, os.WNOHANG)\n\n```",
    "answer": "The parent process needs to wait for the child in order for the child process' resources to be released. Until then the process still exists in a \"zombie\" state, and it will still appear in ps and top etc.\nYou can call one of os.wait(), os.waitpid(),  os.wait3(), or os.wait4().\nos.wait3() with the os.WNOHANG option might be most useful to you as it will wait for any child process and the parent will not block until a child terminates (or it's state changes - wait will return child processes that have been stopped or restarted too).\nMore details on the underlying system calls can be found in the Linux man page: man 2 wait."
  },
  {
    "question": "Python multiprocessing Pool.map uses all cores instead of the specified number I am using the multiprocessing module of Python and more precisely the Pool class and its map method to run a function (called evaluate) in parallel onto a list of Python objects (called object_list).\nThe problem occurs with a computer that has 2CPUs of 20 cores each:\n\n```\n>>> import multiprocessing as mp\n>>> mp.cpu_count()\n40\n\n```\n\nEach run of the function is quite long and also independent, so I chose multiprocessing module to run my function on my list instead of doing it in serial with a for loop for instance.\nSo basically I am using the Pool class, the map method and specifies the processes argument to be 8. The syntax is then:\n\n```\n# import the module\nimport multiprocessing as mp\n\n# creating a Pool instance that should run 8 cores\npool = mp.Pool(processes=8)\n\n# running my evaluations in parallel using Pool.map\npool.map(evaluate, object_list)\n\n```\n\nUnfortunately, my code is too long, and I have not been able to provide a minimum working example. All I can do is describe what syntax I use (like above). So here is my problem.\nI run my script on 2 computers. The first one is composed of a CPU of 12 cores. The second one is composed of 2 CPUs of 20 cores each.\n\nWhen I run my script on the first computer, on 8 cores (processes=8), the terminal command htop shows, as expected, 8 lines for my script (called EA_launch.py):\n\n\nNow, for the second computer, all cores (40) are running, even if it is the exact same script, and only 8 cores are specified. As we can see, 8 processes are written in white, and the rest are in green. I don't really understand the green ones.\n\n\n\nNote that in the screenshot above, the lines with green python EA_launch.py keeps going over 200 lines. The htop commands shows also that all 40 cores are running (instead of 8). The evaluation is incredibly slowed. I understand that running a code in parallel does not automatically mean better performances. But in my case, with the first computer, running in parallel makes the code much faster (as expected), but with the second computer, this \"strange\" behavior with the cores make it incredibly slower.\nCan anyone enlighten me on that matter ? It seems, the fact that the second computer has 2 CPUs messes with multiprocessing module.",
    "answer": "import os\nos.environ[\"MKL_NUM_THREADS\"] = \"1\"\nos.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\n\nThank you a lot @CharlesDuffy ! That post: limit number of threads in numpy? (and the relevant comments) has solved my problem.\nAdding those lines:\n\nBEFORE IMPORTING numpy, made my code work well on both systems !"
  },
  {
    "question": "free the memory from a process in linux? when I run pmap <pid> It is giving me below list\n\n```\n 00007f545bbc5000   1016K rw---    [ anon ]\n 00007f545bd0c000     76K r-x--  /opt/openmrs/.openmrs-lib-cache/bahmnimsf/org/bytedeco/javacpp/linux-x86_64/libjniswscale.so\n 00007f545bd1f000   2044K -----  /opt/openmrs/.openmrs-lib-cache/bahmnimsf/org/bytedeco/javacpp/linux-x86_64/libjniswscale.so\n 00007f545bf1e000      4K rw---  /opt/openmrs/.openmrs-lib-cache/bahmnimsf/org/bytedeco/javacpp/linux-x86_64/libjniswscale.so\n 00007f545c1bc000     52K r-x--  /opt/openmrs/.openmrs-lib-cache/bahmnimsf/org/bytedeco/javacpp/linux-x86_64/libjniswresample.so\n 00007f545c3ca000     12K -----    [ anon ]\n 00007f545c3cd000   1016K rw---    [ anon ]\n 00007f545c4cd000   2048K rw---    [ anon ]\n 00007f545c6cd000     12K -----    [ anon ]\n 00007f545c6d0000   1016K rw---    [ anon ]\n 00007f545c812000   1676K r--s-  /opt/openmrs/lucene/indexes/org.openmrs.PersonAttribute/_1y_Lucene41_0.pos\n 00007f545c9b5000    108K r-x--  /opt/openmrs/.openmrs-lib-cache/bahmnimsf/org/bytedeco/javacpp/linux-x86_64/libswresample.so.2\n 00007f545cbd2000     12K -----    [ anon ]\n 00007f545cbd5000   1016K rw---    [ anon ]\n\n```\n\nI can see the details of one process when I runt the command. But, \nHere [anon] is taking some memory right. Can I free that memory? \nHere I don't want to kill the process. Just want to free the memory from that process.\nThanks in advance",
    "answer": "That depends on what you mean.  Most likely, the answer is no because this is a list of memory consumptions of different parts of the given process.  You typically have no influence over these parts.  [ anon ] just means that this part has no specific name which could be shown, so it is declared anonymous.  (It could be the main program of the process as opposed to some library used within that process.)\nYour only way of freeing that memory is to end the process, be it by urging it to terminate (maybe via a GUI) or by sending it a signal (e. g. via kill (1)).\nAny other way of freeing the memory from the outside (for which there is no interfaces in existence, but one might be able to do this using a debugger or similar) would mean to meddle in the affairs of the running process.  That is not advisable.  The process will sooner or later access the memory it allocated.  If that isn't allocated anymore, it will be sent a SEGFAULT and consequently typically terminate.\nThat being said, it could be that you are considering changing the program to allocate less memory.  In this case the question is way too broad and would rely heavily on the program you are looking at."
  },
  {
    "question": "C++ Stack and Heap in RAM I'm running the following c++ code on Ubuntu with 4GBs of RAM\n\n```\nconst long long  nSize = 400000000;\ndouble Array1[nSize];\nfor(int i=0; i<nSize; i++)\n    Array1[i]= 2*2; // store on the stack\n\n```\n\nAnd this fits on RAM (and my computer doesn't complain). Confusingly... htop says barely any additional RAM is being used during runtime... Why?. (I usually let it sleep for 100 seconds in case it needs to update)\nOn the other hand, if I dynamically allocate the huge array (as tutorials like this recommend I should do) -- htop tells me it's using up most of the RAM (if not all of it and it crashes):\n\n```\ndouble *pnArray2 = new double[nSize];\nfor(int i=0; i<nSize; i++)\n    pnArray2[i] = 2*2; // store on the heap\n\n```\n\nSo why should I use the heap to store big data structures... if (like in this example) the stack can handle even bigger arrays?\nI thought the heap was supposed to be bigger than the stack! Please tell me where I'm going so wrong.",
    "answer": "std::cout << Array1[42];\n\nIt's probably just the optimizer doing its job (or not, in the second case). It's probably harder for it to optimize the second allocation away since you could theoretically access that memory outside the scope of the pointer.\nI managed to repro this in MSVS 2010, on Release mode, and adding a simple\n\nbrought the memory usage to the same value. (granted, I did use a lower value)\nThere's also no code generated for the first snipped, but there is for the second one."
  },
  {
    "question": "How can I gracefully recover from an attached Docker container terminating? Say I run this Docker command in one Terminal window:\n\n```\n$ docker run --name stackoverflow --rm ubuntu /bin/bash -c \"sleep 5\"\n\n```\n\nAnd before it exits I run this in a second Terminal window:\n\n```\n$ docker run -it --rm --pid=container:stackoverflow terencewestphal/htop\n\n```\n\nI'll successfully see htop running in the second container, displaying the bash sleep process running. So far so good.\nAfter 5 seconds, the first container will exit with code 0. All good.\nAt this time, the second container will exit with code 137 (SIGILL). This also makes sense to me since the second container is just attached to the first one.\nThe problem is that this messes up macOS's Terminal.app's state:\n\nThe Terminal's cursor disappears.\nClicking the Terminal window causes mouse location characters to be entered as input.\n\n\nI'm hoping to find a way to avoid messing up Terminal.app state. Any suggestions?",
    "answer": "You can't avoid such behaviour, because it is the htop duty to setup the terminal state after its termination, but it can't do it when terminated with SIGKILL. However, you can fix this terminal window yourself with the reset command, which is intended to initialize the terminal state.\nAbout the \"attached\" container:\nThe --pid=container:<name> option means that the new container would be run in the PID namespace of first container and as the pid_namespaces(7) man page says:\n\nIf the \"init\" process of a PID namespace terminates, the kernel\nterminates all of the processes in the namespace via a SIGKILL signal."
  },
  {
    "question": "Web Server extremely slow, while CPU at 0-5% and RAM at 500/1024 I programmed a web chat, which displayed new entries by refreshing a  via an ajax command. This command launched a php file which then created the chat log showing the latest 25 entries. Ajax refreshed this  every second for every user. \nThen, when there were around 10 users online, the whole website went unbelievably slow and kind of crashed. I figured, that the slowness came from the php script refreshing so often (>10 times every second).\nI took a look at my vserver, launched htop to view the processes. The CPU was barely used. It was between 0% and peaked up to 5%. RAM was only half used, at around 500 MB of 1GB (which is standard, even before the chat went online).\nI resolved the issue by creating a cron job that creates only 1 html cache-site of the chat log and giving that one out to the users.\nBut still, I wonder why would the server go so slow, even though CPU and RAM weren't busy at all?",
    "answer": "If you aren't maxing out cpu then it's blocking on some other resource.  Some possible candidates:\n\nis there a db query for every request?  If so, how long are they taking?\nhow many threads do you have available to serve requests?  How long does each request take?  If there is only one thread serving requests then an incoming load of 100 requests per second (which it sounds like you were doing) would start backing up once a request took longer than 10ms, at best.\n\nAs a general strategy, I'd add timers and try to bisect the problem.  Take the time at the beginning and end of the request, if this is small then you know the delay is elsewhere.\nAlso, for this kind of workload you should consider pushing updates rather than polling.  That way you can push a single message to update listening clients and they don't have to poll frequently to get new messages right away."
  },
  {
    "question": "How do you work out what&#39;s making up the total CPU% on Linux? My fan is whirring on my Ubuntu laptop and htop is showing my CPU as maxed:\n\nHowever, looking at the processes ordered by CPU it doesn't seem like too much is going on other than gjs at 41.3%.\n\nI'm assuming there are just a ton of gjs processes that are adding up to the rest of the CPU.\nIs there anyway to work this out other than manually adding up the CPU%?\n\n```\nNAME=\"Ubuntu\"\nVERSION_ID=\"21.10\"\n\n```",
    "answer": "ps -eo pcpu,command --sort=-pcpu | grep gjs | awk '{sum+=$1} END {print sum}'\nps -eo pcpu,command --sort=-pcpu | grep \"$1\" | awk '{sum+=$1} END {print sum}'\n\nYou can sum up CPU usage as shown here.\n\nThe solution they linked actually sums memory, not CPU usage (probably a bug they never caught), I've fixed it so it should work for you.\nIf you want to make a shell script to reuse, write this to cpusum for example:\n\nthen make it executable: chmod +x cpusum,\nand run it ./cpusum gjs"
  },
  {
    "question": "&quot;used memory&quot; high, but seems inaccurate - ksysguardd compared to htop? I'm monitoring a remote machine using ksysguardd, and it reports a flat unchanging timeline of 3.6 gigs, with free memory of .20 (the machine has 3.85 gigs available)\nYet with htop a range of 400-600 / 3830MB is used.(Which it signifies by a full yellow row of bars.)\nAs usual I assume its me that is confused. \nAm I reading the numbers wrong that ksysguardd says things are running high, or is htop saying things are indeed ok at 600/3830MB used?",
    "answer": "The yellow bars in htop are for memory used for disk caching, which makes the system run more smoothly. This memory is given to applications whenever they need it, so you're fine!\nYou can read more on this (pretty awesome) website.\nTo know what all of the colours in htop correspond to, read this.\nHope that helps!"
  },
  {
    "question": "Why is the status of top or htop always &quot;R&quot; (running)? top and htop are commonly used tools to monitor processes and computer resources, but why is the status of top itself always R (in the S column of the top)? For example, here is a screenshot of the top command:\n\n```\ntop - 12:42:33 up  2:48,  1 user,  load average: 0,11, 0,17, 0,17\nTasks: 319 total,   1 running, 318 sleeping,   0 stopped,   0 zombie\n%Cpu(s):  1,1 us,  0,1 sy,  0,0 ni, 98,8 id,  0,0 wa,  0,0 hi,  0,0 si,  0,0 st\nMiB Mem :  15968,5 total,   4031,8 free,   2196,1 used,   9740,7 buff/cache\nMiB Swap: 122069,0 total, 122069,0 free,      0,0 used.  13325,4 avail Mem \n\n    PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND\n   2264 xxxxxx    20   0 2784640 320344 184092 S   6,0   2,0   9:14.83 Web Content\n   2195 xxxxxx    20   0 3996144 481160 197788 S   2,7   2,9   7:46.13 firefox\n   1704 root      20   0  227712 101204  82140 S   1,3   0,6   3:26.54 Xorg\n   2993 xxxxxx    20   0  817416  54452  41724 S   1,0   0,3   0:04.27 gnome-terminal-\n   1856 xxxxxx    20   0 4521152 385832 123440 S   0,7   2,4   2:39.79 gnome-shell\n   1202 root     -51   0       0      0      0 S   0,3   0,0   0:52.07 irq/44-nvidia\n   5048 xxxxxx    20   0   11980   3888   3132 R   0,3   0,0   0:00.12 top\n      1 root      20   0  167972  11864   8480 S   0,0   0,1   0:03.91 systemd\n      2 root      20   0       0      0      0 S   0,0   0,0   0:00.00 kthreadd\n      3 root       0 -20       0      0      0 I   0,0   0,0   0:00.00 rcu_gp\n      4 root       0 -20       0      0      0 I   0,0   0,0   0:00.00 rcu_par_gp\n      6 root       0 -20       0      0      0 I   0,0   0,0   0:00.00 kworker/0:0H-kblockd\n      9 root       0 -20       0      0      0 I   0,0   0,0   0:00.00 mm_percpu_wq\n     10 root      20   0       0      0      0 S   0,0   0,0   0:00.09 ksoftirqd/0\n     11 root      20   0       0      0      0 I   0,0   0,0   0:01.49 rcu_sched\n     12 root      rt   0       0      0      0 S   0,0   0,0   0:00.02 migration/0\n     13 root     -51   0       0      0      0 S   0,0   0,0   0:00.00 idle_inject/0\n\n\n```\n\nFrom the source code of top (procps/top), it gets process statuses from the file /proc/<pid>/stat, while the status of top, for most of the time, is S (sleeping) if it is continuously printed with the following command:\n\n```\nwatch -n.1 \"cat /proc/<top-pid>/stat | grep -o \\\"[S|R]\\\"\"\n\n```",
    "answer": "cat /proc/self/stat\n(cat) R ...\nhead /proc/self/stat\n(head) R ...\ntail /proc/self/stat\n(tail) R ...\nless /proc/self/stat\n(less) R ...\n\nWell... if you are running top (or htop), and top (or htop) gets its own status from /proc/self/stat... then it must be running, otherwise how would it be possible that it got its status while not running? The sole fact that a process is reading its own status means that the status must be running at the time of reading.\nYou can try this with other programs too, if you want:"
  },
  {
    "question": "Prevent excessive CPU usage of Golang background process I am writing a Go program that watches files and run a system command if one of the files has changed. So far, it works well. I have questions regarding my main \"infinite\" loop because I don't want it to take all system CPU resources:\n\n```\nruntime.GOMAXPROCS(1)\nfor {\n    updatedFiles, _ := GetWatchMap(config)\n    if !reflect.DeepEqual(filesToWatch, updatedFiles) {\n        start := time.Now()\n        _, _ = colorstring.Println(fmt.Sprintf(\" [yellow] \u2b07 Update detected[white] at [green]%s[white] > updating...\", start.Format(\"15:04:05\")))\n        _, _ = Update(config)\n        end := time.Now()\n        elapsed := end.Sub(start)\n        _, _ = colorstring.Println(fmt.Sprintf(\"  [green]\u2705  Done![white] in [yellow]%.2f[white] second(s).\", elapsed.Seconds()))\n        filesToWatch = updatedFiles\n    } else {\n        time.Sleep(config.SleepTime)\n    }\n}\n\n```\n\nSo, what I have done is setting GOMAXPROCS, so it only uses \"1 CPU/Core\", and I have added a configurable sleep time in the else branch. \nWithout the sleep time, htop shows that the process takes 100% of CPU time (I guess it is 100% of one core?) whatever If I call runtime.GOMAXPROCS(1) or not.\nIf I use a sleep time of 30ms on my machine (MacMini i7, 12 Core) htop reports 20% CPU utilisation from the process, which seems OK, but I guess this will vary depending on the computer running the program.\nWhat is the best practice here?",
    "answer": "GOMAXPROCS does not what you think it does. From package runtime docu:\n\nThe GOMAXPROCS variable limits the number of operating system threads that can execute user-level Go code simultaneously. There is no limit to the number of threads that can be blocked in system calls on behalf of Go code; those do not count against the GOMAXPROCS limit.\n\nIt limits OS threads. If your code doesn't use goroutines which could be scheduled to OS threads limiting the number of threads does literally nothing.\nJust remove the GOMAXPROCS stuff, it does nothing. (If you set GOMAXPROCS to 12 you will have at most 12 OS threads actually executing goroutine code; If you have just one goroutine limiting  the number of OS threads it can run on is a noop.)\nAll you can do is not busy-looping as you did with time.Sleep(). Depending on your requirements you could call the actual code regularly, e.g. via a time.Ticker. There is no single \"best practice\" (excepet not fiddling with GOMAXPROCS)."
  },
  {
    "question": "htop reporting Tomcat 8.5 as consuming 4GB when I&#39;ve set -Xmx1024M I have configured my Tomcat instance to not use more than 1GB of RAM by setting the following in setenv.sh:\n\n```\n$ cat /opt/tomcat8.5/bin/setenv.sh \n#!/bin/sh\n\nexport JAVA_OPTS=\"-Djava.awt.headless=true -server -Xms48m -Xmx1024M -XX:MaxPermSize=512m\"\n\n```\n\nIndeed, this appears to be taken into account by the running Tomcat instance:\n\n```\n$ ps aux | grep -i tomcat-juli.jar | grep -v grep | awk '{print $2}'\n26105\n$ sudo jinfo 26105 | grep VM\\ flagsNon-default VM flags: -XX:CICompilerCount=3 -XX:InitialHeapSize=50331648 -XX:MaxHeapSize=1073741824 -XX:MaxNewSize=357564416 -XX:MinHeapDeltaBytes=524288 -XX:NewSize=16777216 -XX:OldSize=33554432 -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseFastUnorderedTimeStamps -XX:+UseParallelGC \n\n```\n\nAnd by attaching to the running instance using Java VisualVM, I see a picture compatible with the above:\n\nWhy is then htop reporting my Tomcat as consuming half of my 8 GB memory?",
    "answer": "Limiting the heap size (-Xmx1024M) only limits the size of the Java heap. The JVM is free to use as much \"native\" memory as it feels is necessary in order to run itself.\nThings that aren't part of the Java heap:\n\nPermGen\nThread stacks\nNative code compiled by the JIT\n\nHave a look at Java native memory usage to see how you can get some insight into what memory is being used outside the Java heap.\nNote that if you are willing to give your JVM a whole gig or RAM, you may as well set -Xms == -Xmx, otherwise you will just waste CPU cycles re-sizing memory at intervals until the heap reaches its maximum size."
  },
  {
    "question": "Several node instances being created for a simple Hello world program I am very new to Javascript and NodeJS.\nI was running a simple helloworld programs as follows\nProgram 1\n\n```\nconst durationInSeconds = 10;\n\nconsole.log('Hello World');\n\nsetTimeout(() => {\n  console.log(`Program has been running for ${durationInSeconds} seconds.`);\n}, durationInSeconds * 1000);\n\n\n```\n\nWhen I ran the program , I was monitoring the processes using the htop command in linux.\nI noticed that the application was creating 7 node instances of the same application.\nWhy is this happening?\nWhy is it not creating only one node instance for a single simple application?\nI had this question because if I run a similar program in python I see only one instance of the python application running.",
    "answer": "Nodejs is needs threads to do other tasks that are handled automatically for your by the V8 engine. Some of the things are\n\nInterpreter\nEvent loop\nGarbage collector\nBlocking I/O executor\nand others...\n\nNodejs make programming easy by hiding these complexities from the programmer.\nIf you need more control on these lower level 'stuff' then you can use C,C++ or other low level lanuages where you have to decide what should in which thread."
  },
  {
    "question": "Cassandra: Is it normal to have so many gc.log processes I am quite new to cassandra, so if someone can explain me what I see here. I have a cassandra ring with 16 nodes(simple strategy) and if I use htop on the nodes I see too many gc.log processes taking place which also I think occupy a lot of memory! 16 of them belong to the 16 nodes, but what about the rest? Is that normal to have so many logging events?",
    "answer": "It's not gc.log processes, it's the single Cassandra process that has an option that specifies where to store gc.log file.  You see so many because by default htop shows all threads of the process, not single process item (see this answer for explanation).  You can check this by executing ps -aef|grep gc.log - you should see only one item for Cassandra process."
  },
  {
    "question": "OpenCV forEach function parallel access I'm processing RGB images and doing the same thing for each channel (R+G+B) so I've been looking for parallel functions that could help me improve my code and run it (3*?) faster. Now Im using forEach function like so:\n\n```\nunsigned char lut[256];\nfor (int i = 0; i < 256; i++)\n    lut[i] = cv::saturate_cast<uchar>(pow((float)(i / 255.0), fGamma) * 255.0f); //pow: power exponent\n\ndst.forEach<cv::Vec3b> //dst is an RGB image\n(\n    [&lut](cv::Vec3b &pixel, const int* po) -> void\n    {\n        pixel[0] = lut[(pixel[0])];\n        pixel[1] = lut[(pixel[1])];\n        pixel[2] = lut[(pixel[2])];\n    }\n);\n\n```\n\nBut when I use htop to see the number of threads running, I only find one or two threads on..\n\nAm I doing something wrong or forEach isn't suppose to run on multi-threading? Do you have any resources to help me with to get to multi-threading computations?\nI'm running my code on ubuntu with this:\n\n```\ng++ -std=c++1z -Wall -Ofast -march=native test3.cpp -o test3 `pkg-config --cflags --libs opencv`\n\n```",
    "answer": "Have you taken a look in TBB already? Threading Building Blocks is an apache licensed lib for parallel computing which you can use just compiling OpenCV with the flag -D WITH_TBB=ON\nSee this example of parallel_for: http://www.jayrambhia.com/blog/opencv-with-tbb\nIf you decide to adopt TBB follow these steps:\n1 - Rebuild OpenCV with TBB support. If you are running into a Linux machine just do:\ncmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local -D WITH_TBB=ON BUILD_TBB=ON ..\n2 - Rewrite your program to use TBB\nSee the answers there: Simplest TBB example focusing in the most recent ones."
  },
  {
    "question": "Linux Webserver - htop shows extreme cpu usage? I am a bit confused about what the tool \"htop\" shows as cpu usage and average load. I was asked to have a look at a webserver which is performing incredibly slow.\nI googled a bit and always found the statement that everyting above 1.00 in average load is terrible when you only have one cpu in the machine.\nHowever, my \"htop\" experience looks like this:\nhtop screenshot\nCan someone please tell me what exactly is going on here? Is this bad or do I misunderstand everything?\nThank you for your help.",
    "answer": "In your screenshot the CPU usage bars are colored in green and red. Press '?' in htop for a help screen to show up. From there you will see that green color is for a normal priority userspace applications CPU usage and the red color is for kernel threads.\nBasically, in your screenshot all the CPU cores are 100% busy and most of the time they spend in the kernel.\nYes, this is bad. Further investigation is needed to tell what exactly is going on here."
  },
  {
    "question": "Get thread Id for a process programmatically that matches htop&#39;s corresponding pid I have seen that in htop's tree mode my multithreaded program has several processes under it. I know they are thread ids. But this id doesn't match the thread id that was returned by the pthread_create function. \nint _id = pthread_create(&m_iAudioThreadID, NULL, AudioRecvThread, this);\nIs the m_iAudioThreadID supposed to be equal to the PID we see in htop's tree mode for a process? It doesn't though. How do I find the PID of htop's programmatically from my program? Thanks.",
    "answer": "pid_t gettid(void)\n{\nreturn syscall(SYS_gettid);\n}\n\nIs the m_iAudioThreadID supposed to be equal to the PID we see in htop's tree mode for a process?\n\nNo, they are not. htop shows you process-ids, PIDs. PThread-IDs as set by pthread_create() are different: Distinction between processes and threads in Linux\nOne main difference is that PIDs are uniquely indentifying a process within the existing processes of a system, PThread-IDs are uniquely indentifying a thread within the existing threads of a process.\n\nHow do I find the PID of htop's programmatically from my program?\n\nAt least on a recent Linux: To get the PID associated to a certain PThread use the gettid() system call from within the thread in question:\n\n(inspired by http://man7.org/linux/man-pages/man2/syscall.2.html)"
  },
  {
    "question": "Extracting certain information from htop command I want to extract the highest CPU load, the process ID causing the high CPU problem and the app name from the htop command. Please take a look at this screenshot.\n\nThe first rectangle on the left is to get the PID of the highest CPU load process.\nThe one in the middle is the CPU load. I need this to compare with a threshold value.\nThe third one is the specific server causing this problem. I need 204-8204 information to kill that instance.\nI got some example commands from chatGPT but it's always working. For instance, it gives me 345 where the actual load is over 3000. Since the threshold is much higher than 345, I cannot kill the process.\nHow can I solve this?",
    "answer": "top -o \"%CPU\" -c -b -n 1 | awk 'NR==8{for(i=13;i<=NF;i++){command = command\" \"$i}; printf \"%s\\t%s\\t%s\\n\",$1,$9,command}'\n29.4     top -o %CPU -c -b -n 1\ntop -o \"%CPU\" -c -b -n 1 |awk 'BEGIN{p=0} $1==\"PID\"{p=1;next} {if(p==1){command=gensub(/^.*-D\\[Server:([0-9-]+).*/,\"\\\\1\",\"1\"); printf \"%s\\t%s\\t%s\\n\",$1,$9,command}p=0;command=\"\"}\n\nOn my machine (not as many cores as yours) this works, the first line of process data sits on line number 8, hence the  NR==8 in the awk command. The  little for-loop joins all fields from # 13 (beginning of command) to NF (number of fields in line). Top is instructed to sort by %CPU, output the full command rather than the process name and output one line in batch mode.\n\nI don't have sample process names that match yours and have no intention to type all that ... so the parsing out of your [Server-mumble]1 is up to you.\nP.S.: I adjusted the awk to cater to your special needs, extracting the numeric substring from your -D parameter ...\n\nHint: There's a reason that images of text are discouraged"
  },
  {
    "question": "Aggregate Top CPU Using Processes When I run top -n 1 -d 2 | head -n 12; it returns processor usage for some processes sorted by %cpu desc as desired, but I'm not convinced that the results are aggregated as they should be. I'm wanting to put these results in a file maybe like\n\n```\nwhile true; do\n    top -n 1 -d 2 | head -n 12;\ndone > top_cpu_users;\n\n```\n\nWhen I run top -d 2; interactively, I first see some results, then two seconds later I see the results updated and they appear to be aggregated over the last two seconds. The first results do not appear to be aggregated in the same way.\nHow do I get top cpu users every two seconds aggregated over the previous two seconds?",
    "answer": "output=\"top_cpu_users\"\nrm -f ${output} ${output}.tmp\n\nsnapshots=5\ninterval=2\nprocess_count=6         ### Number of heavy hitter processes being monitored\ntop_head=7          ### Number of header lines in top report\nlines=$(( ${process_count} + ${top_head} )) ### total lines saved from each report run\n\necho -e \"\\n Collecting process snapshots every ${interval} seconds ...\"\ntop -b -n $(( ${snapshots} + 1 )) -d ${interval} > ${output}.tmp\n\necho -e \"\\n Parsing snapshots ...\"\nawk -v max=\"${lines}\" 'BEGIN{\ndoprint=0 ;\nfirst=1 ;\n}\n{\nif( $1 == \"top\" ){\nif( first == 1 ){\nfirst=0 ;\n}else{\nprint NR | \"cat >&2\" ;\nprint \"\" ;\ndoprint=1 ;\nentry=0 ;\n} ;\n} ;\nif( doprint == 1 ){\nentry++ ;\nprint $0 ;\nif( entry == max ){\ndoprint=0 ;\n} ;\n} ;\n}' ${output}.tmp >${output}\n\nmore ${output}\nCollecting process snapshots every 2 seconds ...\n\nParsing snapshots ...\ntop - 20:14:02 up  8:37,  1 user,  load average: 0.15, 0.13, 0.15\nTasks: 257 total,   1 running, 256 sleeping,   0 stopped,   0 zombie\n%Cpu(s):  0.5 us,  1.0 sy,  0.0 ni, 98.5 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\nMiB Mem :   3678.9 total,    157.6 free,   2753.7 used,    767.6 buff/cache\nMiB Swap:   2048.0 total,   1116.4 free,    931.6 used.    629.2 avail Mem\n\nPID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND\nroot      20   0       0      0      0 I   1.5   0.0   0:09.08 kworker/0:3-events\nericthe+  20   0   14500   3876   3092 R   1.0   0.1   0:00.04 top\nmysql     20   0 2387360  20664   2988 S   0.5   0.5   3:10.11 mysqld\nericthe+  20   0 1949412 130004  20272 S   0.5   3.5   0:46.16 caja\nericthe+  20   0 4837044 461944 127416 S   0.5  12.3  81:26.50 firefox\nericthe+  20   0 2636764 165632  54700 S   0.5   4.4   0:36.97 Isolated Web Co\n\ntop - 20:14:04 up  8:37,  1 user,  load average: 0.14, 0.13, 0.15\nTasks: 257 total,   1 running, 256 sleeping,   0 stopped,   0 zombie\n%Cpu(s):  1.5 us,  0.7 sy,  0.0 ni, 97.4 id,  0.4 wa,  0.0 hi,  0.0 si,  0.0 st\nMiB Mem :   3678.9 total,    157.5 free,   2753.7 used,    767.6 buff/cache\nMiB Swap:   2048.0 total,   1116.4 free,    931.6 used.    629.2 avail Mem\n\nPID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND\nericthe+  20   0 4837044 462208 127416 S   3.0  12.3  81:26.56 firefox\nmysql     20   0 2387360  20664   2988 S   1.0   0.5   3:10.13 mysqld\nericthe+  20   0   14500   3876   3092 R   1.0   0.1   0:00.06 top\nroot      20   0  546692  61584  48956 S   0.5   1.6  17:23.78 Xorg\nericthe+  20   0  303744  11036   7500 S   0.5   0.3   4:46.84 compton\nericthe+  20   0 2617520 127452  44768 S   0.5   3.4   1:41.13 Isolated Web Co\n\ntop - 20:14:06 up  8:37,  1 user,  load average: 0.14, 0.13, 0.15\nTasks: 257 total,   1 running, 256 sleeping,   0 stopped,   0 zombie\n%Cpu(s):  0.6 us,  0.4 sy,  0.0 ni, 99.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\nMiB Mem :   3678.9 total,    157.5 free,   2753.7 used,    767.6 buff/cache\nMiB Swap:   2048.0 total,   1116.4 free,    931.6 used.    629.2 avail Mem\n\nPID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND\nroot      20   0  546700  61584  48956 S   1.5   1.6  17:23.81 Xorg\nericthe+  20   0 4837044 462208 127416 S   1.5  12.3  81:26.59 firefox\nmysql     20   0 2387360  20664   2988 S   0.5   0.5   3:10.14 mysqld\nericthe+  20   0  303744  11036   7500 S   0.5   0.3   4:46.85 compton\nericthe+  20   0  346156  10368   8792 S   0.5   0.3   0:22.97 mate-cpufreq-ap\nericthe+  20   0  346540  11148   9168 S   0.5   0.3   0:41.73 mate-sensors-ap\n\ntop - 20:14:08 up  8:37,  1 user,  load average: 0.14, 0.13, 0.15\nTasks: 257 total,   1 running, 256 sleeping,   0 stopped,   0 zombie\n%Cpu(s):  0.6 us,  0.5 sy,  0.0 ni, 98.9 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\nMiB Mem :   3678.9 total,    157.5 free,   2753.6 used,    767.7 buff/cache\nMiB Swap:   2048.0 total,   1116.4 free,    931.6 used.    629.3 avail Mem\n\nPID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND\nericthe+  20   0   14500   3876   3092 R   1.0   0.1   0:00.08 top\nericthe+  20   0 4837044 462208 127416 S   0.5  12.3  81:26.60 firefox\nericthe+  20   0 2682392  97268  45144 S   0.5   2.6   0:55.36 Isolated Web Co\nericthe+  20   0 2618496 123608  52540 S   0.5   3.3   1:55.08 Isolated Web Co\nericthe+  20   0 2690464 179020  56060 S   0.5   4.8   1:45.57 Isolated Web Co\nericthe+  20   0 2636764 165632  54700 S   0.5   4.4   0:36.98 Isolated Web Co\n\ntop - 20:14:10 up  8:37,  1 user,  load average: 0.13, 0.13, 0.15\nTasks: 257 total,   1 running, 256 sleeping,   0 stopped,   0 zombie\n%Cpu(s):  2.5 us,  0.9 sy,  0.0 ni, 96.6 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\nMiB Mem :   3678.9 total,    157.5 free,   2753.6 used,    767.7 buff/cache\nMiB Swap:   2048.0 total,   1116.4 free,    931.6 used.    629.3 avail Mem\n\nPID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND\nericthe+  20   0 4837076 463000 127416 S   7.5  12.3  81:26.75 firefox\nroot      20   0  546716  61584  48956 S   1.5   1.6  17:23.84 Xorg\nmysql     20   0 2387360  20664   2988 S   1.0   0.5   3:10.16 mysqld\nericthe+  20   0   14500   3876   3092 R   1.0   0.1   0:00.10 top\nericthe+  20   0  303744  11036   7500 S   0.5   0.3   4:46.86 compton\nericthe+  20   0  346540  11148   9168 S   0.5   0.3   0:41.74 mate-sensors-ap\n\ntop will always capture a first full scan of process info for use as a baseline.  It uses that to initialize the utility's database of values used for later comparative reporting.  That is the basis of the first report presented to the screen.\nThe follow-on reports are the true measures for the specified evaluation intervals.\nYour code snippet will therefore never provide what you are really looking for.\nYou need to skip the results from the first scan and only use the follow on reports, but the only way to do that is to generate them from a single command by specifying the count of scans desired, then parse the resulting combined report.\nTo that end, here is a proposed solution:\n\nThe session output for that will look like this:"
  },
  {
    "question": "How to show processes list in zabbix in ubuntu 16.04 I am new in Zabbix, which I am running on about 20 Linux machine (Ubuntu 16.04). I want to see all processes there are on each machine just like PS or hop. Are there any solutions you suggest? I want to have it like the CPU utilization and memory use...\nI tried these commands, but I think it is not supported for Ubuntu. \n\n```\nproc.mem\nproc.cpu.util\n\n```\n\nThank you!",
    "answer": "UserParameter=proc.list,ps -AH\n\nZabbix does not have a built-in item to list all the processes. It would be of a limited use, as you cannot really graph such data, or show a compact history.\nYou can easily add one yourself, though - just add a new userparameter, using ps or top. For example, adding the following in the agent daemon configuration file and restarting the agent will give you a new proc.list item:\n\nAdjust ps or top parameters as necessary."
  },
  {
    "question": "What does the &quot;yield&quot; keyword do in Python? What functionality does the yield keyword in Python provide?\nFor example, I'm trying to understand this code1:\n\n```\ndef _get_child_candidates(self, distance, min_dist, max_dist):\n    if self._leftchild and distance - max_dist < self._median:\n        yield self._leftchild\n    if self._rightchild and distance + max_dist >= self._median:\n        yield self._rightchild  \n\n```\n\nAnd this is the caller:\n\n```\nresult, candidates = [], [self]\nwhile candidates:\n    node = candidates.pop()\n    distance = node._get_dist(obj)\n    if distance <= max_dist and distance >= min_dist:\n        result.extend(node._values)\n    candidates.extend(node._get_child_candidates(distance, min_dist, max_dist))\nreturn result\n\n```\n\nWhat happens when the method _get_child_candidates is called?\nIs a list returned? A single element? Is it called again? When will subsequent calls stop?\n\n\n1. This piece of code was written by Jochen Schulz (jrschulz), who made a great Python library for metric spaces. This is the link to the complete source: Module mspace.",
    "answer": ">>> mylist = [1, 2, 3]\n>>> for i in mylist:\n...    print(i)\n>>> mylist = [x*x for x in range(3)]\n>>> for i in mylist:\n...    print(i)\n>>> mygenerator = (x*x for x in range(3))\n>>> for i in mygenerator:\n...    print(i)\n>>> def create_generator():\n...    mylist = range(3)\n...    for i in mylist:\n...        yield i*i\n...\n>>> mygenerator = create_generator() # create a generator\n>>> print(mygenerator) # mygenerator is an object!\n<generator object create_generator at 0xb7555c34>\n>>> for i in mygenerator:\n...     print(i)\ndef _get_child_candidates(self, distance, min_dist, max_dist):\n\nif self._leftchild and distance - max_dist < self._median:\nyield self._leftchild\n\nif self._rightchild and distance + max_dist >= self._median:\nyield self._rightchild\nresult, candidates = list(), [self]\n\nwhile candidates:\n\nnode = candidates.pop()\n\ndistance = node._get_dist(obj)\n\nif distance <= max_dist and distance >= min_dist:\nresult.extend(node._values)\n\ncandidates.extend(node._get_child_candidates(distance, min_dist, max_dist))\n\nreturn result\n>>> a = [1, 2]\n>>> b = [3, 4]\n>>> a.extend(b)\n>>> print(a)\n[1, 2, 3, 4]\n>>> class Bank(): # Let's create a bank, building ATMs\n...    crisis = False\n...    def create_atm(self):\n...        while not self.crisis:\n...            yield \"$100\"\n>>> hsbc = Bank() # When everything's ok the ATM gives you as much as you want\n>>> corner_street_atm = hsbc.create_atm()\n>>> print(corner_street_atm.next())\n>>> print(corner_street_atm.next())\n>>> print([corner_street_atm.next() for cash in range(5)])\n['$100', '$100', '$100', '$100', '$100']\n>>> hsbc.crisis = True # Crisis is coming, no more money!\n>>> print(corner_street_atm.next())\n<type 'exceptions.StopIteration'>\n>>> wall_street_atm = hsbc.create_atm() # It's even true for new ATMs\n>>> print(wall_street_atm.next())\n<type 'exceptions.StopIteration'>\n>>> hsbc.crisis = False # The trouble is, even post-crisis the ATM remains empty\n>>> print(corner_street_atm.next())\n<type 'exceptions.StopIteration'>\n>>> brand_new_atm = hsbc.create_atm() # Build a new one to get back in business\n>>> for cash in brand_new_atm:\n...    print cash\n...\n>>> horses = [1, 2, 3, 4]\n>>> races = itertools.permutations(horses)\n>>> print(races)\n<itertools.permutations object at 0xb754f1dc>\n>>> print(list(itertools.permutations(horses)))\n[(1, 2, 3, 4),\n(1, 2, 4, 3),\n(1, 3, 2, 4),\n(1, 3, 4, 2),\n(1, 4, 2, 3),\n(1, 4, 3, 2),\n(2, 1, 3, 4),\n(2, 1, 4, 3),\n(2, 3, 1, 4),\n(2, 3, 4, 1),\n(2, 4, 1, 3),\n(2, 4, 3, 1),\n(3, 1, 2, 4),\n(3, 1, 4, 2),\n(3, 2, 1, 4),\n(3, 2, 4, 1),\n(3, 4, 1, 2),\n(3, 4, 2, 1),\n(4, 1, 2, 3),\n(4, 1, 3, 2),\n(4, 2, 1, 3),\n(4, 2, 3, 1),\n(4, 3, 1, 2),\n(4, 3, 2, 1)]\n\nTo understand what yield does, you must understand what generators are. And before you can understand generators, you must understand iterables.\nIterables\nWhen you create a list, you can read its items one by one. Reading its items one by one is called iteration:\n\nmylist is an iterable. When you use a list comprehension, you create a list, and so an iterable:\n\nEverything you can use \"for... in...\" on is an iterable; lists, strings, files...\nThese iterables are handy because you can read them as much as you wish, but you store all the values in memory and this is not always what you want when you have a lot of values.\nGenerators\nGenerators are iterators, a kind of iterable you can only iterate over once. Generators do not store all the values in memory, they generate the values on the fly:\n\nIt is just the same except you used () instead of []. BUT, you cannot perform for i in mygenerator a second time since generators can only be used once: they calculate 0, then forget about it and calculate 1, and end after calculating 4, one by one.\nYield\nyield is a keyword that is used like return, except the function will return a generator.\n\nHere it's a useless example, but it's handy when you know your function will return a huge set of values that you will only need to read once.\nTo master yield, you must understand that when you call the function, the code you have written in the function body does not run. The function only returns the generator object, this is a bit tricky.\nThen, your code will continue from where it left off each time for uses the generator.\nNow the hard part:\nThe first time the for calls the generator object created from your function, it will run the code in your function from the beginning until it hits yield, then it'll return the first value of the loop. Then, each subsequent call will run another iteration of the loop you have written in the function and return the next value. This will continue until the generator is considered empty, which happens when the function runs without hitting yield. That can be because the loop has come to an end, or because you no longer satisfy an \"if/else\".\n\nYour code explained\nGenerator:\n\nCaller:\n\nThis code contains several smart parts:\n\nThe loop iterates on a list, but the list expands while the loop is being iterated. It's a concise way to go through all these nested data even if it's a bit dangerous since you can end up with an infinite loop. In this case, candidates.extend(node._get_child_candidates(distance, min_dist, max_dist)) exhausts all the values of the generator, but while keeps creating new generator objects which will produce different values from the previous ones since it's not applied on the same node.\n\nThe extend() method is a list object method that expects an iterable and adds its values to the list.\n\nUsually, we pass a list to it:\n\nBut in your code, it gets a generator, which is good because:\n\nYou don't need to read the values twice.\nYou may have a lot of children and you don't want them all stored in memory.\n\nAnd it works because Python does not care if the argument of a method is a list or not. Python expects iterables so it will work with strings, lists, tuples, and generators! This is called duck typing and is one of the reasons why Python is so cool. But this is another story, for another question...\nYou can stop here, or read a little bit to see an advanced use of a generator:\nControlling a generator exhaustion\n\nNote: For Python 3, useprint(corner_street_atm.__next__()) or print(next(corner_street_atm))\nIt can be useful for various things like controlling access to a resource.\nItertools, your best friend\nThe itertools module contains special functions to manipulate iterables. Ever wish to duplicate a generator?\nChain two generators? Group values in a nested list with a one-liner? Map / Zip without creating another list?\nThen just import itertools.\nAn example? Let's see the possible orders of arrival for a four-horse race:\n\nUnderstanding the inner mechanisms of iteration\nIteration is a process implying iterables (implementing the __iter__() method) and iterators (implementing the __next__() method).\nIterables are any objects you can get an iterator from. Iterators are objects that let you iterate on iterables.\nThere is more about it in this article about how for loops work."
  },
  {
    "question": "What does if __name__ == &quot;__main__&quot;: do? What does this do, and why should one include the if statement?\n\n```\nif __name__ == \"__main__\":\n    print(\"Hello, World!\")\n\n```\n\n\nIf you are trying to close a question where someone should be using this idiom and isn't, consider closing as a duplicate of Why is Python running my module when I import it, and how do I stop it? instead. For questions where someone simply hasn't called any functions, or incorrectly expects a function named main to be used as an entry point automatically, use Why doesn't the main() function run when I start a Python script? Where does the script start running?.",
    "answer": "print(\"before import\")\nimport math\n\nprint(\"before function_a\")\ndef function_a():\nprint(\"Function A\")\n\nprint(\"before function_b\")\ndef function_b():\nprint(\"Function B {}\".format(math.sqrt(100)))\n\nprint(\"before __name__ guard\")\nif __name__ == '__main__':\nfunction_a()\nfunction_b()\nprint(\"after __name__ guard\")\npython foo.py\n__name__ = \"__main__\"\nimport foo\n__name__ = \"foo\"\nmath = __import__(\"math\")\nbefore import\nbefore function_a\nbefore function_b\nbefore __name__ guard\nFunction A\nFunction B 10.0\nafter __name__ guard\nbefore import\nbefore function_a\nbefore function_b\nbefore __name__ guard\nafter __name__ guard\nimport os, sys; sys.path.insert(0, os.path.dirname(__file__)) # needed for some interpreters\n\ndef function_a():\nprint(\"a1\")\nfrom foo2 import function_b\nprint(\"a2\")\nfunction_b()\nprint(\"a3\")\n\ndef function_b():\nprint(\"b\")\n\nprint(\"t1\")\nif __name__ == \"__main__\":\nprint(\"m1\")\nfunction_a()\nprint(\"m2\")\nprint(\"t2\")\nimport os, sys; sys.path.insert(0, os.path.dirname(__file__)) # needed for some interpreters\n\ndef function_a():\nprint(\"a1\")\nfrom foo3 import function_b\nprint(\"a2\")\nfunction_b()\nprint(\"a3\")\n\ndef function_b():\nprint(\"b\")\n\nprint(\"t1\")\nprint(\"m1\")\nfunction_a()\nprint(\"m2\")\nprint(\"t2\")\n__name__ = \"__main__\"\n\ndef bar():\nprint(\"bar\")\n\nprint(\"before __name__ guard\")\nif __name__ == \"__main__\":\nbar()\nprint(\"after __name__ guard\")\n\nShort Answer\nIt's boilerplate code that protects users from accidentally invoking the script when they didn't intend to. Here are some common problems when the guard is omitted from a script:\n\nIf you import the guardless script in another script (e.g. import my_script_without_a_name_eq_main_guard), then the latter script will trigger the former to run at import time and using the second script's command line arguments. This is almost always a mistake.\n\nIf you have a custom class in the guardless script and save it to a pickle file, then unpickling it in another script will trigger an import of the guardless script, with the same problems outlined in the previous bullet.\n\nLong Answer\nTo better understand why and how this matters, we need to take a step back to understand how Python initializes scripts and how this interacts with its module import mechanism.\nWhenever the Python interpreter reads a source file, it does two things:\n\nit sets a few special variables like __name__, and then\n\nit executes all of the code found in the file.\n\nLet's see how this works and how it relates to your question about the __name__ checks we always see in Python scripts.\nCode Sample\nLet's use a slightly different code sample to explore how imports and scripts work.  Suppose the following is in a file called foo.py.\n\nSpecial Variables\nWhen the Python interpreter reads a source file, it first defines a few special variables. In this case, we care about the __name__ variable.\nWhen Your Module Is the Main Program\nIf you are running your module (the source file) as the main program, e.g.\n\nthe interpreter will assign the hard-coded string \"__main__\" to the __name__ variable, i.e.\n\nWhen Your Module Is Imported By Another\nOn the other hand, suppose some other module is the main program and it imports your module. This means there's a statement like this in the main program, or in some other module the main program imports:\n\nThe interpreter will search for your foo.py file (along with searching for a few other variants), and prior to executing that module, it will assign the name \"foo\" from the import statement to the __name__ variable, i.e.\n\nExecuting the Module's Code\nAfter the special variables are set up, the interpreter executes all the code in the module, one statement at a time. You may want to open another window on the side with the code sample so you can follow along with this explanation.\nAlways\n\nIt prints the string \"before import\" (without quotes).\n\nIt loads the math module and assigns it to a variable called math. This is equivalent to replacing import math with the following (note that __import__ is a low-level function in Python that takes a string and triggers the actual import):\n\nIt prints the string \"before function_a\".\n\nIt executes the def block, creating a function object, then assigning that function object to a variable called function_a.\n\nIt prints the string \"before function_b\".\n\nIt executes the second def block, creating another function object, then assigning it to a variable called function_b.\n\nIt prints the string \"before __name__ guard\".\n\nOnly When Your Module Is the Main Program\n\nIf your module is the main program, then it will see that __name__ was indeed set to \"__main__\" and it calls the two functions, printing the strings \"Function A\" and \"Function B 10.0\".\n\nOnly When Your Module Is Imported by Another\n\n(instead) If your module is not the main program but was imported by another one, then __name__ will be \"foo\", not \"__main__\", and it'll skip the body of the if statement.\n\nAlways\n\nIt will print the string \"after __name__ guard\" in both situations.\n\nSummary\nIn summary, here's what'd be printed in the two cases:\n\nWhy Does It Work This Way?\nYou might naturally wonder why anybody would want this.  Well, sometimes you want to write a .py file that can be both used by other programs and/or modules as a module, and can also be run as the main program itself.  Examples:\n\nYour module is a library, but you want to have a script mode where it runs some unit tests or a demo.\n\nYour module is only used as a main program, but it has some unit tests, and the testing framework works by importing .py files like your script and running special test functions. You don't want it to try running the script just because it's importing the module.\n\nYour module is mostly used as a main program, but it also provides a programmer-friendly API for advanced users.\n\nBeyond those examples, it's elegant that running a script in Python is just setting up a few magic variables and importing the script. \"Running\" the script is a side effect of importing the script's module.\nFood for Thought\n\nQuestion: Can I have multiple __name__ checking blocks?  Answer: it's strange to do so, but the language won't stop you.\n\nSuppose the following is in foo2.py.  What happens if you say python foo2.py on the command-line? Why?\n\nNow, figure out what will happen in foo3.py (having removed the __name__ check):\n\nWhat will this do when used as a script?  When imported as a module?"
  },
  {
    "question": "Does Python have a ternary conditional operator? Is there a ternary conditional operator in Python?",
    "answer": "a if condition else b\n>>> 'true' if True else 'false'\n'true'\n>>> 'true' if False else 'false'\n'false'\n>>> pass if False else pass\nFile \"<stdin>\", line 1\npass if False else pass\n^\nSyntaxError: invalid syntax\n\n>>> # Python parses this as `x = (1 if False else y) = 2`\n>>> # The `(1 if False else x)` part is actually valid, but\n>>> # it can't be on the left-hand side of `=`.\n>>> x = 1 if False else y = 2\nFile \"<stdin>\", line 1\nSyntaxError: cannot assign to conditional expression\n\n>>> # If we parenthesize it instead...\n>>> (x = 1) if False else (y = 2)\nFile \"<stdin>\", line 1\n(x = 1) if False else (y = 2)\n^\nSyntaxError: invalid syntax\na if True\nx = a if True else b\ndef my_max(a, b):\nreturn a if a > b else b\n\nYes, it was added in version 2.5. The expression syntax is:\n\nFirst condition is evaluated, then exactly one of either a or b is evaluated and returned based on the Boolean value of condition. If condition evaluates to True, then a is evaluated and returned but b is ignored, or else when b is evaluated and returned but a is ignored.\nThis allows short-circuiting because when condition is true only a is evaluated and b is not evaluated at all, but when condition is false only b is evaluated and a is not evaluated at all.\nFor example:\n\nNote that conditionals are an expression, not a statement. This means you can't use statements such as pass, or assignments with = (or \"augmented\" assignments like +=), within a conditional expression:\n\n(In 3.8 and above, the := \"walrus\" operator allows simple assignment of values as an expression, which is then compatible with this syntax. But please don't write code like that; it will quickly become very difficult to understand.)\nSimilarly, because it is an expression, the else part is mandatory:\n\nYou can, however, use conditional expressions to assign a variable like so:\n\nOr for example to return a value:\n\nThink of the conditional expression as switching between two values. We can use it when we are in a 'one value or another' situation, where we will do the same thing with the result, regardless of whether the condition is met. We use the expression to compute the value, and then do something with it. If you need to do something different depending on the condition, then use a normal if statement instead.\n\nKeep in mind that it's frowned upon by some Pythonistas for several reasons:\n\nThe order of the arguments is different from those of the classic condition ? a : b ternary operator from many other languages (such as C, C++, Go, Perl, Ruby, Java, JavaScript, etc.), which may lead to bugs when people unfamiliar with Python's \"surprising\" behaviour use it (they may reverse the argument order).\nSome find it \"unwieldy\", since it goes contrary to the normal flow of thought (thinking of the condition first and then the effects).\nStylistic reasons. (Although the 'inline if' can be really useful, and make your script more concise, it really does complicate your code)\n\nIf you're having trouble remembering the order, then remember that when read aloud, you (almost) say what you mean. For example, x = 4 if b > 8 else 9 is read aloud as x will be 4 if b is greater than 8 otherwise 9.\nOfficial documentation:\n\nConditional expressions\nIs there an equivalent of C\u2019s \u201d?:\u201d ternary operator?"
  },
  {
    "question": "What are metaclasses in Python? What are metaclasses? What are they used for?",
    "answer": "def make_hook(f):\n\"\"\"Decorator to turn 'foo' method into '__foo__'\"\"\"\nf.is_hook = 1\nreturn f\n\nclass MyType(type):\ndef __new__(mcls, name, bases, attrs):\n\nif name.startswith('None'):\nreturn None\n\nnewattrs = {}\nfor attrname, attrvalue in attrs.iteritems():\nif getattr(attrvalue, 'is_hook', 0):\nnewattrs['__%s__' % attrname] = attrvalue\nelse:\nnewattrs[attrname] = attrvalue\n\nreturn super(MyType, mcls).__new__(mcls, name, bases, newattrs)\n\ndef __init__(self, name, bases, attrs):\nsuper(MyType, self).__init__(name, bases, attrs)\n\nprint \"Would register class %s now.\" % self\n\ndef __add__(self, other):\nclass AutoClass(self, other):\npass\nreturn AutoClass\n\ndef unregister(self):\n\nprint \"Would unregister class %s now.\" % self\n\nclass MyObject:\n__metaclass__ = MyType\n\nclass NoneSample(MyObject):\npass\n\nprint type(NoneSample), repr(NoneSample)\n\nclass Example(MyObject):\ndef __init__(self, value):\nself.value = value\n@make_hook\ndef add(self, other):\nreturn self.__class__(self.value + other.value)\n\nExample.unregister()\n\ninst = Example(10)\n\nprint inst + inst\nclass Sibling(MyObject):\npass\n\nExampleSibling = Example + Sibling\n\nprint ExampleSibling\nprint ExampleSibling.__mro__\n\nA metaclass is the class of a class. A class defines how an instance of the class (i.e. an object) behaves while a metaclass defines how a class behaves. A class is an instance of a metaclass.\nWhile in Python you can use arbitrary callables for metaclasses (like Jerub shows), the better approach is to make it an actual class itself. type is the usual metaclass in Python. type is itself a class, and it is its own type. You won't be able to recreate something like type purely in Python, but Python cheats a little. To create your own metaclass in Python you really just want to subclass type.\nA metaclass is most commonly used as a class-factory. When you create an object by calling the class, Python creates a new class (when it executes the 'class' statement) by calling the metaclass. Combined with the normal __init__ and __new__ methods, metaclasses therefore allow you to do 'extra things' when creating a class, like registering the new class with some registry or replace the class with something else entirely.\nWhen the class statement is executed, Python first executes the body of the class statement as a normal block of code. The resulting namespace (a dict) holds the attributes of the class-to-be. The metaclass is determined by looking at the baseclasses of the class-to-be (metaclasses are inherited), at the __metaclass__ attribute of the class-to-be (if any) or the __metaclass__ global variable. The metaclass is then called with the name, bases and attributes of the class to instantiate it.\nHowever, metaclasses actually define the type of a class, not just a factory for it, so you can do much more with them. You can, for instance, define normal methods on the metaclass. These metaclass-methods are like classmethods in that they can be called on the class without an instance, but they are also not like classmethods in that they cannot be called on an instance of the class. type.__subclasses__() is an example of a method on the type metaclass. You can also define the normal 'magic' methods, like __add__, __iter__ and __getattr__, to implement or change how the class behaves.\nHere's an aggregated example of the bits and pieces:"
  },
  {
    "question": "How do I merge two dictionaries in a single expression in Python? I want to merge two dictionaries into a new dictionary.\n\n```\nx = {'a': 1, 'b': 2}\ny = {'b': 3, 'c': 4}\nz = merge(x, y)\n\n>>> z\n{'a': 1, 'b': 3, 'c': 4}\n\n```\n\nWhenever a key k is present in both dictionaries, only the value y[k] should be kept.",
    "answer": "z = x | y\nz = {**x, **y}\ndef merge_two_dicts(x, y):\nz = x.copy()   # start with keys and values of x\nz.update(y)    # modifies z with keys and values of y\nreturn z\nz = merge_two_dicts(x, y)\nx = {'a': 1, 'b': 2}\ny = {'b': 3, 'c': 4}\n>>> z\n{'a': 1, 'b': 3, 'c': 4}\nz = {**x, **y}\nz = {**x, 'foo': 1, 'bar': 2, **y}\n>>> z\n{'a': 1, 'b': 3, 'foo': 1, 'bar': 2, 'c': 4}\nz = x.copy()\nz.update(y) # which returns None since it mutates z\ndef merge_two_dicts(x, y):\n\"\"\"Given two dictionaries, merge them into a new dict as a shallow copy.\"\"\"\nz = x.copy()\nz.update(y)\nreturn z\nz = merge_two_dicts(x, y)\ndef merge_dicts(*dict_args):\n\"\"\"\nGiven any number of dictionaries, shallow copy and merge into a new dict,\nprecedence goes to key-value pairs in latter dictionaries.\n\"\"\"\nresult = {}\nfor dictionary in dict_args:\nresult.update(dictionary)\nreturn result\nz = merge_dicts(a, b, c, d, e, f, g)\nz = dict(x.items() + y.items())\n>>> c = dict(a.items() + b.items())\nTraceback (most recent call last):\nFile \"<stdin>\", line 1, in <module>\nTypeError: unsupported operand type(s) for +: 'dict_items' and 'dict_items'\n>>> c = dict(a.items() | b.items())\n>>> x = {'a': []}\n>>> y = {'b': []}\n>>> dict(x.items() | y.items())\nTraceback (most recent call last):\nFile \"<stdin>\", line 1, in <module>\nTypeError: unhashable type: 'list'\n>>> x = {'a': 2}\n>>> y = {'a': 1}\n>>> dict(x.items() | y.items())\n{'a': 2}\nz = dict(x, **y)\n>>> c = dict(a, **b)\nTraceback (most recent call last):\nFile \"<stdin>\", line 1, in <module>\nTypeError: keyword arguments must be strings\ndict(a=1, b=10, c=11)\n{'a': 1, 'b': 10, 'c': 11}\n>>> foo(**{('a', 'b'): None})\nTraceback (most recent call last):\nFile \"<stdin>\", line 1, in <module>\nTypeError: foo() keywords must be strings\n>>> dict(**{('a', 'b'): None})\n{('a', 'b'): None}\nfrom copy import deepcopy\n\ndef dict_of_dicts_merge(x, y):\nz = {}\noverlapping_keys = x.keys() & y.keys()\nfor key in overlapping_keys:\nz[key] = dict_of_dicts_merge(x[key], y[key])\nfor key in x.keys() - overlapping_keys:\nz[key] = deepcopy(x[key])\nfor key in y.keys() - overlapping_keys:\nz[key] = deepcopy(y[key])\nreturn z\n>>> x = {'a':{1:{}}, 'b': {2:{}}}\n>>> y = {'b':{10:{}}, 'c': {11:{}}}\n>>> dict_of_dicts_merge(x, y)\n{'b': {2: {}, 10: {}}, 'a': {1: {}}, 'c': {11: {}}}\n{k: v for d in dicts for k, v in d.items()} # iteritems in Python 2.7\ndict((k, v) for d in dicts for k, v in d.items()) # iteritems in Python 2\nfrom itertools import chain\nz = dict(chain(x.items(), y.items())) # iteritems in Python 2\nfrom timeit import repeat\nfrom itertools import chain\n\nx = dict.fromkeys('abcdefg')\ny = dict.fromkeys('efghijk')\n\ndef merge_two_dicts(x, y):\nz = x.copy()\nz.update(y)\nreturn z\n\nmin(repeat(lambda: {**x, **y}))\nmin(repeat(lambda: merge_two_dicts(x, y)))\nmin(repeat(lambda: {k: v for d in (x, y) for k, v in d.items()}))\nmin(repeat(lambda: dict(chain(x.items(), y.items()))))\nmin(repeat(lambda: dict(item for d in (x, y) for item in d.items())))\n>>> min(repeat(lambda: {**x, **y}))\n1.0804965235292912\n>>> min(repeat(lambda: merge_two_dicts(x, y)))\n1.636518670246005\n>>> min(repeat(lambda: {k: v for d in (x, y) for k, v in d.items()}))\n3.1779992282390594\n>>> min(repeat(lambda: dict(chain(x.items(), y.items()))))\n2.740647904574871\n>>> min(repeat(lambda: dict(item for d in (x, y) for item in d.items())))\n4.266070580109954\nuname -a\nLinux nixos 4.19.113 #1-NixOS SMP Wed Mar 25 07:06:15 UTC 2020 x86_64 GNU/Linux\n\nHow can I merge two Python dictionaries in a single expression?\nFor dictionaries x and y, their shallowly-merged dictionary z takes values from y, replacing those from x.\n\nIn Python 3.9.0 or greater (released 17 October 2020, PEP-584, discussed here):\n\nIn Python 3.5 or greater:\n\nIn Python 2, (or 3.4 or lower) write a function:\n\nand now:\n\nExplanation\nSay you have two dictionaries and you want to merge them into a new dictionary without altering the original dictionaries:\n\nThe desired result is to get a new dictionary (z) with the values merged, and the second dictionary's values overwriting those from the first.\n\nA new syntax for this, proposed in PEP 448 and available as of Python 3.5, is\n\nAnd it is indeed a single expression.\nNote that we can merge in with literal notation as well:\n\nand now:\n\nIt is now showing as implemented in the release schedule for 3.5, PEP 478, and it has now made its way into the What's New in Python 3.5 document.\nHowever, since many organizations are still on Python 2, you may wish to do this in a backward-compatible way. The classically Pythonic way, available in Python 2 and Python 3.0-3.4, is to do this as a two-step process:\n\nIn both approaches, y will come second and its values will replace x's values, thus b will point to 3 in our final result.\nNot yet on Python 3.5, but want a single expression\nIf you are not yet on Python 3.5 or need to write backward-compatible code, and you want this in a single expression, the most performant while the correct approach is to put it in a function:\n\nand then you have a single expression:\n\nYou can also make a function to merge an arbitrary number of dictionaries, from zero to a very large number:\n\nThis function will work in Python 2 and 3 for all dictionaries. e.g. given dictionaries a to g:\n\nand key-value pairs in g will take precedence over dictionaries a to f, and so on.\nCritiques of Other Answers\nDon't use what you see in the formerly accepted answer:\n\nIn Python 2, you create two lists in memory for each dict, create a third list in memory with length equal to the length of the first two put together, and then discard all three lists to create the dict. In Python 3, this will fail because you're adding two dict_items objects together, not two lists -\n\nand you would have to explicitly create them as lists, e.g. z = dict(list(x.items()) + list(y.items())). This is a waste of resources and computation power.\nSimilarly, taking the union of items() in Python 3 (viewitems() in Python 2.7) will also fail when values are unhashable objects (like lists, for example). Even if your values are hashable, since sets are semantically unordered, the behavior is undefined in regards to precedence. So don't do this:\n\nThis example demonstrates what happens when values are unhashable:\n\nHere's an example where y should have precedence, but instead the value from x is retained due to the arbitrary order of sets:\n\nAnother hack you should not use:\n\nThis uses the dict constructor and is very fast and memory-efficient (even slightly more so than our two-step process) but unless you know precisely what is happening here (that is, the second dict is being passed as keyword arguments to the dict constructor), it's difficult to read, it's not the intended usage, and so it is not Pythonic.\nHere's an example of the usage being remediated in django.\nDictionaries are intended to take hashable keys (e.g. frozensets or tuples), but this method fails in Python 3 when keys are not strings.\n\nFrom the mailing list, Guido van Rossum, the creator of the language, wrote:\n\nI am fine with\ndeclaring dict({}, **{1:3}) illegal, since after all it is abuse of\nthe ** mechanism.\n\nand\n\nApparently dict(x, **y) is going around as \"cool hack\" for \"call\nx.update(y) and return x\". Personally, I find it more despicable than\ncool.\n\nIt is my understanding (as well as the understanding of the creator of the language) that the intended usage for dict(**y) is for creating dictionaries for readability purposes, e.g.:\n\ninstead of\n\nResponse to comments\n\nDespite what Guido says, dict(x, **y) is in line with the dict specification, which btw. works for both Python 2 and 3. The fact that this only works for string keys is a direct consequence of how keyword parameters work and not a short-coming of dict. Nor is using the ** operator in this place an abuse of the mechanism, in fact, ** was designed precisely to pass dictionaries as keywords.\n\nAgain, it doesn't work for 3 when keys are not strings. The implicit calling contract is that namespaces take ordinary dictionaries, while users must only pass keyword arguments that are strings. All other callables enforced it. dict broke this consistency in Python 2:\n\nThis inconsistency was bad given other implementations of Python (PyPy, Jython, IronPython). Thus it was fixed in Python 3, as this usage could be a breaking change.\nI submit to you that it is malicious incompetence to intentionally write code that only works in one version of a language or that only works given certain arbitrary constraints.\nMore comments:\n\ndict(x.items() + y.items()) is still the most readable solution for Python 2. Readability counts.\n\nMy response: merge_two_dicts(x, y) actually seems much clearer to me, if we're actually concerned about readability. And it is not forward compatible, as Python 2 is increasingly deprecated.\n\n{**x, **y} does not seem to handle nested dictionaries. the contents of nested keys are simply overwritten, not merged [...] I ended up being burnt by these answers that do not merge recursively and I was surprised no one mentioned it. In my interpretation of the word \"merging\" these answers describe \"updating one dict with another\", and not merging.\n\nYes. I must refer you back to the question, which is asking for a shallow merge of two dictionaries, with the first's values being overwritten by the second's - in a single expression.\nAssuming two dictionaries of dictionaries, one might recursively merge them in a single function, but you should be careful not to modify the dictionaries from either source, and the surest way to avoid that is to make a copy when assigning values. As keys must be hashable and are usually therefore immutable, it is pointless to copy them:\n\nUsage:\n\nComing up with contingencies for other value types is far beyond the scope of this question, so I will point you at my answer to the canonical question on a \"Dictionaries of dictionaries merge\".\nLess Performant But Correct Ad-hocs\nThese approaches are less performant, but they will provide correct behavior.\nThey will be much less performant than copy and update or the new unpacking because they iterate through each key-value pair at a higher level of abstraction, but they do respect the order of precedence (latter dictionaries have precedence)\nYou can also chain the dictionaries manually inside a dict comprehension:\n\nor in Python 2.6 (and perhaps as early as 2.4 when generator expressions were introduced):\n\nitertools.chain will chain the iterators over the key-value pairs in the correct order:\n\nPerformance Analysis\nI'm only going to do the performance analysis of the usages known to behave correctly. (Self-contained so you can copy and paste yourself.)\n\nIn Python 3.8.1, NixOS:\n\nResources on Dictionaries\n\nMy explanation of Python's dictionary implementation, updated for 3.6.\nAnswer on how to add new keys to a dictionary\nMapping two lists into a dictionary\nThe official Python docs on dictionaries\nThe Dictionary Even Mightier - talk by Brandon Rhodes at Pycon 2017\nModern Python Dictionaries, A Confluence of Great Ideas - talk by Raymond Hettinger at Pycon 2017"
  },
  {
    "question": "How do I create a directory, and any missing parent directories? How do I create a directory at a given path, and also create any missing parent directories along that path? For example, the Bash command mkdir -p /path/to/nested/directory does this.",
    "answer": "from pathlib import Path\nPath(\"/my/directory\").mkdir(parents=True, exist_ok=True)\nimport os\nif not os.path.exists(directory):\nos.makedirs(directory)\nimport os, errno\n\ntry:\nos.makedirs(directory)\nexcept OSError as e:\nif e.errno != errno.EEXIST:\nraise\ntry:\nos.makedirs(\"path/to/directory\")\nexcept FileExistsError:\n\npass\nos.makedirs(\"path/to/directory\", exist_ok=True)  # succeeds even if directory exists.\n\nOn Python \u2265 3.5, use pathlib.Path.mkdir:\n\nFor older versions of Python, I see two answers with good qualities, each with a small flaw, so I will give my take on it:\nTry os.path.exists, and consider os.makedirs for the creation.\n\nAs noted in comments and elsewhere, there's a race condition \u2013 if the directory is created between the os.path.exists and the os.makedirs calls, the os.makedirs will fail with an OSError. Unfortunately, blanket-catching OSError and continuing is not foolproof, as it will ignore a failure to create the directory due to other factors, such as insufficient permissions, full disk, etc.\nOne option would be to trap the OSError and examine the embedded error code (see Is there a cross-platform way of getting information from Python\u2019s OSError):\n\nAlternatively, there could be a second os.path.exists, but suppose another created the directory after the first check, then removed it before the second one \u2013 we could still be fooled.\nDepending on the application, the danger of concurrent operations may be more or less than the danger posed by other factors such as file permissions. The developer would have to know more about the particular application being developed and its expected environment before choosing an implementation.\nModern versions of Python improve this code quite a bit, both by exposing FileExistsError (in 3.3+)...\n\n...and by allowing a keyword argument to os.makedirs called exist_ok (in 3.2+)."
  },
  {
    "question": "How can I access the index value in a &#39;for&#39; loop? How do I access the index while iterating over a sequence with a for loop?\n\n```\nxs = [8, 23, 45]\n\nfor x in xs:\n    print(\"item #{} = {}\".format(index, x))\n\n```\n\nDesired output:\n\n```\nitem #1 = 8\nitem #2 = 23\nitem #3 = 45\n\n```",
    "answer": "for idx, x in enumerate(xs):\nprint(idx, x)\n\nUse the built-in function enumerate():\n\nIt is non-Pythonic to manually index via for i in range(len(xs)): x = xs[i] or manually manage an additional state variable.\nCheck out PEP 279 for more."
  },
  {
    "question": "How do I make a flat list out of a list of lists? I have a list of lists like\n\n```\n[\n    [1, 2, 3],\n    [4, 5, 6],\n    [7],\n    [8, 9]\n]\n\n```\n\nHow can I flatten it to get [1, 2, 3, 4, 5, 6, 7, 8, 9]?\n\nIf your list of lists comes from a nested list comprehension, the problem can be solved more simply/directly by fixing the comprehension; please see How can I get a flat result from a list comprehension instead of a nested list?.\nThe most popular solutions here generally only flatten one \"level\" of the nested list. See Flatten an irregular (arbitrarily nested) list of lists for solutions that completely flatten a deeply nested structure (recursively, in general).",
    "answer": "flat_list = [\nx\nfor xs in xss\nfor x in xs\n]\nflat_list = []\n\nfor xs in xss:\nfor x in xs:\nflat_list.append(x)\ndef flatten(xss):\nreturn [x for xs in xss for x in xs]\npython -mtimeit -s'xss=[[1,2,3],[4,5,6],[7],[8,9]]*99' '[x for xs in xss for x in xs]'\nloops, best of 3: 143 usec per loop\npython -mtimeit -s'xss=[[1,2,3],[4,5,6],[7],[8,9]]*99' 'sum(xss, [])'\nloops, best of 3: 969 usec per loop\npython -mtimeit -s'xss=[[1,2,3],[4,5,6],[7],[8,9]]*99' 'reduce(lambda xs, ys: xs + ys, xss)'\nloops, best of 3: 1.1 msec per loop\n\nA list of lists named xss can be flattened using a nested list comprehension:\n\nThe above is equivalent to:\n\nHere is the corresponding function:\n\nThis is the fastest method.\nAs evidence, using the timeit module in the standard library, we see:\n\nExplanation: the methods based on + (including the implied use in sum) are, of necessity, O(L**2) when there are L sublists -- as the intermediate result list keeps getting longer, at each step a new intermediate result list object gets allocated, and all the items in the previous intermediate result must be copied over (as well as a few new ones added at the end). So, for simplicity and without actual loss of generality, say you have L sublists of M items each: the first M items are copied back and forth L-1 times, the second M items L-2 times, and so on; total number of copies is M times the sum of x for x from 1 to L excluded, i.e., M * (L**2)/2.\nThe list comprehension just generates one list, once, and copies each item over (from its original place of residence to the result list) also exactly once."
  },
  {
    "question": "What is the difference between @staticmethod and @classmethod in Python? What is the difference between a method decorated with @staticmethod and one decorated with @classmethod?",
    "answer": "class A(object):\ndef foo(self, x):\nprint(f\"executing foo({self}, {x})\")\n\n@classmethod\ndef class_foo(cls, x):\nprint(f\"executing class_foo({cls}, {x})\")\n\n@staticmethod\ndef static_foo(x):\nprint(f\"executing static_foo({x})\")\n\na = A()\na.foo(1)\na.class_foo(1)\nA.class_foo(1)\na.static_foo(1)\n\nA.static_foo('hi')\nprint(a.foo)\nprint(a.class_foo)\nprint(a.static_foo)\nprint(A.static_foo)\n\nMaybe a bit of example code will help: Notice the difference in the call signatures of foo, class_foo and static_foo:\n\nBelow is the usual way an object instance calls a method. The object instance, a, is implicitly passed as the first argument.\n\nWith classmethods, the class of the object instance is implicitly passed as the first argument instead of self.\n\nYou can also call class_foo using the class. In fact, if you define something to be\na classmethod, it is probably because you intend to call it from the class rather than from a class instance. A.foo(1) would have raised a TypeError, but A.class_foo(1) works just fine:\n\nOne use people have found for class methods is to create inheritable alternative constructors.\n\nWith staticmethods, neither self (the object instance) nor  cls (the class) is implicitly passed as the first argument. They behave like plain functions except that you can call them from an instance or the class:\n\nStaticmethods are used to group functions which have some logical connection with a class to the class.\n\nfoo is just a function, but when you call a.foo you don't just get the function,\nyou get a \"partially applied\" version of the function with the object instance a bound as the first argument to the function. foo expects 2 arguments, while a.foo only expects 1 argument.\na is bound to foo. That is what is meant by the term \"bound\" below:\n\nWith a.class_foo, a is not bound to class_foo, rather the class A is bound to class_foo.\n\nHere, with a staticmethod, even though it is a method, a.static_foo just returns\na good 'ole function with no arguments bound. static_foo expects 1 argument, and\na.static_foo expects 1 argument too.\n\nAnd of course the same thing happens when you call static_foo with the class A instead."
  },
  {
    "question": "How slicing in Python works How does Python's slice notation work? That is: when I write code like a[x:y:z], a[:], a[::2] etc., how can I understand which elements end up in the slice?\n\nSee Why are slice and range upper-bound exclusive? to learn why xs[0:2] == [xs[0], xs[1]], not [..., xs[2]].\nSee Make a new list containing every Nth item in the original list for xs[::N].\nSee How does assignment work with list slices? to learn what xs[0:2] = [\"a\", \"b\"] does.",
    "answer": "a[start:stop]  # items start through stop-1\na[start:]      # items start through the rest of the array\na[:stop]       # items from the beginning through stop-1\na[:]           # a copy of the whole array\na[start:stop:step] # start through not past stop, by step\na[-1]    # last item in the array\na[-2:]   # last two items in the array\na[:-2]   # everything except the last two items\na[::-1]    # all items in the array, reversed\na[1::-1]   # the first two items, reversed\na[:-3:-1]  # the last two items, reversed\na[-3::-1]  # everything except the last two items, reversed\na[start:stop:step]\na[slice(start, stop, step)]\n\nThe syntax is:\n\nThere is also the step value, which can be used with any of the above:\n\nThe key point to remember is that the :stop value represents the first value that is not in the selected slice. So, the difference between stop and start is the number of elements selected (if step is 1, the default).\nThe other feature is that start or stop may be a negative number, which means it counts from the end of the array instead of the beginning. So:\n\nSimilarly, step may be a negative number:\n\nPython is kind to the programmer if there are fewer items than you ask for. For example, if you ask for a[:-2] and a only contains one element, you get an empty list instead of an error. Sometimes you would prefer the error, so you have to be aware that this may happen.\nRelationship with the slice object\nA slice object can represent a slicing operation, i.e.:\n\nis equivalent to:\n\nSlice objects also behave slightly differently depending on the number of arguments, similar to range(), i.e. both slice(stop) and slice(start, stop[, step]) are supported.\nTo skip specifying a given argument, one might use None, so that e.g. a[start:] is equivalent to a[slice(start, None)] or a[::-1] is equivalent to a[slice(None, None, -1)].\nWhile the :-based notation is very helpful for simple slicing, the explicit use of slice() objects simplifies the programmatic generation of slicing."
  },
  {
    "question": "How can I find the index for a given item in a list? Given a list [\"foo\", \"bar\", \"baz\"] and an item in the list \"bar\", how do I get its index 1?",
    "answer": ">>> [\"foo\", \"bar\", \"baz\"].index(\"bar\")\nlist.index(x[, start[, end]])\n>>> import timeit\n>>> timeit.timeit('l.index(999_999)', setup='l = list(range(0, 1_000_000))', number=1000)\n9.356267921015387\n>>> timeit.timeit('l.index(999_999, 999_990, 1_000_000)', setup='l = list(range(0, 1_000_000))', number=1000)\n0.0004404920036904514\n>>> [1, 1].index(1) # the `1` index is not found.\n>>> # A list comprehension gives a list of indices directly:\n>>> [i for i, e in enumerate([1, 2, 1]) if e == 1]\n[0, 2]\n>>> # A generator comprehension gives us an iterable object...\n>>> g = (i for i, e in enumerate([1, 2, 1]) if e == 1)\n>>> # which can be used in a `for` loop, or manually iterated with `next`:\n>>> next(g)\n>>> next(g)\n>>> [1, 1].index(2)\nTraceback (most recent call last):\nFile \"<stdin>\", line 1, in <module>\nValueError: 2 is not in list\n\nSee the documentation for the built-in .index() method of the list:\n\nReturn zero-based index in the list of the first item whose value is equal to x. Raises a ValueError if there is no such item.\nThe optional arguments start and end are interpreted as in the slice notation and are used to limit the search to a particular subsequence of the list. The returned index is computed relative to the beginning of the full sequence rather than the start argument.\n\nCaveats\nLinear time-complexity in list length\nAn index call checks every element of the list in order, until it finds a match. If the list is long, and if there is no guarantee that the value will be near the beginning, this can slow down the code.\nThis problem can only be completely avoided by using a different data structure. However, if the element is known to be within a certain part of the list, the start and end parameters can be used to narrow the search.\nFor example:\n\nThe second call is orders of magnitude faster, because it only has to search through 10 elements, rather than all 1 million.\nOnly the index of the first match is returned\nA call to index searches through the list in order until it finds a match, and stops there. If there could be more than one occurrence of the value, and all indices are needed, index cannot solve the problem:\n\nInstead, use a list comprehension or generator expression to do the search, with enumerate to get indices:\n\nThe list comprehension and generator expression techniques still work if there is only one match, and are more generalizable.\nRaises an exception if there is no match\nAs noted in the documentation above, using .index will raise an exception if the searched-for value is not in the list:\n\nIf this is a concern, either explicitly check first using item in my_list, or handle the exception with try/except as appropriate.\nThe explicit check is simple and readable, but it must iterate the list a second time. See What is the EAFP principle in Python? for more guidance on this choice."
  },
  {
    "question": "Iterating over dictionaries using &#39;for&#39; loops ```\nd = {'x': 1, 'y': 2, 'z': 3}\n\nfor key in d:\n    print(key, 'corresponds to', d[key])\n\n```\n\nHow does Python recognize that it needs only to read the key from the dictionary? Is key a special keyword, or is it simply a variable?",
    "answer": "for key in d:\nfor key, value in d.items():\nfor key, value in d.iteritems():\n\nkey is just a variable name.\n\nwill simply loop over the keys in the dictionary, rather than the keys and values.  To loop over both key and value you can use the following:\nFor Python 3.x:\n\nFor Python 2.x:\n\nTo test for yourself, change the word key to poop.\nIn Python 3.x, iteritems() was replaced with simply items(), which returns a set-like view backed by the dict, like iteritems() but even better.\nThis is also available in 2.7 as viewitems().\nThe operation items() will work for both 2 and 3, but in 2 it will return a list of the dictionary's (key, value) pairs, which will not reflect changes to the dict that happen after the items() call. If you want the 2.x behavior in 3.x, you can call list(d.items())."
  },
  {
    "question": "How can I iterate over rows in a Pandas DataFrame? I have a pandas dataframe, df:\n\n```\n   c1   c2\n0  10  100\n1  11  110\n2  12  120\n\n```\n\nHow do I iterate over the rows of this dataframe? For every row, I want to access its elements (values in cells) by the name of the columns. For example:\n\n```\nfor row in df.rows:\n    print(row['c1'], row['c2'])\n\n```\n\n\nI found a similar question, which suggests using either of these:\n\n\n\n```\nfor date, row in df.T.iteritems():\n\n```\n\n\n\n\n```\nfor row in df.iterrows():\n\n```\n\n\n\nBut I do not understand what the row object is and how I can work with it.",
    "answer": "import pandas as pd\n\ndf = pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 110, 120]})\ndf = df.reset_index()  # make sure indexes pair with number of rows\n\nfor index, row in df.iterrows():\nprint(row['c1'], row['c2'])\n100\n110\n120\n\nDataFrame.iterrows is a generator which yields both the index and row (as a Series):\n\nObligatory disclaimer from the documentation\n\nIterating through pandas objects is generally slow. In many cases, iterating manually over the rows is not needed and can be avoided with one of the following approaches:\n\nLook for a vectorized solution: many operations can be performed using built-in methods or NumPy functions, (boolean) indexing, \u2026\nWhen you have a function that cannot work on the full DataFrame/Series at once, it is better to use apply() instead of iterating over the values. See the docs on function application.\nIf you need to do iterative manipulations on the values but performance is important, consider writing the inner loop with cython or numba. See the enhancing performance section for some examples of this approach.\n\nOther answers in this thread delve into greater depth on alternatives to iter* functions if you are interested to learn more."
  },
  {
    "question": "How can I use a global variable in a function? How do I create or use a global variable inside a function?\nHow do I use a global variable that was defined in one function inside other functions?\n\nFailing to use the global keyword where appropriate often causes UnboundLocalError. The precise rules for this are explained at UnboundLocalError on local variable when reassigned after first use. Generally, please close other questions as a duplicate of that question when an explanation is sought, and this question when someone simply needs to know the global keyword.",
    "answer": "globvar = 0\n\ndef set_globvar_to_one():\nglobal globvar    # Needed to modify global copy of globvar\nglobvar = 1\n\ndef print_globvar():\nprint(globvar)     # No need for global declaration to read value of globvar\n\nset_globvar_to_one()\nprint_globvar()       # Prints 1\n\nYou can use a global variable within other functions by declaring it as global within each function that assigns a value to it:\n\nSince it's unclear whether globvar = 1 is creating a local variable or changing a global variable, Python defaults to creating a local variable, and makes you explicitly choose the other behavior with the global keyword.\nSee other answers if you want to share a global variable across modules."
  },
  {
    "question": "How do I get the current time in Python? How do I get the current time in Python?",
    "answer": ">>> import datetime\n>>> now = datetime.datetime.now()\n>>> now\ndatetime.datetime(2009, 1, 6, 15, 8, 24, 78915)\n>>> print(now)\n2009-01-06 15:08:24.789150\n>>> now.time()\ndatetime.time(15, 8, 24, 78915)\n>>> print(now.time())\n15:08:24.789150\n>>> from datetime import datetime\n\nUse datetime:\n\nFor just the clock time without the date:\n\nTo save typing, you can import the datetime object from the datetime module:\n\nThen remove the prefix datetime. from all of the above."
  },
  {
    "question": "How can I catch multiple exceptions in one line? (in the &quot;except&quot; block) I know that I can do:\n\n```\ntry:\n    # do something that may fail\nexcept:\n    # do this if ANYTHING goes wrong\n\n```\n\nI can also do this:\n\n```\ntry:\n    # do something that may fail\nexcept IDontLikeYouException:\n    # say please\nexcept YouAreTooShortException:\n    # stand on a ladder\n\n```\n\nBut if I want to do the same thing inside two different exceptions, the best I can think of right now is to do this:\n\n```\ntry:\n    # do something that may fail\nexcept IDontLikeYouException:\n    # say please\nexcept YouAreBeingMeanException:\n    # say please\n\n```\n\nIs there a way that I can do something like this (since the action to take in both exceptions is to say please):\n\n```\ntry:\n    # do something that may fail\nexcept IDontLikeYouException, YouAreBeingMeanException:\n    # say please\n\n```\n\nNow this really won't work, as it matches the syntax for:\n\n```\ntry:\n    # do something that may fail\nexcept Exception, e:\n    # say please\n\n```\n\nSo, my effort to catch the two distinct exceptions doesn't exactly come through.\nIs there a way to do this?",
    "answer": "except (IDontLikeYouException, YouAreBeingMeanException) as e:\npass\nexcept (IDontLikeYouException, YouAreBeingMeanException), e:\npass\n\nFrom Python Documentation:\n\nAn except clause may name multiple exceptions as a parenthesized tuple, for example\n\nOr, for Python 2 only:\n\nSeparating the exception from the variable with a comma will still work in Python 2.6 and 2.7, but is now deprecated and does not work in Python 3; now you should be using as."
  },
  {
    "question": "How do I copy a file? How do I copy a file in Python?",
    "answer": "import shutil\n\nshutil.copyfile(src, dst)\n\nshutil.copy(src, dst)  # dst can be a folder; use shutil.copy2() to preserve timestamp\n\nshutil has many methods you can use. One of which is:\n\nCopy the contents of the file named src to a file named dst. Both src and dst need to be the entire filename of the files, including path.\nThe destination location must be writable; otherwise, an IOError exception will be raised.\nIf dst already exists, it will be replaced.\nSpecial files such as character or block devices and pipes cannot be copied with this function.\nWith copy, src and dst are path names given as strs.\n\nAnother shutil method to look at is shutil.copy2(). It's similar but preserves more metadata (e.g. time stamps).\nIf you use os.path operations, use copy rather than copyfile. copyfile will only accept strings."
  },
  {
    "question": "What is __init__.py for? What is __init__.py for in a Python source directory?",
    "answer": "It used to be a required part of a package (old, pre-3.3 \"regular package\", not newer 3.3+ \"namespace package\").\nHere's the documentation.\n\nPython defines two types of packages, regular packages and namespace packages. Regular packages are traditional packages as they existed in Python 3.2 and earlier. A regular package is typically implemented as a directory containing an __init__.py file. When a regular package is imported, this __init__.py file is implicitly executed, and the objects it defines are bound to names in the package\u2019s namespace. The __init__.py file can contain the same Python code that any other module can contain, and Python will add some additional attributes to the module when it is imported.\n\nBut just click the link, it contains an example, more information, and an explanation of namespace packages, the kind of packages without __init__.py."
  },
  {
    "question": "Convert bytes to a string in Python 3 I captured the standard output of an external program into a bytes object:\n\n```\n>>> from subprocess import *\n>>> stdout = Popen(['ls', '-l'], stdout=PIPE).communicate()[0]\n>>> stdout\nb'total 0\\n-rw-rw-r-- 1 thomas thomas 0 Mar  3 07:03 file1\\n-rw-rw-r-- 1 thomas thomas 0 Mar  3 07:03 file2\\n'\n\n```\n\nI want to convert that to a normal Python string, so that I can print it like this:\n\n```\n>>> print(stdout)\n-rw-rw-r-- 1 thomas thomas 0 Mar  3 07:03 file1\n-rw-rw-r-- 1 thomas thomas 0 Mar  3 07:03 file2\n\n```\n\nHow do I convert the bytes object to a str with Python 3?\n\nSee Best way to convert string to bytes in Python 3? for the other way around.",
    "answer": ">>> b\"abcde\".decode(\"utf-8\")\n'abcde'\n\nDecode the bytes object to produce a string:\n\nThe above example assumes that the bytes object is in UTF-8, because it is a common encoding. However, you should use the encoding your data is actually in!"
  },
  {
    "question": "What is the difference between __str__ and __repr__? What is the difference between __str__ and __repr__ in Python?",
    "answer": "return \"%s(%r)\" % (self.__class__, self.__dict__)\nreturn f\"{self.__class__!s}({self.__dict__!r})\"\nlog(INFO, \"I am in the weird function and a is\", a, \"and b is\", b, \"but I got a null C \u2014 using default\", default_c)\n[moshe is, 3, hello\nworld, this is a list, oh I don't know, containing just 4 elements]\nprint(\"[\" + \", \".join(lst) + \"]\")\n\nAlex Martelli summarized well but, surprisingly, was too succinct.\nFirst, let me reiterate the main points in Alex\u2019s post:\n\nThe default implementation is useless (it\u2019s hard to think of one which wouldn\u2019t be, but yeah)\n__repr__ goal is to be unambiguous\n__str__ goal is to be readable\nContainer\u2019s __str__ uses contained objects\u2019 __repr__\n\nDefault implementation is useless\nThis is mostly a surprise because Python\u2019s defaults tend to be fairly useful. However, in this case, having a default for __repr__ which would act like:\n\nOr in new f-string formatting:\n\nwould have been too dangerous (for example, too easy to get into infinite recursion if objects reference each other). So Python cops out. Note that there is one default which is true: if __repr__ is defined, and __str__ is not, the object will behave as though __str__=__repr__.\nThis means, in simple terms: almost every object you implement should have a functional __repr__ that\u2019s usable for understanding the object. Implementing __str__ is optional: do that if you need a \u201cpretty print\u201d functionality (for example, used by a report generator).\nThe goal of __repr__ is to be unambiguous\nLet me come right out and say it \u2014 I do not believe in debuggers. I don\u2019t really know how to use any debugger, and have never used one seriously. Furthermore, I believe that the big fault in debuggers is their basic nature \u2014 most failures I debug happened a long long time ago, in a galaxy far far away. This means that I do believe, with religious fervor, in logging. Logging is the lifeblood of any decent fire-and-forget server system. Python makes it easy to log: with maybe some project specific wrappers, all you need is a\n\nBut you have to do the last step \u2014 make sure every object you implement has a useful repr, so code like that can just work. This is why the \u201ceval\u201d thing comes up: if you have enough information so eval(repr(c))==c, that means you know everything there is to know about c. If that\u2019s easy enough, at least in a fuzzy way, do it. If not, make sure you have enough information about c anyway. I usually use an eval-like format: \"MyClass(this=%r,that=%r)\" % (self.this,self.that). It does not mean that you can actually construct MyClass, or that those are the right constructor arguments \u2014 but it is a useful form to express \u201cthis is everything you need to know about this instance\u201d.\nNote: I used %r above, not %s. You always want to use repr() [or %r formatting character, equivalently] inside __repr__ implementation, or you\u2019re defeating the goal of repr. You want to be able to differentiate MyClass(3) and MyClass(\"3\").\nThe goal of __str__ is to be readable\nSpecifically, it is not intended to be unambiguous \u2014 notice that str(3)==str(\"3\"). Likewise, if you implement an IP abstraction, having the str of it look like 192.168.1.1 is just fine. When implementing a date/time abstraction, the str can be \"2010/4/12 15:35:22\", etc. The goal is to represent it in a way that a user, not a programmer, would want to read it. Chop off useless digits, pretend to be some other class \u2014 as long is it supports readability, it is an improvement.\nContainer\u2019s __str__ uses contained objects\u2019 __repr__\nThis seems surprising, doesn\u2019t it? It is a little, but how readable would it be if it used their __str__?\n\nNot very. Specifically, the strings in a container would find it way too easy to disturb its string representation. In the face of ambiguity, remember, Python resists the temptation to guess. If you want the above behavior when you\u2019re printing a list, just\n\n(you can probably also figure out what to do about dictionaries).\nSummary\nImplement __repr__ for any class you implement. This should be second nature. Implement __str__ if you think it would be useful to have a string version which errs on the side of readability."
  },
  {
    "question": "How do I select rows from a DataFrame based on column values? How can I select rows from a DataFrame based on values in some column in Pandas?\nIn SQL, I would use:\n\n```\nSELECT *\nFROM table\nWHERE column_name = some_value\n\n```",
    "answer": "df.loc[df['column_name'] == some_value]\ndf.loc[df['column_name'].isin(some_values)]\ndf.loc[(df['column_name'] >= A) & (df['column_name'] <= B)]\ndf['column_name'] >= A & df['column_name'] <= B\ndf['column_name'] >= (A & df['column_name']) <= B\ndf.loc[df['column_name'] != some_value]\ndf = df.loc[~df['column_name'].isin(some_values)] # .loc is not in-place replacement\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'A': 'foo bar foo bar foo bar foo foo'.split(),\n'B': 'one one two three two two one three'.split(),\n'C': np.arange(8), 'D': np.arange(8) * 2})\nprint(df)\n\nprint(df.loc[df['A'] == 'foo'])\nA      B  C   D\nfoo    one  0   0\nfoo    two  2   4\nfoo    two  4   8\nfoo    one  6  12\nfoo  three  7  14\nprint(df.loc[df['B'].isin(['one','three'])])\nA      B  C   D\nfoo    one  0   0\nbar    one  1   2\nbar  three  3   6\nfoo    one  6  12\nfoo  three  7  14\ndf = df.set_index(['B'])\nprint(df.loc['one'])\nA  C   D\nB\none  foo  0   0\none  bar  1   2\none  foo  6  12\ndf.loc[df.index.isin(['one','two'])]\nA  C   D\nB\none  foo  0   0\none  bar  1   2\ntwo  foo  2   4\ntwo  foo  4   8\ntwo  bar  5  10\none  foo  6  12\n\nTo select rows whose column value equals a scalar, some_value, use ==:\n\nTo select rows whose column value is in an iterable, some_values, use isin:\n\nCombine multiple conditions with &:\n\nNote the parentheses. Due to Python's operator precedence rules, & binds more tightly than <= and >=. Thus, the parentheses in the last example are necessary. Without the parentheses\n\nis parsed as\n\nwhich results in a Truth value of a Series is ambiguous error.\n\nTo select rows whose column value does not equal some_value, use !=:\n\nThe isin returns a boolean Series, so to select rows whose value is not in some_values, negate the boolean Series using ~:\n\nFor example,\n\nyields\n\nIf you have multiple values you want to include, put them in a\nlist (or more generally, any iterable) and use isin:\n\nyields\n\nNote, however, that if you wish to do this many times, it is more efficient to\nmake an index first, and then use df.loc:\n\nyields\n\nor, to include multiple values from the index use df.index.isin:\n\nyields"
  },
  {
    "question": "How can I add new keys to a dictionary? How do I add a new key to an existing dictionary? It doesn't have an .add() method.",
    "answer": "d = {'key': 'value'}\nprint(d)  # {'key': 'value'}\n\nd['mynewkey'] = 'mynewvalue'\n\nprint(d)  # {'key': 'value', 'mynewkey': 'mynewvalue'}\n\nYou create a new key/value pair on a dictionary by assigning a value to that key\n\nIf the key doesn't exist, it's added and points to that value. If it exists, the current value it points to is overwritten."
  },
  {
    "question": "Does Python have a string &#39;contains&#39; substring method? I'm looking for a string.contains or string.indexof method in Python.\nI want to do:\n\n```\nif not somestring.contains(\"blah\"):\n   continue\n\n```",
    "answer": "if \"blah\" not in somestring:\ncontinue\n\nUse the in operator:\n\nNote: This is case-sensitive."
  },
  {
    "question": "How can I delete a file or folder in Python? How can I delete a file or folder in Python?",
    "answer": "Use one of these methods:\n\npathlib.Path.unlink() removes a file or symbolic link.\n\npathlib.Path.rmdir() removes an empty directory.\n\nshutil.rmtree() deletes a directory and all its contents.\n\nOn Python 3.3 and below, you can use these methods instead of the pathlib ones:\n\nos.remove() removes a file.\n\nos.unlink() removes a symbolic link.\n\nos.rmdir() removes an empty directory."
  },
  {
    "question": "&quot;Least Astonishment&quot; and the Mutable Default Argument Anyone tinkering with Python long enough has been bitten (or torn to pieces) by the following issue:\n\n```\ndef foo(a=[]):\n    a.append(5)\n    return a\n\n```\n\nPython novices would expect this function called with no parameter to always return a list with only one element: [5]. The result is instead very different, and very astonishing (for a novice):\n\n```\n>>> foo()\n[5]\n>>> foo()\n[5, 5]\n>>> foo()\n[5, 5, 5]\n>>> foo()\n[5, 5, 5, 5]\n>>> foo()\n\n```\n\nA manager of mine once had his first encounter with this feature, and called it \"a dramatic design flaw\" of the language. I replied that the behavior had an underlying explanation, and it is indeed very puzzling and unexpected if you don't understand the internals. However, I was not able to answer (to myself) the following question: what is the reason for binding the default argument at function definition, and not at function execution? I doubt the experienced behavior has a practical use (who really used static variables in C, without breeding bugs?)\nEdit:\nBaczek made an interesting example. Together with most of your comments and Utaal's in particular, I elaborated further:\n\n```\ndef a():\n    print(\"a executed\")\n    return []\n\n           \ndef b(x=a()):\n    x.append(5)\n    print(x)\n\na executed\n>>> b()\n[5]\n>>> b()\n[5, 5]\n\n```\n\nTo me, it seems that the design decision was relative to where to put the scope of parameters: inside the function, or \"together\" with it?\nDoing the binding inside the function would mean that x is effectively bound to the specified default when the function is called, not defined, something that would present a deep flaw: the def line would be \"hybrid\" in the sense that part of the binding (of the function object) would happen at definition, and part (assignment of default parameters) at function invocation time.\nThe actual behavior is more consistent: everything of that line gets evaluated when that line is executed, meaning at function definition.",
    "answer": "Actually, this is not a design flaw, and it is not because of internals or performance. It comes simply from the fact that functions in Python are first-class objects, and not only a piece of code.\nAs soon as you think of it this way, then it completely makes sense: a function is an object being evaluated on its definition; default parameters are kind of \"member data\" and therefore their state may change from one call to the other - exactly as in any other object.\nIn any case, the Effbot (Fredrik Lundh) has a very nice explanation of the reasons for this behavior in Default Parameter Values in Python. I found it very clear, and I really suggest reading it for a better knowledge of how function objects work."
  },
  {
    "question": "What does ** (double star/asterisk) and * (star/asterisk) do for parameters? What do *args and **kwargs mean in these function definitions?\n\n```\ndef foo(x, y, *args):\n    pass\n\ndef bar(x, y, **kwargs):\n    pass\n\n```\n\n\nSee What do ** (double star/asterisk) and * (star/asterisk) mean in a function call? for the complementary question about arguments.",
    "answer": "def foo(*args):\nfor a in args:\nprint(a)\n\nfoo(1)\n\nfoo(1, 2, 3)\ndef bar(**kwargs):\nfor a in kwargs:\nprint(a, kwargs[a])\n\nbar(name='one', age=27)\ndef foo(kind, *args, bar=None, **kwargs):\nprint(kind, args, bar, kwargs)\n\nfoo(123, 'a', 'b', apple='red')\ndef foo(a, b, c):\nprint(a, b, c)\n\nobj = {'b':10, 'c':'lee'}\n\nfoo(100, **obj)\ndef foo(bar, lee):\nprint(bar, lee)\n\nbaz = [1, 2]\n\nfoo(*baz)\nfirst, *rest = [1, 2, 3, 4]\ndef func(arg1, arg2, arg3, *, kwarg1, kwarg2):\npass\n\nThe *args and **kwargs are common idioms to allow an arbitrary number of arguments to functions, as described in the section more on defining functions in the Python tutorial.\nThe *args will give you all positional arguments as a tuple:\n\nThe **kwargs will give you all\nkeyword arguments as a dictionary:\n\nBoth idioms can be mixed with normal arguments to allow a set of fixed and some variable arguments:\n\nIt is also possible to use this the other way around:\n\nAnother usage of the *l idiom is to unpack argument lists when calling a function.\n\nIn Python 3 it is possible to use *l on the left side of an assignment (Extended Iterable Unpacking), though it gives a list instead of a tuple in this context:\n\nAlso Python 3 adds a new semantic (refer PEP 3102):\n\nSuch function accepts only 3 positional arguments, and everything after * can only be passed as keyword arguments.\nNote:\nA Python dict, semantically used for keyword argument passing, is arbitrarily ordered. However, in Python 3.6+, keyword arguments are guaranteed to remember insertion order.\n\"The order of elements in **kwargs now corresponds to the order in which keyword arguments were passed to the function.\" - What\u2019s New In Python 3.6.\nIn fact, all dicts in CPython 3.6 will remember insertion order as an implementation detail, and this becomes standard in Python 3.7."
  },
  {
    "question": "How can I access environment variables in Python? How can I get the value of an environment variable in Python?",
    "answer": "import os\nprint(os.environ['HOME'])\nprint(os.environ)\nprint(os.environ.get('KEY_THAT_MIGHT_EXIST'))\n\nprint(os.environ.get('KEY_THAT_MIGHT_EXIST', default_value))\n\nprint(os.getenv('KEY_THAT_MIGHT_EXIST', default_value))\n\nEnvironment variables are accessed through os.environ:\n\nTo see a list of all environment variables:\n\nIf a key is not present, attempting to access it will raise a KeyError. To avoid this:"
  },
  {
    "question": "How do I sort a dictionary by value? I have a dictionary of values read from two fields in a database: a string field and a numeric field. The string field is unique, so that is the key of the dictionary.\nI can sort on the keys, but how can I sort based on the values?\nNote: I have read Stack Overflow question here How do I sort a list of dictionaries by a value of the dictionary? and probably could change my code to have a list of dictionaries, but since I do not really need a list of dictionaries I wanted to know if there is a simpler solution to sort either in ascending or descending order.",
    "answer": ">>> x = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\n>>> {k: v for k, v in sorted(x.items(), key=lambda item: item[1])}\n{0: 0, 2: 1, 1: 2, 4: 3, 3: 4}\n>>> dict(sorted(x.items(), key=lambda item: item[1]))\n{0: 0, 2: 1, 1: 2, 4: 3, 3: 4}\nimport operator\nx = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\nsorted_x = sorted(x.items(), key=operator.itemgetter(1))\nimport operator\nx = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\nsorted_x = sorted(x.items(), key=operator.itemgetter(0))\nx = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\nsorted_x = sorted(x.items(), key=lambda kv: kv[1])\nimport collections\n\nsorted_dict = collections.OrderedDict(sorted_x)\n\nPython 3.7+ or CPython 3.6\nDicts preserve insertion order in Python 3.7+. Same in CPython 3.6, but it's an implementation detail.\n\nor\n\nOlder Python\nIt is not possible to sort a dictionary, only to get a representation of a dictionary that is sorted. Dictionaries are inherently orderless, but other types, such as lists and tuples, are not. So you need an ordered data type to represent sorted values, which will be a list\u2014probably a list of tuples.\nFor instance,\n\nsorted_x will be a list of tuples sorted by the second element in each tuple. dict(sorted_x) == x.\nAnd for those wishing to sort on keys instead of values:\n\nIn Python3 since unpacking is not allowed we can use\n\nIf you want the output as a dict, you can use collections.OrderedDict:"
  },
  {
    "question": "How do I clone a list so that it doesn&#39;t change unexpectedly after assignment? While using new_list = my_list, any modifications to new_list changes my_list every time. Why is this, and how can I clone or copy the list to prevent it? For example:\n\n```\n>>> my_list = [1, 2, 3]\n>>> new_list = my_list\n>>> new_list.append(4)\n>>> my_list\n[1, 2, 3, 4]\n\n```",
    "answer": "new_list = old_list.copy()\nnew_list = old_list[:]\nnew_list = list(old_list)\nimport copy\nnew_list = copy.copy(old_list)\nimport copy\nnew_list = copy.deepcopy(old_list)\nimport copy\n\nclass Foo(object):\ndef __init__(self, val):\nself.val = val\n\ndef __repr__(self):\nreturn f'Foo({self.val!r})'\n\nfoo = Foo(1)\n\na = ['foo', foo]\nb = a.copy()\nc = a[:]\nd = list(a)\ne = copy.copy(a)\nf = copy.deepcopy(a)\n\na.append('baz')\nfoo.val = 5\n\nprint(f'original: {a}\\nlist.copy(): {b}\\nslice: {c}\\nlist(): {d}\\ncopy: {e}\\ndeepcopy: {f}')\noriginal: ['foo', Foo(5), 'baz']\nlist.copy(): ['foo', Foo(5)]\nslice: ['foo', Foo(5)]\nlist(): ['foo', Foo(5)]\ncopy: ['foo', Foo(5)]\ndeepcopy: ['foo', Foo(1)]\n\nnew_list = my_list doesn't actually create a second list. The assignment just copies the reference to the list, not the actual list, so both new_list and my_list refer to the same list after the assignment.\nTo actually copy the list, you have several options:\n\nYou can use the built-in list.copy() method (available since Python 3.3):\n\nYou can slice it:\n\nAlex Martelli's opinion (at least back in 2007) about this is, that it is a weird syntax and it does not make sense to use it ever. ;) (In his opinion, the next one is more readable).\n\nYou can use the built-in list() constructor:\n\nYou can use generic copy.copy():\n\nThis is a little slower than list() because it has to find out the datatype of old_list first.\n\nIf you need to copy the elements of the list as well, use generic copy.deepcopy():\n\nObviously the slowest and most memory-needing method, but sometimes unavoidable. This operates recursively; it will handle any number of levels of nested lists (or other containers).\n\nExample:\n\nResult:"
  },
  {
    "question": "How do I pass a variable by reference? I wrote this class for testing:\n\n```\nclass PassByReference:\n    def __init__(self):\n        self.variable = 'Original'\n        self.change(self.variable)\n        print(self.variable)\n\n    def change(self, var):\n        var = 'Changed'\n\n```\n\nWhen I tried creating an instance, the output was Original. So it seems like parameters in Python are passed by value. Is that correct? How can I modify the code to get the effect of pass-by-reference, so that the output is Changed?\n\nSometimes people are surprised that code like x = 1, where x is a parameter name, doesn't impact on the caller's argument, but code like x[0] = 1 does. This happens because item assignment and slice assignment are ways to mutate an existing object, rather than reassign a variable, despite the = syntax. See Why can a function modify some arguments as perceived by the caller, but not others? for details.\nSee also What's the difference between passing by reference vs. passing by value? for important, language-agnostic terminology discussion.",
    "answer": "def try_to_change_list_contents(the_list):\nprint('got', the_list)\nthe_list.append('four')\nprint('changed to', the_list)\n\nouter_list = ['one', 'two', 'three']\n\nprint('before, outer_list =', outer_list)\ntry_to_change_list_contents(outer_list)\nprint('after, outer_list =', outer_list)\nbefore, outer_list = ['one', 'two', 'three']\ngot ['one', 'two', 'three']\nchanged to ['one', 'two', 'three', 'four']\nafter, outer_list = ['one', 'two', 'three', 'four']\ndef try_to_change_list_reference(the_list):\nprint('got', the_list)\nthe_list = ['and', 'we', 'can', 'not', 'lie']\nprint('set to', the_list)\n\nouter_list = ['we', 'like', 'proper', 'English']\n\nprint('before, outer_list =', outer_list)\ntry_to_change_list_reference(outer_list)\nprint('after, outer_list =', outer_list)\nbefore, outer_list = ['we', 'like', 'proper', 'English']\ngot ['we', 'like', 'proper', 'English']\nset to ['and', 'we', 'can', 'not', 'lie']\nafter, outer_list = ['we', 'like', 'proper', 'English']\ndef try_to_change_string_reference(the_string):\nprint('got', the_string)\nthe_string = 'In a kingdom by the sea'\nprint('set to', the_string)\n\nouter_string = 'It was many and many a year ago'\n\nprint('before, outer_string =', outer_string)\ntry_to_change_string_reference(outer_string)\nprint('after, outer_string =', outer_string)\nbefore, outer_string = It was many and many a year ago\ngot It was many and many a year ago\nset to In a kingdom by the sea\nafter, outer_string = It was many and many a year ago\ndef return_a_whole_new_string(the_string):\nnew_string = something_to_do_with_the_old_string(the_string)\nreturn new_string\n\nmy_string = return_a_whole_new_string(my_string)\ndef use_a_wrapper_to_simulate_pass_by_reference(stuff_to_change):\nnew_string = something_to_do_with_the_old_string(stuff_to_change[0])\nstuff_to_change[0] = new_string\n\nwrapper = [my_string]\nuse_a_wrapper_to_simulate_pass_by_reference(wrapper)\n\ndo_something_with(wrapper[0])\n\nArguments are passed by assignment. The rationale behind this is twofold:\n\nthe parameter passed in is actually a reference to an object (but the reference is passed by value)\nsome data types are mutable, but others aren't\n\nSo:\n\nIf you pass a mutable object into a method, the method gets a reference to that same object and you can mutate it to your heart's delight, but if you rebind the reference in the method, the outer scope will know nothing about it, and after you're done, the outer reference will still point at the original object.\nIf you pass an immutable object to a method, you still can't rebind the outer reference, and you can't even mutate the object.\n\nTo make it even more clear, let's have some examples.\nList - a mutable type\nLet's try to modify the list that was passed to a method:\n\nOutput:\n\nSince the parameter passed in is a reference to outer_list, not a copy of it, we can use the mutating list methods to change it and have the changes reflected in the outer scope.\nNow let's see what happens when we try to change the reference that was passed in as a parameter:\n\nOutput:\n\nSince the the_list parameter was passed by value, assigning a new list to it had no effect that the code outside the method could see. The the_list was a copy of the outer_list reference, and we had the_list point to a new list, but there was no way to change where outer_list pointed.\nString - an immutable type\nIt's immutable, so there's nothing we can do to change the contents of the string\nNow, let's try to change the reference\n\nOutput:\n\nAgain, since the the_string parameter was passed by value, assigning a new string to it had no effect that the code outside the method could see. The the_string was a copy of the outer_string reference, and we had the_string point to a new string, but there was no way to change where outer_string pointed.\nI hope this clears things up a little.\nEDIT: It's been noted that this doesn't answer the question that @David originally asked, \"Is there something I can do to pass the variable by actual reference?\". Let's work on that.\nHow do we get around this?\nAs @Andrea's answer shows, you could return the new value. This doesn't change the way things are passed in, but does let you get the information you want back out:\n\nIf you really wanted to avoid using a return value, you could create a class to hold your value and pass it into the function or use an existing class, like a list:\n\nAlthough this seems a little cumbersome."
  },
  {
    "question": "Manually raising (throwing) an exception in Python How do I raise an exception in Python so that it can later be caught via an except block?",
    "answer": "raise ValueError('A very specific bad thing happened.')\nraise Exception('I know Python!') # Don't! If you catch, likely to hide bugs.\ndef demo_bad_catch():\ntry:\nraise ValueError('Represents a hidden bug, do not catch this')\nraise Exception('This is the exception you expect to handle')\nexcept Exception as error:\nprint('Caught this error: ' + repr(error))\n\n>>> demo_bad_catch()\nCaught this error: ValueError('Represents a hidden bug, do not catch this',)\ndef demo_no_catch():\ntry:\nraise Exception('general exceptions not caught by specific handling')\nexcept ValueError as e:\nprint('we will not catch exception: Exception')\n\n>>> demo_no_catch()\nTraceback (most recent call last):\nFile \"<stdin>\", line 1, in <module>\nFile \"<stdin>\", line 3, in demo_no_catch\nException: general exceptions not caught by specific handling\nraise ValueError('A very specific bad thing happened')\nraise ValueError('A very specific bad thing happened', 'foo', 'bar', 'baz')\ntry:\nsome_code_that_may_raise_our_value_error()\nexcept ValueError as err:\nprint(err.args)\n('message', 'foo', 'bar', 'baz')\nlogger = logging.getLogger(__name__)\n\ntry:\ndo_something_in_app_that_breaks_easily()\nexcept AppError as error:\nlogger.error(error)\nraise                 # just this!\ntype, value, traceback = sys.exc_info()\nraise AppError, error, sys.exc_info()[2] # avoid this.\n\nraise sys.exc_info()[0], sys.exc_info()[1], sys.exc_info()[2]\ndef error():\nraise ValueError('oops!')\n\ndef catch_error_modify_message():\ntry:\nerror()\nexcept ValueError:\nerror_type, error_instance, traceback = sys.exc_info()\nerror_instance.args = (error_instance.args[0] + ' <modification>',)\nraise error_type, error_instance, traceback\n>>> catch_error_modify_message()\nTraceback (most recent call last):\nFile \"<stdin>\", line 1, in <module>\nFile \"<stdin>\", line 3, in catch_error_modify_message\nFile \"<stdin>\", line 2, in error\nValueError: oops! <modification>\nraise error.with_traceback(sys.exc_info()[2])\nraise RuntimeError('specific message') from error\nraise ValueError, 'message' # Don't do this, it's deprecated!\nraise 'message' # really really wrong. don't do this.\ndef api_func(foo):\n'''foo should be either 'baz' or 'bar'. returns something very useful.'''\nif foo not in _ALLOWED_ARGS:\nraise ValueError('{foo} wrong, use \"baz\" or \"bar\"'.format(foo=repr(foo)))\nclass MyAppLookupError(LookupError):\n'''raise this when there's a lookup error for my app'''\nif important_key not in resource_dict and not ok_to_be_missing:\nraise MyAppLookupError('resource is missing, and that is not ok.')\n\nHow do I manually throw/raise an exception in Python?\n\nUse the most specific Exception constructor that semantically fits your issue.\nBe specific in your message, e.g.:\n\nDon't raise generic exceptions\nAvoid raising a generic Exception. To catch it, you'll have to catch all other more specific exceptions that subclass it.\nProblem 1: Hiding bugs\n\nFor example:\n\nProblem 2: Won't catch\nAnd more specific catches won't catch the general exception:\n\nBest Practices: raise statement\nInstead, use the most specific Exception constructor that semantically fits your issue.\n\nwhich also handily allows an arbitrary number of arguments to be passed to the constructor:\n\nThese arguments are accessed by the args attribute on the Exception object. For example:\n\nprints\n\nIn Python 2.5, an actual message attribute was added to BaseException in favor of encouraging users to subclass Exceptions and stop using args, but the introduction of message and the original deprecation of args has been retracted.\nBest Practices: except clause\nWhen inside an except clause, you might want to, for example, log that a specific type of error happened, and then re-raise. The best way to do this while preserving the stack trace is to use a bare raise statement. For example:\n\nDon't modify your errors... but if you insist.\nYou can preserve the stacktrace (and error value) with sys.exc_info(), but this is way more error prone and has compatibility problems between Python 2 and 3, prefer to use a bare raise to re-raise.\nTo explain - the sys.exc_info() returns the type, value, and traceback.\n\nThis is the syntax in Python 2 - note this is not compatible with Python 3:\n\nIf you want to, you can modify what happens with your new raise - e.g. setting new args for the instance:\n\nAnd we have preserved the whole traceback while modifying the args. Note that this is not a best practice and it is invalid syntax in Python 3 (making keeping compatibility much harder to work around).\n\nIn Python 3:\n\nAgain: avoid manually manipulating tracebacks. It's less efficient and more error prone. And if you're using threading and sys.exc_info you may even get the wrong traceback (especially if you're using exception handling for control flow - which I'd personally tend to avoid.)\nPython 3, Exception chaining\nIn Python 3, you can chain Exceptions, which preserve tracebacks:\n\nBe aware:\n\nthis does allow changing the error type raised, and\nthis is not compatible with Python 2.\n\nDeprecated Methods:\nThese can easily hide and even get into production code. You want to raise an exception, and doing them will raise an exception, but not the one intended!\nValid in Python 2, but not in Python 3 is the following:\n\nOnly valid in much older versions of Python (2.4 and lower), you may still see people raising strings:\n\nIn all modern versions, this will actually raise a TypeError, because you're not raising a BaseException type. If you're not checking for the right exception and don't have a reviewer that's aware of the issue, it could get into production.\nExample Usage\nI raise Exceptions to warn consumers of my API if they're using it incorrectly:\n\nCreate your own error types when apropos\n\n\"I want to make an error on purpose, so that it would go into the except\"\n\nYou can create your own error types, if you want to indicate something specific is wrong with your application, just subclass the appropriate point in the exception hierarchy:\n\nand usage:"
  },
  {
    "question": "Understanding Python super() with __init__() methods Why is super() used?\nIs there a difference between using Base.__init__ and super().__init__?\n\n```\nclass Base(object):\n    def __init__(self):\n        print \"Base created\"\n        \nclass ChildA(Base):\n    def __init__(self):\n        Base.__init__(self)\n        \nclass ChildB(Base):\n    def __init__(self):\n        super(ChildB, self).__init__()\n        \nChildA() \nChildB()\n\n```",
    "answer": "super() lets you avoid referring to the base class explicitly, which can be nice. But the main advantage comes with multiple inheritance, where all sorts of fun stuff can happen. See the standard docs on super if you haven't already.\nNote that the syntax changed in Python 3.0: you can just say super().__init__() instead of super(ChildB, self).__init__() which IMO is quite a bit nicer. The standard docs also refer to a guide to using super() which is quite explanatory."
  },
  {
    "question": "How do I concatenate two lists in Python? How do I concatenate two lists in Python?\nExample:\n\n```\nlistone = [1, 2, 3]\nlisttwo = [4, 5, 6]\n\n```\n\nExpected outcome:\n\n```\n>>> joinedlist\n[1, 2, 3, 4, 5, 6]\n\n```",
    "answer": "listone = [1, 2, 3]\nlisttwo = [4, 5, 6]\n\njoinedlist = listone + listtwo\n>>> joinedlist\n[1, 2, 3, 4, 5, 6]\n\nUse the + operator to combine the lists:\n\nOutput:\n\nNOTE: This will create a new list with a shallow copy of the items in the first list, followed by a shallow copy of the items in the second list. Use copy.deepcopy() to get deep copies of lists."
  },
  {
    "question": "How do I check if a list is empty? For example, if passed the following:\n\n```\na = []\n\n```\n\nHow do I check to see if a is empty?",
    "answer": "if not a:\nprint(\"List is empty\")\n\nUsing the implicit booleanness of the empty list is quite Pythonic."
  },
  {
    "question": "How do I make function decorators and chain them together? How do I make two decorators in Python that would do the following?\n\n```\n@make_bold\n@make_italic\ndef say():\n   return \"Hello\"\n\n```\n\nCalling say() should return:\n\n```\n\"<b><i>Hello</i></b>\"\n\n```",
    "answer": "from functools import wraps\n\ndef makebold(fn):\n@wraps(fn)\ndef wrapper(*args, **kwargs):\nreturn \"<b>\" + fn(*args, **kwargs) + \"</b>\"\nreturn wrapper\n\ndef makeitalic(fn):\n@wraps(fn)\ndef wrapper(*args, **kwargs):\nreturn \"<i>\" + fn(*args, **kwargs) + \"</i>\"\nreturn wrapper\n\n@makebold\n@makeitalic\ndef hello():\nreturn \"hello world\"\n\n@makebold\n@makeitalic\ndef log(s):\nreturn s\n\nprint hello()        # returns \"<b><i>hello world</i></b>\"\nprint hello.__name__ # with functools.wraps() this returns \"hello\"\nprint log('hello')   # returns \"<b><i>hello</i></b>\"\n\nCheck out the documentation to see how decorators work. Here is what you asked for:"
  },
  {
    "question": "How do I split a list into equally-sized chunks? How do I split a list of arbitrary length into equal sized chunks?\n\nSee also: How to iterate over a list in chunks.\nTo chunk strings, see Split string every nth character?.",
    "answer": "def chunks(lst, n):\n\"\"\"Yield successive n-sized chunks from lst.\"\"\"\nfor i in range(0, len(lst), n):\nyield lst[i:i + n]\nimport pprint\npprint.pprint(list(chunks(range(10, 75), 10)))\n[[10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n[20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n[30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\n[40, 41, 42, 43, 44, 45, 46, 47, 48, 49],\n[50, 51, 52, 53, 54, 55, 56, 57, 58, 59],\n[60, 61, 62, 63, 64, 65, 66, 67, 68, 69],\n[70, 71, 72, 73, 74]]\ndef chunks(lst, n):\n\"\"\"Yield successive n-sized chunks from lst.\"\"\"\nfor i in xrange(0, len(lst), n):\nyield lst[i:i + n]\n[lst[i:i + n] for i in range(0, len(lst), n)]\n[lst[i:i + n] for i in xrange(0, len(lst), n)]\n\nHere's a generator that yields evenly-sized chunks:\n\nFor Python 2, using xrange instead of range:\n\nBelow is a list comprehension one-liner. The method above is preferable, though, since using named functions makes code easier to understand. For Python 3:\n\nFor Python 2:"
  },
  {
    "question": "Find the current directory and file&#39;s directory How do I determine:\n\nthe current directory (where I was in the shell when I ran the Python script), and\nwhere the Python file I am executing is?",
    "answer": "import os\ndir_path = os.path.dirname(os.path.realpath(__file__))\nimport os\ncwd = os.getcwd()\n\nTo get the full path to the directory a Python file is contained in, write this in that file:\n\n(Note that the incantation above won't work if you've already used os.chdir() to change your current working directory, since the value of the __file__ constant is relative to the current working directory and is not changed by an os.chdir() call.)\n\nTo get the current working directory use\n\nDocumentation references for the modules, constants and functions used above:\n\nThe os and os.path modules.\nThe __file__ constant\nos.path.realpath(path) (returns \"the canonical path of the specified filename, eliminating any symbolic links encountered in the path\")\nos.path.dirname(path) (returns \"the directory name of pathname path\")\nos.getcwd() (returns \"a string representing the current working directory\")\nos.chdir(path) (\"change the current working directory to path\")"
  },
  {
    "question": "What is the difference between Python&#39;s list methods append and extend? What's the difference between the list methods append() and extend()?",
    "answer": ">>> x = [1, 2, 3]\n>>> x.append([4, 5])\n>>> print(x)\n[1, 2, 3, [4, 5]]\n>>> x = [1, 2, 3]\n>>> x.extend([4, 5])\n>>> print(x)\n[1, 2, 3, 4, 5]\n\n.append() appends a single object at the end of the list:\n\n.extend() appends multiple objects that are taken from inside the specified iterable:"
  },
  {
    "question": "Why is &quot;1000000000000000 in range(1000000000000001)&quot; so fast in Python 3? It is my understanding that the range() function, which is actually an object type in Python 3, generates its contents on the fly, similar to a generator.\nThis being the case, I would have expected the following line to take an inordinate amount of time because, in order to determine whether 1 quadrillion is in the range, a quadrillion values would have to be generated:\n\n```\n1_000_000_000_000_000 in range(1_000_000_000_000_001)\n\n```\n\nFurthermore: it seems that no matter how many zeroes I add on, the calculation more or less takes the same amount of time (basically instantaneous).\nI have also tried things like this, but the calculation is still almost instant:\n\n```\n# count by tens\n1_000_000_000_000_000_000_000 in range(0,1_000_000_000_000_000_000_001,10)\n\n```\n\nIf I try to implement my own range function, the result is not so nice!\n\n```\ndef my_crappy_range(N):\n    i = 0\n    while i < N:\n        yield i\n        i += 1\n    return\n\n```\n\nWhat is the range() object doing under the hood that makes it so fast?\n\nMartijn Pieters's answer was chosen for its completeness, but also see abarnert's first answer for a good discussion of what it means for range to be a full-fledged sequence in Python 3, and some information/warning regarding potential inconsistency for __contains__ function optimization across Python implementations. abarnert's other answer goes into some more detail and provides links for those interested in the history behind the optimization in Python 3 (and lack of optimization of xrange in Python 2). Answers by poke and by wim provide the relevant C source code and explanations for those who are interested.",
    "answer": "class my_range:\ndef __init__(self, start, stop=None, step=1, /):\nif stop is None:\nstart, stop = 0, start\nself.start, self.stop, self.step = start, stop, step\nif step < 0:\nlo, hi, step = stop, start, -step\nelse:\nlo, hi = start, stop\nself.length = 0 if lo > hi else ((hi - lo - 1) // step) + 1\n\ndef __iter__(self):\ncurrent = self.start\nif self.step < 0:\nwhile current > self.stop:\nyield current\ncurrent += self.step\nelse:\nwhile current < self.stop:\nyield current\ncurrent += self.step\n\ndef __len__(self):\nreturn self.length\n\ndef __getitem__(self, i):\nif i < 0:\ni += self.length\nif 0 <= i < self.length:\nreturn self.start + i * self.step\nraise IndexError('my_range object index out of range')\n\ndef __contains__(self, num):\nif self.step < 0:\nif not (self.stop < num <= self.start):\nreturn False\nelse:\nif not (self.start <= num < self.stop):\nreturn False\nreturn (num - self.start) % self.step == 0\n\nThe Python 3 range() object doesn't produce numbers immediately; it is a smart sequence object that produces numbers on demand. All it contains is your start, stop and step values, then as you iterate over the object the next integer is calculated each iteration.\nThe object also implements the object.__contains__ hook, and calculates if your number is part of its range. Calculating is a (near) constant time operation *. There is never a need to scan through all possible integers in the range.\nFrom the range() object documentation:\n\nThe advantage of the range type over a regular list or tuple is that a range object will always take the same (small) amount of memory, no matter the size of the range it represents (as it only stores the start, stop and step values, calculating individual items and subranges as needed).\n\nSo at a minimum, your range() object would do:\n\nThis is still missing several things that a real range() supports (such as the .index() or .count() methods, hashing, equality testing, or slicing), but should give you an idea.\nI also simplified the __contains__ implementation to only focus on integer tests; if you give a real range() object a non-integer value (including subclasses of int), a slow scan is initiated to see if there is a match, just as if you use a containment test against a list of all the contained values. This was done to continue to support other numeric types that just happen to support equality testing with integers but are not expected to support integer arithmetic as well. See the original Python issue that implemented the containment test.\n\n* Near constant time because Python integers are unbounded and so math operations also grow in time as N grows, making this a O(log N) operation. Since it\u2019s all executed in optimised C code and Python stores integer values in 30-bit chunks, you\u2019d run out of memory before you saw any performance impact due to the size of the integers involved here."
  },
  {
    "question": "Renaming column names in Pandas I want to change the column labels of a Pandas DataFrame from\n\n```\n['$a', '$b', '$c', '$d', '$e']\n\n```\n\nto\n\n```\n['a', 'b', 'c', 'd', 'e']\n\n```",
    "answer": ">>> df = pd.DataFrame({'$a':[1,2], '$b': [10,20]})\n>>> df\na  $b\n1  10\n2  20\n\n>>> df.columns = ['a', 'b']\n>>> df\na   b\n1  10\n2  20\n\nJust assign it to the .columns attribute:"
  },
  {
    "question": "Convert string &quot;Jun 1 2005 1:33PM&quot; into datetime I have a huge list of datetime strings like the following\n\n```\n[\"Jun 1 2005 1:33PM\", \"Aug 28 1999 12:00AM\"]\n\n```\n\nHow do I convert them into datetime objects?",
    "answer": ">>> from datetime import datetime\n>>> datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\ndatetime.datetime(2005, 6, 1, 13, 33)\n>>> datetime.strptime('Jun 1 2005', '%b %d %Y').date()\ndate(2005, 6, 1)\n\ndatetime.strptime parses an input string in the user-specified format into a timezone-naive datetime object:\n\nTo obtain a date object using an existing datetime object, convert it using .date():\n\nLinks:\n\nstrptime docs: Python 2, Python 3\n\nstrptime/strftime format string docs: Python 2, Python 3\n\nstrftime.org format string cheatsheet\n\nNotes:\n\nstrptime = \"string parse time\"\nstrftime = \"string format time\""
  },
  {
    "question": "How can I remove a key from a Python dictionary? I want to remove a key from a dictionary if it is present. I currently use this code:\n\n```\nif key in my_dict:\n    del my_dict[key]\n\n```\n\nWithout the if statement, the code will raise KeyError if the key is not present. How can I handle this more simply?\n\nSee Delete an element from a dictionary for more general approaches to the problem of removing a key from a dict (including ones which produce a modified copy).",
    "answer": "my_dict.pop('key', None)\ndel my_dict['key']\n\nTo delete a key regardless of whether it is in the dictionary, use the two-argument form of dict.pop():\n\nThis will return my_dict[key] if key exists in the dictionary, and None otherwise. If the second parameter is not specified (i.e. my_dict.pop('key')) and key does not exist, a KeyError is raised.\nTo delete a key that is guaranteed to exist, you can also use\n\nThis will raise a KeyError if the key is not in the dictionary."
  },
  {
    "question": "How to upgrade all Python packages with pip Is it possible to upgrade all Python packages at one time with pip?\nNote: that there is a feature request for this on the official issue tracker.",
    "answer": "pip --disable-pip-version-check list --outdated --format=json | python -c \"import json, sys; print('\\n'.join([x['name'] for x in json.load(sys.stdin)]))\" | xargs -n1 pip install -U\npip list --outdated --format=freeze | grep -v '^\\-e' | cut -d = -f 1  | xargs -n1 pip install -U\npip freeze --local | grep -v '^\\-e' | cut -d = -f 1  | xargs -n1 pip install -U\n\nThere isn't a built-in flag yet. Starting with pip version 22.3, the --outdated and --format=freeze have become mutually exclusive. Use Python, to parse the JSON output:\n\nIf you are using pip<22.3 you can use:\n\nFor older versions of pip:\n\nThe grep is to skip editable (\"-e\") package definitions, as suggested by @jawache. (Yes, you could replace grep+cut with sed or awk or perl or...).\n\nThe -n1 flag for xargs prevents stopping everything if updating one package fails (thanks @andsens).\n\nNote: there are infinite potential variations for this. I'm trying to keep this answer short and simple, but please do suggest variations in the comments!"
  },
  {
    "question": "How to sort a list of dictionaries by a value of the dictionary in Python? How do I sort a list of dictionaries by a specific key's value? Given:\n\n```\n[{'name': 'Homer', 'age': 39}, {'name': 'Bart', 'age': 10}]\n\n```\n\nWhen sorted by name, it should become:\n\n```\n[{'name': 'Bart', 'age': 10}, {'name': 'Homer', 'age': 39}]\n\n```",
    "answer": "newlist = sorted(list_to_be_sorted, key=lambda d: d['name'])\nfrom operator import itemgetter\nnewlist = sorted(list_to_be_sorted, key=itemgetter('name'))\nnewlist = sorted(list_to_be_sorted, key=itemgetter('name'), reverse=True)\n\nThe sorted() function takes a key= parameter\n\nAlternatively, you can use operator.itemgetter instead of defining the function yourself\n\nFor completeness, add reverse=True to sort in descending order"
  },
  {
    "question": "How do I get the last element of a list? How do I get the last element of a list? Which way is preferred?\n\n```\nalist[-1]\nalist[len(alist) - 1]\n\n```",
    "answer": ">>> some_list = [1, 2, 3]\n>>> some_list[-1] = 5 # Set the last element\n>>> some_list[-2] = 3 # Set the second to last element\n>>> some_list\n[1, 3, 5]\n\nsome_list[-1] is the shortest and most Pythonic.\nIn fact, you can do much more with this syntax. The some_list[-n] syntax gets the nth-to-last element. So some_list[-1] gets the last element, some_list[-2] gets the second to last, etc, all the way down to some_list[-len(some_list)], which gives you the first element.\nYou can also set list elements in this way. For instance:\n\nNote that getting a list item by index will raise an IndexError if the expected item doesn't exist. This means that some_list[-1] will raise an exception if some_list is empty, because an empty list can't have a last element."
  },
  {
    "question": "How to leave/exit/deactivate a Python virtualenv I'm using virtualenv and the virtualenvwrapper. I can switch between virtualenv's just fine using the workon command.\n\n```\nme@mymachine:~$ workon env1\n(env1)me@mymachine:~$ workon env2\n(env2)me@mymachine:~$ workon env1\n(env1)me@mymachine:~$ \n\n```\n\nHow do I exit all virtual environments and work on my system environment again? Right now, the only way I have of getting back to me@mymachine:~$ is to exit the shell and start a new one. That's kind of annoying. Is there a command to work on \"nothing\", and if so, what is it? If such a command does not exist, how would I go about creating it?",
    "answer": "deactivate\nconda deactivate\nsource deactivate\n\nUsually, activating a virtualenv gives you a shell function named:\n\nwhich puts things back to normal.\nI have just looked specifically again at the code for virtualenvwrapper, and, yes, it too supports deactivate as the way to escape from all virtualenvs.\nIf you are trying to leave an Anaconda environment, the command depends upon your version of conda. Recent versions (like 4.6) install a conda function directly in your shell, in which case you run:\n\nOlder conda versions instead implement deactivation using a stand-alone script:"
  },
  {
    "question": "How do I install pip on Windows? pip is a replacement for easy_install. But should I install pip using easy_install on Windows? Is there a better way?",
    "answer": "py -3 -m ensurepip\npython3 -m ensurepip\npython get-pip.py\npip install httpie\nhttp://proxy_url:port\nhttp://username:password@proxy_url:port\n\nPython 3.4+ and 2.7.9+\nGood news! Python 3.4 (released March 2014) and Python 2.7.9 (released December 2014) ship with Pip. This is the best feature of any Python release. It makes the community's wealth of libraries accessible to everyone. Newbies are no longer excluded from using community libraries by the prohibitive difficulty of setup. In shipping with a package manager, Python joins Ruby, Node.js, Haskell, Perl, Go\u2014almost every other contemporary language with a majority open-source community. Thank you, Python.\nIf you do find that pip is not available, simply run ensurepip.\n\nOn Windows:\n\nOtherwise:\n\nOf course, that doesn't mean Python packaging is problem solved. The experience remains frustrating. I discuss this in the Stack Overflow question Does Python have a package/module management system?.\nPython 3 \u2264 3.3 and 2 \u2264 2.7.8\nFlying in the face of its 'batteries included' motto, Python ships without a package manager. To make matters worse, Pip was\u2014until recently\u2014ironically difficult to install.\nOfficial instructions\nPer https://pip.pypa.io/en/stable/installing/#do-i-need-to-install-pip:\nDownload get-pip.py, being careful to save it as a .py file rather than .txt. Then, run it from the command prompt:\n\nYou possibly need an administrator command prompt to do this. Follow Start a Command Prompt as an Administrator (Microsoft TechNet).\nThis installs the pip package, which (in Windows) contains ...\\Scripts\\pip.exe that path must be in PATH environment variable to use pip from the command line (see the second part of 'Alternative Instructions' for adding it to your PATH,\nAlternative instructions\nThe official documentation tells users to install Pip and each of its dependencies from source. That's tedious for the experienced and prohibitively difficult for newbies.\nFor our sake, Christoph Gohlke prepares Windows installers (.msi) for popular Python packages. He builds installers for all Python versions, both 32 and 64 bit. You need to:\n\nInstall setuptools\nInstall pip\n\nFor me, this installed Pip at C:\\Python27\\Scripts\\pip.exe. Find pip.exe on your computer, then add its folder (for example, C:\\Python27\\Scripts) to your path (Start / Edit environment variables). Now you should be able to run pip from the command line. Try installing a package:\n\nThere you go (hopefully)! Solutions for common problems are given below:\nProxy problems\nIf you work in an office, you might be behind an HTTP proxy. If so, set the environment variables http_proxy and https_proxy. Most Python applications (and other free software) respect these. Example syntax:\n\nIf you're really unlucky, your proxy might be a Microsoft NTLM proxy. Free software can't cope. The only solution is to install a free software friendly proxy that forwards to the nasty proxy. http://cntlm.sourceforge.net/\nUnable to find vcvarsall.bat\nPython modules can be partly written in C or C++. Pip tries to compile from source. If you don't have a C/C++ compiler installed and configured, you'll see this cryptic error message.\n\nError: Unable to find vcvarsall.bat\n\nYou can fix that by installing a C++ compiler such as MinGW or Visual C++. Microsoft actually ships one specifically for use with Python. Or try Microsoft Visual C++ Compiler for Python 2.7.\nOften though it's easier to check Christoph's site for your package."
  },
  {
    "question": "How do I parse a string to a float or int? How can I convert an str to a float?\n\n```\n\"545.2222\" -> 545.2222\n\n```\n\nOr an str to a int?\n\n```\n\"31\" -> 31\n\n```\n\n\nFor the reverse, see Convert integer to string in Python and Converting a float to a string without rounding it.\nPlease instead use How can I read inputs as numbers? to close duplicate questions where OP received a string from user input and immediately wants to convert it, or was hoping for input (in 3.x) to convert the type automatically.",
    "answer": ">>> a = \"545.2222\"\n>>> float(a)\n545.22220000000004\n>>> int(float(a))"
  },
  {
    "question": "How do I get a substring of a string in Python? I want to get a new string from the third character to the end of the string, e.g. myString[2:end]. If omitting the second part means 'to the end', and if you omit the first part, does it start from the start?",
    "answer": ">>> x = \"Hello World!\"\n>>> x[2:]\n'llo World!'\n>>> x[:2]\n'He'\n>>> x[:-2]\n'Hello Worl'\n>>> x[-2:]\n'd!'\n>>> x[2:-2]\n'llo Worl'\n\nPython calls this concept \"slicing\" and it works on more than just strings. Take a look here for a comprehensive introduction."
  },
  {
    "question": "Installing specific package version with pip I am trying to install version 1.2.2 of MySQL_python, using a fresh virtualenv created with the --no-site-packages option. The current version shown in PyPi is 1.2.3. Is there a way to install the older version? I have tried:\n\n```\npip install MySQL_python==1.2.2\n\n```\n\nHowever, when installed, it still shows MySQL_python-1.2.3-py2.6.egg-info in the site packages. Is this a problem specific to this package, or am I doing something wrong?",
    "answer": "pip uninstall MySQL_python\npip install -Iv http://sourceforge.net/projects/mysql-python/files/mysql-python/1.2.2/MySQL-python-1.2.2.tar.gz/download\n\nTL;DR:\nUpdate as of 2022-12-28:\npip install --force-reinstall -v\nFor example: pip install --force-reinstall -v \"MySQL_python==1.2.2\"\nWhat these options mean:\n\n--force-reinstall is an option to reinstall all packages even if they are already up-to-date.\n-v is for verbose. You can combine for even more verbosity (i.e. -vv) up to 3 times (e.g. --force-reinstall -vvv).\n\nThanks to @Peter for highlighting this (and it seems that the context of the question has broadened given the time when the question was first asked!), the documentation for Python discusses a caveat with using -I, in that it can break your installation if it was installed with a different package manager or if if your package is/was a different version.\n\nOriginal answer:\n\npip install -Iv (i.e. pip install -Iv MySQL_python==1.2.2)\n\nWhat these options mean:\n\n-I stands for --ignore-installed which will ignore the installed packages, overwriting them.\n-v is for verbose. You can combine for even more verbosity (i.e. -vv) up to 3 times (e.g. -Ivvv).\n\nFor more information, see pip install --help\nFirst, I see two issues with what you're trying to do. Since you already have an installed version, you should either uninstall the current existing driver or use pip install -I MySQL_python==1.2.2\nHowever, you'll soon find out that this doesn't work. If you look at pip's installation log, or if you do a pip install -Iv MySQL_python==1.2.2 you'll find that the PyPI URL link does not work for MySQL_python v1.2.2. You can verify this here: http://pypi.python.org/pypi/MySQL-python/1.2.2\nThe download link 404s and the fallback URL links are re-directing infinitely due to sourceforge.net's recent upgrade and PyPI's stale URL.\nSo to properly install the driver, you can follow these steps:"
  },
  {
    "question": "How can I install packages using pip according to the requirements.txt file from a local directory? Here is the problem:\nI have a requirements.txt file that looks like:\n\n```\nBeautifulSoup==3.2.0\nDjango==1.3\nFabric==1.2.0\nJinja2==2.5.5\nPyYAML==3.09\nPygments==1.4\nSQLAlchemy==0.7.1\nSouth==0.7.3\namqplib==0.6.1\nanyjson==0.3\n...\n\n```\n\nI have a local archive directory containing all the packages + others.\nI have created a new virtualenv with\n\n```\nbin/virtualenv testing\n\n```\n\nUpon activating it, I tried to install the packages according to requirements.txt from the local archive directory.\n\n```\nsource bin/activate\npip install -r /path/to/requirements.txt -f file:///path/to/archive/\n\n```\n\nI got some output that seems to indicate that the installation is fine:\n\n```\nDownloading/unpacking Fabric==1.2.0 (from -r ../testing/requirements.txt (line 3))\n  Running setup.py egg_info for package Fabric\n    warning: no previously-included files matching '*' found under directory 'docs/_build'\n    warning: no files found matching 'fabfile.py'\nDownloading/unpacking South==0.7.3 (from -r ../testing/requirements.txt (line 8))\n  Running setup.py egg_info for package South\n....\n\n```\n\nBut a later check revealed that none of the packages are installed properly. I cannot import the packages, and none are found in the site-packages directory of my virtualenv. So what went wrong?",
    "answer": "pip install -r requirements.txt --no-index --find-links file:///tmp/packages\n\nThis works for me:\n\n--no-index - Ignore package index (only look at --find-links URLs instead).\n-f, --find-links <URL> - If <URL> is a URL or a path to an HTML file, then parse for links to archives. If <URL> is a local path or a file:// URL that's a directory, then look for archives in the directory listing."
  },
  {
    "question": "How do I remove all packages installed by pip? How do I uninstall all packages installed by pip from my currently activated virtual environment?",
    "answer": "pip freeze | xargs pip uninstall -y\npip freeze --exclude-editable | xargs pip uninstall -y\npip freeze | cut -d \"@\" -f1 | xargs pip uninstall -y\n\nI've found this snippet as an alternative solution. It's a more graceful removal of libraries than remaking the virtualenv:\n\nIn case you have packages installed via VCS, you need to exclude those lines and remove the packages manually (elevated from the comments below):\n\nIf you have packages installed directly from github/gitlab, those will have @.\nLike:\ndjango @ git+https://github.com/django.git@<sha>\nYou can add cut -d \"@\" -f1 to get just the package name that is required to uninstall it."
  },
  {
    "question": "How do I get a list of locally installed Python modules? How do I get a list of Python modules installed on my computer?",
    "answer": "import pip\ninstalled_packages = pip.get_installed_distributions()\ninstalled_packages_list = sorted([\"%s==%s\" % (i.key, i.version)\nfor i in installed_packages])\nprint(installed_packages_list)\nsorted([\"%s==%s\" % (i.key, i.version) for i in pip.get_installed_distributions()])\n['behave==1.2.4', 'enum34==1.0', 'flask==0.10.1', 'itsdangerous==0.24',\n'jinja2==2.7.2', 'jsonschema==2.3.0', 'markupsafe==0.23', 'nose==1.3.3',\n'parse-type==0.3.4', 'parse==1.6.4', 'prettytable==0.7.2', 'requests==2.3.0',\n'six==1.6.1', 'vioozer-metadata==0.1', 'vioozer-users-server==0.1',\n'werkzeug==0.9.4']\ncd /tmp\nvirtualenv test_env\nNew python executable in test_env/bin/python\nInstalling setuptools, pip...done.\nsource test_env/bin/activate\n(test_env) $\n(test_env) $ git clone https://github.com/behave/behave.git\nCloning into 'behave'...\nremote: Reusing existing pack: 4350, done.\nremote: Total 4350 (delta 0), reused 0 (delta 0)\nReceiving objects: 100% (4350/4350), 1.85 MiB | 418.00 KiB/s, done.\nResolving deltas: 100% (2388/2388), done.\nChecking connectivity... done.\n(test_env) $ ls /tmp/behave/setup.py\n/tmp/behave/setup.py\n(test_env) $ cd /tmp/behave && pip install .\nrunning install\n...\nInstalled /private/tmp/test_env/lib/python2.7/site-packages/enum34-1.0-py2.7.egg\nFinished processing dependencies for behave==1.2.5a1\n>>> import pip\n>>> sorted([\"%s==%s\" % (i.key, i.version) for i in pip.get_installed_distributions()])\n['behave==1.2.5a1', 'enum34==1.0', 'parse-type==0.3.4', 'parse==1.6.4', 'six==1.6.1']\n>>> import os\n>>> os.getcwd()\n'/private/tmp'\n>>> import pip\n>>> sorted([\"%s==%s\" % (i.key, i.version) for i in pip.get_installed_distributions()])\n['enum34==1.0', 'parse-type==0.3.4', 'parse==1.6.4', 'six==1.6.1']\n>>> import os\n>>> os.getcwd()\n'/private/tmp/behave'\n\nSolution\nDo not use with pip > 10.0!\nMy 50 cents for getting a pip freeze-like list from a Python script:\n\nAs a (too long) one liner:\n\nGiving:\n\nScope\nThis solution applies to the system scope or to a virtual environment scope, and covers packages installed by setuptools, pip and (god forbid) easy_install.\nMy use case\nI added the result of this call to my Flask server, so when I call it with http://example.com/exampleServer/environment I get the list of packages installed on the server's virtualenv. It makes debugging a whole lot easier.\nCaveats\nI have noticed a strange behaviour of this technique - when the Python interpreter is invoked in the same directory as a setup.py file, it does not list the package installed by setup.py.\nSteps to reproduce:\nCreate a virtual environment\n\nClone a Git repository with setup.py\n\nWe have behave's setup.py in /tmp/behave:\n\nInstall the Python package from the Git repository\n\nIf we run the aforementioned solution from /tmp\n\nIf we run the aforementioned solution from /tmp/behave\n\nbehave==1.2.5a1 is missing from the second example, because the working directory contains behave's setup.py file.\nI could not find any reference to this issue in the documentation. Perhaps I shall open a bug for it."
  },
  {
    "question": "How do I install a Python package with a .whl file? I'm trying to install a Python package on Windows, using Christoph Gohlke's Windows binaries here:\nhttp://www.lfd.uci.edu/~gohlke/pythonlibs/#jpype\nThe package is made available here as a .whl file. How do I install that?",
    "answer": "pip install some-package.whl\npip install C:/some-dir/some-file.whl\n\nI just used the following which was quite simple. First open a console then cd to where you've downloaded your file like some-package.whl and use\n\nNote: if pip.exe is not recognized, you may find it in the \"Scripts\" directory from where python has been installed. If pip is not installed, this page can help:\nHow do I install pip on Windows?\nNote: for clarification\nIf you copy the *.whl file to your local drive (ex. C:\\some-dir\\some-file.whl) use the following command line parameters --"
  },
  {
    "question": "Find which version of package is installed with pip Using pip, is it possible to figure out which version of a package is currently installed?\nI know about pip install XYZ --upgrade but I am wondering if there is anything like pip info XYZ.  If not what would be the best way to tell what version I am currently using.",
    "answer": "pip show Jinja2\n---\nName: Jinja2\nVersion: 2.7.3\nLocation: /path/to/virtualenv/lib/python2.7/site-packages\nRequires: markupsafe\npip freeze | grep Jinja2\nJinja2==2.7.3\n\nAs of pip 1.3, there is a pip show command.\n\nIn older versions, pip freeze and grep should do the job nicely."
  },
  {
    "question": "pip install from git repo branch Trying to pip install a repo's specific branch. Google tells me to\n\n```\npip install https://github.com/user/repo.git@branch\n\n```\n\nThe branch's name is issue/34/oscar-0.6 so I did pip  install https://github.com/tangentlabs/django-oscar-paypal.git@/issue/34/oscar-0.6 but its returning a 404.\nHow do I install this branch?",
    "answer": "pip install git+https://github.com/tangentlabs/django-oscar-paypal.git@issue/34/oscar-0.6\n\nPrepend the url prefix git+ (See VCS Support):\n\nAnd specify the branch name without the leading /."
  },
  {
    "question": "What is the difference between pip and Conda? I know pip is a package manager for Python packages. However, I saw the installation on IPython's website use Conda to install IPython.\nCan I use pip to install IPython? Why should I use Conda as another Python package manager when I already have pip?\nWhat is the difference between pip and Conda?\nWhat is the difference between pip and Conda?",
    "answer": "Quoting from the Conda blog:\n\nHaving been involved in the python world for so long, we are all aware of pip, easy_install, and virtualenv, but these tools did not meet all of our specific requirements. The main problem is that they are focused around Python, neglecting non-Python library dependencies, such as HDF5, MKL, LLVM, etc., which do not have a setup.py in their source code and also do not install files into Python\u2019s site-packages directory.\n\nSo Conda is a packaging tool and installer that aims to do more than what pip does; handle library dependencies outside of the Python packages as well as the Python packages themselves. Conda also creates a virtual environment, like virtualenv does.\nAs such, Conda should be compared to Buildout perhaps, another tool that lets you handle both Python and non-Python installation tasks.\nBecause Conda introduces a new packaging format, you cannot use pip and Conda interchangeably;  pip cannot install the Conda package format. You can use the two tools side by side (by installing pip with conda install pip) but they do not interoperate either.\nSince writing this answer, Anaconda has published a new page on Understanding Conda and Pip, which echoes this as well:\n\nThis highlights a key difference between conda and pip. Pip installs Python packages whereas conda installs packages which may contain software written in any language. For example, before using pip, a Python interpreter must be installed via a system package manager or by downloading and running an installer. Conda on the other hand can install Python packages as well as the Python interpreter directly.\n\nand further on\n\nOccasionally a package is needed which is not available as a conda package but is available on PyPI and can be installed with pip. In these cases, it makes sense to try to use both conda and pip."
  },
  {
    "question": "How do I update/upgrade pip itself from inside my virtual environment? I'm able to update pip-managed packages, but how do I update pip itself? According to pip --version, I currently have pip 1.1 installed in my virtualenv and I want to update to the latest version. \nWhat's the command for that? Do I need to use distribute or is there a native pip or virtualenv command? I've already tried pip update and pip update pip with no success.",
    "answer": "pip install --upgrade pip\npy -m pip install --upgrade pip\n\npip is just a PyPI package like any other; you could use it to upgrade itself the same way you would upgrade any package:\n\nOn Windows the recommended command is:"
  },
  {
    "question": "Why use pip over easy_install? A tweet reads: \n\nDon't use easy_install, unless you\n  like stabbing yourself in the face.\n  Use pip.\n\nWhy use pip over easy_install? Doesn't the fault lie with PyPI and package authors mostly? If an author uploads crap source tarball (eg: missing files, no setup.py) to PyPI, then both pip and easy_install will fail. Other than cosmetic differences, why do Python people (like in the above tweet) seem to strongly favor pip over easy_install?\n(Let's assume that we're talking about easy_install from the Distribute package, that is maintained by the community)",
    "answer": "Many of the answers here are out of date for 2015 (although the initially accepted one from Daniel Roseman is not). Here's the current state of things:\n\nBinary packages are now distributed as wheels (.whl files)\u2014not just on PyPI, but in third-party repositories like Christoph Gohlke's Extension Packages for Windows. pip can handle wheels; easy_install cannot.\nVirtual environments (which come built-in with 3.4, or can be added to 2.6+/3.1+ with virtualenv) have become a very important and prominent tool (and recommended in the official docs); they include pip out of the box, but don't even work properly with easy_install.\nThe distribute package that included easy_install is no longer maintained. Its improvements over setuptools got merged back into setuptools. Trying to install distribute will just install setuptools instead.\neasy_install itself is only quasi-maintained.\nAll of the cases where pip used to be inferior to easy_install\u2014installing from an unpacked source tree, from a DVCS repo, etc.\u2014are long-gone; you can pip install ., pip install git+https://.\npip comes with the official Python 2.7 and 3.4+ packages from python.org, and a pip bootstrap is included by default if you build from source.\nThe various incomplete bits of documentation on installing, using, and building packages have been replaced by the Python Packaging User Guide. Python's own documentation on Installing Python Modules now defers to this user guide, and explicitly calls out pip as \"the preferred installer program\".\nOther new features have been added to pip over the years that will never be in easy_install. For example, pip makes it easy to clone your site-packages by building a requirements file and then installing it with a single command on each side. Or to convert your requirements file to a local repo to use for in-house development. And so on.\n\nThe only good reason that I know of to use easy_install in 2015 is the special case of using Apple's pre-installed Python versions with OS X 10.5-10.8. Since 10.5, Apple has included easy_install, but as of 10.10 they still don't include pip. With 10.9+, you should still just use get-pip.py, but for 10.5-10.8, this has some problems, so it's easier to sudo easy_install pip. (In general, easy_install pip is a bad idea; it's only for OS X 10.5-10.8 that you want to do this.) Also, 10.5-10.8 include readline in a way that easy_install knows how to kludge around but pip doesn't, so you also want to sudo easy_install readline if you want to upgrade that."
  },
  {
    "question": "pip install mysql-python fails with EnvironmentError: mysql_config not found This is the error I get\n\n```\n(mysite)zjm1126@zjm1126-G41MT-S2:~/zjm_test/mysite$ pip install mysql-python\nDownloading/unpacking mysql-python\n  Downloading MySQL-python-1.2.3.tar.gz (70Kb): 70Kb downloaded\n  Running setup.py egg_info for package mysql-python\n    sh: mysql_config: not found\n    Traceback (most recent call last):\n      File \"<string>\", line 14, in <module>\n      File \"/home/zjm1126/zjm_test/mysite/build/mysql-python/setup.py\", line 15, in <module>\n        metadata, options = get_config()\n      File \"setup_posix.py\", line 43, in get_config\n        libs = mysql_config(\"libs_r\")\n      File \"setup_posix.py\", line 24, in mysql_config\n        raise EnvironmentError(\"%s not found\" % (mysql_config.path,))\n    EnvironmentError: mysql_config not found\n    Complete output from command python setup.py egg_info:\n    sh: mysql_config: not found\n\nTraceback (most recent call last):\n\n  File \"<string>\", line 14, in <module>\n\n  File \"/home/zjm1126/zjm_test/mysite/build/mysql-python/setup.py\", line 15, in <module>\n\n    metadata, options = get_config()\n\n  File \"setup_posix.py\", line 43, in get_config\n\n    libs = mysql_config(\"libs_r\")\n\n  File \"setup_posix.py\", line 24, in mysql_config\n\n    raise EnvironmentError(\"%s not found\" % (mysql_config.path,))\n\nEnvironmentError: mysql_config not found\n\n----------------------------------------\nCommand python setup.py egg_info failed with error code 1\nStoring complete log in /home/zjm1126/.pip/pip.log\n(mysite)zjm1126@zjm1126-G41MT-S2:~/zjm_test/mysite$ pip install mysql-python\nDownloading/unpacking mysql-python\n  Running setup.py egg_info for package mysql-python\n    sh: mysql_config: not found\n    Traceback (most recent call last):\n      File \"<string>\", line 14, in <module>\n      File \"/home/zjm1126/zjm_test/mysite/build/mysql-python/setup.py\", line 15, in <module>\n        metadata, options = get_config()\n      File \"setup_posix.py\", line 43, in get_config\n        libs = mysql_config(\"libs_r\")\n      File \"setup_posix.py\", line 24, in mysql_config\n        raise EnvironmentError(\"%s not found\" % (mysql_config.path,))\n    EnvironmentError: mysql_config not found\n    Complete output from command python setup.py egg_info:\n    sh: mysql_config: not found\n\nTraceback (most recent call last):\n\n  File \"<string>\", line 14, in <module>\n\n  File \"/home/zjm1126/zjm_test/mysite/build/mysql-python/setup.py\", line 15, in <module>\n\n    metadata, options = get_config()\n\n  File \"setup_posix.py\", line 43, in get_config\n\n    libs = mysql_config(\"libs_r\")\n\n  File \"setup_posix.py\", line 24, in mysql_config\n\n    raise EnvironmentError(\"%s not found\" % (mysql_config.path,))\n\nEnvironmentError: mysql_config not found\n\n----------------------------------------\nCommand python setup.py egg_info failed with error code 1\nStoring complete log in /home/zjm1126/.pip/pip.log\n\n```\n\nWhat can I do to resolve this?",
    "answer": "sudo apt-get install libmysqlclient-dev\nsudo apt install default-libmysqlclient-dev\n\nIt seems mysql_config is missing on your system or the installer could not find it.\nBe sure mysql_config is really installed.\nFor example on Debian/Ubuntu you must install the package:\n\nMaybe the mysql_config is not in your path, it will be the case when you compile by yourself\nthe mysql suite.\nUpdate: For recent versions of debian/ubuntu (as of 2018) it is"
  },
  {
    "question": "Can I force pip to reinstall the current version? I've come across situations where a current version of a package seems not to be working and requires reinstallation. But pip install -U won't touch a package that is already up-to-date. I see how to force a reinstallation by first uninstalling (with pip uninstall) and then installing, but is there a way to simply force an \"update\" to a nominally current version in a single step?",
    "answer": "pip install --upgrade --force-reinstall <package>\npip install -I <package>\npip install --ignore-installed <package>\n\nWhen upgrading, reinstall all packages even if they are already up-to-date.\n\nIgnore the installed packages (reinstalling instead)."
  },
  {
    "question": "How to list all available package versions with pip? Given the name of a Python package that can be installed with pip, is there any way to find out a list of all the possible versions of it that pip could install? Right now it's trial and error.\nI'm trying to install a version for a third party library, but the newest version is too new, there were backwards incompatible changes made. So I'd like to somehow have a list of all the versions that pip knows about, so that I can test them.",
    "answer": "pip install yolk3k\nyolk -V django\nDjango 1.3\nDjango 1.2.5\nDjango 1.2.4\nDjango 1.2.3\nDjango 1.2.2\nDjango 1.2.1\nDjango 1.2\nDjango 1.1.4\nDjango 1.1.3\nDjango 1.1.2\nDjango 1.0.4\n\n(update: As of March 2020, many people have reported that yolk, installed via pip install yolk3k, only returns latest version.  Chris's answer seems to have the most upvotes and worked for me)\nThe script at pastebin does work. However it's not very convenient if you're working with multiple environments/hosts because you will have to copy/create it every time.\nA better all-around solution would be to use yolk3k, which is available to install with pip. E.g. to see what versions of Django are available:\n\nyolk3k is a fork of the original yolk which ceased development in 2012. Though yolk is no longer maintained (as indicated in comments below), yolk3k appears to be and supports Python 3.\nNote: I am not involved in the development of yolk3k. If something doesn't seem to work as it should, leaving a comment here should not make much difference. Use the yolk3k issue tracker instead and consider submitting a fix, if possible."
  },
  {
    "question": "How to update/upgrade a package using pip? What is the way to update a package using pip?\nthose do not work:\n\n```\npip update\npip upgrade\n\n```\n\nI know this is a simple question but it is needed as it is not so easy to find (pip documentation doesn't pop up and other questions from stack overflow are relevant but are not exactly about that)",
    "answer": "pip install <package_name> --upgrade\npip install <package_name> -U\npip install <package_name> --upgrade --user\n\nThis is the way\n\nor in short\n\nUsing sudo will ask to enter your root password to confirm the action, but although common, is considered unsafe.\nIf you do not have a root password (if you are not the admin) you should probably work with virtualenv.\nYou can also use the user flag to install it on this user only."
  },
  {
    "question": "How to state in requirements.txt a direct github source I've installed a library using the command\n\n```\npip install git+git://github.com/mozilla/elasticutils.git\n\n```\n\nwhich installs it directly from a Github repository.  This works fine and I want to have that dependency in my requirements.txt.  I've looked at other tickets like this but that didn't solve my problem.  If I put something like\n\n```\n-f git+git://github.com/mozilla/elasticutils.git\nelasticutils==0.7.dev\n\n```\n\nin the requirements.txt file, a pip install -r requirements.txt results in the following output:\n\n```\nDownloading/unpacking elasticutils==0.7.dev (from -r requirements.txt (line 20))\n  Could not find a version that satisfies the requirement elasticutils==0.7.dev (from -r requirements.txt (line 20)) (from versions: )\nNo distributions matching the version for elasticutils==0.7.dev (from -r requirements.txt (line 20))\n\n```\n\nThe documentation of the requirements file does not mention links using the git+git protocol specifier, so maybe this is just not supported.\nDoes anybody have a solution for my problem?",
    "answer": "package-one==1.9.4\npackage-two==3.7.1\npackage-three==1.0.1\n...\npackage-one==1.9.4\npackage-two @ git+https://github.com/owner/repo@41b95ec\npackage-three==1.0.1\npackage-two @ git+https://github.com/owner/repo@main\npackage-two @ git+https://github.com/owner/repo@0.1\npackage-two @ git+https://github.com/owner/repo@releases/tag/v3.7.1\n\nNormally your requirements.txt file would look something like this:\n\nTo specify a Github repo, you do not need the package-name== convention.\nThe examples below update package-two using a GitHub repo. The text after @ denotes the specifics of the package.\nSpecify commit hash (41b95ec in the context of updated requirements.txt):\n\nSpecify branch name (main):\n\nSpecify tag (0.1):\n\nSpecify release (3.7.1):\n\nNote that in certain versions of pip you will need to update the package version in the package's setup.py, or pip will assume the requirement is already satisfied and not install the new version. For instance, if you have 1.2.1 installed, and want to fork this package with your own version, you could use the above technique in your requirements.txt and then update setup.py to 1.2.1.1.\nSee also the pip documentation on VCS support."
  },
  {
    "question": "Where does pip install its packages? I activated a virtualenv which has pip installed. I did\n\n```\npip3 install Django==1.8\n\n```\n\nand Django successfully downloaded. Now, I want to open up the Django folder. Where is the folder located?\nNormally it would be in \"downloads\", but I'm not sure where it would be if I installed it using pip in a virtualenv.",
    "answer": "pip when used with virtualenv will generally install packages in the path <virtualenv_name>/lib/<python_ver>/site-packages.\nFor example, I created a test virtualenv named venv_test with Python 2.7, and the django folder is in venv_test/lib/python2.7/site-packages/django."
  },
  {
    "question": "Dealing with multiple Python versions and PIP Is there a way to make pip play well with multiple versions of Python? For example, I want to use pip to explicitly install things to either my site 2.5 installation or my site 2.6 installation.\nFor example, with easy_install, I use easy_install-2.{5,6}.\nAnd, yes \u2014 I know about virtualenv, and no \u2014 it's not a solution to this particular problem.",
    "answer": "python -m pip install fish\n\n.env/bin/python -m pip install fish\n\npython-3.6 -m pip install fish\npip-2.5 install myfoopackage\npip-2.6 install otherpackage\npip-2.7 install mybarpackage\npip2.6 install otherpackage\npip2.7 install mybarpackage\n\nThe current recommendation is to use python -m pip, where python is the version of Python you would like to use. This is the recommendation because it works across all versions of Python, and in all forms of virtualenv. For example:\n\nPrevious answer, left for posterity:\nSince version 0.8, Pip supports pip-{version}. You can use it the same as easy_install-{version}:\n\npip changed its schema to use pipVERSION instead of pip-VERSION in version 1.5. You should use the following if you have pip >= 1.5:\n\nCheck Versioned commands consistent with Python. #1053 for more details\n\nReferences:\n\nNeed pip-x.y scripts #200\nv0.8 changelog or News for pip, v0.8"
  },
  {
    "question": "How to install psycopg2 with &quot;pip&quot; on Python? I'm using virtualenv and I need to install \"psycopg2\".\nI have done the following:\n\n```\npip install http://pypi.python.org/packages/source/p/psycopg2/psycopg2-2.4.tar.gz#md5=24f4368e2cfdc1a2b03282ddda814160\n\n```\n\nAnd I have the following messages:\n\n```\nDownloading/unpacking http://pypi.python.org/packages/source/p/psycopg2/psycopg2\n-2.4.tar.gz#md5=24f4368e2cfdc1a2b03282ddda814160\n  Downloading psycopg2-2.4.tar.gz (607Kb): 607Kb downloaded\n  Running setup.py egg_info for package from http://pypi.python.org/packages/sou\nrce/p/psycopg2/psycopg2-2.4.tar.gz#md5=24f4368e2cfdc1a2b03282ddda814160\n    Error: pg_config executable not found.\n\n    Please add the directory containing pg_config to the PATH\n    or specify the full executable path with the option:\n\n        python setup.py build_ext --pg-config /path/to/pg_config build ...\n\n    or with the pg_config option in 'setup.cfg'.\n    Complete output from command python setup.py egg_info:\n    running egg_info\n\ncreating pip-egg-info\\psycopg2.egg-info\n\nwriting pip-egg-info\\psycopg2.egg-info\\PKG-INFO\n\nwriting top-level names to pip-egg-info\\psycopg2.egg-info\\top_level.txt\n\nwriting dependency_links to pip-egg-info\\psycopg2.egg-info\\dependency_links.txt\n\nwriting manifest file 'pip-egg-info\\psycopg2.egg-info\\SOURCES.txt'\n\nwarning: manifest_maker: standard file '-c' not found\n\nError: pg_config executable not found.\n\n\n\nPlease add the directory containing pg_config to the PATH\n\nor specify the full executable path with the option:\n\n\n\n    python setup.py build_ext --pg-config /path/to/pg_config build ...\n\n\n\nor with the pg_config option in 'setup.cfg'.\n\n----------------------------------------\nCommand python setup.py egg_info failed with error code 1\nStoring complete log in C:\\Documents and Settings\\anlopes\\Application Data\\pip\\p\nip.log\n\n```\n\nMy question, I only need to do this to get the psycopg2 working?\n\n```\npython setup.py build_ext --pg-config /path/to/pg_config build ...\n\n```",
    "answer": "pip install psycopg2-binary\nsudo apt install build-essential\nsudo apt install postgresql-server-dev-all\n\nNote: Since a while back, there are binary wheels for Windows in PyPI, so this should no longer be an issue for Windows users. Below are solutions for Linux, Mac users, since lots of them find this post through web searches.\n\nOption 1\nInstall the psycopg2-binary PyPI package instead, it has Python wheels for Linux and Mac OS.\n\nOption 2\nInstall the prerequsisites for building the psycopg2 package from source:\nDebian/Ubuntu\n\nPython version\nCommand\nNote\n\nDefault Python 3\nsudo apt install libpq-dev python3-dev\n\nPython 3.x\nsudo apt install libpq-dev python3.x-dev\nsubstitute x in command\n\nPython 2\nsudo apt install libpq-dev python-dev\n\nIf that's not enough, you might additionally need to install\n\nor\n\nas well before installing psycopg2 again.\nCentOS 6\nSee Banjer's answer\nmacOS\nSee nichochar's answer"
  },
  {
    "question": "No module named pkg_resources I'm deploying a Django app to a dev server and am hitting this error when I run pip install -r requirements.txt:\n\n```\nTraceback (most recent call last):\n  File \"/var/www/mydir/virtualenvs/dev/bin/pip\", line 5, in <module>\n    from pkg_resources import load_entry_point\nImportError: No module named pkg_resources\n\n```\n\npkg_resources appears to be distributed with setuptools.  Initially I thought this might not be installed to the Python in the virtualenv, so I installed setuptools 2.6 (same version as Python) to the Python site-packages in the virtualenv with the following command:\n\n```\nsh setuptools-0.6c11-py2.6.egg --install-dir /var/www/mydir/virtualenvs/dev/lib/python2.6/site-packages\n\n```\n\nEDIT: This only happens inside the virtualenv.  If I open a console outside the virtualenv then pkg_resources is present, but I am still getting the same error.\nAny ideas as to why pkg_resources is not on the path?",
    "answer": "wget https://bootstrap.pypa.io/ez_setup.py -O - | python\ncurl https://bootstrap.pypa.io/ez_setup.py | python\n\nJuly 2018 Update\nMost people should now use pip install setuptools (possibly with sudo).\nSome may need to (re)install the python-setuptools package via their package manager (apt-get install, yum install, etc.).\nThis issue can be highly dependent on your OS and dev environment. See the legacy/other answers below if the above isn't working for you.\nExplanation\nThis error message is caused by a missing/broken Python setuptools package. Per Matt M.'s comment and setuptools issue #581, the bootstrap script referred to below is no longer the recommended installation method.\nThe bootstrap script instructions will remain below, in case it's still helpful to anyone.\nLegacy Answer\nI encountered the same ImportError today while trying to use pip. Somehow the setuptools package had been deleted in my Python environment.\nTo fix the issue, run the setup script for setuptools:\n\n(or if you don't have wget installed (e.g. OS X), try\n\npossibly with sudo prepended.)\nIf you have any version of distribute, or any setuptools below 0.6, you will have to uninstall it first.*\nSee Installation Instructions for further details.\n\n* If you already have a working distribute, upgrading it to the \"compatibility wrapper\" that switches you over to setuptools is easier. But if things are already broken, don't try that."
  },
  {
    "question": "pip uses incorrect cached package version, instead of the user-specified version I need to install psycopg2 v2.4.1 specifically. I accidentally did:\n\n```\npip install psycopg2\n\n```\n\nInstead of:\n\n```\npip install psycopg2==2.4.1\n\n```\n\nThat installs 2.4.4 instead of the earlier version.\nNow even after I pip uninstall psycopg2 and attempt to reinstall with the correct version, it appears that pip is re-using the cache it downloaded the first time.\nHow can I force pip to clear out its download cache and use the specific version I'm including in the command?",
    "answer": "If using pip 6.0 or newer, try adding the --no-cache-dir option (source).\nIf using pip older than pip 6.0, upgrade it with pip install -U pip."
  },
  {
    "question": "How to install pip with Python 3? I want to install pip. It should support Python 3, but it requires setuptools, which is available only for Python 2.\nHow can I install pip with Python 3?",
    "answer": "sudo apt-get install python-pip\nsudo apt-get install python3-pip\nsudo apt-get update\nsudo yum install python-setuptools\nsudo easy_install pip\nsudo yum install python34-setuptools\nsudo easy_install pip\npython get-pip.py\n\nedit: Manual installation and use of setuptools is not the standard process anymore.\nIf you're running Python 2.7.9+ or Python 3.4+\nCongrats, you should already have pip installed. If you do not, read onward.\nIf you're running a Unix-like System\nYou can usually install the package for pip through your package manager if your version of Python is older than 2.7.9 or 3.4, or if your system did not include it for whatever reason.\nInstructions for some of the more common distros follow.\nInstalling on Debian (Wheezy and newer) and Ubuntu (Trusty Tahr and newer) for Python 2.x\nRun the following command from a terminal:\n\nInstalling on Debian (Wheezy and newer) and Ubuntu (Trusty Tahr and newer) for Python 3.x\nRun the following command from a terminal:\n\nNote:\nOn a fresh Debian/Ubuntu install, the package may not be found until you do:\n\nInstalling pip on CentOS 7 for Python 2.x\nOn CentOS 7, you have to install setup tools first, and then use that to install pip, as there is no direct package for it.\n\nInstalling pip on CentOS 7 for Python 3.x\nAssuming you installed Python 3.4 from EPEL, you can install Python 3's setup tools and use it to install pip.\n\nIf your Unix/Linux distro doesn't have it in package repos\nInstall using the manual way detailed below.\nThe manual way\nIf you want to do it the manual way, the now-recommended method is to install using the get-pip.py script from pip's installation instructions.\n\nInstall pip\nTo install pip, securely download get-pip.py\nThen run the following (which may require administrator access):\n\nIf setuptools is not already installed, get-pip.py will install setuptools for you."
  },
  {
    "question": "How can I upgrade specific packages using pip and a requirements file? I'm using pip with a requirements file, in a virtualenv, for my Django projects.  I'm trying to upgrade some packages, notably Django itself, and I'm getting an error about source code conflicts:\n\nSource in <virtualenv>/build/Django has version 1.2.3 that conflicts with Django==1.2.4 (from -r requirements/apps.txt (line 3))\n\nThat's after updating the version number of Django from 1.2.3 to 1.2.4 in my requirements file.  I'm using this command to actually do the upgrade:\n\n```\npip --install --upgrade -E `<virtualenv dir`> --requirement `<requirements file`>\n\n```\n\nI can't find any flag that triggers a total package re-download.  I even tried running an uninstall command first, and then the install, but no dice. Am I missing something?",
    "answer": "<virtualenv>/bin/pip uninstall Django\n<virtualenv>/bin/pip install Django\n\nFirst make sure you have checked the most voted answer.\n\nI'm not sure if it's exactly your problem, but in my case, I wasn't able to upgrade Django to 1.2.4 - I was always finishing with 1.2.3 version, so I uninstalled Django with:\n\nThen I removed <virtualenv>/build/Django directory and finally I installed the proper version with:"
  },
  {
    "question": "Install a Python package into a different directory using pip? I know the obvious answer is to use virtualenv and virtualenvwrapper, but for various reasons I can't/don't want to do that.\nSo how do I modify the command\n\n```\npip install package_name\n\n```\n\nto make pip install the package somewhere other than the default site-packages?",
    "answer": "pip install --install-option=\"--prefix=$PREFIX_PATH\" package_name\n\nUse:\n\nYou might also want to use --ignore-installed to force all dependencies to be reinstalled using this new prefix.  You can use --install-option to multiple times to add any of the options you can use with python setup.py install (--prefix is probably what you want, but there are a bunch more options you could use)."
  },
  {
    "question": "&#39;pip&#39; is not recognized as an internal or external command I'm running into a weird error when trying to install Django on my computer.\nThis is the sequence that I typed into my command line:\n\n```\nC:\\Python34> python get-pip.py\nRequirement already up-to-date: pip in c:\\python34\\lib\\site-packages\nCleaning up...\n\nC:\\Python34> pip install Django\n'pip' is not recognized as an internal or external command,\noperable program or batch file.\n\nC:\\Python34> lib\\site-packages\\pip install Django\n'lib\\site-packages\\pip' is not recognized as an internal or external command,\noperable program or batch file.\n\n```\n\nWhat could be causing this?\nThis is what I get when I type in echo %PATH%:\n\n```\nC:\\Python34>echo %PATH%\nC:\\Program Files\\ImageMagick-6.8.8-Q16;C:\\Program Files (x86)\\Intel\\iCLS Client\\\n;C:\\Program Files\\Intel\\iCLS Client\\;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\S\nystem32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Program Files (x86)\\\nWindows Live\\Shared;C:\\Program Files (x86)\\Intel\\OpenCL SDK\\2.0\\bin\\x86;C:\\Progr\nam Files (x86)\\Intel\\OpenCL SDK\\2.0\\bin\\x64;C:\\Program Files\\Intel\\Intel(R) Mana\ngement Engine Components\\DAL;C:\\Program Files\\Intel\\Intel(R) Management Engine C\nomponents\\IPT;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\n\\DAL;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\IPT;C:\\P\nrogram Files (x86)\\nodejs\\;C:\\Program Files (x86)\\Heroku\\bin;C:\\Program Files (x\n86)\\git\\cmd;C:\\RailsInstaller\\Ruby2.0.0\\bin;C:\\RailsInstaller\\Git\\cmd;C:\\RailsIn\nstaller\\Ruby1.9.3\\bin;C:\\Users\\Javi\\AppData\\Roaming\\npm\n\n```",
    "answer": "setx PATH \"%PATH%;C:\\Python34\\Scripts\"\n\nYou need to add the path of your pip installation to your PATH system variable. By default, pip is installed to C:\\Python34\\Scripts\\pip (pip now comes bundled with new versions of python), so the path \"C:\\Python34\\Scripts\" needs to be added to your PATH variable.\nTo check if it is already in your PATH variable, type echo %PATH% at the CMD prompt\nTo add the path of your pip installation to your PATH variable, you can use the Control Panel or the setx command. For example:\n\nNote:\nAccording to the official documentation, \"[v]ariables set with setx variables are available in future command windows only, not in the current command window\". In particular, you will need to start a new cmd.exe instance after entering the above command in order to utilize the new environment variable.\nThanks to Scott Bartell for pointing this out."
  },
  {
    "question": "Is it possible to use pip to install a package from a private GitHub repository? I am trying to install a Python package from a private GitHub repository. For a public repository, I can issue the following command which works fine:\n\n```\npip install git+git://github.com/django/django.git\n\n```\n\nHowever, if I try this for a private repository:\n\n```\npip install git+git://github.com/echweb/echweb-utils.git\n\n```\n\nI get the following output:\n\n```\nDownloading/unpacking git+git://github.com/echweb/echweb-utils.git\nCloning Git repository git://github.com/echweb/echweb-utils.git to /var/folders/cB/cB85g9P7HM4jcPn7nrvWRU+++TI/-Tmp-/pip-VRsIoo-build\nComplete output from command /usr/local/bin/git clone git://github.com/echweb/echweb-utils.git /var/folders/cB/cB85g9P7HM4jcPn7nrvWRU+++TI/-Tmp-/pip-VRsIoo-build:\nfatal: The remote end hung up unexpectedly\n\nCloning into /var/folders/cB/cB85g9P7HM4jcPn7nrvWRU+++TI/-Tmp-/pip-VRsIoo-build...\n\n----------------------------------------\nCommand /usr/local/bin/git clone git://github.com/echweb/echweb-utils.git /var/folders/cB/cB85g9P7HM4jcPn7nrvWRU+++TI/-Tmp-/pip-VRsIoo-build failed with error code 128\n\n```\n\nI guess this is because I am trying to access a private repository without providing any authentication. I therefore tried to use Git + ssh hoping that pip would use my SSH public key to authenticate:\n\n```\npip install git+ssh://github.com/echweb/echweb-utils.git\n\n```\n\nThis gives the following output:\n\n```\nDownloading/unpacking git+ssh://github.com/echweb/echweb-utils.git\nCloning Git repository ssh://github.com/echweb/echweb-utils.git to /var/folders/cB/cB85g9P7HM4jcPn7nrvWRU+++TI/-Tmp-/pip-DQB8s4-build\nComplete output from command /usr/local/bin/git clone ssh://github.com/echweb/echweb-utils.git /var/folders/cB/cB85g9P7HM4jcPn7nrvWRU+++TI/-Tmp-/pip-DQB8s4-build:\nCloning into /var/folders/cB/cB85g9P7HM4jcPn7nrvWRU+++TI/-Tmp-/pip-DQB8s4-build...\n\nPermission denied (publickey).\n\nfatal: The remote end hung up unexpectedly\n\n----------------------------------------\nCommand /usr/local/bin/git clone ssh://github.com/echweb/echweb-utils.git /var/folders/cB/cB85g9P7HM4jcPn7nrvWRU+++TI/-Tmp-/pip-DQB8s4-build failed with error code 128\n\n```\n\nIs what I am trying to achieve even possible? If so, how can I do it?",
    "answer": "pip install git+ssh://git@github.com/echweb/echweb-utils.git\npip install -e URI#egg=EggName\ngit remote -v\norigin  git@github.com:echweb/echweb-utils.git (fetch)\nssh: Could not resolve hostname github.com:echweb:\nnodename nor servname provided, or not known\n\nYou can use the git+ssh URI scheme, but you must set a username. Notice the git@ part in the URI:\n\nAlso read about deploy keys.\nPS: In my installation, the \"git+ssh\" URI scheme works only with \"editable\" requirements:\n\nRemember: Change the : character that git remote -v prints to a / character before using the remote's address in the pip command:\n\nIf you forget, you will get this error:"
  },
  {
    "question": "Installing Python packages from local file system folder to virtualenv with pip Is it possible to install packages using pip from the local filesystem?\nI have run python setup.py sdist for my package, which has created the appropriate tar.gz file. This file is stored on my system at /srv/pkg/mypackage/mypackage-0.1.0.tar.gz.\nNow in a virtual environment I would like to install packages either coming from pypi or from the specific local location /srv/pkg.\nIs this possible?\nPS\nI know that I can specify pip install /srv/pkg/mypackage/mypackage-0.1.0.tar.gz. That will work, but I am talking about using the /srv/pkg location as another place for pip to search if I typed pip install mypackage.",
    "answer": "pip install mypackage --no-index --find-links file:///srv/pkg/mypackage\n\nI am pretty sure that what you are looking for is called --find-links option.\nYou can do"
  },
  {
    "question": "How can I Install a Python module with Pip programmatically (from my code)? I need to install a package from PyPI straight within my script.\nIs there maybe some module or distutils (distribute, pip, etc.) feature which allows me to just execute something like pypi.install('requests') and requests will be installed into my virtualenv?",
    "answer": "import subprocess\nimport sys\n\ndef install(package):\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n\nThe officially recommended way to install packages from a script is by calling pip's command-line interface via a subprocess. Most other answers presented here are not supported by pip. Furthermore since pip v10, all code has been moved to pip._internal precisely in order to make it clear to users that programmatic use of pip is not allowed.\nUse sys.executable to ensure that you will call the same pip associated with the current runtime."
  },
  {
    "question": "setup script exited with error: command &#39;x86_64-linux-gnu-gcc&#39; failed with exit status 1 When I try to install odoo-server, I got the following error: \n\n```\nerror: Setup script exited with error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n\n```\n\nCould anyone help me to solve this issue?",
    "answer": "sudo apt-get install build-essential autoconf libtool pkg-config python-opengl python-pil python-pyrex python-pyside.qtopengl idle-python2.7 qt4-dev-tools qt4-designer libqtgui4 libqtcore4 libqt4-xml libqt4-test libqt4-script libqt4-network libqt4-dbus python-qt4 python-qt4-gl libgle3 python-dev libssl-dev\n\nsudo easy_install greenlet\n\nsudo easy_install gevent\n\nTry installing these packages."
  },
  {
    "question": "How do I install the yaml package for Python? I have a Python program that uses YAML.  I attempted to install it on a new server using pip install yaml and it returns the following:\n\n```\n$ sudo pip install yaml\nDownloading/unpacking yaml\n  Could not find any downloads that satisfy the requirement yaml\nNo distributions at all found for yaml\nStoring complete log in /home/pa/.pip/pip.log\n\n```\n\nHow do I install the yaml package for Python?  I'm running Python 2.7.  (OS: Debian Wheezy)",
    "answer": "pip install pyyaml\nsudo apt-get install python-yaml\nsudo yum install python-yaml\n\nYou could try the search the feature on https://pypi.org/search (via a browser) and look for packages in PyPI with yaml in the short description.  That reveals various packages, including PyYaml, yamltools, and PySyck, among others (Note that PySyck docs recommend using PyYaml, since syck is out of date).  Now you know a specific package name, you can install it:\n\nIf you want to install python yaml system-wide in linux, you can also use a package manager, like aptitude or yum:"
  },
  {
    "question": "Error after upgrading pip: cannot import name &#39;main&#39; Whenever I am trying to install any package using pip, I am getting this import error:\n\n```\nguru@guru-notebook:~$ pip3 install numpy\nTraceback (most recent call last):\n  File \"/usr/bin/pip3\", line 9, in <module>\n    from pip import main\nImportError: cannot import name 'main'\n\n```\n\n\n\n```\nguru@guru-notebook:~$ cat `which pip3`\n#!/usr/bin/python3\n# GENERATED BY DEBIAN\n\nimport sys\n\n# Run the main entry point, similarly to how setuptools does it, but because\n# we didn't install the actual entry point from setup.py, don't use the\n# pkg_resources API.\nfrom pip import main\nif __name__ == '__main__':\n    sys.exit(main())\n\n```\n\nIt was working fine earlier, I am not sure why it is throwing this error.\nI have searched about this error, but can't find anything to fix it.\nPlease let me know if you need any further detail, I will update my question.",
    "answer": "You must have inadvertently upgraded your system pip (probably through something like sudo pip install pip --upgrade)\npip 10.x adjusts where its internals are situated.  The pip3 command you're seeing is one provided by your package maintainer (presumably debian based here?) and is not a file managed by pip.\nYou can read more about this on pip's issue tracker\nYou'll probably want to not upgrade your system pip and instead use a virtualenv.\nTo recover the pip3 binary you'll need to sudo python3 -m pip uninstall pip && sudo apt install python3-pip --reinstall\nIf you want to continue in \"unsupported territory\" (upgrading a system package outside of the system package manager), you can probably get away with python3 -m pip ... instead of pip3."
  },
  {
    "question": "Using Pip to install packages to an Anaconda environment On\n\nConda 4.2.13\nMac OS X v10.12.1 (Sierra)\n\nI am trying to install packages from pip to a fresh environment (virtual) created using anaconda.  In the Anaconda documentation it says this is perfectly fine.  It is done the same way as for virtualenv.\n\nActivate the environment where you want to put the program, then pip install a program...\n\nI created an empty environment in Anaconda like this:\n\n```\nconda create -n shrink_venv\n\n```\n\nActivate it:\n\n```\nsource activate shrink_venv\n\n```\n\nI then can see in the terminal that I am working in my environment (shrink_venv). A problem is coming up when I try to install a package using pip:\n\n```\npip install Pillow\n\n```\n\nOutput:\n\n```\nRequirement already satisfied (use --upgrade to upgrade): Pillow in /Library/Python/2.7/site-packages\n\n```\n\nSo I can see it thinks the requirement is satisfied from the system-wide package.  So it seems the environment is not working correctly, definitely not like it said in the documentation.  Am I doing something wrong here?\nJust a note, I know you can use conda install for the packages, but I have had an issue with Pillow from anaconda, so I wanted to get it from pip, and since the docs say that is fine.\nOutput of which -a pip:\n\n```\n/usr/local/bin/pip\n/Users/my_user/anaconda/bin/pip\n\n```\n\nI see this is pretty common issue. I have found that the Conda environment doesn't play well with the PYTHONPATH. The system seems to always look in the PYTHONPATH locations even when you're using a Conda environment. Now, I always run unset PYTHONPATH when using a Conda environment, and it works much better. I'm on a Mac.",
    "answer": "For others who run into this situation, I found this to be the most straightforward solution:\n\nRun conda create -n venv_name and conda activate venv_name, where venv_name is the name of your virtual environment.\n\nRun conda install pip. This will install pip to your venv directory.\n\nFind your anaconda directory, and find the actual venv folder. It should be somewhere like /anaconda/envs/venv_name/; or, you could also run conda activate venv_name.\n\nInstall new packages by doing /anaconda/envs/venv_name/bin/pip install package_name; or, simply run pip install package_name.\n\nThis should now successfully install packages using that virtual environment's pip!"
  },
  {
    "question": "Error &quot;filename.whl is not a supported wheel on this platform&quot; I would like to install scipy-0.15.1-cp33-none-win_amd64.whl that I have saved to the local drive. I am using:\n\n```\npip 6.0.8 from C:\\Python27\\Lib\\site-packages\npython 2.7.9 (default, Dec 10 2014, 12:28:03) [MSC v.1500 64 bit (AMD64)]\n\n```\n\nWhen I run:\n\n```\npip install scipy-0.15.1-cp33-none-win_amd64.whl\n\n```\n\nI get the following error:\n\nscipy-0.15.1-cp33-none-win_amd64.whl is not a supported wheel on this platform\n\nWhat is the problem?",
    "answer": "cp33 means CPython 3.3.\nYou need scipy\u20110.15.1\u2011cp27\u2011none\u2011win_amd64.whl instead."
  },
  {
    "question": "Can I add comments to a pip requirements file? I'd like to add comments for a few packages in a pip requirements file. (Just to explain why that package is on the list.) Can I do this?\nI'm imagining something like\n\n```\nBabel==0.9.5 # translation\nCherryPy==3.2.0 # web server\nCreoleparser==0.7.1 # wiki formatting\nGenshi==0.5.1 # templating\n\n```",
    "answer": "Sure, you can, just use #\npip docs:\n\nA line that begins with # is treated as a comment and ignored. Whitespace followed by a # causes the # and the remainder of the line to be treated as a comment."
  },
  {
    "question": "How to pip install a package with min and max version range? I'm wondering if there's any way to tell pip, specifically in a requirements file, to install a package with both a minimum version (pip install package>=0.2) and a maximum version which should never be installed (theoretical api: pip install package<0.3).\nI ask because I am using a third party library that's in active development. I'd like my pip requirements file to specify that it should always install the most recent minor release of the 0.5.x branch, but I don't want pip to ever try to install any newer major versions (like 0.6.x) since the API is different.  This is important because even though the 0.6.x branch is available, the devs are still releasing patches and bugfixes to the 0.5.x branch, so I don't want to use a static package==0.5.9 line in my requirements file.\nIs there any way to do that?",
    "answer": "pip install \"package>=0.2,<0.3\"\n\nYou can do:\n\nAnd pip will look for the best match, assuming the version is at least 0.2, and less than 0.3.\nThis also applies to pip requirements files.  See the full details on version specifiers in PEP 440."
  },
  {
    "question": "Reference requirements.txt for the install_requires kwarg in setuptools setup.py file I have a requirements.txt file that I'm using with Travis-CI.  It seems silly to duplicate the requirements in both requirements.txt and setup.py, so I was hoping to pass a file handle to the install_requires kwarg in setuptools.setup.\nIs this possible? If so, how should I go about doing it?\nHere is my requirements.txt file:\n\n```\nguessit>=0.5.2\ntvdb_api>=1.8.2\nhachoir-metadata>=1.3.3\nhachoir-core>=1.3.3\nhachoir-parser>=1.3.4\n\n```",
    "answer": "install_reqs = parse_requirements('requirements.txt', session='hack')\nfrom pip.req import parse_requirements\n\ninstall_reqs = parse_requirements(<requirements_path>)\n\nreqs = [str(ir.req) for ir in install_reqs]\n\nsetup(\n...\ninstall_requires=reqs\n)\n\nUpdate 12/2024:\nThis does not work in recent versions of Python, primarily because the function has been moved around in the pip module. Either way, as stated below by maintainers of the library, it is not recommended to import and use this function, as it is an internal function which is subject to change and being moved around.\n\nOld answer:\nYou can flip it around and list the dependencies in setup.py and have a single character \u2014 a dot . \u2014 in requirements.txt instead.\n\nAlternatively, even if not advised, it is still possible to parse the requirements.txt file (if it doesn't refer any external requirements by URL) with the following hack (tested with pip 9.0.1):\n\nThis doesn't filter environment markers though.\n\nIn old versions of pip, more specifically older than 6.0, there is a public API that can be used to achieve this. A requirement file can contain comments (#) and can include some other files (--requirement or -r). Thus, if you really want to parse a requirements.txt you can use the pip parser:"
  },
  {
    "question": "How to install python3 version of package via pip on Ubuntu? I have both python2.7 and python3.2 installed in Ubuntu 12.04.\nThe symbolic link python links to python2.7.\nWhen I type:\n\n```\nsudo pip install package-name\n\n```\n\nIt will default install python2 version of package-name.\nSome package supports both python2 and python3.\nHow to install python3 version of package-name via pip?",
    "answer": "virtualenv -p /usr/bin/python3 py3env\nsource py3env/bin/activate\npip install package-name\n\nYou may want to build a virtualenv of python3, then install packages of python3 after activating the virtualenv. So your system won't be messed up :)\nThis could be something like:"
  },
  {
    "question": "What is the purpose of &quot;pip install --user ...&quot;? From pip install --help:\n\n```\n--user  Install to the Python user install directory for your platform.\n        Typically ~/.local/, or %APPDATA%\\Python on Windows.\n        (See the Python documentation for site.USER_BASE for full details.)\n\n```\n\nThe documentation for site.USER_BASE is a terrifying wormhole of interesting Unix-like subject matter that I don't understand.\nWhat is the purpose of --user in plain English? Why would installing the package to ~/.local/ matter? Why not just put an executable somewhere in my $PATH?",
    "answer": "pip defaults to installing Python packages to a system directory (such as /usr/local/lib/python3.4). This requires root access.\n--user makes pip install packages in your home directory instead, which doesn't require any special privileges."
  },
  {
    "question": "What is pyproject.toml file for? Background\nI was about to try Python package downloaded from GitHub, and realized that it did not have a setup.py, so I could not install it with\n\n```\npip install -e <folder>\n\n```\n\nInstead, the package had a pyproject.toml file which seems to have very similar entries as the setup.py usually has.\nWhat I found\nGoogling lead me into PEP-518 and it gives some critique to setup.py in Rationale section. However, it does not clearly tell that usage of setup.py should be avoided, or that pyproject.toml would as such completely replace setup.py.\nQuestions\nIs the pyproject.toml something that is used to replace setup.py? Or should a package come with both, a pyproject.toml and a setup.py?\nHow would one install a project with pyproject.toml in an editable state?",
    "answer": "What is it for?\nCurrently there are multiple packaging tools being popular in Python community and while setuptools still seems to be prevalent it's not a de\u00a0facto standard anymore. This situation creates a number of hassles for both end\u00a0users and developers:\n\nFor setuptools-based packages installation from source / build of a distribution can fail if one doesn't have setuptools installed;\npip doesn't support the installation of packages based on other packaging tools from source, so these tools had to generate a setup.py file to produce a compatible package. To build a distribution package one has to install the packaging tool first and then use tool-specific commands;\nIf package author decides to change the packaging tool, workflows must be changed as\u00a0well to use different tool-specific commands.\n\npyproject.toml is a new configuration file introduced by PEP\u00a0517 and PEP\u00a0518 to solve these problems:\n\n... think of the (rough) steps required to produce a built artifact for a project:\n\nThe source checkout of the project.\nInstallation of the build system.\nExecute the build system.\n\nThis PEP [518] covers step #2. PEP\u00a0517 covers step #3 ...\n\nAny tool can also extend this file with its own section (table) to accept tool-specific options, but it's up to them and not required.\nPEP\u00a0621 suggests using pyproject.toml to specify package core\u00a0metadata in static, tool-agnostic way. Which backends currently support this is shown in the following table:\n\nenscons\nflit_core\nhatchling\npdm-backend\npoetry-core\nsetuptools\n\n0.26.0+\n3.2+\n0.3+\n0.3.0+\n2.0.0+\n61.0.0+\n\nDoes it replace setup.py?\nFor setuptools-based packages pyproject.toml is not strictly meant to replace setup.py, but rather to ensure its correct execution if it's still needed. For other packaging tools \u2013 yes, it is:\n\nWhere the build-backend key exists, this takes precedence and the source tree follows the format and conventions of the specified backend (as such no setup.py is needed unless the backend requires it). Projects may still wish to include a setup.py for compatibility with tools that do not use this spec.\n\nHow to install a package in editable mode?\nOriginally \"editable install\" was a setuptools-specific feature and as such it was not supported by PEP\u00a0517. Later\u00a0on PEP\u00a0660 extended this concept to packages using pyproject.toml.\nThere are two possible conditions for installing a package in editable mode using pip:\n\nModern:\nBoth the frontend (pip) and a backend must support PEP\u00a0660.\npip supports it since version\u00a021.3;\nLegacy:\nPackaging tool must provide a setup.py file which supports the develop command.\nSince version\u00a021.1 pip can also install packages using only setup.cfg file in editable mode.\n\nThe following table describes the support of editable installs by various backends:\n\nenscons\nflit_core\nhatchling\npdm-backend\npoetry-core\nsetuptools\n\n0.28.0+\n3.4+\n0.3+\n0.8.0+\n1.0.8+\n64.0.0+"
  },
  {
    "question": "Could not find a version that satisfies the requirement tensorflow I installed the latest version of Python (3.6.4 64-bit) and the latest version of PyCharm (2017.3.3 64-bit). Then I installed some modules in PyCharm (Numpy, Pandas, etc), but when I tried installing Tensorflow it didn't install, and I got the error message: \n\nCould not find a version that satisfies the requirement TensorFlow (from versions: )\n      No matching distribution found for TensorFlow.\n\nThen I tried installing TensorFlow from the command prompt and I got the same error message.\nI did however successfully install tflearn. \nI also installed Python 2.7, but I got the same error message again. I googled the error and tried some of the things which were suggested to other people, but nothing worked (this included installing Flask). \nHow can I install Tensorflow? Thanks.",
    "answer": "The latest requirements for running TensorFlow are documented in the installation documentation.\n\nTensorFlow only supports 64-bit Python\n\nTensorFlow only supports certain versions of Python (for example, Python 3.6 is not supported)\n\nSo, if you're using an out-of-range version of Python (older or newer) or a 32-bit version, then you'll need to use a different version."
  },
  {
    "question": "Upgrade Python in a virtual environment Is there a way to upgrade the version of Python used in a virtual environment (e.g. if a bugfix release comes out)?\nI could pip freeze --local > requirements.txt, then remove the directory and pip install -r requirements.txt, but this requires a lot of reinstallation of large libraries, for instance, numpy, which I use a lot.\nI can see this is an advantage when upgrading from, e.g., 2.6 -> 2.7, but what about 2.7.x -> 2.7.y?",
    "answer": "Did you see this? If I haven't misunderstand that answer, you may try to create a new virtualenv on top of the old one. You just need to know which python is going to use your virtualenv (you will need to see your virtualenv version).\nIf your virtualenv is installed with the same python version of the old one and upgrading your virtualenv package is not an option, you may want to read this in order to install a virtualenv with the python version you want.\nEDIT\nI've tested this approach (the one that create a new virtualenv on top of the old one) and it worked fine for me. I think you may have some problems if you change from python 2.6 to 2.7 or 2.7 to 3.x but if you just upgrade inside the same version (staying at 2.7 as you want) you shouldn't have any problem, as all the packages are held in the same folders for both python versions (2.7.x and 2.7.y packages are inside your_env/lib/python2.7/).\nIf you change your virtualenv python version, you will need to install all your packages again for that version (or just link the packages you need into the new version packages folder, i.e: your_env/lib/python_newversion/site-packages)"
  },
  {
    "question": "What is pip&#39;s `--no-cache-dir` good for? I've recently seen the --no-cache-dir being used in a Docker file. I've never seen that flag before and the help is not explaining it:\n\n```\n --no-cache-dir              Disable the cache.\n\n```\n\n\nQuestion: What is cached?\nQuestion: What is the cache used for?\nQuestion: Why would I want to disable it?",
    "answer": "Cached is: store away in hiding or for future use\nUsed for\n\nstore the installation files(.whl, etc) of the modules that you install through pip\nstore the source files (.tar.gz, etc) to avoid re-download when not expired\n\nPossible Reason you might want to disable cache:\n\nyou don't have space on your hard drive\npreviously run pip install with unexpected settings\n\neg:\n\npreviously run export PYCURL_SSL_LIBRARY=nss and pip install pycurl\nwant new run export PYCURL_SSL_LIBRARY=openssl and pip install pycurl --compile --no-cache-dir\n\nyou want to keep a Docker image as small as possible\n\nLinks to documentation\nhttps://pip.pypa.io/en/stable/topics/caching"
  },
  {
    "question": "How to install multiple python packages at once using pip  I know it's an  easy way of doing it but i didn't find it neither here nor on google. \nSo i was curious if there is a way to install multiple packages using pip. \nSomething like: \n\n```\npip install progra1 , progra2 ,progra3 ,progra4 . \n\n```\n\nor:\n\n```\npip install (command to read some txt containing the name of the modules) \n\n```",
    "answer": "pip install wsgiref boto\npip freeze\nboto==2.3.0\nwsgiref==0.1.2\n\nFor installing multiple packages on the command line, just pass them as a space-delimited list, e.g.:\n\nFor installing from a text file, then, from pip install --help:\n\n-r FILENAME, --requirement=FILENAME\nInstall all the packages listed in the given requirements file.  This option can be used multiple times.\n\nTake a look at the pip documentation regarding requirements files for their general layout and syntax - note that you can generate one based on current environment / site-packages with pip freeze if you want a quick example - e.g. (based on having installed wsgiref and boto in a clean virtualenv):"
  },
  {
    "question": "How to install Python package from GitHub? I want to use a new feature of httpie. This feature is in the github repo https://github.com/jkbr/httpie but not in the release on the python package index https://pypi.python.org/pypi/httpie \nHow can I install the httpie package from the github repo? I tried\n\n```\npip install https://github.com/jkbr/httpie\n\n```\n\nBut I got  an error 'could not unpack'\n\nIn Nodejs, I can install packages from github like this\n\n```\nnpm install git+https://github.com/substack/node-optimist.git\n\n```",
    "answer": "pip install git+https://github.com/jkbr/httpie.git#egg=httpie\n\nYou need to use the proper git URL:\n\nAlso see the VCS Support section of the pip documentation.\nDon\u2019t forget to include the egg=<projectname> part to explicitly name the project; this way pip can track metadata for it without having to have run the setup.py script."
  },
  {
    "question": "Could not find a version that satisfies the requirement &lt;package&gt; I'm installing several Python packages in Ubuntu 12.04 using the following requirements.txt file:\n\n```\nnumpy>=1.8.2,<2.0.0\nmatplotlib>=1.3.1,<2.0.0\nscipy>=0.14.0,<1.0.0\nastroML>=0.2,<1.0\nscikit-learn>=0.14.1,<1.0.0\nrpy2>=2.4.3,<3.0.0\n\n```\n\nand these two commands:\n\n```\n$ pip install --download=/tmp -r requirements.txt\n$ pip install --user --no-index --find-links=/tmp -r requirements.txt\n\n```\n\n(the first one downloads the packages and the second one installs them).\nThe process is frequently stopped with the error:\n\n```\n  Could not find a version that satisfies the requirement <package> (from matplotlib<2.0.0,>=1.3.1->-r requirements.txt (line 2)) (from versions: )\nNo matching distribution found for <package> (from matplotlib<2.0.0,>=1.3.1->-r requirements.txt (line 2))\n\n```\n\nwhich I fix manually with:\n\n```\npip install --user <package>\n\n```\n\nand then run the second pip install command again.\nBut that only works for that particular package. When I run the second pip install command again, the process is stopped now complaining about another required package and I need to repeat the process again, ie: install the new required package manually (with the command above) and then run the second pip install command.\nSo far I've had to manually install six, pytz, nose, and now it's complaining about needing mock.\nIs there a way to tell pip to automatically install all needed dependencies so I don't have to do it manually one by one?\nAdd: This only happens in Ubuntu 12.04 BTW. In Ubuntu 14.04 the pip install commands applied on the requirements.txt file work without issues.",
    "answer": "This approach (having all dependencies in a directory and not downloading from an index) only works when the directory contains all packages. The directory should therefore contain all dependencies but also all packages that those dependencies depend on (e.g., six, pytz etc).\nYou should therefore manually include these in requirements.txt (so that the first step downloads them explicitly) or you should install all packages using PyPI and then pip freeze > requirements.txt to store the list of all packages needed."
  },
  {
    "question": "How to install Python MySQLdb module using pip? How can I install the MySQLdb module for Python using pip?",
    "answer": "pip install mysqlclient\nsudo apt-get install python-pip python-dev libmysqlclient-dev\nsudo dnf install python python-devel mysql-devel redhat-rpm-config gcc\nbrew install mysql-connector-c\nbrew install mysql\n\nIt's easy to do, but hard to remember the correct spelling:\n\nIf you need 1.2.x versions (legacy Python only), use pip install MySQL-python\nNote: Some dependencies might have to be in place when running the above command. Some hints on how to install these on various platforms:\nUbuntu 14, Ubuntu 16, Debian 8.6 (jessie)\n\nFedora 24:\n\nMac OS\n\nif that fails, try"
  },
  {
    "question": "Does uninstalling a package with &quot;pip&quot; also remove the dependent packages? When you use pip to install a package, all the required packages will also be installed with it (dependencies). Does uninstalling that package also remove the dependent packages?",
    "answer": "pip install specloud\npip freeze # all the packages here are dependencies of specloud package\npip uninstall specloud\npip freeze\n\nNo, it doesn't uninstall the dependencies packages. It only removes the specified package:\n\nfigleaf==0.6.1\nnose==1.1.2\npinocchio==0.3\nspecloud==0.4.5\n\nfigleaf==0.6.1\nnose==1.1.2\npinocchio==0.3\n\nAs you can see those packages are dependencies from specloud and they're still there, but not the specloud package itself.\nAs mentioned below, you can install and use the pip-autoremove utility to remove a package plus unused dependencies."
  },
  {
    "question": "How to install packages offline? What's the best way to download a python package and its dependencies from pypi for offline installation on another machine? Is there any easy way to do this with pip or easy_install? I'm trying to install the requests library on a FreeBSD box that is not connected to the internet.",
    "answer": "mkdir /pypi && cd /pypi\nls -la\n-rw-r--r--   1 pavel  staff   237954 Apr 19 11:31 Flask-WTF-0.6.tar.gz\n-rw-r--r--   1 pavel  staff   389741 Feb 22 17:10 Jinja2-2.6.tar.gz\n-rw-r--r--   1 pavel  staff    70305 Apr 11 00:28 MySQL-python-1.2.3.tar.gz\n-rw-r--r--   1 pavel  staff  2597214 Apr 10 18:26 SQLAlchemy-0.7.6.tar.gz\n-rw-r--r--   1 pavel  staff  1108056 Feb 22 17:10 Werkzeug-0.8.2.tar.gz\n-rw-r--r--   1 pavel  staff   488207 Apr 10 18:26 boto-2.3.0.tar.gz\n-rw-r--r--   1 pavel  staff   490192 Apr 16 12:00 flask-0.9-dev-2a6c80a.tar.gz\ninstall_requires=[\n'boto',\n'Flask',\n'Werkzeug',\ncd ~/src/myapp\npython setup.py develop --always-unzip --allow-hosts=None --find-links=/pypi\ncd ~/src/myapp\neasy_install --always-unzip --allow-hosts=None --find-links=/pypi .\n\nIf the package is on PYPI, download it and its dependencies to some local directory.\nE.g.\n\nSome packages may have to be archived into similar looking tarballs by hand. I do it a lot when I want a more recent (less stable) version of something. Some packages aren't on PYPI, so same applies to them.\nSuppose you have a properly formed Python application in ~/src/myapp. ~/src/myapp/setup.py will have install_requires list that mentions one or more things that you have in your /pypi directory. Like so:\n\nIf you want to be able to run your app with all the necessary dependencies while still hacking on it, you'll do something like this:\n\nThis way your app will be executed straight from your source directory. You can hack on things, and then rerun the app without rebuilding anything.\nIf you want to install your app and its dependencies into the current python environment, you'll do something like this:\n\nIn both cases, the build will fail if one or more dependencies aren't present in /pypi directory. It won't attempt to promiscuously install missing things from Internet.\nI highly recommend to invoke setup.py develop ... and easy_install ... within an active virtual environment to avoid contaminating your global Python environment. It is (virtualenv that is) pretty much the way to go. Never install anything into global Python environment.\nIf the machine that you've built your app has same architecture as the machine on which you want to deploy it, you can simply tarball the entire virtual environment directory into which you easy_install-ed everything. Just before tarballing though, you must make the virtual environment directory relocatable (see --relocatable option). NOTE: the destination machine needs to have the same version of Python installed, and also any C-based dependencies your app may have must be preinstalled there too (e.g. say if you depend on PIL, then libpng, libjpeg, etc must be preinstalled)."
  },
  {
    "question": "After installing with pip, &quot;jupyter: command not found&quot; After installing with pip install jupyter, terminal still cannot find jupyter notebook.  \nUbuntu simply says command not found.  Similar with ipython.  Did pip not get install properly or something?  How does Ubuntu know where to look for executables installed with pip?",
    "answer": "~/.local/bin/jupyter-notebook\n\nyou did not log out and log in ? It should be on your path to execute.\nIf not, pip installed executables in .local, so in a terminal:\n\nshould start notebook"
  },
  {
    "question": "How do I prevent Conda from activating the base environment by default? I recently installed anaconda2 on my Mac. By default Conda is configured to activate the base environment when I open a fresh terminal session.\nI want access to the Conda commands (i.e. I want the path to Conda added to my $PATH which Conda does when initialised so that's fine).\nHowever I don't ordinarily program in python, and I don't want Conda to activate the base environment by default.\nWhen first executing conda init from the prompt, Conda adds the following to my .bash_profile:\n\n```\n# >>> conda initialize >>>\n# !! Contents within this block are managed by 'conda init' !!\n__conda_setup=\"$('/Users/geoff/anaconda2/bin/conda' 'shell.bash' 'hook' 2> /dev/null)\"\nif [ $? -eq 0 ]; then\n    eval \"$__conda_setup\"\nelse\nif [ -f \"/Users/geoff/anaconda2/etc/profile.d/conda.sh\" ]; then\n    . \"/Users/geoff/anaconda2/etc/profile.d/conda.sh\"\nelse\n    export PATH=\"/Users/geoff/anaconda2/bin:$PATH\"\nfi\n# fi\nunset __conda_setup\n# <<< conda initialize <<<\n\n```\n\nIf I comment out the whole block, then I can't activate any Conda environments.\nI tried to comment out the whole block except for\n\n```\nexport PATH=\"/Users/geoff/anaconda2/bin:$PATH\"\n\n```\n\nBut then when I started a new session and tried to activate an environment, I got this error message:\n\n```\nCommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.\n\n```\n\nThis question (and others like it) are helpful, but doesn't ultimately answer my question and is more suited for linux users.\nTo be clear, I'm not asking to remove the (base) from my $PS1 I'm asking for Conda not to activate base when I open a terminal session.",
    "answer": "conda config --set auto_activate_base false\n\nI have conda 4.6 with a similar block of code that was added by conda. In my case, there's a conda configuration setting to disable the automatic base activation:\n\nThe first time you run it, it'll create a .condarc in your home directory with that setting to override the default.\nThis wouldn't de-clutter your .bash_profile but it's a cleaner solution without manual editing that section that conda manages."
  },
  {
    "question": "How can I rename a conda environment? I have a conda environment named old_name, how can I change its name to new_name without breaking references?",
    "answer": "conda rename -n old_name  new_name\nconda rename -n old_name -d new_name\nconda create --name new_name --clone old_name\nconda remove --name old_name --all # or its alias: `conda env remove --name old_name`\n\nNew answer:\nFrom Conda 4.14 you will be able to use just:\n\nAlthough, under the hood, conda rename still uses [1][2]  undermentioned combination of conda create and conda remove.\nUse the -d flag for dry-run (not destination, as of v22.11.0)\n\nOld answer:\nYou can't.\nOne workaround is to create clone a new environment and then remove the original one.\nFirst, remember to deactivate your current environment. You can do this with the commands:\n\ndeactivate on Windows or\nsource deactivate on macOS/Linux.\n\nThen:\n\nNotice there are several drawbacks of this method:\n\nIt redownloads packages (you can use --offline flag to disable it)\nTime consumed on copying environment's files\nTemporary double disk usage\n\nThere is an open issue requesting this feature."
  },
  {
    "question": "How to update an existing Conda environment with a .yml file How can a pre-existing conda environment be updated with another .yml file. This is extremely helpful when working on projects that have multiple requirement files, i.e. base.yml, local.yml, production.yml, etc.\nFor example, below is a base.yml file has conda-forge, conda, and pip packages:\nbase.yml\n\n```\nname: myenv\nchannels:\n  - conda-forge\ndependencies:\n  - django=1.10.5\n  - pip:\n    - django-crispy-forms==1.6.1\n\n```\n\nThe actual environment is created with:\nconda env create -f base.yml.\nLater on, additional packages need to be added to base.yml. Another file, say local.yml, needs to import those updates.\nPrevious attempts to accomplish this include:\ncreating a local.yml file with an import definition:\n\n```\nchannels:\n\ndependencies:\n  - pip:\n    - boto3==1.4.4\nimports:\n  - requirements/base. \n\n```\n\nAnd then run the command:\nconda install -f local.yml. \nThis does not work. Any thoughts?",
    "answer": "conda activate myenv\nconda env update --file local.yml --prune\nconda env update --name myenv --file local.yml --prune\n\nTry using conda env update:\n\n--prune uninstalls dependencies which were removed from local.yml, as pointed out in this answer by @Blink.\nAttention: if there is a name tag with a name other than that of your environment in local.yml, the command above will create a new environment with that name. To avoid this, use (thanks @NumesSanguis):\n\nSee Updating an environment in Conda User Guide."
  },
  {
    "question": "Does Conda replace the need for virtualenv? I recently discovered Conda after I was having trouble installing SciPy, specifically on a Heroku app that I am developing.\nWith Conda you create environments, very similar to what virtualenv does. My questions are:\n\nIf I use Conda will it replace the need for virtualenv? If not, how do I use the two together? Do I install virtualenv in Conda, or Conda in virtualenv?\nDo I still need to use pip? If so, will I still be able to install packages with pip in an isolated environment?",
    "answer": "conda list\nsphinx_rtd_theme          0.1.7                    py35_0    defaults\nwxpython-common           3.0.0.0                   <pip>\n\nConda replaces virtualenv. In my opinion it is better. It is not limited to Python but can be used for other languages too. In my experience it provides a much smoother experience, especially for scientific packages. The first time I got MayaVi properly installed on Mac was with conda.\nYou can still use pip. In fact, conda installs pip in each new environment. It knows about pip-installed packages.\n\nFor example:\n\nlists all installed packages in your current environment.\nConda-installed packages show up like this:\n\nand the ones installed via pip have the <pip> marker:"
  },
  {
    "question": "&#39;Conda&#39; is not recognized as internal or external command I installed Anaconda3 4.4.0 (32 bit) on my Windows 7 Professional machine and imported NumPy and Pandas on Jupyter notebook so I assume Python was installed correctly. But when I type conda list and conda --version in command prompt, it says conda is not recognized as internal or external command.\nI have set environment variable for Anaconda3; Variable Name: Path, Variable Value: C:\\Users\\dipanwita.neogy\\Anaconda3\nHow do I make it work?",
    "answer": "Although you were offered a good solution by others I think it is helpful to point out what is really happening. As per the Anaconda 4.4 changelog, https://docs.anaconda.com/anaconda/reference/release-notes/#what-s-new-in-anaconda-4-4:\n\nOn Windows, the PATH environment variable is no longer changed by default, as this can cause trouble with other software. The recommended approach is to instead use Anaconda Navigator or the Anaconda Command Prompt (located in the Start Menu under \u201cAnaconda\u201d) when you wish to use Anaconda software.\n\n(Note: recent Win 10 does not assume you have privileges to install or update.  If the command fails, right-click on the Anaconda Command Prompt, choose \"More\", chose \"Run as administrator\")\nThis is a change from previous installations. It is suggested to use Navigator or the Anaconda Prompt although you can always add it to your PATH as well. During the install the box to add Anaconda to the PATH is now unchecked but you can select it."
  },
  {
    "question": "How do I upgrade to Python 3.6 with Conda? I want to get the latest version of Python to use f-strings in my code. Currently my version is (python -V):\n\n```\nPython 3.5.2 :: Anaconda 4.2.0 (x86_64)\n\n```\n\nHow would I upgrade to Python 3.6?",
    "answer": "conda install python=$pythonversion$\nconda create --name py36 python=3.6\nconda create --name py365 python=3.6.5 --channel conda-forge\n\nAnaconda had not updated Python internally to 3.6, but later versions of Anaconda has a Python 3.6 version here.\na) Method 1\n\nIf you wanted to update, you will type conda update python\n\nTo update Anaconda, type conda update conda\n\nIf you want to upgrade between major python versions, like 3.5 to 3.6, you'll have to do\n\nb) Method 2 - Create a new environment (the better method)\n\nc) To get the absolute latest Python (3.6.5 at time of writing)\n\nYou can see all this from here.\nAlso, refer to this for force upgrading."
  },
  {
    "question": "Anaconda export Environment file How can I make anaconda environment file which could be use on other computers?\nI exported my anaconda python environment to YML using conda env export > environment.yml. The exported environment.yml contains this line prefix: /home/superdev/miniconda3/envs/juicyenv which maps to my anaconda's location which will be different on other's pcs.",
    "answer": "conda env export | grep -v \"^prefix: \" > environment.yml\nconda env create -f environment.yml\nconda env create -f environment.yml -p /home/user/anaconda3/envs/env_name\n\nI can't find anything in the conda specs which allows you to export an environment file without the prefix: ... line. However, like Alex pointed out in the comments, conda doesn't seem to care about the prefix line when creating an environment from the file.\nWith that in mind, if you want the other user to have no knowledge of your default install path, you can remove the prefix line with grep before writing to environment.yml.\n\nEither way, the other user then runs:\n\nand the environment will get installed in their default conda environment path.\nIf you want to specify a different install path than the default for your system (not related to 'prefix' in the environment.yml), just use the -p flag followed by the required path.\n\nNote that Conda recommends creating the environment.yml by hand, which is especially important if you are wanting to share your environment across platforms (Windows/Linux/Mac). In this case, you can just leave out the prefix line."
  },
  {
    "question": "anaconda update all possible packages? I tried the conda search --outdated, there are lots of outdated packages, for example the scipy is 0.17.1 but the latest is 0.18.0. However, when I do the conda update --all. It will not update any packages.\nupdate 1\n\n```\nconda update --all --alt-hint\n\nFetching package metadata .......\nSolving package specifications: ..........\n\n# All requested packages already installed.\n# packages in environment at /home/user/opt/anaconda2:\n#\n\n```\n\nupdate 2\nI can update those packages separately. I can do conda update scipy. But why I cannot update all of them in one go?",
    "answer": "conda update --all\n\nTL;DR: dependency conflicts: Updating one requires (by its requirements) to downgrade another\nYou are right:\n\nis actually the way to go1. Conda always tries to upgrade the packages to the newest version in the series (say Python 2.x or 3.x).\nDependency conflicts\nBut it is possible that there are dependency conflicts (which prevent a further upgrade). Conda usually warns very explicitly if they occur.\ne.g. X requires Y <5.0, so Y will never be >= 5.0\nThat's why you 'cannot' upgrade them all.\nResolving\nUpdate 1: since a while, mamba has proven to be an extremely powerful drop-in replacement for conda in terms of dependency resolution and (IMH experience) finds solutions to problems where conda fails. A way to invoke it without installing mamba is via the --solver=libmamba flag (requires conda-libmamba-solver), as pointed out by matteo in the comments.\nTo add: maybe it could work but a newer version of X working with Y > 5.0 is not available in conda. It is possible to install with pip, since more packages are available in pip. But be aware that pip also installs packages if dependency conflicts exist and that it usually breaks your conda environment in the sense that you cannot reliably install with conda anymore. If you do that, do it as a last resort and after all packages have been installed with conda. It's rather a hack.\nA safe way you can try is to add conda-forge as a channel when upgrading (add -c conda-forge as a flag) or any other channel you find that contains your package if you really need this new version. This way conda does also search in this places for available packages.\nConsidering your update: You can upgrade them each separately, but doing so will not only include an upgrade but also a downgrade of another package as well. Say, to add to the example above:\nX > 2.0 requires Y < 5.0, X < 2.0 requires Y > 5.0\nSo upgrading Y > 5.0 implies downgrading X to < 2.0 and vice versa.\n(this is a pedagogical example, of course, but it's the same in reality, usually just with more complicated dependencies and sub-dependencies)\nSo you still cannot upgrade them all by doing the upgrades separately; the dependencies are just not satisfiable so earlier or later, an upgrade will downgrade an already upgraded package again. Or break the compatibility of the packages (which you usually don't want!), which is only possible by explicitly invoking an ignore-dependencies and force-command. But that is only to hack your way around issues, definitely not the normal-user case!\n\n1 If you actually want to update the packages of your installation, which you usually don't. The command run in the base environment will update the packages in this, but usually you should work with virtual environments (conda create -n myenv and then conda activate myenv). Executing conda update --all inside such an environment will update the packages inside this environment. However, since the base environment is also an environment, the answer applies to both cases in the same way."
  },
  {
    "question": "How can I run Conda? I installed Anaconda and can run Python, so I assume that I installed it correctly. Following this introductory documentation, I am trying to install Python v3.3, so I am copying and pasting the following line into my console:\n\n```\nconda create -n py33 python=3.3 anaconda\n\n```\n\nHowever, that gives me an error:\n\n-bash: conda: command not found\n\nWhat do I need to do to run Conda?\nI am working on a Linux system.",
    "answer": "PATH=$PATH:$HOME/anaconda/bin\n\nIt turns out that I had not set the path.\nTo do so, I first had to edit .bash_profile (I downloaded it to my local desktop to do that; I do not know how to text edit a file from Linux)\nThen add this to .bash_profile:"
  },
  {
    "question": "From conda create requirements.txt for pip3 I usually use conda to manage my environments, but now I am on a project that needs a little more horsepower than my laptop. So I am trying to use my university's workstations which have new Intel Xeons. But I don't have admin rights and the workstation does not have conda so I am forced to work with virtualenv and pip3.\nHow do I generate a requirements.txt from conda that will work with pip3 and venv?\n\n```\nconda list -e > requirements.txt\n\n```\n\ndoes not generate a compatible file:\n\n```\n= is not a valid operator. Did you mean == ?\n\n```\n\nThe conda output is:\n\n```\n# This file may be used to create an environment using:\n# $ conda create --name <env> --file <this file>\n# platform: osx-64\ncertifi=2016.2.28=py36_0\ncycler=0.10.0=py36_0\nfreetype=2.5.5=2\nicu=54.1=0\nlibpng=1.6.30=1\nmatplotlib=2.0.2=np113py36_0\nmkl=2017.0.3=0\nnumpy=1.13.1=py36_0\nopenssl=1.0.2l=0\npip=9.0.1=py36_1\npyparsing=2.2.0=py36_0\npyqt=5.6.0=py36_2\npython=3.6.2=0\npython-dateutil=2.6.1=py36_0\npytz=2017.2=py36_0\nqt=5.6.2=2\nreadline=6.2=2\nscikit-learn=0.19.0=np113py36_0\nscipy=0.19.1=np113py36_0\nsetuptools=36.4.0=py36_1\nsip=4.18=py36_0\nsix=1.10.0=py36_0\nsqlite=3.13.0=0\ntk=8.5.18=0\nwheel=0.29.0=py36_0\nxz=5.2.3=0\nzlib=1.2.11=0\n\n```\n\nI thought I would just manually change all = to == but the there are two = in the conda output. Which one to change? Surely there is an easier way?\nEDIT: pip freeze > requirements.txt gives:\n\n```\ncertifi==2016.2.28\ncycler==0.10.0\nmatplotlib==2.0.2\nmatplotlib-venn==0.11.5\nnumpy==1.13.1\npyparsing==2.2.0\npython-dateutil==2.6.1\npytz==2017.2\nscikit-learn==0.19.0\nscipy==0.19.1\nsix==1.10.0\n\n```",
    "answer": "conda activate <env>\nconda install pip\npip freeze > requirements.txt\npython3 -m venv env\nsource env/bin/activate\npip install -r requirements.txt\npip list --format=freeze > requirements.txt\n\nAs the comment at the top indicates, the output of\nconda list -e > requirements.txt\ncan be used to create a conda virtual environment with\nconda create --name <env> --file requirements.txt\nbut this output isn't in the right format for pip.\nIf you want a file which you can use to create a pip virtual environment (i.e. a requirements.txt in the right format)\nyou can install pip within the conda environment, then use pip to create requirements.txt.\n\nThen use the resulting requirements.txt to create a pip virtual environment:\n\nWhen I tested this, the packages weren't identical across the outputs (pip included fewer packages) but it was sufficient to set up a functional environment.\nFor those getting odd path references in requirements.txt, use:"
  },
  {
    "question": "Combining conda environment.yml with pip requirements.txt I work with conda environments and need some pip packages as well, e.g. pre-compiled wheels from ~gohlke.\nAt the moment I have two files: environment.yml for conda with:\n\n```\n# run: conda env create --file environment.yml\nname: test-env\ndependencies:\n- python>=3.5\n- anaconda\n\n```\n\nand requirements.txt for pip which can be used after activating above conda environment:\n\n```\n# run: pip install -i requirements.txt\ndocx\ngooey\nhttp://www.lfd.uci.edu/~gohlke/pythonlibs/bofhrmxk/opencv_python-3.1.0-cp35-none-win_amd64.whl\n\n```\n\nIs there a possibility to combine them in one file (for conda)?",
    "answer": "name: test-env\ndependencies:\n- python>=3.5\n- anaconda\n- pip\n- numpy=1.13.3  # pin version for conda\n- pip:\n\n- docx\n- gooey\n- matplotlib==2.0.0  # pin version for pip\n\n- http://www.lfd.uci.edu/~gohlke/pythonlibs/bofhrmxk/opencv_python-3.1.0-cp35-none-win_amd64.whl\n\nPip dependencies can be included in the environment.yml file like this (docs):\n\nIt also works for .whl files in the same directory (see Dengar's answer) as well as with common pip packages."
  },
  {
    "question": "The environment is inconsistent, please check the package plan carefully I tried to update or install new packages from anaconda and lately, this message has appeared:\n\n```\nThe environment is inconsistent, please check the package plan carefully\nThe following package are causing the inconsistency:\n\n   - defaults/win-32::anaconda==5.3.1=py37_0\n\ndone\n\n```\n\nI tried with conda clean --all and then conda update --all but it persists.\nConda Info\n\n```\nactive environment : base\n    active env location : C:\\Users\\NAME\\Continuum\n            shell level : 1\n       user config file : C:\\Users\\NAME\\.condarc\n populated config files : C:\\Users\\NAME\\.condarc\n          conda version : 4.6.11\n    conda-build version : 3.17.7\n         python version : 3.7.3.final.0\n       base environment : C:\\Users\\NAME\\Continuum  (writable)\n           channel URLs : https://repo.anaconda.com/pkgs/main/win-32\n                          https://repo.anaconda.com/pkgs/main/noarch\n                          https://repo.anaconda.com/pkgs/free/win-32\n                          https://repo.anaconda.com/pkgs/free/noarch\n                          https://repo.anaconda.com/pkgs/r/win-32\n                          https://repo.anaconda.com/pkgs/r/noarch\n                          https://repo.anaconda.com/pkgs/msys2/win-32\n                          https://repo.anaconda.com/pkgs/msys2/noarch\n          package cache : C:\\Users\\NAME\\Continuum\\pkgs\n                          C:\\Users\\NAME\\.conda\\pkgs\n                          C:\\Users\\NAME\\AppData\\Local\\conda\\conda\\pkgs\n       envs directories : C:\\Users\\NAME\\Continuum\\envs\n                          C:\\Users\\NAME\\.conda\\envs\n                          C:\\Users\\NAME\\AppData\\Local\\conda\\conda\\envs\n               platform : win-32\n             user-agent : conda/4.6.11 requests/2.21.0 CPython/3.7.3 Windows/10 Windows/10.0.17763\n          administrator : False\n             netrc file : None\n           offline mode : False\n\n```",
    "answer": "conda install anaconda\n\nI had faced the same problem. Simply running\n\nsolved the problem for me."
  },
  {
    "question": "How to change default Anaconda python environment I've installed Anaconda and created two extra environments: py3k (which holds Python 3.3) and py34 (which holds Python 3.4). Besides those, I have a default environment named 'root' which the Anaconda installer created by default and which holds Python 2.7. This last one is the default, whenever I launch 'ipython' from the terminal it gives me version 2.7. In order to work with Python 3.4, I need to issue the commands (in the shell)\n\n```\nsource activate py34\nipython\n\n```\n\nwhich change the default environment to Python 3.4. This works fine, but it's annoying since most of the time I work on Python 3.4, instead of Python 2.7 (which I hold for teaching purposes, it's a rather long story). Anyway, I'll like to know how to change the default environment to Python 3.4, bearing in mind that I don't want to reinstall everything from scratch.",
    "answer": "source activate environment-name\nsource conda activate environment-name\nconda update conda\nconda install python=3.5\n\nIf you just want to temporarily change to another environment, use\n\nETA: This may be deprecated. I believe the current correct command is:\n\n(you can create environment-name with conda create)\n\nTo change permanently, there is no method except creating a startup script that runs the above code.\n\nTypically it's best to just create new environments. However, if you really want to change the Python version in the default environment, you can do so as follows:\nFirst, make sure you have the latest version of conda by running\n\nThen run\n\nThis will attempt to update all your packages in your root environment to Python 3 versions. If it is not possible (e.g., because some package is not built for Python 3.5), it will give you an error message indicating which package(s) caused the issue.\nIf you installed packages with pip, you'll have to reinstall them."
  },
  {
    "question": "How to activate an Anaconda environment I'm on Windows 8, using Anaconda 1.7.5 64bit.\nI created a new Anaconda environment with\nconda create -p ./test python=2.7 pip\nfrom C:\\Pr\\TEMP\\venv\\.\nThis worked well (there is a folder with a new python distribution). conda tells me to type \nactivate C:\\PR\\TEMP\\venv\\test \nto activate the environment, however this returns:\nNo environment named \"C:\\PR\\temp\\venv\\test\" exists in C:\\PR\\Anaconda\\envs\nHow can I activate the environment? What am I doing wrong?",
    "answer": "conda create -n py33 python=3.3 anaconda\nset PATH=C:\\Anaconda\\envs\\py33\\Scripts;C:\\Anaconda\\envs\\py33;%PATH%\nactivate py33\nsource activate py33\n\nIf this happens you would need to set the PATH for your environment (so that it gets the right Python from the environment and Scripts\\ on Windows).\nImagine you have created an environment called py33 by using:\n\nHere the folders are created by default in Anaconda\\envs, so you need to set the PATH as:\n\nNow it should work in the command window:\n\nThe line above is the Windows equivalent to the code that normally appears in the tutorials for Mac and Linux:\n\nMore info:\nhttps://groups.google.com/a/continuum.io/forum/#!topic/anaconda/8T8i11gO39U\nDoes `anaconda` create a separate PYTHONPATH variable for each new environment?"
  },
  {
    "question": "Should conda, or conda-forge be used for Python environments? Conda and conda-forge are both Python package managers. What is the appropriate choice when a package exists in both repositories? Django, for example, can be installed with either, but the difference between the two is several dependencies (conda-forge has many more). There is no explanation for these differences, not even a simple README.\nWhich one should be used? Conda or conda-forge? Does it matter?",
    "answer": "conda install -c some-channel packagename\nconda install some-channel::packagename\nconda config --show channels\nconda config --add channels some-channel\nconda config --append channels some-channel\nconda config --remove channels some-channel\nconda config -h\n\nThe short answer is that, in my experience generally, it doesn't matter which you use, with one exception. If you work for a company with more than 200 employees then the default conda channel is not free as of 2020.\nThe long answer:\nSo conda-forge is an additional channel from which packages may be installed. In this sense, it is not any more special than the default channel, or any of the other hundreds (thousands?) of channels that people have posted packages to. You can add your own channel if you sign up at https://anaconda.org and upload your own Conda packages.\nHere we need to make the distinction, which I think you're not clear about from your phrasing in the question, between conda, the cross-platform package manager, and conda-forge, the package channel. Anaconda Inc. (formerly Continuum IO), the main developers of the conda software, also maintain a separate channel of packages, which is the default when you type conda install packagename without changing any options.\nThere are three ways to change the options for channels. The first two are done every time you install a package and the last one is persistent. The first one is to specify a channel every time you install a package:\n\nOf course, the package has to exist on that channel. This way will install packagename and all its dependencies from some-channel. Alternately, you can specify:\n\nThe package still has to exist on some-channel, but now, only packagename will be pulled from some-channel. Any other packages that are needed to satisfy dependencies will be searched for from your default list of channels.\nTo see your channel configuration, you can write:\n\nYou can control the order that channels are searched with conda config. You can write:\n\nto add the channel some-channel to the top of the channels configuration list. This gives some-channel the highest priority. Priority determines (in part) which channel is selected when more than one channel has a particular package. To add the channel to the end of the list and give it the lowest priority, type\n\nIf you would like to remove the channel that you added, you can do so by writing\n\nSee\n\nfor more options.\nWith all of that said, there are five main reasons to use the conda-forge channel instead of the defaults channel maintained by Anaconda:\n\nPackages on conda-forge may be more up-to-date than those on the defaults channel\nThere are packages on the conda-forge channel that aren't available from defaults\nYou would prefer to use a dependency such as openblas (from conda-forge) instead of mkl (from defaults).\nIf you are installing a package that requires a compiled library (e.g., a C extension or a wrapper around a C library), it may reduce the chance of incompatibilities if you install all of the packages in an environment from a single channel due to binary compatibility of the base C library (but this advice may be out of date/change in the future). For reference, see the Conda Forge post on mixing channels.\nconda-forge is free to use even in large companies, while the default conda channel is not. See here."
  },
  {
    "question": "How to create conda environment with specific python version? I have miniconda3 installed and since I would like to have an environment with python version 3.3.0, I create it via\n\n```\nconda create -n \"myenv\" python=3.3.0\n\n```\n\nHowever when I activate the environment via\n\n```\nconda activate myenv\n\n```\n\npython has version 2.7.15 and path\n\n```\n/usr/bin/python\n\n```\n\nand ipython has python version 3.6.8 and path\n\n```\n/home/myname/.local/bin/ipython\n\n```\n\nI can access the correct python with python3 which is at\n\n```\n/home/myname/miniconda3/envs/myenv/bin/python3\n\n```\n\nhowever, ipython3 has python version 3.6.8 again.\n\n```\nconda install python=3.3.0\n\n```\n\nleft the situation unchanged.\nA solution would be to open IPython via\n\n```\npython3 -m IPython\n\n```\n\nhowever, while this works fine for python here I get the error message\n\n```\n/home/myname/miniconda3/envs/myenv/bin/python3: No module named IPython\n\n```\n\nIs it possible to access with the commands python and ipython both python version 3.3.0 in that specific environment, i.e. not by setting an alias in the .bashrc?\nEDIT:\nTurns out that this problem does not occur if you select version 3.3 instead of 3.3.0 together with @ilmarinen's answer\n\n```\nconda create -n \"myenv\" python=3.3 ipython\n\n```\n\neverything works fine and python as well as ipython result to version python 3.3.5.",
    "answer": "conda create -n \"myenv\" python=3.3.0 ipython\n\nYou need to install ipython as well into your given environment\n\nThe conda environments are prepended to your PATH variable, so when you are trying to run the executable \"ipython\", Linux will not find \"ipython\" in your activated environment (since it doesn't exist there), but it will continue searching for it, and eventually find it wherever you have it installed."
  },
  {
    "question": "Bulk package updates using Conda Is there a way (using conda update) that I can list outdated packages and select or bulk update (compatible) packages in Anaconda?\nIt doesn't make much sense updating the packages individually as there are hundreds of them.",
    "answer": "You want conda update --all.\nconda search --outdated will show outdated packages, and conda update --all will update them (note that the latter will not update you from Python 2 to Python 3, but the former will show Python as being outdated if you do use Python 2)."
  },
  {
    "question": "Conda command is not recognized on Windows 10 I installed Anaconda 4.4.0 (Python 3.6 version) on Windows 10 by following the instructions here: https://www.continuum.io/downloads. However, when I open the Command prompt window and try to write \n\n```\nconda list\n\n```\n\nI get the \n\n'conda' command is not recognized...\n\nerror. \nI tried to run\n\n```\nset PATH=%PATH%;C:\\Users\\Alex\\Anaconda3\n\n```\n\nbut it didn't help. I also read that I might need to edit my .bashrc file, but I don't know how to access this file, and how I should edit it.",
    "answer": "In Windows, you will have to set the path to the location where you installed Anaconda3 to.\nFor me, I installed anaconda3 into C:\\Anaconda3. Therefore you need to add C:\\Anaconda3 as well as C:\\Anaconda3\\Scripts\\ to your path variable, e.g. set PATH=%PATH%;C:\\Anaconda3;C:\\Anaconda3\\Scripts\\.\nYou can do this via powershell (see above, https://msdn.microsoft.com/en-us/library/windows/desktop/bb776899(v=vs.85).aspx ), or hit the windows key \u2192 enter environment \u2192 choose from settings \u2192 edit environment variables for your account \u2192 select Path variable \u2192 Edit \u2192 New.\nTo test it, open a new dos shell, and you should be able to use conda commands now. E.g., try conda --version."
  },
  {
    "question": "Does it make sense to use Conda + Poetry? Does it make sense to use Conda + Poetry for a Machine Learning project? Allow me to share my (novice) understanding and please correct or enlighten me:\nAs far as I understand, Conda and Poetry have different purposes but are largely redundant:\n\nConda is primarily a environment manager (in fact not necessarily Python), but it can also manage packages and dependencies.\nPoetry is primarily a Python package manager (say, an upgrade of pip), but it can also create and manage Python environments (say, an upgrade of Pyenv).\n\nMy idea is to use both and compartmentalize their roles: let Conda be the environment manager and Poetry the package manager. My reasoning is that (it sounds like) Conda is best for managing environments and can be used for compiling and installing non-python packages, especially CUDA drivers (for GPU capability), while Poetry is more powerful than Conda as a Python package manager.\nI've managed to make this work fairly easily by using Poetry within a Conda environment. The trick is to not use Poetry to manage the Python environment: I'm not using commands like poetry shell or poetry run, only poetry init, poetry install etc (after activating the Conda environment).\nFor full disclosure, my environment.yml file (for Conda) looks like this:\n\n```\nname: N\n\nchannels:\n  - defaults\n  - conda-forge\n\ndependencies:\n  - python=3.9\n  - cudatoolkit\n  - cudnn\n\n```\n\nand my poetry.toml file looks like that:\n\n```\n[tool.poetry]\nname = \"N\"\nauthors = [\"B\"]\n\n[tool.poetry.dependencies]\npython = \"3.9\"\ntorch = \"^1.10.1\"\n\n[build-system]\nrequires = [\"poetry-core>=1.0.0\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\n```\n\nTo be honest, one of the reasons I proceeded this way is that I was struggling to install CUDA (for GPU support) without Conda.\nDoes this project design look reasonable to you?",
    "answer": "ln -s /path/to/my/project/.pixi/envs/default /path/to/conda/base/envs/conda-name-of-my-env\nname: my_project_env\nchannels:\n- pytorch\n- conda-forge\n\n- nodefaults\ndependencies:\n- python=3.10.*  # or don't specify the version and use the latest stable Python\n- mamba\n- pip  # pip must be mentioned explicitly, or conda-lock will fail\n- poetry=1.*  # or 1.1.*, or no version at all -- as you want\n- tensorflow=2.8.0\n- pytorch::pytorch=1.11.0\n- pytorch::torchaudio=0.11.0\n- pytorch::torchvision=0.12.0\n\nplatforms:\n- linux-64\nsubdirs:\nlinux-64:\npackages:\n__cuda: 11.5\nconda create -p /tmp/bootstrap -c conda-forge mamba conda-lock poetry='1.*'\nconda activate /tmp/bootstrap\n\nconda-lock -k explicit --conda mamba\n\npoetry init --python=~3.10  # version spec should match the one from environment.yml\n\npoetry add --lock tensorflow=2.8.0 torch=1.11.0 torchaudio=0.11.0 torchvision=0.12.0\n\npoetry add --lock conda-lock\n\nconda deactivate\nrm -rf /tmp/bootstrap\n\ngit add environment.yml virtual-packages.yml conda-linux-64.lock\n\ngit add pyproject.toml poetry.lock\ngit commit\nconda create --name my_project_env --file conda-linux-64.lock\nconda activate my_project_env\npoetry install\nconda activate my_project_env\nconda-lock -k explicit --conda mamba\n\nmamba update --file conda-linux-64.lock\n\npoetry update\n\n2024-04-05 update:\nIt looks like my tips proved to be useful to many people, but they are not needed anymore. Just use Pixi. It's still alpha, but it works great, and provides the features of the Conda + Poetry setup in a simpler and more unified way. In particular, Pixi supports:\n\ninstalling packages both from Conda channels and from PyPi,\nlockfiles,\ncreating multiple features and environments (prod, dev, etc.),\nvery efficient package version resolution, not just faster than Conda (which is very slow), but in my experience also faster than Mamba, Poetry and pip.\n\nMaking a Pixi env look like a Conda env\nOne non-obvious tip about Pixi is that you can easily make your project's Pixi environment visible as a Conda environment, which may be useful e.g. in VS Code, which allows choosing Python interpreters and Jupyter kernels from detected Conda environments. All you need to do is something like:\n\nThe first path is the path to your Pixi environment, which resides in your project directory, under .pixi/envs, and the second path needs to be within one of Conda's environment directories, which can be found with conda config --show envs_dirs.\nOriginal answer:\nI have experience with a Conda + Poetry setup, and it's been working fine. The great majority of my dependencies are specified in pyproject.toml, but when there's something that's unavailable in PyPI, or installing it with Conda is easier, I add it to environment.yml. Moreover, Conda is used as a virtual environment manager, which works well with Poetry: there is no need to use poetry run or poetry shell, it is enough to activate the right Conda environment.\nTips for creating a reproducible environment\n\nAdd Poetry, possibly with a version number (if needed), as a dependency in environment.yml, so that you get Poetry installed when you run conda create, along with Python and other non-PyPI dependencies.\nAdd conda-lock, which gives you lock files for Conda dependencies, just like you have poetry.lock for Poetry dependencies.\nConsider using mamba which is generally compatible with conda, but is better at resolving conflicts, and is also much faster. An additional benefit is that all users of your setup will use the same  package resolver, independent from the locally-installed version of Conda.\nBy default, use Poetry for adding Python dependencies. Install packages via Conda if there's a reason to do so (e.g. in order to get a CUDA-enabled version). In such a case, it is best to specify the package's exact version in environment.yml, and after it's installed, to add an entry with the same version specification to Poetry's pyproject.toml (without ^ or ~ before the version number). This will let Poetry know that the package is there and should not be upgraded.\nIf you use a different channels that provide the same packages, it might be not obvious which channel a particular package will be downloaded from. One solution is to specify the channel for the package using the :: notation (see the pytorch entry below), and another solution is to enable strict channel priority. Unfortunately, in Conda 4.x there is no way to enable this option through environment.yml.\nNote that Python adds user site-packages to sys.path, which may cause lack of reproducibility if the user has installed Python packages outside Conda environments. One possible solution is to make sure that the PYTHONNOUSERSITE environment variable is set to True (or to any other non-empty value).\n\nExample\nenvironment.yml:\n\nvirtual-packages.yml (may be used e.g. when we want conda-lock to generate CUDA-enabled lock files even on platforms without CUDA):\n\nFirst-time setup\nYou can avoid playing with the bootstrap env and simplify the example below if you have conda-lock, mamba and poetry already installed outside your target environment.\n\nUsage\nThe above setup may seem complex, but it can be used in a fairly simple way.\nCreating the environment\n\nActivating the environment\n\nUpdating the environment"
  },
  {
    "question": "What are the differences between Conda and Anaconda? Post-question update:\nSee Introduction to Conda for more details.\n\nThe problem:\nI first installed Anaconda on my ubuntu at ~/anaconda, when I was trying to update my anaconda, according to the documentation from Continuum Analytics, I should use the following commands:\n\n```\nconda update conda\nconda update anaconda\n\n```\n\nThen I realized that I did not have conda installed, so I installed it using the documentation from here.\nAfter conda is installed, when I run conda update anaconda, I got the following error:\n\nError: package 'anaconda' is not installed in /home/xiang/miniconda\n\nIt appears conda is assuming my anaconda is installed under /home/xiang/miniconda which is NOT true.\nThe questions:\n\nWhat are the differences between conda and anaconda?\nHow can I tell conda where my anaconda is installed?",
    "answer": "conda is the package manager. Anaconda is a set of about a hundred packages including conda, numpy, scipy, ipython notebook, and so on.\nYou installed Miniconda, which is a smaller alternative to Anaconda that is just conda and its dependencies, not those listed above.\nOnce you have Miniconda, you can easily install Anaconda into it with conda install anaconda."
  },
  {
    "question": "How to change Python version of existing conda virtual environment? I created a conda environment with Python version 3.8, but it doesn't support matplotlib... So I am looking for something like this to change the Python version: conda env my_env update to python=3.6. Is this possible or do I need to recreate the environment?\nI have miniconda installed.",
    "answer": "conda activate my_env\nconda install python=3.6\n\nActivate the relevant environment, then install your target python version."
  },
  {
    "question": "Conda: Installing / upgrading directly from github Can I install/upgrade packages from GitHub using conda?\nFor example, with pip I can do:\n\n```\npip install git+git://github.com/scrappy/scrappy@master\n\n```\n\nto install scrappy directly from the master branch in GitHub. Can I do something equivalent with conda?\nIf this is not possible, would it make any sense to install pip with conda and manage such local installations with pip?",
    "answer": "name: sample_env\nchannels:\ndependencies:\n- requests\n- bokeh>=0.10.0\n- pip:\n- \"--editable=git+https://github.com/pythonforfacebook/facebook-sdk.git@8c0d34291aaafec00e02eaa71cc2a242790a0fcc#egg=facebook_sdk-master\"\n\nThere's better support for this now through conda-env.  You can, for example, now do:\n\nIt's still calling pip under the covers, but you can now unify your conda and pip package specifications in a single environment.yml file.\nIf you wanted to update your root environment with this file, you would need to save this to a file (for example, environment.yml), then run the command: conda env update -f environment.yml.\nIt's more likely that you would want to create a new environment:\nconda env create -f environment.yml (changed as supposed in the comments)"
  },
  {
    "question": "How do I revert to a previous package in Anaconda? If I do \n\n```\nconda info pandas\n\n```\n\nI can see all of the packages available.\nI updated my pandas to the latest this morning, but I need to revert to a prior version now. I tried\n\n```\nconda update pandas 0.13.1\n\n```\n\nbut that didn't work. How do I specify which version to use?",
    "answer": "conda install pandas=0.13.1\n\nI had to use the install function instead:"
  },
  {
    "question": "How to use Jupyter notebooks in a conda environment? Typically one runs jupyter notebook or jupyter-notebook or ipython notebook in a terminal to\nstart a Jupyter notebook webserver locally (and open the URL in the browser). When using conda\nand conda environments, what is the best way to run a Jupyter notebook which allows to\nimport Python modules installed in the conda environment?\nAs it seems, this is not quite straight forward and many\nusers have similar troubles.\nMost common error message seems to be: after installing a package XYZ in a conda environment\nmy-env one can run import XYZ in a python console started in my-env, but running the same\ncode in the Jupyter notebook will lead to an ImportError.\nThis question has been asked many times, but there is no good place to answer it, most Q&A's and\nGithub tickets are quite messy so let's start a new Q&A here.",
    "answer": "conda create -n my-conda-env         # creates new virtual env\nconda activate my-conda-env          # activate environment in terminal\nconda install jupyter                # install jupyter + notebook\njupyter notebook                     # start server + kernel inside my-conda-env\nconda create -n my-conda-env                               # creates new virtual env\nconda activate my-conda-env                                # activate environment in terminal\nconda install ipykernel                                    # install Python kernel in new conda env\nipython kernel install --user --name=my-conda-env-kernel   # configure Jupyter to use Python kernel\nconda deactivate          # this step can be omitted by using a different terminal window than before\nconda install jupyter     # optional, might be installed already in system e.g. by 'apt install jupyter' on debian-based systems\njupyter notebook          # run jupyter from system\n{\n\"argv\": [\n\"/opt/miniconda3/envs/my-conda-env/bin/python\",\n\"-m\",\n\"ipykernel_launcher\",\n\"-f\",\n\"{connection_file}\"\n],\n\"display_name\": \"my-conda-env-kernel\",\n\"language\": \"python\"\n}\nconda activate my-conda-env    # this is the environment for your project and code\nconda install ipykernel\nconda deactivate\n\nconda activate base            # could be also some other environment\nconda install nb_conda_kernels\njupyter notebook\nwhich jupyter\n/opt/miniconda3/envs/my-conda-env/bin/jupyter\nwhich jupyter-notebook   # this might be different than 'which jupyter'! (see below)\n/opt/miniconda3/envs/my-conda-env/bin/jupyter-notebook\n[1] !which python\n/opt/miniconda3/envs/my-conda-env/bin/python\n[2] import sys; sys.executable\n'/opt/miniconda3/envs/my-conda-env/bin/python'\n['/home/my_user',\n'/opt/miniconda3/envs/my-conda-env/lib/python37.zip',\n'/opt/miniconda3/envs/my-conda-env/lib/python3.7',\n'/opt/miniconda3/envs/my-conda-env/lib/python3.7/lib-dynload',\n'',\n'/opt/miniconda3/envs/my-conda-env/lib/python3.7/site-packages',\n'/opt/miniconda3/envs/my-conda-env/lib/python3.7/site-packages/IPython/extensions',\n'/home/my_user/.ipython']\n!jupyter-troubleshoot\njupyter kernelspec list\n(my-conda-env) $ which jupyter-notebook\n/usr/bin/jupyter\n(my-conda-env) $ which jupyter-notebook\n/opt/miniconda3/envs/my-conda-env/bin/jupyter-notebook\nconda create -n my-conda-env\nconda activate my-conda-env\nconda install ipykernel\nwhich jupyter            # this looks good, but is misleading!\n/opt/miniconda3/envs/my-conda-env/bin/jupyter\nwhich jupyter-notebook   # jupyter simply runs jupyter-notebook from system...\n/usr/bin/jupyter-notebook\nconda activate my-conda-env\nconda install jupyter\njupyter notebook\nconda activate my-conda-env\njupyter kernelspec list\nAvailable kernels:\npython3 /opt/miniconda3/envs/my-conda-env/share/jupyter/kernels/python3\ncat ~/.local/share/jupyter/kernels/python3/kernel.json\n{\n\"argv\": [\n\"python\",\n\"-m\",\n\"ipykernel_launcher\",\n\"-f\",\n\"{connection_file}\"\n],\n\"display_name\": \"Python 3\",\n\"language\": \"python\"\n}\nconda activate my-conda-env\njupyter notebook\nconda create -n my-conda-env\nconda activate my-conda-env\nconda install python=2\nconda install jupyter\njupyter notebook\ncat ~/.local/share/jupyter/kernels/python3/kernel.json\n{\n\"argv\": [\n\"python\",\n\"-m\",\n\"ipykernel_launcher\",\n\"-f\",\n\"{connection_file}\"\n],\n\"display_name\": \"Python 3\",\n\"language\": \"python\"\n}\n\nDisclaimer: ATM tested only in Ubuntu and Windows (see comments to this answer).\n\nJupyter runs the user's code in a separate process called kernel. The kernel can be a different Python installation (in a different conda environment or virtualenv or Python 2 instead of Python 3) or even an interpreter for a different language (e.g. Julia or R). Kernels are configured by specifying the interpreter and a name and some other parameters (see Jupyter documentation) and configuration can be stored system-wide, for the active environment (or virtualenv) or per user. If nb_conda_kernels is used, additional to statically configured kernels, a separate kernel for each conda environment with ipykernel installed will be available in Jupyter notebooks.\nIn short, there are three options how to use a conda environment and Jupyter:\nOption 1: Run Jupyter server and kernel inside the conda environment\nDo something like:\n\nJupyter will be completely installed in the conda environment. Different versions of Jupyter can be used\nfor different conda environments, but this option might be a bit of overkill. It is enough to\ninclude the kernel in the environment, which is the component wrapping Python which runs the code.\nThe rest of Jupyter notebook can be considered as editor or viewer and it is not necessary to\ninstall this separately for every environment and include it in every env.yml file. Therefore one\nof the next two options might be preferable, but this one is the simplest one and definitely fine.\nOption 2: Create special kernel for the conda environment\nDo something like:\n\nThen run jupyter from the system installation or a different conda environment:\n\nName of the kernel and the conda environment are independent from each other, but it might make sense to use a similar name.\nOnly the Python kernel will be run inside the conda environment, Jupyter from system or a different conda environment will be used - it is not installed in the conda environment. By calling ipython kernel install the jupyter is configured to use the conda environment as kernel, see Jupyter documentation and IPython documentation for more information. In most Linux installations this configuration is a *.json file in ~/.local/share/jupyter/kernels/my-conda-env-kernel/kernel.json:\n\nOption 3: Use nb_conda_kernels to use a kernel in the conda environment\nWhen the package nb_conda_kernels is installed, a separate kernel is available automatically for each\nconda environment containing the conda package ipykernel or a different kernel (R, Julia, ...).\n\nYou should be able to choose the Kernel Python [conda env:my-conda-env]. Note that nb_conda_kernels seems to be available only via conda and not via pip or other package managers like apt.\nTroubleshooting\nUsing Linux/Mac the command which on the command line will tell you which jupyter is used, if you\nare using option 1 (running Jupyter from inside the conda environment), it should be an executable\nfrom your conda environment:\n\nInside the notebook you should see that Python uses Python paths from the conda environment:\n\nJupyter provides the command jupyter-troubleshoot or in a Jupyter notebook:\n\nThis will print a lot of helpful information about including the outputs mentioned above as well as installed libraries and others. When\nasking for help regarding Jupyter installations questions, it might be good idea to provide this information in bug reports or questions.\nTo list all configured Jupyter kernels run:\n\nCommon errors and traps\nJupyter notebook not installed in conda environment\nNote: symptoms are not unique to the issue described here.\nSymptoms: ImportError in Jupyter notebooks for modules installed in the conda environment (but\nnot installed system wide), but no error when importing in a Python terminal\nExplaination: You tried to run jupyter notebook from inside your conda environment\n(option 1, see above), there is no configuration for a kernel for this conda environment (this\nwould be option 2) and nb_conda_kernels is not installed (option 3), but jupyter notebook is not (fully)\ninstalled in the conda environment, even if which jupyter might make you believe it was.\nIn GNU/Linux you can type which jupyter to check which executable of Jupyter is run.\nThis means that system's Jupyter is used, probably because Jupyter is not installed:\n\nIf the path points to a file in your conda environment, Jupyter is run from inside Jupyter:\n\nNote that when the conda package ipykernel is installed, an executable jupyter is shipped, but\nno executable jupyter-notebook. This means that which jupyter will return a path to the conda\nenvironment but jupyter notebook will start system's jupyter-nootebook (see also here):\n\nThis happens because jupyter notebook searches for jupyter-notebook, finds\n/usr/bin/jupyter-notebook and\ncalls it\nstarting a new Python process. The shebang in /usr/bin/jupyter-notebook is #!/usr/bin/python3\nand not a dynamic\n#!/usr/bin/env python.\nTherefore Python manages to break out of the conda environment. I guess jupyter could call\npython /usr/bin/jupyter-notebook instead to overrule the shebang, but mixing\nsystem's bin files and the environment's python path can't work well anyway.\nSolution: Install jupyter notebook inside the conda environment:\n\nWrong kernel configuration: Kernel is configured to use system Python\nNote: symptoms are not unique to the issue described here.\nSymptoms: ImportError in Jupyter notebooks for modules installed in the conda environment (but\nnot installed system wide), but no error when importing in a Python terminal\nExplanation: Typically the system provides a kernel called python3 (display name \"Python 3\")\nconfigured to use /usr/bin/python3, see e.g. /usr/share/jupyter/kernels/python3/kernel.json.\nThis is usually overridden by a kernel in the conda environment, which points to the environments\npython binary /opt/miniconda3/envs/my-conda-env/bin/python. Both are generated by the package\nipykernel (see here\nand here).\nA user kernel specification in ~/.local/share/jupyter/kernels/python3/kernel.json might override\nthe system-wide and environment kernel. If the environment kernel is missing or the user kernel\npoints to a python installation outside the environment option 1 (installation of jupyter in the\nenvironment) will fail.\nFor occurrences and discussions of this problem and variants see here,\nhere,\nhere\nand also here,\nhere and\nhere.\nSolution: Use jupyter kernelspec list to list the location active kernel locations.\n\nIf the kernel in the environment is missing, you can try creating it manually using\nipython kernel install --sys-prefix in the activated environment, but it is probably better to\ncheck your installation, because conda install ipykernel should have created the environment\n(maybe try re-crate the environment and re-install all packages?).\nIf a user kernel specification is blocking the environment kernel specification, you can either\nremove it or use a relative python path which will use $PATH to figure out which python to use.\nSo something like this, should be totally fine:\n\nCorrect conda environment not activated\nSymptoms: ImportError for modules installed in the conda environment (but not installed system\nwide) in Jupyter notebooks and Python terminals\nExplanation: Each terminal has a set of environment variables, which are lost when the terminal\nis closed. In order to use a conda environment certain environment variables need to be set, which\nis done by activating it using conda activate my-conda-env. If you attempted to run Jupyter\nnotebook from inside the conda environment (option 1), but did not activate the conda environment\nbefore running it, it might run the system's jupyter.\nSolution: Activate conda environment before running Jupyter.\n\nBroken kernel configuration\nSymptoms: Strange things happening. Maybe similar symptoms as above, e.g. ImportError\nExplanation: If you attempted to use option 2, i.e. running Jupyter from system and the Jupyter\nkernel inside the conda environment by using an explicit configuration for the kernel, but it does\nnot behave as you expect, the configuration might be corrupted in some way.\nSolution: Check configuration in ~/.local/share/jupyter/kernels/my-kernel-name/kernel.json\nand fix mistakes manually or remove the entire directory and re-create it using the command\nprovided above for option 2. If you can't find the kernel configuration there run\njupyter kernelspec list.\nPython 2 vs 3\nSymptoms: ImportError due to wrong Python version of the Jupyter kernel or other problems\nwith Python 2/3\nExplanation: The kernel configuration can have all sorts of confusing and misleading effects.\nFor example the default Python 3 kernel configuration will allow me to launch a Jupyter notebook\nrunning on Python 2:\n\nThe default Python 3 kernel:\n\nAfter creating a new Jupyter Notebook with the Python 3 kernel, Python 2 from the conda\nenvironment will be used even if \"Python 3\" is displayed by Jupyter.\nSolution: Don't use Python 2 ;-)"
  },
  {
    "question": "How to list package versions available with conda Is there a way to see what package versions are available with conda? I am getting an error with jupyter but it was working before. Something like yolk?",
    "answer": "conda search\nFetching package metadata .........\naffine                       2.0.0                    py27_0  defaults\n2.0.0                    py35_0  defaults\n2.0.0                    py36_0  defaults\nalabaster                    0.7.3                    py27_0  defaults\n0.7.3                    py34_0  defaults\n0.7.7                    py27_0  defaults\n0.7.7                    py34_0  defaults\n0.7.7                    py35_0  defaults\n0.7.9                    py27_0  defaults\n\nYou can just type \"conda search\" which will give you something like the following."
  },
  {
    "question": "How do I keep track of pip-installed packages in an Anaconda (Conda) environment? I've installed and have been using the Anaconda Python distribution, and I have started using the Anaconda (Conda) environment. I can use the standard conda install... command to put packages from the distribution into my environments, but to use anything outside (i.e. Flask-WTF, flask-sqlalchemy, and alembic) I need to use pip install in the active environment. However, when I look at the contents of the environment, either in the directory, or using conda list these pip installed packages don't show up. \nUsing pip freeze and pip list just lists every package I've ever installed. \nIs there a way to keep track of what is in each of my Anaconda envs (both pip and conda installed)?",
    "answer": "conda env export -n <env-name> > environment.yml\nname: stats\nchannels:\n- javascript\ndependencies:\n- python=3.4\n- bokeh=0.9.2\n- numpy=1.9.*\n- nodejs=0.10.*\n- flask\n- pip:\n- Flask-Testing\nconda env create -f path/to/environment.yml\n\nconda-env now does this automatically (if pip was installed with conda).\nYou can see how this works by using the export tool used for migrating an environment:\n\nThe file will list both conda packages and pip packages:\n\nIf you're looking to follow through with exporting the environment, move environment.yml to the new host machine and run:"
  },
  {
    "question": "How to remove (base) from terminal prompt after updating conda After updating miniconda3, whenever I open a terminal it shows \"(base)\" in front of my username and host.\nIn this answer post https://askubuntu.com/a/1113206/315699 it was suggested to use\n\n```\nconda config --set changeps1 False\n\n```\n\nTo remove it.\nBut that would remove the indication for any conda environment. I would like to remove it only for the base one, so that I can maintain it always active and have access to its python and installed packages without having to always see this (base) taking up space.",
    "answer": "mkdir -p miniconda3/etc/conda/activate.d\nPS1=\"$(echo \"$PS1\" | sed 's/(base) //')\"\nPROMPT=$(echo $PROMPT | sed 's/(base) //')\n\nUse the base env's activation hook\nFor each env, any scripts in the etc/conda/activate.d directory will be executed post-activation (likewise etc/conda/deactivate.d scripts for deactivation).  If you add a script to remove the (base), similar to @ewindes suggestion, you'll get the behavior you desire.\nI had to create this directory for base, which is just the root of your Anaconda/Miniconda folder. E.g.,\n\nThen made a simple file in there (e.g., remove_base_ps1.sh) with one line:\n\nIf you are using zsh, use this instead.\n\nLaunching a new shell then does not show (base), and deactivating out of nested envs also takes care of the PS1 change.\nNote: You must add quotes around $PS1 if you want to preserve ending spaces."
  },
  {
    "question": "Zsh: Conda/Pip installs command not found So I installed Anaconda and everything is working. After I installed it I decided to switch to oh-my-zsh. I am now getting:\n\n```\nzsh: command not found: conda\n\n```\n\nwhen trying to use pip or conda installs\n\n```\necho $ZSH_VERSION\n\n```\n\n\n5.0.5\n\nI have added to my zshenv.sh \n\n```\nexport PATH =\"/Users/Dz/anaconda/bin:$PATH\"\n\n```\n\nWhat is it that I'm missing?",
    "answer": "export PATH=\"$PATH;/Users/Dz/anaconda/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/opt/X11/bin:/Users/Dz/.rvm/bin\"\nexport PATH=\"$PATH:/Users/Dz/anaconda/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/opt/X11/bin:/Users/Dz/.rvm/bin\"\n\nsource ~/.zshrc\necho $HOME\necho $PATH\n\nIt appears that my PATH is broken in my .zshrc file.\nOpen it and add :\n\nDoh! Well that would explain everything. How did I miss that little semicolon? Changed:\n\nWe're good now."
  },
  {
    "question": "How to free disk space taken up by (ana)conda? I am using the conda package manager - a lot. By now I have quite a few environments and a lot of downloaded packages taking a lot of space on my SSD. An obvious path to free some of that space is to use the command\n\n```\nconda env export > environment.yml\n\n```\n\nfrom https://conda.io/docs/user-guide/tasks/manage-environments.html#exporting-the-environment-file to export which packages my old, inactive projects use(d) and then delete these environments. As far as I understand, this should free some of the space in anaconda2/envs/, but not in anaconda2/pkgs/. How do I get rid of these packages? Also, I suspect that there might be quite a few packages still sitting around, to which no environment is linking to - could that happen?\nQuestions:\n\nIn general: What is the best way to reduce the space taken up by conda?\nHow do I get rid of packages that no environment is using anymore? How do I prune my packages? I am searching for something like sudo apt-get autoremove from Ubuntu/Debian.",
    "answer": "conda clean --all\nimport os\nimport subprocess\nfor env in os.listdir('/Users/me/miniconda3/envs'):\nsubprocess.call(['conda', 'list', '-n', env])\n\nYou can free some space with:\n\nclean        Remove unused packages and caches.\n\nConda already use symlinks when possible for packages. So, not much to improve here, I guess.\n\nOk, thanks, but I would like to know \"not for a specific environment, but in general\" - for all environments.\n\nYou can list all packages in all envs with a few lines of Python:"
  },
  {
    "question": "How to uninstall miniconda? I've installed conda packages as follows:\n\n```\n$ wget http://.../miniconda\n$ bash miniconda\n$ conda install numpy pandas scipy matplotlib scikit-learn nltk ipython-notebook seaborn\n\n```\n\nI want to uninstall conda because it's messing up my pips and environment.\n\nHow do I uninstall conda totally?\nWill it uninstall also my pip managed packages? If so, is there a way to uninstall conda safely without uninstalling packages managed by pip?",
    "answer": "rm -r ~/miniconda/\nwget https://repo.continuum.io/miniconda/Miniconda3-3.7.0-Linux-x86_64.sh -O ~/miniconda.sh\nbash miniconda\nconda env remove --yes -n new_env    # remove the environement new_env if it exists (optional)\nconda create --yes -n new_env pip numpy pandas scipy matplotlib scikit-learn nltk ipython-notebook seaborn python=2\nactivate new_env\n\ndeactivate\n\nIn order to uninstall miniconda, simply remove the miniconda folder,\n\nAs for avoiding conflicts between different Python environments, you can use virtual environments. In particular, with Miniconda, the following workflow could be used,"
  },
  {
    "question": "Where is site-packages located in a Conda environment? After installing a package in an Conda environment, I'd like to make some changes to the code in that package.\nWhere can I find the site-packages directory containing the installed packages?\nI have an Anaconda Python 2.7 base distribution, but I do not find a directory:\n\n```\n/Users/username/anaconda/lib/python2.7/site-packages\n\n```",
    "answer": "You can import the module and check the module.__file__ string. It contains the path to the associated source file.\nAlternatively, you can read the File tag in the the module documentation, which can be accessed using help(module), or module? in IPython."
  },
  {
    "question": "Conda - Silently/non-interactively installing a package I am trying to automate the process of setting up a development environment with pandas package using conda.\nI installed conda, created and activated a dev environment. When I tried to install a package as follows, I noticed that there was a prompt to which a user had to key in Y or N (Proceed ([y]/n)?) for the installation to proceed successfully.\n\n```\n$ conda install pandas\nFetching package metadata: ....\nSolving package specifications: ..................\nPackage plan for installation in environment /home/miniconda2/envs/pandas_env:\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    libgfortran-1.0            |                0         170 KB\n    openblas-0.2.14            |                3         3.5 MB\n    numpy-1.10.2               |           py27_0         5.9 MB\n    pytz-2015.7                |           py27_0         174 KB\n    six-1.10.0                 |           py27_0          16 KB\n    python-dateutil-2.4.2      |           py27_0         219 KB\n    pandas-0.17.1              |      np110py27_0        12.4 MB\n    ------------------------------------------------------------\n                                           Total:        22.3 MB\n\nThe following NEW packages will be INSTALLED:\n\n    libgfortran:     1.0-0             \n    numpy:           1.10.2-py27_0     \n    openblas:        0.2.14-3          \n    pandas:          0.17.1-np110py27_0\n    python-dateutil: 2.4.2-py27_0      \n    pytz:            2015.7-py27_0     \n    six:             1.10.0-py27_0     \n\nProceed ([y]/n)? y\n\nFetching packages ...\nlibgfortran-1. 100% |###################################################################################################################################################################| Time: 0:00:00 457.23 kB/s\nopenblas-0.2.1 100% |###################################################################################################################################################################| Time: 0:00:02   1.68 MB/s\nnumpy-1.10.2-p 100% |###################################################################################################################################################################| Time: 0:00:02   2.42 MB/s\npytz-2015.7-py 100% |###################################################################################################################################################################| Time: 0:00:00 388.35 kB/s\nsix-1.10.0-py2 100% |###################################################################################################################################################################| Time: 0:00:00 224.69 kB/s\npython-dateuti 100% |###################################################################################################################################################################| Time: 0:00:00 493.15 kB/s\npandas-0.17.1- 100% |###################################################################################################################################################################| Time: 0:00:04   3.24 MB/s\nExtracting packages ...\n[      COMPLETE      ]|######################################################################################################################################################################################| 100%\nLinking packages ...\n[      COMPLETE      ]|######################################################################################################################################################################################| 100%\n\n```\n\nHow can I override these prompts so that the installation takes place silently/non-interactively?\nI tried using the -f flag, but it does not seem to be existing with the conda install command.",
    "answer": "Used $conda install -y pandas and it installed without any prompts (see documentation)."
  },
  {
    "question": "How can you &quot;clone&quot; a conda environment into the base (root) environment? I'd like the base (root) environment of conda to copy all of the packages in another environment.  How can this be done?",
    "answer": "\u03bb conda activate root\n\u03bb conda env export > environment_root.yml\n\u03bb conda list --explicit > spec_file_root.txt\n\u03bb activate myenv\n\u03bb conda env export > environment.yml\n\u03bb conda env update --name root --file environment.yml\n\u03bb conda create --name myclone --clone root\n\u03bb activate myenv\n\u03bb conda list --explicit > spec_file.txt\n\u03bb conda install --name root --file spec_file.txt\n\u03bb conda create --name myenv2 --file spec_file.txt\n\u03bb conda run --name myenv conda list --explicit > spec_file.txt\n\u03bb activate myenv\n\u03bb conda list --explicit > spec_file.txt\n\u03bb deactivate\n\nThere are options to copy dependency names/urls/versions to files.\nRecommendation\nNormally it is safer to work from a new environment rather than changing root.  However, consider backing up your existing environments before attempting changes. Verify the desired outcome by testing these commands in a demo environment.  To backup your root env for example:\n\nOptions\nOption 1 - YAML file\nWithin the second environment (e.g. myenv), export names+ to a yaml file:\n\nthen update the first environment+ (e.g. root) with the yaml file:\n\nOption 2 - Cloning an environment\nUse the --clone flag to clone environments (see @DevC's answer):\n\nThis basically creates a direct copy of an environment.\nOption 3 - Spec file\nMake a spec-file++ to append dependencies from an env (see @Ormetrom):\n\nAlternatively, replicate a new environment (recommended):\n\nSee Also\n\nconda env for more details on the env sub-commands.\nAnaconada Navigator desktop program for a more graphical experience.\nDocs on updated commands.  With older conda versions use activate (Windows) and source activate (Linux/Mac OS).  Newer versions of conda can use conda activate (this may require some setup with your shell configuration via conda init).\nDiscussion on keeping conda env\n\nExtras\nThere appears to be an undocumented conda run option to help execute commands in specific environments.\n\nThe latter command is effective at running commands in environments without the activation/deactivation steps.  See the equivalent command below:\n\nNote, this is likely an experimental feature, so this may not be appropriate in production until official adoption into the public API.\n+ Conda docs have changed since the original post; links updated.\n++ Spec-files only work with environments created on the same OS.  Unlike the first two options, spec-files only capture links to conda dependencies; pip dependencies are not included."
  },
  {
    "question": "anaconda - graphviz - can&#39;t import after installation Just installed a package through anaconda (conda install graphviz), but ipython wouldn't find it.\nI can see a graphviz folder in C:\\Users\\username\\Anaconda\\pkgs\nBut there's nothing in: C:\\Users\\username\\Anaconda\\Lib\\site-packages",
    "answer": "conda install graphviz\nconda install python-graphviz\nconda install pydot\n\nOn conda:\nFirst install\n\nThen the python-library for graphviz python-graphviz\n\ngv_python is a dynamically loaded extension for python that provides\naccess to the graph facilities of graphviz.\n\nThere is also pydot package, which can parse and dump into DOT language, used by GraphViz"
  },
  {
    "question": "CondaValueError: The target prefix is the base prefix. Aborting I have the following conda environment file environment.yml:\n\n```\nname: testproject\nchannels:\n- defaults\ndependencies:\n- python=3.7\nprefix: /opt/projects/testproject\n\n```\n\nBefore creating the environment, only the base environment exists:\n\n```\n(base) me@mymachine:/opt/projects/testproject$ conda env list\n# conda environments:\n#\nbase                  *  /opt/anaconda/anaconda3\n\n```\n\nWhen trying to create the environment, I get the following error:\n\n```\n(base) me@mymachine:/opt/projects/testproject$ conda create -f environment.yml\n\nCondaValueError: The target prefix is the base prefix. Aborting.\n\n```\n\nWhat does this error mean?",
    "answer": "conda env create -f environment.yml\n\nYou need to use\n\nNotice the extra env after conda and before create.\nFor more information check the documentation."
  },
  {
    "question": "Working with Anaconda in Visual Studio Code I am getting a bit confused here, the latest Anaconda Distribution, 2018.12 at time of writing comes with an option to install Microsoft Visual Studio Code, which is great.\nWhen launching VSC and after Python: Select Interpreter and with a fresh install of Anaconda, I can see ~Anaconda3\\python.exe which I assume is the Anaconda Python Environment, however, when I try to run some commands, I am getting:\n\n```\nPS ~\\Documents\\Python Scripts\\vs> ~/Anaconda3/Scripts/activate\nPS ~\\Documents\\Python Scripts\\vs> conda activate base\n\n```\n\n\nconda : The term 'conda' is not recognized as the name of a cmdlet,\n  function, script file, or operable program. Check the spelling of the\n  name, or if a path was included, verify that the path is correct and\n  try again. At line:1 char:1\n\nNow I know that it might be related to the environment variables but I find it highly odd as during the Anaconda installation, there are specific mentions that it is not required to add the Anaconda path to the environment variables. However after the error, the integrated terminal manages to launch Python and I am able to run code.\nNext in line is that I am unable to view any variables in the debugger after running a simple script, as shown in the tutorial here:\n\n```\nmsg = \"Hello World\"\nprint(msg)\n\n```\n\nI do expect to see similar results as shown in the link such as the dunder variables, I have also updated my launch.json with stopOnEntry = True following the steps.\nI would like to know if it is possible to use Visual Studio Code with Anaconda as a interpreter without registering variables from the original distribution and if I am missing out anything required.\nI expected the experience to be more straight forward but also I might be missing something, I am running on Windows 10.",
    "answer": "Activating a conda environment does not place conda on your PATH. You need to launch the Anaconda Prompt app from your Start menu to get a command-line with conda on your PATH if you didn't check the box to include conda during installation.\nAlso realize that conda only supports PowerShell as of conda 4.6 which was released in January 2019.\nAnd the Python extension for VS Code works with conda fine. Create a conda environment and the extension will allow you to select it as your environment/interpreter."
  },
  {
    "question": "How do I find the name of the conda environment in which my code is running? I'm looking for a good way to figure out the name of the conda environment I'm in from within running code or an interactive python instance. \nThe use-case is that I am running Jupyter notebooks with both Python 2 and Python 3 kernels from a miniconda install. The default environment is Py3. There is a separate environment for Py2. Inside the a notebook file, I want it to attempt to conda install foo. I'm using subcommand to do this for now, since I can't find a programmatic conda equivalent of pip.main(['install','foo']). \nThe problem is that the command needs to know the name of the Py2 environment to install foo there if the notebook is running using the Py2 kernel. Without that info it installs in the default Py3 env. I'd like for the code to figure out which environment it is in and the right name for it on its own.\nThe best solution I've got so far is:\n\n```\nimport sys\n\ndef get_env():\n    sp = sys.path[1].split(\"/\")\n    if \"envs\" in sp:\n        return sp[sp.index(\"envs\") + 1]\n    else:\n        return \"\"\n\n```\n\nIs there a more direct/appropriate way to accomplish this?",
    "answer": "source activate my_env\n(my_env) $ echo $CONDA_DEFAULT_ENV\nmy_env\n\n(my_env) $ echo $CONDA_PREFIX\n/Users/nhdaly/miniconda3/envs/my_env\nsource deactivate\necho $CONDA_DEFAULT_ENV  # (not-defined)\necho $CONDA_PREFIX  # (not-defined)\nimport os\nprint(os.environ['CONDA_DEFAULT_ENV'])\nPython 3.9.0 | packaged by conda-forge | (default, Oct 14 2020, 22:56:29)\n[Clang 10.0.1 ] on darwin\nimport os; print(os.environ[\"CONDA_PREFIX\"])\n/Users/miranda9/.conda/envs/synthesis\n\nYou want $CONDA_DEFAULT_ENV or $CONDA_PREFIX:\n\nIn python:\n\nfor the absolute entire path which is usually more useful:\n\nThe environment variables are not well documented. You can find CONDA_DEFAULT_ENV mentioned here:\nhttps://www.continuum.io/blog/developer/advanced-features-conda-part-1\nThe only info on CONDA_PREFIX I could find is this Issue:\nhttps://github.com/conda/conda/issues/2764"
  },
  {
    "question": "Difference between pip freeze and conda list I am using both \"pip freeze\" and \"conda list\" to list the packages installed in my environment, but what are their differences?",
    "answer": "If the goal only is to list all the installed packages, pip list or conda list are the way to go.\npip freeze, like conda list --export, is more for generating requirements files for your environment. For example, if you have created a package in your customized environment with certain dependencies, you can do conda list --export > requirements.txt. When you are ready to distribute your package to other users, they can easily duplicate your environment and the associated dependencies with conda create --name <envname> --file requirements.txt.\n\nThe differences between conda and pip need a longer discussion. There are plenty of explanations on StackOverflow. This article by Jake VanderPlas is a great read as well.\nYou might also find this table useful. It lists operation equivalences between conda, pip and virtualenv."
  },
  {
    "question": "Conda uninstall one package and one package only When I try to uninstall pandas from my conda virtual env, I see that it tries to uninstall more packages as well:\n\n```\n$ conda uninstall pandas\nUsing Anaconda Cloud api site https://api.anaconda.org\nFetching package metadata: ....\nSolving package specifications: .........\n\nPackage plan for package removal in environment /Users/amelio/anaconda/envs/py35:\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    dask-0.7.6                 |           py35_0         276 KB\n\nThe following packages will be REMOVED:\n\n    blaze:       0.10.1-py35_0\n    odo:         0.5.0-py35_1\n    pandas:      0.18.1-np111py35_0\n    seaborn:     0.7.0-py35_0\n    statsmodels: 0.6.1-np111py35_1\n\nThe following packages will be DOWNGRADED:\n\n    dask:        0.10.1-py35_0      --> 0.7.6-py35_0\n\nProceed ([y]/n)?\n\n```\n\nI would like to uninstall pandas only and not have anything else downgraded. \nI understand that there these packages have dependencies to pandas and even to specific versions of pandas, but is this even possible at all with conda?\nPartial motivation\nMy understanding is that it isn't that easy to install a package from GitHub using conda. At least it isn't as easy as using a one-liner as one can do with pip. \nSince I want to use the version in master, my plan is to uninstall the conda version and use pip to install pandas from master on the official repo GitHub.\nHow can I do this?",
    "answer": "--force               Forces removal of a package without removing packages\nthat depend on it. Using this option will usually\nleave your environment in a broken and inconsistent\nstate\n\nYou can use conda remove --force.\nThe documentation says:"
  },
  {
    "question": "How to install python with conda? I'm trying to install python 3.9 in a conda enviroment. I tried creating a new conda env using the following command,\n\n```\nconda create --name myenv python=3.9\n\n```\n\nBut I got an error saying package not found because python 3.9 is not yet released\nSo, I manually created a folder in envs folder and tried to list all envs. But I couldn't get the manually created new environment.\nSo, how do I install python 3.9 in a conda env with all functionalities like pip working?",
    "answer": "conda create -n py311 python=3.11\npy311 - environment name\nconda create -n py310 python=3.10\npy310 - environment name\nconda create -n py39 python=3.9\npy39 - environment name\npython3.7\npython3.7-config\npython3.7m\npython3.7m-config\npython3.9\npython3.9-config\npip\npip3\npip3.7\npip3.8\npip3.9\npipreqs\npip3.9 install ipython\n\nTo create python 3.11 conda environment use the following command\n\nUpdate 3\nTo create python 3.10 conda environment use the following command\n\nUpdate 2\nYou can now directly create python 3.9 environment using the following command\n\nUpdate 1\nPython 3.9 is now available in conda-forge.\nTo download the tar file - https://anaconda.org/conda-forge/python/3.9.0/download/linux-64/python-3.9.0-h852b56e_0_cpython.tar.bz2\nAnaconda Page - https://anaconda.org/conda-forge/python\n\nAs pointed out in the comments, python 3.9 is not yet there on any channels. So, it cannot be install yet via conda.\nInstead, you can download the python 3.9 executable and install it.\nOnce the installation is done, a new executable will be created for python 3.9 and pip 3.9 will be created.\nPython:\n\npip\n\nIn order to install ipython for python 3.9,"
  },
  {
    "question": "Conda install and update do not work also solving environment get errors I am using anaconda as below:\n\n```\n(base) C:\\Users\\xxx>conda info\n\n     active environment : base\n    active env location : C:\\Users\\xxx\\Documents\\ANACONDA\n            shell level : 1\n       user config file : C:\\Users\\xxx\\.condarc\n populated config files : C:\\Users\\xxx\\.condarc\n          conda version : 4.7.11\n    conda-build version : 3.18.9\n         python version : 3.6.9.final.0\n       virtual packages :\n       base environment : C:\\Users\\xxx\\Documents\\ANACONDA  (writable)\n           channel URLs : https://repo.anaconda.com/pkgs/main/win-64\n                          https://repo.anaconda.com/pkgs/main/noarch\n                          https://repo.anaconda.com/pkgs/free/win-64\n                          https://repo.anaconda.com/pkgs/free/noarch\n                          https://repo.anaconda.com/pkgs/r/win-64\n                          https://repo.anaconda.com/pkgs/r/noarch\n                          https://repo.anaconda.com/pkgs/msys2/win-64\n                          https://repo.anaconda.com/pkgs/msys2/noarch\n          package cache : C:\\Users\\xxx\\Documents\\ANACONDA\\pkgs\n                          C:\\Users\\xxx\\.conda\\pkgs\n                          C:\\Users\\xxx\\AppData\\Local\\conda\\conda\\pkgs\n       envs directories : C:\\Users\\xxx\\Documents\\ANACONDA\\envs\n                          C:\\Users\\xxx\\.conda\\envs\n                          C:\\Users\\xxx\\AppData\\Local\\conda\\conda\\envs\n               platform : win-64\n             user-agent : conda/4.7.11 requests/2.22.0 CPython/3.6.9 Windows/10 Windows/10.0.16299\n          administrator : False\n             netrc file : None\n           offline mode : False\n\n\n```\n\nNow I have 2 issues that stop my work.\n1) I cannot use conda install for any package.\nIt will give me the error in solving environment list this:\n\n```\nfailed with initial frozen solve. Retrying with flexible solve.\n\n\n```\n\nthen it will fail again and give message like this:\n\n```\nFound conflicts! Looking for incompatible packages.\nThis can take several minutes.  Press CTRL-C to abort.\n\n```\n\nEven after the checking for incompatible packages, it didn't give me the solution.\n2) When I want to upgrade or downgrade conda by the command:\n\n```\nconda update -n base conda\n\n```\n\nor\n\n```\nconda install conda = 4.6.11\n\n```\n\nIt will give me errors again in the solving environment, and I think this is related to the first issue.\nNow I cannot use conda for anything, please advise and thank you!",
    "answer": "conda create --name myenv\nconda activate myenv\n\nI ran into the same problem and I couldn't find a solution, but I did find a workaround. If you create an env and activate that env and then do the install, it seems to work just fine. If you don't need a lot of libraries I would try that.\nCommands are:\n\nCreate env\n\nActivate the env"
  },
  {
    "question": "Conda update fails with SSL error CERTIFICATE_VERIFY_FAILED I have a problem with conda update. Specifically, I tried doing\n\n```\nconda update <package>\n\n```\n\n, and I got the following error:\n\n```\nCould not connect to https://repo.continuum.io/pkgs/free/osx-64/decorator-4.0.2-py27_0.tar.   \nbz2 Error: Connection error: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed \n(_ssl.c:590): https://repo.continuum.io/pkgs/free/osx-64/decorator-4.0.2-py27_0.tar.bz2\n\n```\n\nThe full output of the command was the following:\n\n```\nconda update bokeh Fetching package metadata: SSL verification error: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:590) .SSL verification error: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:590) .SSL verification error: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:590) .SSL verification error: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:590) . Solving package specifications: . Package plan for installation in environment //anaconda:\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    decorator-4.0.2            |           py27_0          11 KB\n    ipython_genutils-0.1.0     |           py27_0          32 KB\n    path.py-8.1.1              |           py27_0          45 KB\n    pexpect-3.3                |           py27_0          60 KB\n    pickleshare-0.5            |           py27_0           8 KB\n    simplegeneric-0.8.1        |           py27_0           6 KB\n    traitlets-4.0.0            |           py27_0          88 KB\n    ipython-4.0.0              |           py27_0         916 KB\n    jinja2-2.8                 |           py27_0         263 KB\n    tornado-4.2.1              |           py27_0         515 KB\n    bokeh-0.9.3                |       np19py27_0        14.3 MB\n    ------------------------------------------------------------\n                                           Total:        16.2 MB\n\nThe following NEW packages will be INSTALLED:\n\n    ipython_genutils: 0.1.0-py27_0    \n    path.py:          8.1.1-py27_0    \n    pexpect:          3.3-py27_0      \n    pickleshare:      0.5-py27_0      \n    simplegeneric:    0.8.1-py27_0    \n    traitlets:        4.0.0-py27_0    \n\nThe following packages will be UPDATED:\n\n    bokeh:            0.9.0-np19py27_0 --> 0.9.3-np19py27_0\n    decorator:        3.4.2-py27_0     --> 4.0.2-py27_0    \n    ipython:          3.2.0-py27_0     --> 4.0.0-py27_0    \n    jinja2:           2.7.3-py27_1     --> 2.8-py27_0      \n    tornado:          4.2-py27_0       --> 4.2.1-py27_0    \n\nProceed ([y]/n)? y\n\nFetching packages ... Could not connect to https://repo.continuum.io/pkgs/free/osx-64/decorator-4.0.2-py27_0.tar.bz2 Error: Connection error: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:590): https://repo.continuum.io/pkgs/free/osx-64/decorator-4.0.2-py27_0.tar.bz2\n\ndendisuhubdy:finalproject dendisuhubdy$ brew link --force openssl Linking /usr/local/Cellar/openssl/1.0.2d_1... 1548 symlinks created dendisuhubdy:finalproject dendisuhubdy$ conda update bokeh Fetching package metadata: SSL verification error: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:590) .SSL verification error: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:590) .SSL verification error: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:590) .SSL verification error: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:590) . Solving package specifications: . Package plan for installation in environment //anaconda:\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    decorator-4.0.2            |           py27_0          11 KB\n    ipython_genutils-0.1.0     |           py27_0          32 KB\n    path.py-8.1.1              |           py27_0          45 KB\n    pexpect-3.3                |           py27_0          60 KB\n    pickleshare-0.5            |           py27_0           8 KB\n    simplegeneric-0.8.1        |           py27_0           6 KB\n    traitlets-4.0.0            |           py27_0          88 KB\n    ipython-4.0.0              |           py27_0         916 KB\n    jinja2-2.8                 |           py27_0         263 KB\n    tornado-4.2.1              |           py27_0         515 KB\n    bokeh-0.9.3                |       np19py27_0        14.3 MB\n    ------------------------------------------------------------\n                                           Total:        16.2 MB\n\nThe following NEW packages will be INSTALLED:\n\n    ipython_genutils: 0.1.0-py27_0    \n    path.py:          8.1.1-py27_0    \n    pexpect:          3.3-py27_0      \n    pickleshare:      0.5-py27_0      \n    simplegeneric:    0.8.1-py27_0    \n    traitlets:        4.0.0-py27_0    \n\nThe following packages will be UPDATED:\n\n    bokeh:            0.9.0-np19py27_0 --> 0.9.3-np19py27_0\n    decorator:        3.4.2-py27_0     --> 4.0.2-py27_0    \n    ipython:          3.2.0-py27_0     --> 4.0.0-py27_0    \n    jinja2:           2.7.3-py27_1     --> 2.8-py27_0      \n    tornado:          4.2-py27_0       --> 4.2.1-py27_0    \n\nProceed ([y]/n)? y\n\nFetching packages ... Could not connect to https://repo.continuum.io/pkgs/free/osx-64/decorator-4.0.2-py27_0.tar.bz2 Error: Connection error: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:590): https://repo.continuum.io/pkgs/free/osx-64/decorator-4.0.2-py27_0.tar.bz2\n\n```\n\nPlease advise: what should I do to overcome this error?",
    "answer": "conda config --set ssl_verify false\n\nPlease note that the following solution is not secure. See: https://conda.io/projects/conda/en/latest/user-guide/configuration/disable-ssl-verification.html\naccording to @jreback here https://github.com/conda/conda/issues/1166\n\nwill turn off this feature, e.g. here"
  },
  {
    "question": "Use Conda environment in pycharm Conda env is activated using source activate env_name. \nHow can I activate the environment in pycharm ?",
    "answer": "The best PyCharm specific answer is this one by wasabi (below).\nIn general though, if you want to use an interpreter from within a Conda environment then you can change the location of the interpreter to point to the particular environment that you want to use e.g. /home/username/miniconda/envs/bunnies as mentioned in this comment.\nHowever, as mentioned in this answer by Mark Turner, it is possible to have a shell script executed when activating an environment. This method will not run that shell script, but you can follow his workaround if you need that shell script run:\n\nopen a conda prompt\nactivate the environment\nrun pycharm from the conda prompt"
  },
  {
    "question": "How to share conda environments across platforms The conda docs at http://conda.pydata.org/docs/using/envs.html explain how to share environments with other people.\nHowever, the docs tell us this is not cross platform:\n\n```\nNOTE: These explicit spec files are not usually cross platform, and      \ntherefore have a comment at the top such as # platform: osx-64 showing the  \nplatform where they were created. This platform is the one where this spec\nfile is known to work. On other platforms, the packages specified might not\nbe available or dependencies might be missing for some of the key packages\nalready in the spec.\n\nNOTE: Conda does not check architecture or dependencies when installing \nfrom an explicit specification file. To ensure the packages work correctly,\nbe sure that the file was created from a working environment and that it is \nused on the same architecture, operating system and platform, such as linux-\n64 or osx-64.\n\n```\n\nIs there a good method to share and recreate a conda environment in one platform (e.g. CentOS) in another platform (e.g. Windows)?",
    "answer": "name: basic_analysis\ndependencies:\n- numpy\n- pandas\nname: stats-web\nchannels:\n- javascript\ndependencies:\n- python=3.4   # or 2.7 if you are feeling nostalgic\n- bokeh=0.9.2\n- numpy=1.9\n- nodejs=0.10\n- flask\n- pip:\n- Flask-Testing\n\nThis answer is given with the assumption that you would like to make sure that\nthe same versions of the packages that you generally care about are on\ndifferent platforms and that you don't care about the exact same versions of\nall packages in the entire dependency tree. If you are trying to install the\nexact same version of all packages in your entire dependency tree that has a\nhigh likelihood of failure since some conda packages have different\ndependencies for osx/win/linux. For example, the recipe for\notrobopt\nwill install different packages on Win vs. osx/linux, so the environment list\nwould be different.\nRecommendation: manually create an environment.yaml file and specify or pin\nonly the dependencies that you care about. Let the conda solver do the rest.\nProbably worth noting is that conda-env (the tool that you use to manage conda\nenvironments) explicitly recommends that you \"Always create your\nenvironment.yml file by hand.\"\nThen you would just do conda env create --file environment.yml\nHave a look at the readme for\nconda-env.\nThey can be quite simple:\n\nOr more complex where you pin dependencies and specify anaconda.org channels to\ninstall from:\n\nIn either case, you can create an environment with conda env create --file environment.yaml.\nNOTE: You may need to use .* as a version suffix if you're using an older version of conda."
  },
  {
    "question": "Conda: Creating a virtual environment I'm trying to create a virtual environment. I've followed steps from both Conda and Medium.\nEverything works fine until I need to source the new environment:\n\n```\nconda info -e\n\n# conda environments:\n#\nbase                  *  /Users/fwrenn/anaconda3\ntest_env                 /Users/fwrenn/anaconda3/envs/test_env\n\nsource ~/anaconda3/bin/activate test_env\n\n```\n\n\n_CONDA_ROOT=/Users/fwrenn/anaconda3: Command not found.\nBadly placed ()'s.\n\nI can't figure out the problem. Searching on here has solutions that say adding lines to your bash_profile file, but I don't work in Bash, only C shell (csh). It looks like it's unable to build the directory path in activate.\nMy particulars:\n\nOS X\n\nOutput of python --version:\n\n```\nPython 3.6.3 :: Anaconda custom (64-bit)\n\n```\n\n\nOutput of conda --version:\n\n```\nconda 4.4.7\n\n```",
    "answer": "> conda info -e\n> # conda environments:\n> #\n> base                  *  ~/anaconda3\n> test_env                 ~/anaconda3/envs/test_env\n> bash\n~$ source ~/anaconda3/bin/activate test_env\n(test_env) ~$\n(test_env) ~$ conda info -e\n\ntest_env              *  ~/anaconda3/envs/test_env\nroot                     ~/anaconda3\n\nI was able to solve my problem. Executing the source activate test_env command wasn't picking up my .bash_profile, and I normally work in tcsh. Simply starting a subprocess in Bash was enough to get activate working. I guess I assumed, incorrectly, that the activate command would start a child process in Bash and use Bash environment variables."
  },
  {
    "question": "How to set specific environment variables when activating conda environment? Does anyone know how to automatically set environment variables when activating an env in conda?\nI have tried editing */bin/activate, but that adds the new environment variables for every new env that is created. I want to set env variables that are specific to each env.",
    "answer": "Use the files $CONDA_PREFIX/etc/conda/activate.d and $CONDA_PREFIX/etc/conda/deactivate.d, where $CONDA_PREFIX is the path to the environment.\nSee the section on managing environments in the official documentation for reference."
  },
  {
    "question": "Python - Activate conda env through shell script I am hoping to run a simple shell script to ease the management around some conda environments.  Activating conda environments via conda activate in a linux os works fine in the shell but is problematic within a shell script.  Could someone point me into the right direction as to why this is happening?\nExample to repeat the issue:\n\n```\n# default conda env\n$ conda info | egrep \"conda version|active environment\"\n     active environment : base\n          conda version : 4.6.9\n\n# activate new env to prove that it works\n$ conda activate scratch\n$ conda info | egrep \"conda version|active environment\"\n     active environment : scratch\n          conda version : 4.6.9\n\n# revert back to my original conda env\n$ conda activate base \n\n$ cat shell_script.sh\n#!/bin/bash\nconda activate scratch\n\n# run shell script - this will produce an error even though it succeeded above\n$ ./shell_script.sh\n\nCommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.\nTo initialize your shell, run\n\n    $ conda init <SHELL_NAME>\n\nCurrently supported shells are:\n  - bash\n  - fish\n  - tcsh\n  - xonsh\n  - zsh\n  - powershell\n\nSee 'conda init --help' for more information and options.\n\nIMPORTANT: You may need to close and restart your shell after running 'conda init'.\n\n```",
    "answer": "bash -i shell_script.sh\n\nThe error message is rather helpful - it's telling you that conda is not properly set up from within the subshell that your script is running in. To be able to use conda within a script, you will need to (as the error message says) run conda init bash (or whatever your shell is) first. The behaviour of conda and how it's set up depends on your conda version, but the reason for the version 4.4+ behaviour is that conda is dependent on certain environment variables that are normally set up by the conda shell itself. Most importantly, this changelog entry explains why your conda activate and deactivate commands no longer behave as you expect in versions 4.4 and above.\nFor more discussion of this, see the official conda issue on GitHub.\n\nEdit: Some more research tells me that the conda init function mentioned in the error message is actually a new v4.6.0 feature that allows a quick environment setup so that you can use conda activate instead of the old source activate. However, the reason why this works is that it adds/changes several environment variables of your current shell and also makes changes to your RC file (e.g.: .bashrc), and RC file changes are never picked up in the current shell - only in newly created shells. (Unless of course you source .bashrc again). In fact, conda init --help says as much:\n\nIMPORTANT: After running conda init, most shells will need to be closed and restarted for changes to take effect\n\nHowever, you've clearly already run conda init, because you are able to use conda activate interactively. In fact, if you open up your .bashrc, you should be able to see a few lines added by conda teaching your shell where to look for conda commands. The problem with your script, though, lies in the fact that the .bashrc is not sourced by the subshell that runs shell scripts (see this answer for more info). This means that even though your non-login interactive shell sees the conda commands, your non-interactive script subshells won't - no matter how many times you call conda init.\nThis leads to a conjecture (I don't have conda on Linux myself, so I can't test it) that by running your script like so:\n\nyou should see conda activate work correctly. Why? -i is a bash flag that tells the shell you're starting to run in interactive mode, which means it will automatically source your .bashrc. This should be enough to enable you to use conda within your script as if you were using it normally."
  },
  {
    "question": "No module named &quot;Torch&quot; I successfully installed pytorch via conda:\n\n```\nconda install pytorch-cpu torchvision-cpu -c pytorch\n\n```\n\nI also successfully installed pytorch via pip:\n\n```\npip3 install https://download.pytorch.org/whl/cpu/torch-1.0.1-cp36-cp36m-win_amd64.whl\npip3 install torchvision\n\n```\n\nBut, it only works in a jupyter notebook. Whenever I try to execute a script from the console, I get the error message:\n\n```\nNo module named \"torch\"\n\n```",
    "answer": "conda create -n env_pytorch python=3.6\nconda activate env_pytorch\npip install torchvision\nimport torch\nimport torchvision\n\nTry to install PyTorch using pip:\nFirst create a Conda environment using:\n\nActivate the environment using:\n\nNow install PyTorch using pip:\n\nNote: This will install both torch and torchvision.\nNow go to Python shell and import using the command:"
  },
  {
    "question": "PackageNotInstalledError: Package is not installed in prefix conda update conda >> successful\nconda update anaconda >> gives me error saying package is not installed in prefix.\nI have single installation of Python distribution on my system. How do I solve this issue?\n\n```\n(base) C:\\Users\\asukumari>conda info\n\n```\n\n\n\n```\n active environment : base\nactive env location : C:\\Users\\asukumari\\AppData\\Local\\Continuum\\anaconda3\n        shell level : 1\n   user config file : C:\\Users\\asukumari\\.condarc  populated config files : C:\\Users\\asukumari\\.condarc\n      conda version : 4.5.9\nconda-build version : 3.4.1\n     python version : 3.6.4.final.0\n   base environment : C:\\Users\\asukumari\\AppData\\Local\\Continuum\\anaconda3  (writable)\n       channel URLs : https://repo.anaconda.com/pkgs/main/win-64\n                      https://repo.anaconda.com/pkgs/main/noarch\n                      https://repo.anaconda.com/pkgs/free/win-64\n                      https://repo.anaconda.com/pkgs/free/noarch\n                      https://repo.anaconda.com/pkgs/r/win-64\n                      https://repo.anaconda.com/pkgs/r/noarch\n                      https://repo.anaconda.com/pkgs/pro/win-64\n                      https://repo.anaconda.com/pkgs/pro/noarch\n                      https://repo.anaconda.com/pkgs/msys2/win-64\n                      https://repo.anaconda.com/pkgs/msys2/noarch\n      package cache : C:\\Users\\asukumari\\AppData\\Local\\Continuum\\anaconda3\\pkgs\n                      C:\\Users\\asukumari\\AppData\\Local\\conda\\conda\\pkgs\n   envs directories : C:\\Users\\asukumari\\AppData\\Local\\Continuum\\anaconda3\\envs\n                      C:\\Users\\asukumari\\AppData\\Local\\conda\\conda\\envs\n                      C:\\Users\\asukumari\\.conda\\envs\n           platform : win-64\n         user-agent : conda/4.5.9 requests/2.18.4 CPython/3.6.4 Windows/10 Windows/10.0.16299\n      administrator : False\n         netrc file : None\n       offline mode : False\n\n```",
    "answer": "conda update --name base conda\nconda list --name base conda\nconda                     4.6.14                   py27_0\nconda-env                 2.6.0                h36134e3_0\n\nUsually this error, \"PackageNotInstalledError: Package is not installed in prefix.\" is because your custom environment doesn't have the conda infrastructure. Instead, it is in your base only. To update the base environment:\n\nTo see what version you have installed:\n\nexample output of list;"
  },
  {
    "question": "abjad.show() issues &quot;FileNotFoundError: [WinError 2] The system cannot find the file specified&quot; in Python Here's a basic, simple 'abjad' code that one can find in any 'abjad' documentation:\n\n```\nimport abjad\nn = abjad.Note(\"c'4\")\nabjad.show(n)\n\n```\n\nAnd here's the full traceback produced by the above code:\n\n```\nTraceback (most recent call last):\n  File \"R:\\W\\y.py\", line 122, in <module>\n    abjad.show(n)\n  File \"R:\\W\\venv\\Lib\\site-packages\\abjad\\io.py\", line 672, in show\n    result = illustrator()\n             ^^^^^^^^^^^^^\n  File \"R:\\W\\venv\\Lib\\site-packages\\abjad\\io.py\", line 75, in __call__\n    string = self.string or self.get_string()\n                            ^^^^^^^^^^^^^^^^^\n  File \"R:\\W\\venv\\Lib\\site-packages\\abjad\\io.py\", line 152, in get_string\n    return lilypond_file._get_lilypond_format()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"R:\\W\\venv\\Lib\\site-packages\\abjad\\lilypondfile.py\", line 474, in _get_lilypond_format\n    string = configuration.get_lilypond_version_string()\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"R:\\W\\venv\\Lib\\site-packages\\abjad\\configuration.py\", line 388, in get_lilypond_version_string\n    proc = subprocess.run(command, stdout=subprocess.PIPE)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"G:\\Python3.12\\Lib\\subprocess.py\", line 548, in run\n    with Popen(*popenargs, **kwargs) as process:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"G:\\Python3.12\\Lib\\subprocess.py\", line 1026, in __init__\n    self._execute_child(args, executable, preexec_fn, close_fds,\n  File \"G:\\Python3.12\\Lib\\subprocess.py\", line 1538, in _execute_child\n    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFileNotFoundError: [WinError 2] The system cannot find the file specified\n\n```\n\nNote: I debugged 'subprocess.py' and saw that 'executable' was None, but I couldn't find what it was expected to be.\nI have installed abjad in a virtual environment, using both Python 3.10 and Python 3.12. And I have tried with both abjad versions, 3.19 & 3.21. Same story.\nI have installed hundreds of Python packages ... I can't remember this case evere having occurred.\nAny idea why's this happening?",
    "answer": "command = [\"lilypond\", \"--version\"]\nproc = subprocess.run(command, stdout=subprocess.PIPE)\n\nLooking at the source of \"...\\abjad\\configuration.py\", line 388 (as pointed to by the error message), we have the following lines:\n\nSo the process is trying to run the command \"lilypond --version\", and failing because Windows cannot find an executable file or command script with that name. You should check where that command is installed and ensure that its full path is added to your PATH environment variable."
  },
  {
    "question": "Check if a python module exists with specific venv path If I have a venv path, how can I find out if a Python module is installed inside that venv?\nI normally use importlib.util.find_spec. But this only works for the current venv that is active and does not work if I have a different venv path.\n\n```\nfrom importlib.util import find_spec\n\nif find_spec('numpy'):\n    # Do something\n\n```",
    "answer": "import sys\nimport importlib.util\nimport os\nfrom pathlib import Path\n\ndef is_module_installed_in_venv(module_name, venv_path):\nvenv_python_lib_path = Path(venv_path) / 'lib'\nfor python_dir in venv_python_lib_path.iterdir():\nif python_dir.name.startswith('python'):\nsite_packages_path = python_dir / 'site-packages'\nbreak\n\nif not site_packages_path.exists():\nreturn False\n\nsys.path.insert(0, str(site_packages_path))\n\nmodule_spec = importlib.util.find_spec(module_name)\n\nsys.path.pop(0)\n\nreturn module_spec is not None\n\nvenv_path = '/home/user/anaconda3/envs/env_name/'\nmodule_name = 'numpy'\n\nif is_module_installed_in_venv(module_name, venv_path):\nprint(\"do something\")\n\nthis works , make sure to include the full path"
  },
  {
    "question": "Can 2 VS Code windows for 2 git clones each work with their own venv? Background (adjacent to my question)\nThis doesn't happen often, but sometimes, I get carried away with a proof of concept effort and end up too far down a rapid prototyping rabbit hole, with a bunch of work I want to keep, so I break up and organize my work into a stacked series of small branches using a git squash merge, cherry-pick, followed by a repeating series of git adds, commits, pushes, and branching.  As I create an organized commit history to pick related changes for a coherent commit history, I write tests and do linting, which sometimes requires some code changes along the way, so the further I get out of the disorganized mess, the more the new series of stacked branches diverge.  Since this creates conflicts, I figured out a way to keep these divergent commit histories flowing smoothly...\nI have 1 repo clone containing the messy rapid prototyping code where I organize related changes into separate branches, which I push to github.\nI then have a second clone where I pull those branches to lint, test, tweak, then rebase off the previously linted, tested, and tweaked code from the previous branch.\nQuestion\nI know some of you are probably shaking your heads right now, but it flows surprisingly smoothly this way.  The only problem is that I use 2 VSCode windows and the one for the secondary clone where I do the clean-up cannot resolve the imports.  It's really just a minor annoyance, but I tried to figure it out today by setting up the secondary clone with its own venv, but it didn't work, so this is what I see (secondary window above and primary window below):\n\nI imagine this is due to the fact that when I launch VSCode for the primary clone, it uses its venv... for both windows.  But the 2 clones are in different directories, so it cannot \"find\" the imports?",
    "answer": "OK.  It was pretty simple.  I just had to:\n\nOn the command line: activate my venv: source .venv/bin/activate\ncode .\ncommand-shift-p\nClick \"Python: select interpreter\"\nClick \"Enter Interpreter path\"\nPaste and hit Enter\n\nI was mistakenly guessing that just activating the environment (either in the terminal from which I launch vscode or in vscode's terminal) was enough.  I probably did this for the primary clone long ago and just forgot."
  },
  {
    "question": "Cannot import: `from serpapi import GoogleSearch` I have this app in PyCharm (see screenshot). It won't run because of:\n\nImportError: cannot import name 'GoogleSearch' from 'serpapi' (/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/serpapi/init.py)\nProcess finished with exit code 1\n\nHas anybody faced this issue?\nInvalidate Caches / Restart pycharm. Check my run configuration",
    "answer": "The solution was to remove all system Python installations from my Mac (including the official Python binary and Brew python). Then create a run shell script configuration in PyCharm like this:\nsource venv/bin/activate && streamlit run app/web/app.py"
  }
]